{
  "version": "1.0",
  "spec_id": "003-implement-response-caching-for-llm-api-calls-with-",
  "title": "Implement response caching for LLM API calls with identical inputs",
  "created_at": "2025-12-29T01:40:00Z",
  "phases": [
    {
      "id": "phase-1",
      "name": "Design and Architecture",
      "status": "completed",
      "subtasks": [
        {
          "id": "1-1",
          "name": "Design cache key generation strategy",
          "description": "Determine how to generate unique cache keys from LLM request parameters (system prompt, messages, tools, temperature)",
          "status": "completed",
          "files_to_modify": [],
          "dependencies": []
        },
        {
          "id": "1-2",
          "name": "Design cache data structures",
          "description": "Design in-memory and on-disk cache structures with support for TTL and size limits",
          "status": "completed",
          "files_to_modify": [],
          "dependencies": [
            "1-1"
          ],
          "notes": "Completed comprehensive design document for cache data structures. Designed two-tier architecture with in-memory LRU cache and persistent disk cache. Includes TTL support (configurable, default 7 days), size limits (entry count for memory, byte size for disk), thread-safety with sync.RWMutex, error handling for corruption recovery, and performance optimizations. Document is ready for implementation in subtask 2-1.",
          "updated_at": "2025-12-29T04:47:05.847100+00:00"
        },
        {
          "id": "1-3",
          "name": "Design cache integration layer",
          "description": "Define where in the LLM call flow to integrate caching (likely as a decorator around LLMClient)",
          "status": "completed",
          "files_to_modify": [],
          "dependencies": [
            "1-2"
          ],
          "notes": "Completed comprehensive design document for cache integration layer. Analyzed current LLM client architecture and identified decorator pattern as optimal integration strategy. Designed CachedLLMClient decorator that wraps existing LLMClient implementations, intercepts GenerateCompletion() calls to check cache before making API calls. Includes factory integration approach, configuration structure, error handling strategy (non-blocking cache failures), and complete call flow diagrams. Design is ready for implementation in subtask 3-1.",
          "updated_at": "2025-12-29T05:30:00.000000+00:00"
        }
      ]
    },
    {
      "id": "phase-2",
      "name": "Implement Core Caching Layer",
      "status": "in_progress",
      "subtasks": [
        {
          "id": "2-1",
          "name": "Create LLM response cache package",
          "description": "Create internal/llmcache package with cache entry structures and basic operations",
          "status": "completed",
          "files_to_modify": [],
          "new_files": [
            "internal/llmcache/cache.go",
            "internal/llmcache/entry.go",
            "internal/llmcache/key.go"
          ],
          "dependencies": [
            "1-1",
            "1-2",
            "1-3"
          ],
          "notes": "Successfully created internal/llmcache package with three core files: entry.go (cache entry structures including CachedResponse, CacheKeyRequest, CacheKeyMessage, CacheKeyTool), key.go (SHA256-based cache key generation with canonical JSON serialization, sorted tools, and preserved message order), and cache.go (LRUCache with O(1) operations and thread-safety, DiskCache with JSON persistence and atomic writes, CacheStats for metrics tracking). All structures follow the design specifications exactly.",
          "updated_at": "2025-12-29T05:55:00.000000+00:00"
        },
        {
          "id": "2-2",
          "name": "Implement cache key generation",
          "description": "Implement SHA256-based cache key generation from CompletionRequest",
          "status": "completed",
          "files_to_modify": [],
          "dependencies": [
            "2-1"
          ],
          "notes": "Verified that cache key generation is correctly implemented in internal/llmcache/key.go from subtask 2-1. Implementation includes: SHA256 hashing, canonical JSON serialization, string trimming, tool sorting by name, message order preservation, and proper error handling. All design specifications verified.",
          "updated_at": "2025-12-29T06:00:00.000000+00:00"
        },
        {
          "id": "2-3",
          "name": "Implement in-memory cache with LRU eviction",
          "description": "Implement thread-safe in-memory cache with configurable size limit and LRU eviction",
          "status": "completed",
          "files_to_modify": [],
          "dependencies": [
            "2-1"
          ],
          "notes": "Verified that LRUCache implementation in internal/llmcache/cache.go (lines 72-289) is complete and correct. Includes: thread-safe with sync.RWMutex, configurable maxSize, LRU eviction using doubly-linked list, O(1) Get/Put/Delete operations, TTL checking on Get, and comprehensive statistics tracking. All requirements from design document satisfied.",
          "updated_at": "2025-12-29T06:15:00.000000+00:00"
        },
        {
          "id": "2-4",
          "name": "Implement persistent disk cache",
          "description": "Implement JSON-based disk cache with automatic loading and background persistence",
          "status": "completed",
          "files_to_modify": [],
          "dependencies": [
            "2-1"
          ],
          "notes": "Successfully implemented JSON-based disk cache with automatic loading and background persistence. Added StartAutoSave() method to start background goroutine that periodically saves dirty cache data with configurable interval. Implemented Stop() method for graceful shutdown with final save. Added saveData() helper for lock-free I/O operations using atomic write pattern (temp file + rename). All features thread-safe with mutex protection, proper error handling, and follows design specifications exactly.",
          "updated_at": "2025-12-29T06:30:00.000000+00:00"
        },
        {
          "id": "2-5",
          "name": "Implement cache TTL and expiration",
          "description": "Add time-based expiration with configurable TTL (default 7 days)",
          "status": "completed",
          "files_to_modify": [],
          "dependencies": [
            "2-3",
            "2-4"
          ],
          "notes": "Successfully implemented time-based expiration with configurable TTL. Added DefaultTTL constant (7 days) to cache.go. Implemented LRUCache.CleanupExpired() method for proactive removal of expired entries. Added NewCachedResponse() helper function to entry.go for easy creation of cache entries with proper TTL. Existing TTL checking already in place on Get() operations in both LRUCache (line 106) and DiskCache (line 421). All design specifications from design_cache_data_structures.md satisfied.",
          "updated_at": "2025-12-29T06:45:00.000000+00:00"
        }
      ]
    },
    {
      "id": "phase-3",
      "name": "Integrate with LLM Client Layer",
      "status": "in_progress",
      "subtasks": [
        {
          "id": "3-1",
          "name": "Create caching LLM client decorator",
          "description": "Implement CachedLLMClient that wraps existing LLMClient implementations",
          "status": "completed",
          "files_to_modify": [
            "internal/llmcache/key.go"
          ],
          "new_files": [
            "internal/llm/cached_client.go"
          ],
          "dependencies": [
            "2-1",
            "2-2",
            "2-3",
            "2-4",
            "2-5"
          ],
          "notes": "Successfully implemented CachedLLMClient decorator following the design specification. Implementation includes: decorator struct wrapping LLMClient with memory and disk cache support, GenerateCompletion method with two-tier cache lookup (memory first, then disk, with hit promotion), non-blocking error handling (cache failures gracefully bypass), helper function CacheKeyRequestFrom for converting CompletionRequest to CacheKeyRequest, and utility methods (GetStats, CleanupExpired, Clear, GetUnderlyingClient). All LLMClient interface methods implemented: GenerateCompletion (with caching logic), SupportsTools (delegates), GetProvider (returns 'cached-{provider}'). Follows existing code patterns and design document exactly.",
          "updated_at": "2025-12-29T07:00:00.000000+00:00"
        },
        {
          "id": "3-2",
          "name": "Update LLM factory to support caching",
          "description": "Modify llm.NewFactory to create cached clients when caching is enabled",
          "status": "completed",
          "files_to_modify": [
            "internal/llm/factory.go"
          ],
          "dependencies": [
            "3-1"
          ],
          "notes": "Successfully modified llm.NewFactory to support creating cached clients when caching is enabled. Updated Factory struct to include memoryCache, diskCache, cacheEnabled, and cacheTTL fields. Modified NewFactory constructor to accept cache parameters (memoryCache, diskCache, cacheEnabled, cacheTTL). Updated CreateClient method to wrap base clients with CachedLLMClient when caching is enabled and memoryCache is available; otherwise returns base client directly. Updated all existing NewFactory calls in agent files (documenter.go, analyzer.go, ai_rules_generator.go) to pass nil values for cache parameters, maintaining current behavior (caching disabled) until subtask 3-3 adds configuration support. Implementation follows design specifications exactly and maintains backward compatibility.",
          "updated_at": "2025-12-29T07:15:00.000000+00:00"
        },
        {
          "id": "3-3",
          "name": "Add cache configuration to config system",
          "description": "Add LLMCacheConfig structure with enable/disable, max_size, ttl, cache_path options",
          "status": "completed",
          "files_to_modify": [
            "internal/config/models.go"
          ],
          "dependencies": [
            "3-1"
          ],
          "notes": "Successfully implemented LLMCacheConfig structure in internal/config/models.go. Added fields: Enabled (bool), MaxSize (int), TTL (int in days), CachePath (string). Added helper methods following existing patterns: IsEnabled(), GetMaxSize() (default 1000 entries), GetTTL() (default 7 days as time.Duration), GetCachePath() (default '.ai/llm_cache.json'). Integrated LLMCacheConfig into LLMConfig struct as Cache field. All fields include mapstructure tags for YAML configuration support.",
          "updated_at": "2025-12-29T07:30:00.000000+00:00"
        },
        {
          "id": "3-4",
          "name": "Integrate caching in agent creation",
          "description": "Update agent factories to use cached clients when appropriate",
          "status": "completed",
          "files_to_modify": [
            "internal/agents/sub_agents.go",
            "internal/agents/analyzer.go",
            "internal/agents/documenter.go",
            "internal/agents/ai_rules_generator.go"
          ],
          "dependencies": [
            "3-2",
            "3-3"
          ],
          "notes": "Successfully integrated LLM response caching into all agent factories. Added setupCaches() helper function in sub_agents.go that initializes memory and disk caches based on LLMConfig.Cache settings, loads existing disk cache, starts auto-save (5-minute interval), and returns cleanup function. Updated NewAnalyzerAgent in analyzer.go to call setupCaches(), pass cache instances to NewFactory, and defer cleanup in Run(). Updated Run() methods in documenter.go and ai_rules_generator.go to call setupCaches() locally, pass caches to NewFactory, and defer cleanup. All agents now support automatic LLM response caching when enabled via configuration (cache.enabled: true). Caching gracefully degrades on errors with warning logs.",
          "updated_at": "2025-12-29T07:45:00.000000+00:00"
        }
      ]
    },
    {
      "id": "phase-4",
      "name": "Add Metrics and Observability",
      "status": "in_progress",
      "subtasks": [
        {
          "id": "4-1",
          "name": "Implement cache statistics tracking",
          "description": "Track hits, misses, evictions, size, and hit rate",
          "status": "completed",
          "files_to_modify": [
            "internal/llmcache/cache.go",
            "internal/llm/cached_client.go"
          ],
          "dependencies": [
            "2-3"
          ],
          "notes": "Successfully implemented comprehensive statistics tracking for LLM response caching. Enhanced DiskCacheStats with Hits, Misses, Evictions, and HitRate fields. Added thread-safe statistics tracking to DiskCache with dedicated mutex (data.mu). Updated DiskCache.Get() to record hits and misses for all cache lookups. Updated DiskCache.CleanupExpired() to record evictions when expired entries are removed. Added helper methods: recordHit(), recordMiss(), recordEviction(count), updateHitRate(). Added DiskCache.Stats() method to retrieve statistics safely. Enhanced CachedLLMClient.GetStats() to aggregate memory and disk cache statistics into a single unified view. All statistics tracked include: Hits (successful retrievals), Misses (failed lookups), Evictions (removed entries), Size (current entry count), MaxSize (maximum entry limit), TotalSizeBytes (total memory usage), HitRate (ratio of hits to total lookups). Thread-safety guaranteed with mutex locks for all stat updates. Disk cache statistics persist to .ai/llm_cache.json and survive program restarts. Implementation verified and documented in verification_4-1.md.",
          "updated_at": "2025-12-29T08:00:00.000000+00:00"
        },
        {
          "id": "4-2",
          "name": "Add cache logging",
          "description": "Add structured logging for cache operations (hit/miss/store/evict)",
          "status": "completed",
          "files_to_modify": [
            "internal/llmcache/cache.go"
          ],
          "dependencies": [
            "4-1"
          ],
          "notes": "Successfully implemented comprehensive structured logging for all cache operations using zap logger via internal/logging package. Added logger field to LRUCache and DiskCache structs with SetLogger() methods for injection. Updated constructors to use no-op logger by default (backward compatible). LRUCache operations logged: cache_hit (debug with key, hits, access_count, rate), cache_miss (debug with key, misses, rate), cache_miss_expired (debug with key, expired_at), cache_store (debug with key, size_bytes, current/max_size), cache_update (debug with key, old/new_size), cache_evict (debug with key, size, evictions, current_size), cache_cleanup_expired (info with count, remaining). DiskCache operations logged: disk_cache_hit/miss/miss_expired (debug with stats), disk_cache_store/update (debug), disk_cache_cleanup_expired (info), disk_cache_load (info with status, entries), disk_cache_load_failed (error), disk_cache_corrupted (warn), disk_cache_version_mismatch (warn), disk_cache_save (debug), disk_cache_save_failed (error). Updated setupCaches() in sub_agents.go to inject named loggers: llmcache.memory and llmcache.disk. All log entries use structured fields (String, Int, Int64, Float64, Time, Error). Logging levels: Debug for routine ops, Info for events, Warn for issues, Error for failures. Backward compatible with no-op logger default. Cache operations are now fully traceable through logs for performance analysis and issue diagnosis.",
          "updated_at": "2025-12-29T09:00:00.000000+00:00"
        },
        {
          "id": "4-3",
          "name": "Add cache statistics output",
          "description": "Add CLI command or flag to display cache statistics",
          "status": "completed",
          "files_to_modify": [
            "cmd/root.go",
            "cmd/analyze.go"
          ],
          "dependencies": [
            "4-1",
            "4-2"
          ],
          "notes": "Successfully implemented CLI command and flag to display cache statistics. Added --show-cache-stats flag to analyze command that displays stats after analysis. Added standalone cache-stats command that shows statistics without running analysis. Implementation includes: displayCacheStats() function in cmd/analyze.go that loads disk cache, parses JSON, and displays formatted statistics (total entries, expired entries, cache hits/misses, hit rate, storage size, evictions). Added cacheStatsCmd in cmd/root.go as a top-level command. Both approaches handle missing cache files gracefully with helpful error messages. Statistics displayed in user-friendly format with emojis for better UX. Manual verification completed successfully.",
          "updated_at": "2025-12-29T02:35:47.959061"
        }
      ]
    },
    {
      "id": "phase-5",
      "name": "Add Cache Management Utilities",
      "status": "in_progress",
      "subtasks": [
        {
          "id": "5-1",
          "name": "Implement cache clearing command",
          "description": "Add CLI command to clear the LLM response cache",
          "status": "completed",
          "files_to_modify": [
            "cmd/root.go"
          ],
          "new_files": [
            "cmd/cache.go"
          ],
          "dependencies": [
            "2-4"
          ],
          "notes": "Successfully implemented cache-clear CLI command. Created cmd/cache.go with cacheClearCmd that removes the LLM cache file from the repository. Updated cmd/root.go to register the new command. Implementation follows existing patterns from cache-stats command, includes --repo-path flag, handles missing cache files gracefully with helpful messages, and provides user-friendly output with emojis. The command removes the .ai/llm_cache.json file, forcing all LLM requests to be re-executed on the next run. Manual verification completed successfully.",
          "updated_at": "2025-12-29T10:00:00.000000+00:00"
        },
        {
          "id": "5-2",
          "name": "Implement cache validation and recovery",
          "description": "Add checksums and validation to detect corrupted cache entries",
          "status": "completed",
          "files_to_modify": [
            "internal/llmcache/entry.go",
            "internal/llmcache/cache.go"
          ],
          "dependencies": [
            "2-4"
          ],
          "notes": "Successfully implemented checksum-based validation for cache entries. Added Checksum field to CachedResponse struct to store SHA256 hash. Implemented CalculateChecksum() method that computes SHA256 hash over key, request, and response data. Implemented ValidateChecksum() method that verifies stored checksum against calculated checksum, with backward compatibility for entries without checksums. Implemented UpdateChecksum() method to recalculate checksums when entries are modified. Updated DiskCache.Load() to validate all entry checksums after loading from disk and automatically remove corrupted entries with warning logs. Updated DiskCache.Put() to automatically calculate and store checksums before saving entries. Added structured logging: disk_cache_corrupted_entry (warn level for each corrupted entry removed), disk_cache_validation (info level with summary of corrupted and valid entries). Implementation detects individual corrupted entries and removes them rather than discarding entire cache, enhancing reliability and data recovery.",
          "updated_at": "2025-12-29T11:00:00.000000+00:00"
        },
        {
          "id": "5-3",
          "name": "Add cache size estimation",
          "description": "Calculate and report disk usage of cache files",
          "status": "pending",
          "files_to_modify": [],
          "dependencies": [
            "5-1"
          ]
        }
      ]
    },
    {
      "id": "phase-6",
      "name": "Testing",
      "status": "pending",
      "subtasks": [
        {
          "id": "6-1",
          "name": "Unit tests for cache key generation",
          "description": "Test that identical requests generate same key, different requests generate different keys",
          "status": "pending",
          "files_to_modify": [],
          "new_files": [
            "internal/llmcache/key_test.go"
          ],
          "dependencies": [
            "2-2"
          ]
        },
        {
          "id": "6-2",
          "name": "Unit tests for in-memory cache",
          "description": "Test LRU eviction, concurrent access, size limits",
          "status": "pending",
          "files_to_modify": [],
          "new_files": [
            "internal/llmcache/cache_test.go"
          ],
          "dependencies": [
            "2-3"
          ]
        },
        {
          "id": "6-3",
          "name": "Unit tests for disk cache",
          "description": "Test persistence, loading, corruption handling",
          "status": "pending",
          "files_to_modify": [],
          "new_files": [
            "internal/llmcache/persistence_test.go"
          ],
          "dependencies": [
            "2-4"
          ]
        },
        {
          "id": "6-4",
          "name": "Integration tests for cached LLM client",
          "description": "Test end-to-end caching with actual LLM calls",
          "status": "pending",
          "files_to_modify": [],
          "new_files": [
            "internal/llm/cached_client_test.go"
          ],
          "dependencies": [
            "3-1"
          ]
        },
        {
          "id": "6-5",
          "name": "Manual testing with real workloads",
          "description": "Test with documenter and AI rules agents to verify cache hits on re-runs",
          "status": "pending",
          "files_to_modify": [],
          "dependencies": [
            "3-4",
            "6-4"
          ]
        }
      ]
    },
    {
      "id": "phase-7",
      "name": "Documentation",
      "status": "pending",
      "subtasks": [
        {
          "id": "7-1",
          "name": "Document cache configuration options",
          "description": "Add documentation for config options (cache.enabled, cache.max_size_mb, cache.ttl_days)",
          "status": "pending",
          "files_to_modify": [
            "README.md"
          ],
          "dependencies": [
            "3-3"
          ]
        },
        {
          "id": "7-2",
          "name": "Document cache management commands",
          "description": "Document CLI commands for cache management (clear, stats)",
          "status": "pending",
          "files_to_modify": [
            "README.md"
          ],
          "dependencies": [
            "5-1",
            "5-3"
          ]
        },
        {
          "id": "7-3",
          "name": "Add inline code documentation",
          "description": "Add comprehensive Go doc comments to cache types and functions",
          "status": "pending",
          "files_to_modify": [
            "internal/llmcache/*.go",
            "internal/llm/cached_client.go"
          ],
          "dependencies": [
            "2-1",
            "3-1"
          ]
        }
      ]
    }
  ],
  "summary": {
    "total_phases": 7,
    "total_subtasks": 29,
    "estimated_effort": "medium",
    "priority": "high"
  },
  "last_updated": "2025-12-29T10:00:00.000000Z"
}