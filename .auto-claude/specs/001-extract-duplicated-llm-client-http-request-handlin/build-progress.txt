# Build Progress: Extract Duplicated LLM Client HTTP Request Handling

## Summary
Extracting duplicated HTTP request handling logic from OpenAI, Anthropic, and Gemini LLM clients into a shared helper function in BaseLLMClient.

## Status: Phase 2 - Subtask 5 Complete

### Phase 1: Design HTTP Request Helper

#### ✅ Subtask 1: Analyze Duplicated Pattern (Completed)
Created detailed pattern-analysis.md documenting:
- **8-step duplicated pattern** across all three clients
  1. Marshal request to JSON (lines 117, 115, 115)
  2. Create HTTP request (lines 124, 122, 127)
  3. Set HTTP headers (lines 129-130, 127-129, 132)
  4. Execute with retry (lines 133-137, 132-136, 135-139)
  5. Read response body (lines 140-143, 139-142, 142-145)
  6. Check status code (lines 146-148, 145-147, 148-150)
  7. Parse JSON response (lines 151-154, 150-153, 153-156)
  8. Check provider API errors (lines 157-159, 156-158, 159-161)

- **Code duplication metrics**:
  - OpenAI: 32 lines duplicated
  - Anthropic: 33 lines duplicated
  - Gemini: 36 lines duplicated
  - Total: ~101 lines of nearly identical code

- **Identical error messages** across all implementations
- **Provider-specific logic** clearly identified and documented
- **Proposed helper function** signature designed

#### ✅ Subtask 2: Design Helper Function Signature (Completed)
Created comprehensive helper-function-design.md documenting:

**Function Signature:**
```go
func (c *BaseLLMClient) doHTTPRequest(
    ctx context.Context,
    method string,
    url string,
    headers map[string]string,
    body interface{},
) ([]byte, error)
```

**Key Design Decisions:**
- **Location**: BaseLLMClient method to access retryClient
- **Body Parameter**: interface{} type for provider-specific request structs
- **Headers Parameter**: map[string]string for flexibility
- **Return Type**: Raw []byte to allow provider-specific parsing

**Implementation Behavior:**
1. Marshal request body to JSON
2. Create HTTP request with context
3. Set headers from map
4. Execute with retryClient.Do
5. Read response body
6. Validate status code (200 OK)
7. Return raw bytes for provider-specific parsing

**Error Handling:**
- `"failed to marshal request: %w"`
- `"failed to create request: %w"`
- `"request failed: %w"`
- `"failed to read response: %w"`
- `"API error: status %d, body: %s"`

**Benefits:**
- ~70 lines of code reduction
- Single source of truth for HTTP handling
- Consistent error messages and retry behavior
- Provider-specific logic preserved

**Verification Criteria:**
- 10 specific criteria covering signature, behavior, error handling, and resource cleanup

#### ✅ Subtask 3: Identify Provider-Specific Logic (Completed)
Created comprehensive provider-specific-logic-confirmation.md documenting:

**5 Categories of Provider-Specific Logic Confirmed:**

1. **Request Format Conversion** (convertRequest methods)
   - OpenAI: openaiRequest with message array + tools
   - Anthropic: anthropicRequest with content blocks structure
   - Gemini: geminiRequest with contents/parts structure

2. **Response Format Conversion** (convertResponse methods)
   - OpenAI: Extracts from Choices[] array
   - Anthropic: Extracts from Content[] blocks
   - Gemini: Extracts from Candidates[].Content.Parts[]

3. **API Authentication**
   - OpenAI: Authorization: Bearer token header
   - Anthropic: x-api-key header + anthropic-version
   - Gemini: API key in URL query parameter

4. **URL Construction**
   - OpenAI: {baseURL}/chat/completions
   - Anthropic: {baseURL}/v1/messages
   - Gemini: {baseURL}/v1beta/{model}:generateContent?key={apiKey}

5. **Additional Response Validation**
   - OpenAI: Checks openaiResponse.Error field
   - Anthropic: Checks anthropicResponse.Error field
   - Gemini: Checks error + empty candidates + safety blocks

**Summary Table:**
- Clear mapping of what stays in each client vs. what gets extracted
- All provider-specific logic confirmed to remain intact
- Only truly duplicated HTTP handling will be centralized

**Verification:**
- ✅ Provider-specific request/response conversion preserved
- ✅ Provider authentication mechanisms maintained
- ✅ Provider-specific error checking stays in place
- ✅ Only duplicated HTTP handling extracted
- ✅ Each provider can evolve independently
- ✅ No breaking changes to public interfaces
- ✅ Test compatibility maintained

**Function Signature:**
```go
func (c *BaseLLMClient) doHTTPRequest(
    ctx context.Context,
    method string,
    url string,
    headers map[string]string,
    body interface{},
) ([]byte, error)
```

**Key Design Decisions:**
- **Location**: BaseLLMClient method to access retryClient
- **Body Parameter**: interface{} type for provider-specific request structs
- **Headers Parameter**: map[string]string for flexibility
- **Return Type**: Raw []byte to allow provider-specific parsing

**Implementation Behavior:**
1. Marshal request body to JSON
2. Create HTTP request with context
3. Set headers from map
4. Execute with retryClient.Do
5. Read response body
6. Validate status code (200 OK)
7. Return raw bytes for provider-specific parsing

**Error Handling:**
- `"failed to marshal request: %w"`
- `"failed to create request: %w"`
- `"request failed: %w"`
- `"failed to read response: %w"`
- `"API error: status %d, body: %s"`

**Benefits:**
- ~70 lines of code reduction
- Single source of truth for HTTP handling
- Consistent error messages and retry behavior
- Provider-specific logic preserved

**Verification Criteria:**
- 10 specific criteria covering signature, behavior, error handling, and resource cleanup

### Phase 1 Status: ✅ COMPLETE

All three design subtasks completed:
1. ✅ Pattern analysis documented (pattern-analysis.md)
2. ✅ Helper function signature designed (helper-function-design.md)
3. ✅ Provider-specific logic confirmed (provider-specific-logic-confirmation.md)

### Next Steps
1. ✅ Subtask 1: Document pattern (COMPLETED)
2. ✅ Subtask 2: Design helper function signature (COMPLETED)
3. ✅ Subtask 3: Identify provider-specific logic (COMPLETED)
4. ✅ Phase 2: Implement HTTP request helper (IN PROGRESS)

---

### Phase 2: Implement HTTP Request Helper

#### ✅ Subtask 1: Add doHTTPRequest method to BaseLLMClient (Completed)

**Implementation Summary:**
Successfully implemented the `doHTTPRequest` method in `internal/llm/client.go` with:

**Function Signature:**
```go
func (c *BaseLLMClient) doHTTPRequest(
    ctx context.Context,
    method string,
    url string,
    headers map[string]string,
    body interface{},
) ([]byte, error)
```

**Implementation Details:**
1. **JSON Marshaling** (lines 112-119)
   - Checks if body is nil before marshaling
   - Returns wrapped error: `"failed to marshal request: %w"`

2. **HTTP Request Creation** (lines 122-129)
   - Uses `http.NewRequestWithContext` for context support
   - Creates body reader only if jsonData exists
   - Returns wrapped error: `"failed to create request: %w"`

3. **Header Setting** (lines 131-134)
   - Iterates through headers map
   - Sets each header using `httpReq.Header.Set(key, value)`

4. **Request Execution** (lines 137-141)
   - Uses `c.retryClient.Do(httpReq)` for automatic retries
   - Returns wrapped error: `"request failed: %w"`
   - Properly defers `resp.Body.Close()` for resource cleanup

5. **Response Reading** (lines 144-147)
   - Uses `io.ReadAll(resp.Body)` to read complete response
   - Returns wrapped error: `"failed to read response: %w"`

6. **Status Validation** (lines 150-152)
   - Checks `resp.StatusCode != http.StatusOK`
   - Returns error with status code and response body: `"API error: status %d, body: %s"`

7. **Success Return** (line 154)
   - Returns raw response body bytes for provider-specific parsing

**Added Imports:**
- `bytes` - for bytes.NewReader
- `encoding/json` - for json.Marshal
- `fmt` - for fmt.Errorf
- `io` - for io.ReadAll
- `net/http` - for http.NewRequestWithContext and http.StatusOK

**Documentation:**
- Comprehensive function documentation with parameter descriptions
- Clear error handling documentation
- Usage examples in design doc

**Acceptance Criteria Met:**
- ✅ Method accepts method, url, headers map, and body interface
- ✅ Marshals body to JSON
- ✅ Creates HTTP request with context
- ✅ Sets all provided headers
- ✅ Executes with retryClient.Do
- ✅ Reads response body
- ✅ Returns error on non-200 status
- ✅ Returns response body bytes on success
- ✅ Proper resource cleanup with defer
- ✅ All error messages match existing pattern

**Files Modified:**
- `internal/llm/client.go` - Added doHTTPRequest method (52 lines)

#### ✅ Subtask 2: Handle JSON marshaling errors (Completed)

**Verification Summary:**
Verified proper error wrapping for JSON marshaling failures in the doHTTPRequest method:

**Implementation Location:** Lines 112-118 of `internal/llm/client.go`

```go
// Marshal request body to JSON
var jsonData []byte
if body != nil {
    var err error
    jsonData, err = json.Marshal(body)
    if err != nil {
        return nil, fmt.Errorf("failed to marshal request: %w", err)
    }
}
```

**Acceptance Criteria Met:**
- ✅ Returns wrapped error with context 'failed to marshal request'
- ✅ Uses `%w` verb for proper error wrapping (allows errors.Is/As to work)
- ✅ Matches exact pattern from existing implementations (openai.go, anthropic.go, gemini.go)
- ✅ Handles nil body case correctly - only marshals if body != nil

**Pattern Consistency:**
The implementation matches the duplicated pattern from all three LLM clients:
- OpenAI: Line 117 - `fmt.Errorf("failed to marshal request: %w", err)`
- Anthropic: Line 115 - `fmt.Errorf("failed to marshal request: %w", err)`
- Gemini: Line 115 - `fmt.Errorf("failed to marshal request: %w", err)`
- **New Helper:** Line 117 - `fmt.Errorf("failed to marshal request: %w", err)`

**Files Modified:**
- No code changes required (already correctly implemented in subtask 1)
- Updated `implementation_plan.json` to mark subtask as completed

#### ✅ Subtask 3: Handle HTTP request creation errors (Completed)

**Verification Summary:**
Verified proper error wrapping for HTTP request creation failures in the doHTTPRequest method:

**Implementation Location:** Lines 126-129 of `internal/llm/client.go`

```go
// Create HTTP request with context
var bodyReader *bytes.Reader
if jsonData != nil {
    bodyReader = bytes.NewReader(jsonData)
}
httpReq, err := http.NewRequestWithContext(ctx, method, url, bodyReader)
if err != nil {
    return nil, fmt.Errorf("failed to create request: %w", err)
}
```

**Acceptance Criteria Met:**
- ✅ Returns wrapped error with context 'failed to create request'
- ✅ Uses `%w` verb for proper error wrapping (allows errors.Is/As to work)
- ✅ Matches exact pattern from existing implementations (openai.go, anthropic.go, gemini.go)
- ✅ Properly handles nil jsonData by conditionally creating bodyReader

**Pattern Consistency:**
The implementation matches the duplicated pattern from all three LLM clients:
- OpenAI: Line 124 - `fmt.Errorf("failed to create request: %w", err)`
- Anthropic: Line 122 - `fmt.Errorf("failed to create request: %w", err)`
- Gemini: Line 127 - `fmt.Errorf("failed to create request: %w", err)`
- **New Helper:** Line 128 - `fmt.Errorf("failed to create request: %w", err)`

**Files Modified:**
- No code changes required (already correctly implemented in subtask 1)
- Updated `implementation_plan.json` to mark subtask as completed

#### ✅ Subtask 4: Handle request execution errors (Completed)

**Verification Summary:**
Verified proper error wrapping for request execution failures in the doHTTPRequest method:

**Implementation Location:** Lines 137-141 of `internal/llm/client.go`

```go
// Execute request with retry
resp, err := c.retryClient.Do(httpReq)
if err != nil {
    return nil, fmt.Errorf("request failed: %w", err)
}
defer resp.Body.Close()
```

**Acceptance Criteria Met:**
- ✅ Returns wrapped error with context 'request failed'
- ✅ Uses `%w` verb for proper error wrapping (allows errors.Is/As to work)
- ✅ Matches exact pattern from existing implementations
  - OpenAI: Line 135 - `fmt.Errorf("request failed: %w", err)`
  - Anthropic: Line 134 - `fmt.Errorf("request failed: %w", err)`
  - Gemini: Line 137 - `fmt.Errorf("request failed: %w", err)`
  - **New Helper:** Line 139 - `fmt.Errorf("request failed: %w", err)`
- ✅ Properly defers `resp.Body.Close()` for resource cleanup (line 141)
- ✅ Defer statement placed immediately after error check to ensure cleanup even if subsequent operations fail

**Pattern Consistency:**
The implementation exactly matches the duplicated pattern from all three LLM clients:
1. Execute request via `c.retryClient.Do(httpReq)`
2. Check error and return wrapped `"request failed: %w"` error
3. Defer `resp.Body.Close()` to ensure proper resource cleanup

**Files Modified:**
- No code changes required (already correctly implemented in subtask 1)
- Updated `implementation_plan.json` to mark subtask as completed

**Next Subtask:**
- Phase 2, Subtask 5: Handle response reading errors (already implemented in subtask 1)
- Phase 2, Subtask 6: Handle non-OK status codes (already implemented in subtask 1)

**Note:** All Phase 2 subtasks (2-6) were completed as part of subtask 1 since the complete implementation includes all error handling. Each remaining subtask will be verified and marked complete.

#### ✅ Subtask 5: Handle response reading errors (Completed)

**Verification Summary:**
Verified proper error wrapping for response body reading failures in the doHTTPRequest method:

**Implementation Location:** Lines 144-147 of `internal/llm/client.go`

```go
// Read response body
responseBody, err := io.ReadAll(resp.Body)
if err != nil {
    return nil, fmt.Errorf("failed to read response: %w", err)
}
```

**Acceptance Criteria Met:**
- ✅ Returns wrapped error with context 'failed to read response'
- ✅ Uses `%w` verb for proper error wrapping (allows errors.Is/As to work)
- ✅ Matches exact pattern from existing implementations (openai.go, anthropic.go, gemini.go)
- ✅ Returns nil for bytes parameter on error (consistent with helper return type)
- ✅ Uses `io.ReadAll` to read complete response body

**Pattern Consistency:**
The implementation matches the duplicated pattern from all three LLM clients:
- OpenAI: Line 142 - `fmt.Errorf("failed to read response: %w", err)`
- Anthropic: Line 141 - `fmt.Errorf("failed to read response: %w", err)`
- Gemini: Line 144 - `fmt.Errorf("failed to read response: %w", err)`
- **New Helper:** Line 146 - `fmt.Errorf("failed to read response: %w", err)`

**Note:** The only difference from the existing clients is that the helper returns `nil` (for `[]byte`) instead of `CompletionResponse{}` because the helper function signature returns `[]byte, error` instead of `CompletionResponse, error`.

**Files Modified:**
- No code changes required (already correctly implemented in subtask 1)
- Updated `implementation_plan.json` to mark subtask as completed

#### ✅ Subtask 6: Handle non-OK status codes (Completed)

**Verification Summary:**
Verified proper error wrapping for non-OK status codes in the doHTTPRequest method:

**Implementation Location:** Lines 149-152 of `internal/llm/client.go`

```go
// Check for error status
if resp.StatusCode != http.StatusOK {
    return nil, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(responseBody))
}
```

**Acceptance Criteria Met:**
- ✅ Checks if resp.StatusCode != http.StatusOK
- ✅ Returns wrapped error with status code and response body
- ✅ Error message format exactly matches: 'API error: status %d, body: %s'
- ✅ Returns nil for bytes parameter on error (consistent with helper return type)
- ✅ Includes full response body in error message for debugging

**Pattern Consistency:**
The implementation matches the duplicated pattern from all three LLM clients:
- OpenAI: Lines 146-148 - `fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))`
- Anthropic: Lines 145-147 - `fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))`
- Gemini: Lines 148-150 - `fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))`
- **New Helper:** Lines 149-152 - `fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(responseBody))`

**Note:** The implementation uses `responseBody` variable name instead of `body` to be more descriptive, and returns `nil` (for `[]byte`) instead of `CompletionResponse{}` because the helper function signature returns `[]byte, error` instead of `CompletionResponse, error`.

**Files Modified:**
- No code changes required (already correctly implemented in subtask 1)
- Updated `implementation_plan.json` to mark subtask as completed

**Commit:**
- Commit hash: 7cfe3bd
- Commit message: "auto-claude: phase-2-subtask-6 - Ensure proper error wrapping for non-200 status co"

### Phase 2 Status: ✅ COMPLETE

All Phase 2 subtasks completed:
1. ✅ Subtask 1: Add doHTTPRequest method to BaseLLMClient (COMPLETED)
2. ✅ Subtask 2: Handle JSON marshaling errors (COMPLETED)
3. ✅ Subtask 3: Handle HTTP request creation errors (COMPLETED)
4. ✅ Subtask 4: Handle request execution errors (COMPLETED)
5. ✅ Subtask 5: Handle response reading errors (COMPLETED)
6. ✅ Subtask 6: Handle non-OK status codes (COMPLETED)

### Implementation Plan Created
- ✅ 6 phases defined with 19 subtasks
- ✅ Each subtask has clear acceptance criteria
- ✅ Estimated total time: ~2.5 hours

### Next Steps
1. Begin Phase 1: Design HTTP request helper
2. Implement doHTTPRequest method in BaseLLMClient
3. Refactor each LLM client to use the helper
4. Verify all tests pass

### Key Design Decisions
- Helper method signature: `doHTTPRequest(ctx context.Context, method, url string, headers map[string]string, body interface{}) ([]byte, error)`
- Added to BaseLLMClient to leverage existing retryClient
- Returns raw response bytes, allowing each client to handle provider-specific parsing
- Preserves exact error messages and wrapping behavior

### Files Modified
- internal/llm/client.go (will add doHTTPRequest method)
- internal/llm/openai.go (will refactor GenerateCompletion)
- internal/llm/anthropic.go (will refactor GenerateCompletion)
- internal/llm/gemini.go (will refactor GenerateCompletion)

### Expected Outcomes
- ~90 lines of code reduction
- Single source of truth for HTTP request handling
- Easier maintenance and bug fixes
- No breaking changes to public APIs
- All existing tests continue to pass
