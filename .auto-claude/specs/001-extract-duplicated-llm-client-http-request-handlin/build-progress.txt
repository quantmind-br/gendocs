# Build Progress: Extract Duplicated LLM Client HTTP Request Handling

## Summary
Extracting duplicated HTTP request handling logic from OpenAI, Anthropic, and Gemini LLM clients into a shared helper function in BaseLLMClient.

## Status: Phase 2 - Subtask 1 Complete

### Phase 1: Design HTTP Request Helper

#### ✅ Subtask 1: Analyze Duplicated Pattern (Completed)
Created detailed pattern-analysis.md documenting:
- **8-step duplicated pattern** across all three clients
  1. Marshal request to JSON (lines 117, 115, 115)
  2. Create HTTP request (lines 124, 122, 127)
  3. Set HTTP headers (lines 129-130, 127-129, 132)
  4. Execute with retry (lines 133-137, 132-136, 135-139)
  5. Read response body (lines 140-143, 139-142, 142-145)
  6. Check status code (lines 146-148, 145-147, 148-150)
  7. Parse JSON response (lines 151-154, 150-153, 153-156)
  8. Check provider API errors (lines 157-159, 156-158, 159-161)

- **Code duplication metrics**:
  - OpenAI: 32 lines duplicated
  - Anthropic: 33 lines duplicated
  - Gemini: 36 lines duplicated
  - Total: ~101 lines of nearly identical code

- **Identical error messages** across all implementations
- **Provider-specific logic** clearly identified and documented
- **Proposed helper function** signature designed

#### ✅ Subtask 2: Design Helper Function Signature (Completed)
Created comprehensive helper-function-design.md documenting:

**Function Signature:**
```go
func (c *BaseLLMClient) doHTTPRequest(
    ctx context.Context,
    method string,
    url string,
    headers map[string]string,
    body interface{},
) ([]byte, error)
```

**Key Design Decisions:**
- **Location**: BaseLLMClient method to access retryClient
- **Body Parameter**: interface{} type for provider-specific request structs
- **Headers Parameter**: map[string]string for flexibility
- **Return Type**: Raw []byte to allow provider-specific parsing

**Implementation Behavior:**
1. Marshal request body to JSON
2. Create HTTP request with context
3. Set headers from map
4. Execute with retryClient.Do
5. Read response body
6. Validate status code (200 OK)
7. Return raw bytes for provider-specific parsing

**Error Handling:**
- `"failed to marshal request: %w"`
- `"failed to create request: %w"`
- `"request failed: %w"`
- `"failed to read response: %w"`
- `"API error: status %d, body: %s"`

**Benefits:**
- ~70 lines of code reduction
- Single source of truth for HTTP handling
- Consistent error messages and retry behavior
- Provider-specific logic preserved

**Verification Criteria:**
- 10 specific criteria covering signature, behavior, error handling, and resource cleanup

#### ✅ Subtask 3: Identify Provider-Specific Logic (Completed)
Created comprehensive provider-specific-logic-confirmation.md documenting:

**5 Categories of Provider-Specific Logic Confirmed:**

1. **Request Format Conversion** (convertRequest methods)
   - OpenAI: openaiRequest with message array + tools
   - Anthropic: anthropicRequest with content blocks structure
   - Gemini: geminiRequest with contents/parts structure

2. **Response Format Conversion** (convertResponse methods)
   - OpenAI: Extracts from Choices[] array
   - Anthropic: Extracts from Content[] blocks
   - Gemini: Extracts from Candidates[].Content.Parts[]

3. **API Authentication**
   - OpenAI: Authorization: Bearer token header
   - Anthropic: x-api-key header + anthropic-version
   - Gemini: API key in URL query parameter

4. **URL Construction**
   - OpenAI: {baseURL}/chat/completions
   - Anthropic: {baseURL}/v1/messages
   - Gemini: {baseURL}/v1beta/{model}:generateContent?key={apiKey}

5. **Additional Response Validation**
   - OpenAI: Checks openaiResponse.Error field
   - Anthropic: Checks anthropicResponse.Error field
   - Gemini: Checks error + empty candidates + safety blocks

**Summary Table:**
- Clear mapping of what stays in each client vs. what gets extracted
- All provider-specific logic confirmed to remain intact
- Only truly duplicated HTTP handling will be centralized

**Verification:**
- ✅ Provider-specific request/response conversion preserved
- ✅ Provider authentication mechanisms maintained
- ✅ Provider-specific error checking stays in place
- ✅ Only duplicated HTTP handling extracted
- ✅ Each provider can evolve independently
- ✅ No breaking changes to public interfaces
- ✅ Test compatibility maintained

**Function Signature:**
```go
func (c *BaseLLMClient) doHTTPRequest(
    ctx context.Context,
    method string,
    url string,
    headers map[string]string,
    body interface{},
) ([]byte, error)
```

**Key Design Decisions:**
- **Location**: BaseLLMClient method to access retryClient
- **Body Parameter**: interface{} type for provider-specific request structs
- **Headers Parameter**: map[string]string for flexibility
- **Return Type**: Raw []byte to allow provider-specific parsing

**Implementation Behavior:**
1. Marshal request body to JSON
2. Create HTTP request with context
3. Set headers from map
4. Execute with retryClient.Do
5. Read response body
6. Validate status code (200 OK)
7. Return raw bytes for provider-specific parsing

**Error Handling:**
- `"failed to marshal request: %w"`
- `"failed to create request: %w"`
- `"request failed: %w"`
- `"failed to read response: %w"`
- `"API error: status %d, body: %s"`

**Benefits:**
- ~70 lines of code reduction
- Single source of truth for HTTP handling
- Consistent error messages and retry behavior
- Provider-specific logic preserved

**Verification Criteria:**
- 10 specific criteria covering signature, behavior, error handling, and resource cleanup

### Phase 1 Status: ✅ COMPLETE

All three design subtasks completed:
1. ✅ Pattern analysis documented (pattern-analysis.md)
2. ✅ Helper function signature designed (helper-function-design.md)
3. ✅ Provider-specific logic confirmed (provider-specific-logic-confirmation.md)

### Next Steps
1. ✅ Subtask 1: Document pattern (COMPLETED)
2. ✅ Subtask 2: Design helper function signature (COMPLETED)
3. ✅ Subtask 3: Identify provider-specific logic (COMPLETED)
4. ✅ Phase 2: Implement HTTP request helper (IN PROGRESS)

---

### Phase 2: Implement HTTP Request Helper

#### ✅ Subtask 1: Add doHTTPRequest method to BaseLLMClient (Completed)

**Implementation Summary:**
Successfully implemented the `doHTTPRequest` method in `internal/llm/client.go` with:

**Function Signature:**
```go
func (c *BaseLLMClient) doHTTPRequest(
    ctx context.Context,
    method string,
    url string,
    headers map[string]string,
    body interface{},
) ([]byte, error)
```

**Implementation Details:**
1. **JSON Marshaling** (lines 112-119)
   - Checks if body is nil before marshaling
   - Returns wrapped error: `"failed to marshal request: %w"`

2. **HTTP Request Creation** (lines 122-129)
   - Uses `http.NewRequestWithContext` for context support
   - Creates body reader only if jsonData exists
   - Returns wrapped error: `"failed to create request: %w"`

3. **Header Setting** (lines 131-134)
   - Iterates through headers map
   - Sets each header using `httpReq.Header.Set(key, value)`

4. **Request Execution** (lines 137-141)
   - Uses `c.retryClient.Do(httpReq)` for automatic retries
   - Returns wrapped error: `"request failed: %w"`
   - Properly defers `resp.Body.Close()` for resource cleanup

5. **Response Reading** (lines 144-147)
   - Uses `io.ReadAll(resp.Body)` to read complete response
   - Returns wrapped error: `"failed to read response: %w"`

6. **Status Validation** (lines 150-152)
   - Checks `resp.StatusCode != http.StatusOK`
   - Returns error with status code and response body: `"API error: status %d, body: %s"`

7. **Success Return** (line 154)
   - Returns raw response body bytes for provider-specific parsing

**Added Imports:**
- `bytes` - for bytes.NewReader
- `encoding/json` - for json.Marshal
- `fmt` - for fmt.Errorf
- `io` - for io.ReadAll
- `net/http` - for http.NewRequestWithContext and http.StatusOK

**Documentation:**
- Comprehensive function documentation with parameter descriptions
- Clear error handling documentation
- Usage examples in design doc

**Acceptance Criteria Met:**
- ✅ Method accepts method, url, headers map, and body interface
- ✅ Marshals body to JSON
- ✅ Creates HTTP request with context
- ✅ Sets all provided headers
- ✅ Executes with retryClient.Do
- ✅ Reads response body
- ✅ Returns error on non-200 status
- ✅ Returns response body bytes on success
- ✅ Proper resource cleanup with defer
- ✅ All error messages match existing pattern

**Files Modified:**
- `internal/llm/client.go` - Added doHTTPRequest method (52 lines)

**Next Subtask:**
- Phase 2, Subtask 2: Handle JSON marshaling errors (already implemented in subtask 1)
- Phase 2, Subtask 3: Handle HTTP request creation errors (already implemented in subtask 1)
- Phase 2, Subtask 4: Handle request execution errors (already implemented in subtask 1)
- Phase 2, Subtask 5: Handle response reading errors (already implemented in subtask 1)
- Phase 2, Subtask 6: Handle non-OK status codes (already implemented in subtask 1)

**Note:** All Phase 2 subtasks (2-6) were completed as part of subtask 1 since the complete implementation includes all error handling.

### Implementation Plan Created
- ✅ 6 phases defined with 19 subtasks
- ✅ Each subtask has clear acceptance criteria
- ✅ Estimated total time: ~2.5 hours

### Next Steps
1. Begin Phase 1: Design HTTP request helper
2. Implement doHTTPRequest method in BaseLLMClient
3. Refactor each LLM client to use the helper
4. Verify all tests pass

### Key Design Decisions
- Helper method signature: `doHTTPRequest(ctx context.Context, method, url string, headers map[string]string, body interface{}) ([]byte, error)`
- Added to BaseLLMClient to leverage existing retryClient
- Returns raw response bytes, allowing each client to handle provider-specific parsing
- Preserves exact error messages and wrapping behavior

### Files Modified
- internal/llm/client.go (will add doHTTPRequest method)
- internal/llm/openai.go (will refactor GenerateCompletion)
- internal/llm/anthropic.go (will refactor GenerateCompletion)
- internal/llm/gemini.go (will refactor GenerateCompletion)

### Expected Outcomes
- ~90 lines of code reduction
- Single source of truth for HTTP request handling
- Easier maintenance and bug fixes
- No breaking changes to public APIs
- All existing tests continue to pass
