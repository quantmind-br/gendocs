{
  "feature": "Optimize file scanning with selective hashing and parallel processing",
  "description": "The cache.ScanFiles() function calculates SHA256 hashes for every file sequentially, which becomes a bottleneck for large repositories. Many files haven't changed and don't need re-hashing.",
  "created_at": "2025-12-29T03:58:22.725Z",
  "updated_at": "2025-12-29T04:00:00.000Z",
  "status": "in_progress",
  "planStatus": "in_progress",
  "spec_file": "spec.md",
  "phases": [
    {
      "phase_id": "phase-1",
      "name": "Add mtime-based selective hashing optimization",
      "description": "Implement selective hashing to skip computing hashes for files whose modification time and size haven't changed since the last scan.",
      "order": 1,
      "subtasks": [
        {
          "subtask_id": "phase-1-subtask-1",
          "name": "Refactor ScanFiles to accept optional cache parameter",
          "description": "Modify the ScanFiles function signature to accept an optional *AnalysisCache parameter. When provided, use cached file metadata to skip hashing unchanged files.",
          "status": "completed",
          "files": [
            "internal/cache/cache.go"
          ],
          "implementation_details": [
            "Add optional cache parameter to ScanFiles: func ScanFiles(repoPath string, ignorePatterns []string, cache *AnalysisCache) (map[string]FileInfo, error)",
            "When cache is provided and file exists in cache with same mtime and size, reuse cached hash",
            "Maintain backward compatibility by making cache parameter optional (can be nil)"
          ]
        },
        {
          "subtask_id": "phase-1-subtask-2",
          "name": "Add metrics tracking for cache hits/misses",
          "description": "Track statistics about how many files were skipped due to cache hits vs actual hashes computed, for logging and debugging.",
          "status": "completed",
          "files": [
            "internal/cache/cache.go"
          ],
          "implementation_details": [
            "Define ScanMetrics struct with fields: TotalFiles, CachedFiles, HashedFiles, SkippedFiles",
            "Add metrics parameter to ScanFiles or return as additional value",
            "Log metrics at debug level to show optimization effectiveness"
          ]
        },
        {
          "subtask_id": "phase-1-subtask-3",
          "name": "Update analyzer.go to pass cache to ScanFiles",
          "description": "Modify the analyzer to pass the loaded cache to ScanFiles so it can leverage selective hashing.",
          "status": "completed",
          "files": [
            "internal/agents/analyzer.go"
          ],
          "implementation_details": [
            "Load cache before calling ScanFiles (already done on line 63)",
            "Pass analysisCache to ScanFiles call (line 57)",
            "Log cache hit/miss metrics after scan completes"
          ]
        }
      ]
    },
    {
      "phase_id": "phase-2",
      "name": "Add parallel file hashing",
      "description": "Implement parallel hashing using a worker pool to speed up the CPU-bound hash computation for files that need hashing.",
      "order": 2,
      "subtasks": [
        {
          "subtask_id": "phase-2-subtask-1",
          "name": "Create parallel hash worker implementation",
          "description": "Create a parallel hashing system that uses goroutines to compute multiple file hashes concurrently.",
          "status": "completed",
          "files": [
            "internal/cache/cache.go"
          ],
          "implementation_details": [
            "Create hashFileJob struct to hold file path and result channel",
            "Implement parallelHashFiles function that takes a list of files to hash",
            "Use runtime.NumCPU() to determine optimal worker count for hash computation",
            "Use errgroup or sync.WaitGroup to manage concurrent hashing operations",
            "Limit parallelism to avoid overwhelming the filesystem (e.g., max 4-8 workers)"
          ]
        },
        {
          "subtask_id": "phase-2-subtask-2",
          "name": "Integrate parallel hashing into ScanFiles",
          "description": "Modify ScanFiles to collect files first, then batch the hashing operations for parallel execution.",
          "status": "completed",
          "files": [
            "internal/cache/cache.go"
          ],
          "implementation_details": [
            "Separate file walking from hashing into two phases",
            "Phase 1: Walk directory tree, collect file paths and metadata (mtime, size)",
            "Phase 2: Determine which files need hashing (cache miss), process in parallel",
            "Combine results with cached data for final map"
          ]
        },
        {
          "subtask_id": "phase-2-subtask-3",
          "name": "Add configurable parallelism limit",
          "description": "Add a configuration option or environment variable to control the maximum number of parallel hash workers.",
          "status": "completed",
          "files": [
            "internal/cache/cache.go",
            "internal/config/models.go",
            "internal/config/loader.go",
            "internal/agents/analyzer.go"
          ],
          "implementation_details": [
            "Add MaxHashWorkers config field (default to runtime.NumCPU())",
            "Pass configuration through to ScanFiles",
            "Respect configured limit in parallel hash worker pool",
            "Support configuration via .ai/config.yaml (analyzer.max_hash_workers)",
            "Support configuration via environment variable (GENDOCS_ANALYZER_MAX_HASH_WORKERS)"
          ]
        }
      ]
    },
    {
      "phase_id": "phase-3",
      "name": "Add comprehensive testing",
      "description": "Create unit tests and benchmarks to verify correctness and measure performance improvements.",
      "order": 3,
      "subtasks": [
        {
          "subtask_id": "phase-3-subtask-1",
          "name": "Create unit tests for selective hashing",
          "description": "Write tests that verify selective hashing correctly skips files with unchanged mtime/size and correctly rehashes changed files.",
          "status": "completed",
          "files": [
            "internal/cache/cache_test.go"
          ],
          "implementation_details": [
            "Test cache hit: file with same mtime/size reuses hash",
            "Test cache miss: file with different mtime gets rehashed",
            "Test cache miss: file with different size gets rehashed",
            "Test new files: files not in cache get hashed",
            "Test deleted files: files in cache but not on disk are handled correctly"
          ]
        },
        {
          "subtask_id": "phase-3-subtask-2",
          "name": "Create benchmarks for performance",
          "description": "Create benchmarks to measure the performance improvement from selective hashing and parallel processing.",
          "status": "pending",
          "files": [
            "internal/cache/cache_bench_test.go"
          ],
          "implementation_details": [
            "Benchmark sequential ScanFiles (baseline)",
            "Benchmark ScanFiles with selective hashing only",
            "Benchmark ScanFiles with selective hashing + parallel hashing",
            "Use realistic test dataset with ~1000 files",
            "Report throughput: files/second and MB/second"
          ]
        },
        {
          "subtask_id": "phase-3-subtask-3",
          "name": "Add integration tests",
          "description": "Create integration tests that verify the full workflow with cache load, scan, and save operations.",
          "status": "pending",
          "files": [
            "internal/cache/cache_integration_test.go"
          ],
          "implementation_details": [
            "Test full scan cycle: load cache -> scan files -> detect changes -> save cache",
            "Test incremental scan with no changes",
            "Test incremental scan with some file changes",
            "Verify cache persistence across multiple scan cycles"
          ]
        }
      ]
    },
    {
      "phase_id": "phase-4",
      "name": "Documentation and cleanup",
      "description": "Update documentation, add comments, and perform final code review.",
      "order": 4,
      "subtasks": [
        {
          "subtask_id": "phase-4-subtask-1",
          "name": "Add code comments and documentation",
          "description": "Add comprehensive comments explaining the selective hashing and parallel processing logic.",
          "status": "pending",
          "files": [
            "internal/cache/cache.go"
          ],
          "implementation_details": [
            "Document the selective hashing algorithm and cache hit conditions",
            "Explain the parallel hashing worker pool design",
            "Add package-level documentation for the cache package",
            "Document the ScanMetrics struct and its purpose"
          ]
        },
        {
          "subtask_id": "phase-4-subtask-2",
          "name": "Update README and documentation",
          "description": "Update project documentation to explain the file scanning optimizations.",
          "status": "pending",
          "files": [
            "README.md"
          ],
          "implementation_details": [
            "Document the performance optimizations in the README",
            "Add information about configuration options for parallelism",
            "Include benchmark results showing typical improvements"
          ]
        },
        {
          "subtask_id": "phase-4-subtask-3",
          "name": "Final code review and validation",
          "description": "Perform final code review, run all tests, and verify the implementation meets the spec requirements.",
          "status": "pending",
          "files": [
            "internal/cache/cache.go",
            "internal/agents/analyzer.go"
          ],
          "implementation_details": [
            "Review all code changes for correctness and style",
            "Run full test suite and ensure all tests pass",
            "Verify no regressions in existing functionality",
            "Confirm performance improvements meet expectations",
            "Check for race conditions using go test -race"
          ]
        }
      ]
    }
  ],
  "workflow_type": "development",
  "services_involved": [],
  "final_acceptance": [
    "ScanFiles accepts optional cache parameter and skips hashing unchanged files",
    "File hashing is performed in parallel using worker pool",
    "Unit tests verify selective hashing correctness",
    "Benchmarks show significant performance improvement (target: 3-5x faster for incremental scans)",
    "No race conditions detected in tests",
    "Documentation updated with optimization details"
  ],
  "metadata": {
    "estimated_time": "4-6 hours",
    "complexity": "medium",
    "risk_level": "low",
    "dependencies": [
      "Go 1.21+ (for errgroup if used)"
    ],
    "testing_strategy": "Unit tests, benchmarks, and integration tests to verify correctness and performance",
    "rollback_plan": "Changes are additive; can revert to sequential ScanFiles by passing nil cache parameter"
  }
}
