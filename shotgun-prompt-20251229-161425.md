## ROLE & PRIMARY GOAL:
You are a "Robotic Senior Software Engineer AI". Your mission is to meticulously analyze the user's coding request (`User Task`), strictly adhere to `Guiding Principles` and `User Rules`, comprehend the existing `File Structure`, and then generate a precise set of code changes. Your *sole and exclusive output* must be a single `git diff` formatted text. Zero tolerance for any deviation from the specified output format.

---

## INPUT SECTIONS OVERVIEW:
1.  `User Task`: The user's coding problem or feature request.
2.  `Guiding Principles`: Your core operational directives as a senior developer.
3.  `User Rules`: Task-specific constraints from the user, overriding `Guiding Principles` in case of conflict.
4.  `Output Format & Constraints`: Strict rules for your *only* output: the `git diff` text.
5.  `File Structure Format Description`: How the provided project files are structured in this prompt.
6.  `File Structure`: The current state of the project's files.

---

## 1. User Task
Remova arquivos inÃºteis do repositÃ³rio. Mantenha todos os arquivos e pastas com dotfiles.

---

## 2. Guiding Principles (Your Senior Developer Logic)

### A. Analysis & Planning (Internal Thought Process - Do NOT output this part):
1.  **Deconstruct Request:** Deeply understand the `User Task` â€“ its explicit requirements, implicit goals, and success criteria.
2.  **Identify Impact Zone:** Determine precisely which files/modules/functions will be affected.
3.  **Risk Assessment:** Anticipate edge cases, potential errors, performance impacts, and security considerations.
4.  **Assume with Reason:** If ambiguities exist in `User Task`, make well-founded assumptions based on best practices and existing code context. Document these assumptions internally if complex.
5.  **Optimal Solution Path:** Briefly evaluate alternative solutions, selecting the one that best balances simplicity, maintainability, readability, and consistency with existing project patterns.
6.  **Plan Changes:** Before generating diffs, mentally (or internally) outline the specific changes needed for each affected file.

### B. Code Generation & Standards:
*   **Simplicity & Idiomatic Code:** Prioritize the simplest, most direct solution. Write code that is idiomatic for the language and aligns with project conventions (inferred from `File Structure`). Avoid over-engineering.
*   **Respect Existing Architecture:** Strictly follow the established project structure, naming conventions, and coding style.
*   **Type Safety:** Employ type hints/annotations as appropriate for the language.
*   **Modularity:** Design changes to be modular and reusable where sensible.
*   **Documentation:**
    *   Add concise docstrings/comments for new public APIs, complex logic, or non-obvious decisions.
    *   Update existing documentation if changes render it inaccurate.
*   **Logging:** Introduce logging for critical operations or error states if consistent with the project's logging strategy.
*   **No New Dependencies:** Do NOT introduce external libraries/dependencies unless explicitly stated in `User Task` or `User Rules`.
*   **Atomicity of Changes (Hunks):** Each distinct change block (hunk in the diff output) should represent a small, logically coherent modification.
*   **Testability:** Design changes to be testable. If a testing framework is evident in `File Structure` or mentioned in `User Rules`, ensure new code is compatible.

---

## 3. User Rules

*(These are user-provided, project-specific rules or task constraints. They take precedence over `Guiding Principles`.)*

---

## 4. Output Format & Constraints (MANDATORY & STRICT)

Your **ONLY** output will be a single, valid `git diff` formatted text, specifically in the **unified diff format**. No other text, explanations, or apologies are permitted.

### Git Diff Format Structure:
*   If no changes are required, output an empty string.
*   For each modified, newly created, or deleted file, include a diff block. Multiple file diffs are concatenated directly.

### File Diff Block Structure:
A typical diff block for a modified file looks like this:
```diff
diff --git a/relative/path/to/file.ext b/relative/path/to/file.ext
index <hash_old>..<hash_new> <mode>
--- a/relative/path/to/file.ext
+++ b/relative/path/to/file.ext
@@ -START_OLD,LINES_OLD +START_NEW,LINES_NEW @@
 context line (unchanged)
-old line to be removed
+new line to be added
 another context line (unchanged)
```

*   **`diff --git a/path b/path` line:**
    *   Indicates the start of a diff for a specific file.
    *   `a/path/to/file.ext` is the path in the "original" version.
    *   `b/path/to/file.ext` is the path in the "new" version. Paths are project-root-relative, using forward slashes (`/`).
*   **`index <hash_old>..<hash_new> <mode>` line (Optional Detail):**
    *   This line provides metadata about the change. While standard in `git diff`, if generating precise hashes and modes is overly complex for your internal model, you may omit this line or use placeholder values (e.g., `index 0000000..0000000 100644`). The `---`, `+++`, and `@@` lines are the most critical for applying the patch.
*   **`--- a/path/to/file.ext` line:**
    *   Specifies the original file. For **newly created files**, this should be `--- /dev/null`.
*   **`+++ b/path/to/file.ext` line:**
    *   Specifies the new file. For **deleted files**, this should be `+++ /dev/null`. For **newly created files**, this is `+++ b/path/to/new_file.ext`.
*   **Hunk Header (`@@ -START_OLD,LINES_OLD +START_NEW,LINES_NEW @@`):**
    *   `START_OLD,LINES_OLD`: 1-based start line and number of lines from the original file affected by this hunk.
        *   For **newly created files**, this is `0,0`.
        *   For hunks that **only add lines** (no deletions from original), `LINES_OLD` is `0`. (e.g., `@@ -50,0 +51,5 @@` means 5 lines added after original line 50).
    *   `START_NEW,LINES_NEW`: 1-based start line and number of lines in the new file version affected by this hunk.
        *   For **deleted files** (where the entire file is deleted), this is `0,0` for the `+++ /dev/null` part.
        *   For hunks that **only delete lines** (no additions), `LINES_NEW` is `0`. (e.g., `@@ -25,3 +25,0 @@` means 3 lines deleted starting from original line 25).
*   **Hunk Content:**
    *   Lines prefixed with a space (` `) are context lines (unchanged).
    *   Lines prefixed with a minus (`-`) are lines removed from the original file.
    *   Lines prefixed with a plus (`+`) are lines added to the new file.
    *   Include at least 3 lines of unchanged context around changes, where available. If changes are at the very beginning or end of a file, or if hunks are very close, fewer context lines are acceptable as per standard unified diff practice.

### Specific Cases:
*   **Newly Created Files:**
    ```diff
    diff --git a/relative/path/to/new_file.ext b/relative/path/to/new_file.ext
    new file mode 100644
    index 0000000..<hash_new_placeholder>
    --- /dev/null
    +++ b/relative/path/to/new_file.ext
    @@ -0,0 +1,LINES_IN_NEW_FILE @@
    +line 1 of new file
    +line 2 of new file
    ...
    ```
    *(The `new file mode` and `index` lines should be included. Use `100644` for regular files. For the hash in the `index` line, a placeholder like `abcdef0` is acceptable if the actual hash cannot be computed.)*

*   **Deleted Files:**
    ```diff
    diff --git a/relative/path/to/deleted_file.ext b/relative/path/to/deleted_file.ext
    deleted file mode <mode_old_placeholder>
    index <hash_old_placeholder>..0000000
    --- a/relative/path/to/deleted_file.ext
    +++ /dev/null
    @@ -1,LINES_IN_OLD_FILE +0,0 @@
    -line 1 of old file
    -line 2 of old file
    ...
    ```
    *(The `deleted file mode` and `index` lines should be included. Use a placeholder like `100644` for mode and `abcdef0` for hash if actual values are unknown.)*

*   **Untouched Files:** Do NOT include any diff output for files that have no changes.

### General Constraints on Generated Code:
*   **Minimal & Precise Changes:** Generate the smallest, most targeted diff that correctly implements the `User Task` per all rules.
*   **Preserve Integrity:** Do not break existing functionality unless the `User Task` explicitly requires it. The codebase should remain buildable/runnable.
*   **Leverage Existing Code:** Prefer modifying existing files over creating new ones, unless a new file is architecturally justified or required by `User Task` or `User Rules`.

---

## 5. File Structure Format Description
The `File Structure` (provided in the next section) is formatted as follows:
1.  An initial project directory tree structure (e.g., generated by `tree` or similar).
2.  Followed by the content of each file, using an XML-like structure:
    <file path="RELATIVE/PATH/TO/FILE">
    (File content here)
    </file>
    The `path` attribute contains the project-root-relative path, using forward slashes (`/`).
    File content is the raw text of the file. Each file block is separated by a newline.

---

## 6. File Structure
â””â”€â”€ gendocs/
    â”œâ”€â”€ build/
    â”œâ”€â”€ cmd/
    â”‚   â”œâ”€â”€ analyze.go [5.8KB]
    â”‚   â”œâ”€â”€ cache.go [1.3KB]
    â”‚   â”œâ”€â”€ config.go [1.9KB]
    â”‚   â”œâ”€â”€ cronjob.go [3.3KB]
    â”‚   â”œâ”€â”€ generate.go [9.0KB]
    â”‚   â”œâ”€â”€ helpers.go [5.4KB]
    â”‚   â”œâ”€â”€ helpers_test.go [14.3KB]
    â”‚   â””â”€â”€ root.go [1.7KB]
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ ARCHITECTURE.md [20.2KB]
    â”‚   â”œâ”€â”€ CONTRIBUTING.md [12.2KB]
    â”‚   â”œâ”€â”€ EXPORT.md [12.3KB]
    â”‚   â”œâ”€â”€ JSON_FORMAT.md [15.0KB]
    â”‚   â””â”€â”€ TESTING.md [13.6KB]
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ custom-prompts/
    â”‚   â”‚   â”œâ”€â”€ README.md [4.2KB]
    â”‚   â”‚   â”œâ”€â”€ basic-override.yaml [2.1KB]
    â”‚   â”‚   â”œâ”€â”€ enterprise-docs.yaml [3.9KB]
    â”‚   â”‚   â””â”€â”€ microservices.yaml [4.1KB]
    â”‚   â””â”€â”€ json-export/
    â”‚       â”œâ”€â”€ README.md [6.5KB]
    â”‚       â”œâ”€â”€ complete-example.json [15.7KB]
    â”‚       â””â”€â”€ complete-example.md [3.2KB]
    â”œâ”€â”€ internal/
    â”‚   â”œâ”€â”€ agents/
    â”‚   â”‚   â”œâ”€â”€ sub_agents/
    â”‚   â”‚   â”œâ”€â”€ ai_rules_generator.go [2.9KB]
    â”‚   â”‚   â”œâ”€â”€ analyzer.go [11.5KB]
    â”‚   â”‚   â”œâ”€â”€ analyzer_integration_test.go [10.7KB]
    â”‚   â”‚   â”œâ”€â”€ base.go [8.2KB]
    â”‚   â”‚   â”œâ”€â”€ documenter.go [2.8KB]
    â”‚   â”‚   â”œâ”€â”€ factory.go [3.3KB]
    â”‚   â”‚   â”œâ”€â”€ sub_agents.go [6.4KB]
    â”‚   â”‚   â””â”€â”€ types.go [652B]
    â”‚   â”œâ”€â”€ cache/
    â”‚   â”‚   â”œâ”€â”€ BENCHMARKS.md [4.6KB]
    â”‚   â”‚   â”œâ”€â”€ cache.go [25.9KB]
    â”‚   â”‚   â”œâ”€â”€ cache_bench_test.go [11.6KB]
    â”‚   â”‚   â”œâ”€â”€ cache_integration_test.go [18.5KB]
    â”‚   â”‚   â””â”€â”€ cache_test.go [14.2KB]
    â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”œâ”€â”€ loader.go [11.9KB]
    â”‚   â”‚   â”œâ”€â”€ loader_test.go [17.0KB]
    â”‚   â”‚   â””â”€â”€ models.go [6.3KB]
    â”‚   â”œâ”€â”€ errors/
    â”‚   â”‚   â”œâ”€â”€ agent.go [3.5KB]
    â”‚   â”‚   â”œâ”€â”€ base.go [1.6KB]
    â”‚   â”‚   â”œâ”€â”€ config.go [4.2KB]
    â”‚   â”‚   â”œâ”€â”€ context.go [1.6KB]
    â”‚   â”‚   â”œâ”€â”€ exit_codes.go [400B]
    â”‚   â”‚   â”œâ”€â”€ gitlab.go [2.7KB]
    â”‚   â”‚   â”œâ”€â”€ handler.go [2.4KB]
    â”‚   â”‚   â””â”€â”€ validation.go [2.7KB]
    â”‚   â”œâ”€â”€ export/
    â”‚   â”‚   â”œâ”€â”€ html.go [7.0KB]
    â”‚   â”‚   â”œâ”€â”€ html_test.go [9.0KB]
    â”‚   â”‚   â”œâ”€â”€ json.go [23.2KB]
    â”‚   â”‚   â””â”€â”€ json_test.go [59.6KB]
    â”‚   â”œâ”€â”€ gitlab/
    â”‚   â”‚   â””â”€â”€ client.go [4.7KB]
    â”‚   â”œâ”€â”€ handlers/
    â”‚   â”‚   â”œâ”€â”€ ai_rules.go [1.8KB]
    â”‚   â”‚   â”œâ”€â”€ analyze.go [2.4KB]
    â”‚   â”‚   â”œâ”€â”€ base.go [619B]
    â”‚   â”‚   â”œâ”€â”€ cronjob.go [6.5KB]
    â”‚   â”‚   â””â”€â”€ readme.go [1.8KB]
    â”‚   â”œâ”€â”€ llm/
    â”‚   â”‚   â”œâ”€â”€ anthropic.go [10.1KB]
    â”‚   â”‚   â”œâ”€â”€ anthropic_bench_test.go [11.2KB]
    â”‚   â”‚   â”œâ”€â”€ anthropic_test.go [22.2KB]
    â”‚   â”‚   â”œâ”€â”€ cached_client.go [5.5KB]
    â”‚   â”‚   â”œâ”€â”€ cached_client_test.go [24.5KB]
    â”‚   â”‚   â”œâ”€â”€ client.go [1.5KB]
    â”‚   â”‚   â”œâ”€â”€ example_test.go [8.6KB]
    â”‚   â”‚   â”œâ”€â”€ factory.go [1.7KB]
    â”‚   â”‚   â”œâ”€â”€ gemini.go [12.3KB]
    â”‚   â”‚   â”œâ”€â”€ gemini_bench_test.go [11.1KB]
    â”‚   â”‚   â”œâ”€â”€ gemini_test.go [20.1KB]
    â”‚   â”‚   â”œâ”€â”€ openai.go [9.9KB]
    â”‚   â”‚   â”œâ”€â”€ openai_bench_test.go [12.2KB]
    â”‚   â”‚   â”œâ”€â”€ openai_test.go [20.8KB]
    â”‚   â”‚   â”œâ”€â”€ retry_client.go [11.7KB]
    â”‚   â”‚   â”œâ”€â”€ retry_client_bench_test.go [10.6KB]
    â”‚   â”‚   â”œâ”€â”€ retry_client_test.go [14.6KB]
    â”‚   â”‚   â”œâ”€â”€ sse_parser.go [3.3KB]
    â”‚   â”‚   â”œâ”€â”€ sse_parser_test.go [10.9KB]
    â”‚   â”‚   â””â”€â”€ streaming_integration_test.go [23.9KB]
    â”‚   â”œâ”€â”€ llmcache/
    â”‚   â”‚   â”œâ”€â”€ cache.go [25.2KB]
    â”‚   â”‚   â”œâ”€â”€ cache_test.go [17.0KB]
    â”‚   â”‚   â”œâ”€â”€ entry.go [4.4KB]
    â”‚   â”‚   â”œâ”€â”€ key.go [4.4KB]
    â”‚   â”‚   â”œâ”€â”€ key_test.go [15.5KB]
    â”‚   â”‚   â””â”€â”€ persistence_test.go [25.7KB]
    â”‚   â”œâ”€â”€ llmtypes/
    â”‚   â”‚   â””â”€â”€ types.go [1.3KB]
    â”‚   â”œâ”€â”€ logging/
    â”‚   â”‚   â””â”€â”€ logger.go [4.1KB]
    â”‚   â”œâ”€â”€ prompts/
    â”‚   â”‚   â”œâ”€â”€ integration_test.go [4.3KB]
    â”‚   â”‚   â”œâ”€â”€ manager.go [7.2KB]
    â”‚   â”‚   â”œâ”€â”€ manager_test.go [9.0KB]
    â”‚   â”‚   â””â”€â”€ manager_test_override.go [8.6KB]
    â”‚   â”œâ”€â”€ testing/
    â”‚   â”‚   â”œâ”€â”€ fixtures.go [5.3KB]
    â”‚   â”‚   â””â”€â”€ helpers.go [4.9KB]
    â”‚   â”œâ”€â”€ tools/
    â”‚   â”‚   â”œâ”€â”€ base.go [1.7KB]
    â”‚   â”‚   â”œâ”€â”€ file_read.go [5.2KB]
    â”‚   â”‚   â”œâ”€â”€ file_read_test.go [7.8KB]
    â”‚   â”‚   â”œâ”€â”€ list_files.go [3.2KB]
    â”‚   â”‚   â”œâ”€â”€ list_files_test.go [9.4KB]
    â”‚   â”‚   â””â”€â”€ utils.go [7.9KB]
    â”‚   â”œâ”€â”€ tui/
    â”‚   â”‚   â”œâ”€â”€ config.go [8.1KB]
    â”‚   â”‚   â”œâ”€â”€ progress.go [8.7KB]
    â”‚   â”‚   â”œâ”€â”€ progress_test.go [23.1KB]
    â”‚   â”‚   â””â”€â”€ styles.go [3.0KB]
    â”‚   â”œâ”€â”€ validation/
    â”‚   â”‚   â”œâ”€â”€ markdown.go [5.9KB]
    â”‚   â”‚   â””â”€â”€ markdown_test.go [9.4KB]
    â”‚   â””â”€â”€ worker_pool/
    â”‚       â””â”€â”€ pool.go [1.9KB]
    â”œâ”€â”€ pkg/
    â”œâ”€â”€ prompts/
    â”‚   â”œâ”€â”€ ai_rules_generator.yaml [7.4KB]
    â”‚   â”œâ”€â”€ analyzer.yaml [9.0KB]
    â”‚   â””â”€â”€ documenter.yaml [5.1KB]
    â”œâ”€â”€ CLAUDE.md [3.8KB]
    â”œâ”€â”€ INSTALL.md [7.0KB]
    â”œâ”€â”€ Makefile [3.1KB]
    â”œâ”€â”€ README.md [10.5KB]
    â”œâ”€â”€ go.mod [2.2KB]
    â”œâ”€â”€ go.sum [10.8KB]
    â”œâ”€â”€ install.sh [1.5KB]
    â”œâ”€â”€ main.go [83B]
    â””â”€â”€ uninstall.sh [795B]

<file path="cmd/analyze.go">
package cmd

import (
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/handlers"
	"github.com/user/gendocs/internal/llmcache"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/tui"
)

var (
	repoPath         string
	excludeStructure bool
	excludeDataFlow  bool
	excludeDeps      bool
	excludeReqFlow   bool
	excludeAPI       bool
	maxWorkers       int
	forceAnalysis    bool
	showCacheStats   bool
)

// analyzeCmd represents the analyze command
var analyzeCmd = &cobra.Command{
	Use:   "analyze",
	Short: "Analyze codebase structure and dependencies",
	Long: `Analyze the codebase to generate detailed documentation about:
  - Code structure and architecture
  - Dependencies and imports
  - Data flow through the system
  - Request/response flow
  - API endpoints and contracts

Results are written to .ai/docs/ directory.

By default, incremental analysis is used which only re-analyzes files
that have changed since the last run. Use --force to perform a full
re-analysis ignoring the cache.`,
	RunE: runAnalyze,
}

func init() {
	rootCmd.AddCommand(analyzeCmd)

	analyzeCmd.Flags().StringVar(&repoPath, "repo-path", ".", "Path to repository")
	analyzeCmd.Flags().BoolVar(&excludeStructure, "exclude-code-structure", false, "Exclude structure analysis")
	analyzeCmd.Flags().BoolVar(&excludeDataFlow, "exclude-data-flow", false, "Exclude data flow analysis")
	analyzeCmd.Flags().BoolVar(&excludeDeps, "exclude-dependencies", false, "Exclude dependency analysis")
	analyzeCmd.Flags().BoolVar(&excludeReqFlow, "exclude-request-flow", false, "Exclude request flow analysis")
	analyzeCmd.Flags().BoolVar(&excludeAPI, "exclude-api-analysis", false, "Exclude API analysis")
	analyzeCmd.Flags().IntVar(&maxWorkers, "max-workers", 0, "Maximum concurrent workers (0=auto)")
	analyzeCmd.Flags().BoolVarP(&forceAnalysis, "force", "f", false, "Force full re-analysis, ignoring cache")
	analyzeCmd.Flags().BoolVar(&showCacheStats, "show-cache-stats", false, "Show LLM cache statistics after analysis")
}

func runAnalyze(cmd *cobra.Command, args []string) error {
	cliOverrides := map[string]interface{}{
		"repo_path":              repoPath,
		"exclude_code_structure": excludeStructure,
		"exclude_data_flow":      excludeDataFlow,
		"exclude_dependencies":   excludeDeps,
		"exclude_request_flow":   excludeReqFlow,
		"exclude_api_analysis":   excludeAPI,
		"max_workers":            maxWorkers,
		"debug":                  debugFlag,
		"force":                  forceAnalysis,
	}

	cfg, err := config.LoadAnalyzerConfig(repoPath, cliOverrides)
	if err != nil {
		return fmt.Errorf("failed to load configuration: %w", err)
	}

	// Initialize logger using helper
	logger, err := InitLogger(cfg.RepoPath, debugFlag, verboseFlag)
	if err != nil {
		return err
	}
	defer func() { _ = logger.Sync() }()

	showProgress := !verboseFlag

	logger.Info("Starting gendocs analyze",
		logging.String("repo_path", cfg.RepoPath),
		logging.Int("max_workers", cfg.MaxWorkers),
	)

	handler := handlers.NewAnalyzeHandler(*cfg, logger)

	var progress *tui.Progress
	if showProgress {
		progress = tui.NewProgress("Gendocs Analyze")
		handler.SetProgressReporter(progress)
		progress.Start()
	}

	err = handler.Handle(cmd.Context())

	if showProgress {
		progress.Stop()
		progress.PrintSummary()
	}

	if err != nil {
		if docErr, ok := err.(*errors.AIDocGenError); ok {
			if !showProgress {
				fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
			}
			return docErr
		}
		return err
	}

	if !showProgress {
		logger.Info("Analysis complete")
	}

	// Show cache statistics if requested
	if showCacheStats {
		displayCacheStats(repoPath)
	}

	return nil
}

// displayCacheStats loads and displays cache statistics from the disk cache
func displayCacheStats(repoPath string) {
	cachePath := filepath.Join(repoPath, llmcache.DefaultCacheFileName)

	// Check if cache file exists
	fileInfo, err := os.Stat(cachePath)
	if os.IsNotExist(err) {
		fmt.Println("\nğŸ“Š LLM Cache Statistics")
		fmt.Println("   Cache file not found. Run analysis with caching enabled first.")
		fmt.Printf("   Expected location: %s\n\n", cachePath)
		return
	}

	// Get actual file size on disk
	actualFileSize := fileInfo.Size()

	// Read cache file
	data, err := os.ReadFile(cachePath)
	if err != nil {
		fmt.Printf("\nâŒ Failed to read cache file: %v\n\n", err)
		return
	}

	// Parse cache data
	var cacheData llmcache.DiskCacheData
	if err := json.Unmarshal(data, &cacheData); err != nil {
		fmt.Printf("\nâŒ Failed to parse cache file: %v\n\n", err)
		return
	}

	// Display statistics
	fmt.Println("\nğŸ“Š LLM Cache Statistics")
	fmt.Println("======================")
	fmt.Printf("Cache File: %s\n", cachePath)
	fmt.Printf("Version: %d\n", cacheData.Version)
	fmt.Printf("Created: %s\n", cacheData.CreatedAt.Format("2006-01-02 15:04:05"))
	fmt.Printf("Last Updated: %s\n\n", cacheData.UpdatedAt.Format("2006-01-02 15:04:05"))

	stats := cacheData.Stats
	fmt.Println("Entries:")
	fmt.Printf("  Total Entries: %d\n", stats.TotalEntries)
	fmt.Printf("  Expired Entries: %d\n", stats.ExpiredEntries)
	fmt.Printf("  Active Entries: %d\n\n", stats.TotalEntries-stats.ExpiredEntries)

	fmt.Println("Performance:")
	fmt.Printf("  Cache Hits: %d\n", stats.Hits)
	fmt.Printf("  Cache Misses: %d\n", stats.Misses)
	fmt.Printf("  Hit Rate: %.2f%%\n\n", stats.HitRate*100)

	fmt.Println("Disk Usage:")
	fmt.Printf("  Actual File Size: %.2f MB\n", float64(actualFileSize)/(1024*1024))
	fmt.Printf("  Logical Data Size: %.2f MB\n", float64(stats.TotalSizeBytes)/(1024*1024))
	if stats.TotalSizeBytes > 0 {
		efficiency := float64(stats.TotalSizeBytes) / float64(actualFileSize) * 100
		fmt.Printf("  Storage Efficiency: %.1f%% (data size / file size)\n", efficiency)
	}
	fmt.Printf("  Evictions: %d\n\n", stats.Evictions)

	fmt.Println("======================")
}
</file>
<file path="cmd/cache.go">
package cmd

import (
	"fmt"
	"os"
	"path/filepath"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/llmcache"
)

var (
	cacheClearRepoPath string
)

// cacheClearCmd represents the cache-clear command
var cacheClearCmd = &cobra.Command{
	Use:   "cache-clear",
	Short: "Clear the LLM response cache",
	Long: `Clear the LLM response cache by removing all cached entries.

This command removes the disk cache file, forcing all LLM requests to be
executed again on the next run. Use this to:
  - Free up disk space
  - Force fresh LLM responses
  - Reset cache statistics`,
	RunE: runCacheClear,
}

func runCacheClear(cmd *cobra.Command, args []string) error {
	return clearCache(cacheClearRepoPath)
}

// clearCache removes the LLM cache file
func clearCache(repoPath string) error {
	cachePath := filepath.Join(repoPath, llmcache.DefaultCacheFileName)

	// Check if cache file exists
	if _, err := os.Stat(cachePath); os.IsNotExist(err) {
		fmt.Println("â„¹ï¸  Cache file not found.")
		fmt.Printf("   Expected location: %s\n", cachePath)
		fmt.Println("   No action taken.")
		return nil
	}

	// Remove the cache file
	if err := os.Remove(cachePath); err != nil {
		return fmt.Errorf("failed to remove cache file: %w", err)
	}

	fmt.Println("âœ… Cache cleared successfully!")
	fmt.Printf("   Removed: %s\n\n", cachePath)
	return nil
}
</file>
<file path="cmd/config.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/charmbracelet/bubbles/textinput"
	tea "github.com/charmbracelet/bubbletea"
	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/tui"
)

// configCmd represents the config command
var configCmd = &cobra.Command{
	Use:   "config",
	Short: "Configure gendocs settings",
	Long: `Launch an interactive configuration wizard to set up your LLM provider,
API key, and model preferences. Configuration is saved to ~/.gendocs.yaml.`,
	RunE: runConfig,
}

func init() {
	rootCmd.AddCommand(configCmd)
}

func runConfig(cmd *cobra.Command, args []string) error {
	// Initialize text inputs
	apiKeyInput := textinput.New()
	apiKeyInput.Placeholder = "Enter your API key"
	apiKeyInput.EchoMode = textinput.EchoPassword
	apiKeyInput.EchoCharacter = 'â€¢'
	apiKeyInput.Focus()

	modelInput := textinput.New()
	modelInput.Placeholder = "Press Enter for default model"

	baseURLInput := textinput.New()
	baseURLInput.Placeholder = "https://api.example.com (optional)"

	// Initialize Bubble Tea model
	model := tui.Model{
		Step:         0,
		Provider:     "",
		Model:        "",
		BaseURL:      "",
		Quitting:     false,
		APIKeyInput:  apiKeyInput,
		ModelInput:   modelInput,
		BaseURLInput: baseURLInput,
	}

	// Start Bubble Tea program
	p := tea.NewProgram(
		model,
		tea.WithAltScreen(),       // Use full screen mode
		tea.WithMouseCellMotion(), // Enable mouse motion
	)

	finalModel, err := p.Run()
	if err != nil {
		return fmt.Errorf("error running config wizard: %w", err)
	}

	// Type assertion to get our model back
	m, ok := finalModel.(tui.Model)
	if !ok {
		return fmt.Errorf("unexpected model type")
	}

	// Show final status
	if m.Err != nil {
		fmt.Fprintf(os.Stderr, "\nError: %v\n", m.Err)
		return m.Err
	}

	if m.SavedConfig {
		fmt.Printf("\nConfiguration saved to: %s\n", m.GetConfigPath())
		fmt.Println("\nYou can now run:")
		fmt.Println("  gendocs analyze --repo-path .")
	}

	return nil
}
</file>
<file path="cmd/cronjob.go">
package cmd

import (
	"os"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/handlers"
	"github.com/user/gendocs/internal/logging"
)

// cronjobCmd represents the cronjob command
var cronjobCmd = &cobra.Command{
	Use:   "cronjob",
	Short: "Automated batch processing for GitLab projects",
	Long: `Process multiple GitLab projects automatically, analyzing each
and creating merge requests with the results.`,
}

var (
	cronjobMaxDays     int
	cronjobWorkingPath string
	cronjobGroupID     int
)

// cronjobAnalyzeCmd represents the cronjob analyze command
var cronjobAnalyzeCmd = &cobra.Command{
	Use:   "analyze",
	Short: "Analyze all applicable GitLab projects in a group",
	Long: `Fetch all projects in a GitLab group, filter them based on
configuration, and run analysis on each applicable project.
Creates branches and merge requests automatically.`,
	RunE: runCronjobAnalyze,
}

func init() {
	rootCmd.AddCommand(cronjobCmd)
	cronjobCmd.AddCommand(cronjobAnalyzeCmd)

	cronjobAnalyzeCmd.Flags().IntVar(&cronjobMaxDays, "max-days-since-last-commit", 14, "Skip projects with no commits in N days")
	cronjobAnalyzeCmd.Flags().StringVar(&cronjobWorkingPath, "working-path", "./work", "Working directory for cloning repos")
	cronjobAnalyzeCmd.Flags().IntVar(&cronjobGroupID, "group-project-id", 0, "GitLab group/project ID to analyze")
	_ = cronjobAnalyzeCmd.MarkFlagRequired("group-project-id")
}

func runCronjobAnalyze(cmd *cobra.Command, args []string) error {
	// Validate required GitLab configuration
	gitLabToken := os.Getenv("GITLAB_OAUTH_TOKEN")
	if gitLabToken == "" {
		return errors.NewMissingEnvVarError("GITLAB_OAUTH_TOKEN", "GitLab API authentication token")
	}

	gitLabURL := os.Getenv("GITLAB_API_URL")
	if gitLabURL == "" {
		gitLabURL = "https://gitlab.com"
	}

	// Build configurations
	cronjobCfg := config.CronjobConfig{
		MaxDaysSinceLastCommit: cronjobMaxDays,
		WorkingPath:            cronjobWorkingPath,
		GroupProjectID:         cronjobGroupID,
	}

	gitLabCfg := config.GitLabConfig{
		APIURL:       gitLabURL,
		OAuthToken:   gitLabToken,
		UserName:     os.Getenv("GITLAB_USER_NAME"),
		UserUsername: os.Getenv("GITLAB_USER_USERNAME"),
		UserEmail:    os.Getenv("GITLAB_USER_EMAIL"),
	}

	// Set defaults for GitLab user info
	if gitLabCfg.UserName == "" {
		gitLabCfg.UserName = "AI Analyzer"
	}
	if gitLabCfg.UserUsername == "" {
		gitLabCfg.UserUsername = "agent_doc"
	}

	cliOverrides := map[string]interface{}{
		"debug":       debugFlag,
		"max_workers": 0,
	}

	analyzerCfgPtr, err := config.LoadAnalyzerConfig(".", cliOverrides)
	if err != nil {
		return err
	}
	analyzerCfg := *analyzerCfgPtr

	// Initialize logger (verbose=true to enable console output for cronjob)
	logger, err := InitLogger(cronjobWorkingPath, debugFlag, true)
	if err != nil {
		return err
	}
	defer func() { _ = logger.Sync() }()

	logger.Info("Starting cronjob analysis",
		logging.Int("group_id", cronjobGroupID),
		logging.Int("max_days", cronjobMaxDays),
	)

	// Create and run CronjobHandler
	handler := handlers.NewCronjobHandler(cronjobCfg, gitLabCfg, analyzerCfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		return HandleCommandError(err, nil, false)
	}

	logger.Info("Cronjob analysis complete")
	return nil
}
</file>
<file path="cmd/generate.go">
package cmd

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/export"
	"github.com/user/gendocs/internal/handlers"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/tui"
)

// generateCmd represents the generate command
var generateCmd = &cobra.Command{
	Use:   "generate",
	Short: "Generate documentation from analysis results",
	Long:  `Generate documentation files (README.md, AI rules) from existing analysis results.`,
}

var (
	readmeRepoPath string
	autoExportHTML bool
)

// readmeCmd represents the generate readme command
var readmeCmd = &cobra.Command{
	Use:   "readme",
	Short: "Generate README.md from analysis results",
	Long: `Generate a comprehensive README.md file based on existing analysis documents
in .ai/docs/. This synthesizes information from structure, dependency, data flow,
request flow, and API analyses into a user-friendly README.`,
	RunE: runReadme,
}

func init() {
	rootCmd.AddCommand(generateCmd)
	generateCmd.AddCommand(readmeCmd)

	readmeCmd.Flags().StringVar(&readmeRepoPath, "repo-path", ".", "Path to repository")
	readmeCmd.Flags().BoolVar(&autoExportHTML, "export-html", false, "Also export to HTML after generation")
}

func runReadme(cmd *cobra.Command, args []string) error {
	cliOverrides := map[string]interface{}{
		"repo_path": readmeRepoPath,
		"debug":     debugFlag,
	}

	cfg, err := config.LoadDocumenterConfig(readmeRepoPath, cliOverrides)
	if err != nil {
		return fmt.Errorf("failed to load configuration: %w", err)
	}

	// Initialize logger using helper
	logger, err := InitLogger(readmeRepoPath, debugFlag, verboseFlag)
	if err != nil {
		return err
	}
	defer func() { _ = logger.Sync() }()

	showProgress := !verboseFlag
	logger.Info("Starting README generation",
		logging.String("repo_path", readmeRepoPath),
	)

	var progress *tui.SimpleProgress
	if showProgress {
		progress = tui.NewSimpleProgress("Gendocs Generate README")
		progress.Start()
		progress.Step("Loading analysis documents...")
		progress.Info(fmt.Sprintf("Repository: %s", readmeRepoPath))
		progress.Step("Generating README.md...")
	}

	handler := handlers.NewReadmeHandler(*cfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		return HandleCommandError(err, progress, showProgress)
	}

	if showProgress {
		progress.Success("README.md generated successfully")
	} else {
		logger.Info("README.md generation complete")
	}

	if autoExportHTML {
		readmePath := filepath.Join(readmeRepoPath, "README.md")
		htmlPath := filepath.Join(readmeRepoPath, "README.html")

		if showProgress {
			progress.Step("Exporting to HTML...")
		} else {
			fmt.Println("\nExporting to HTML...")
		}

		if err := exportToHTML(readmePath, htmlPath); err != nil {
			if showProgress {
				progress.Warning(fmt.Sprintf("HTML export failed: %v", err))
			} else {
				fmt.Fprintf(os.Stderr, "Warning: HTML export failed: %v\n", err)
			}
		} else if showProgress {
			progress.Success(fmt.Sprintf("Exported to %s", htmlPath))
		}
	}

	if showProgress {
		progress.Done()
	}

	return nil
}

// aiRulesCmd represents the generate ai-rules command
var aiRulesCmd = &cobra.Command{
	Use:   "ai-rules",
	Short: "Generate AI assistant configuration files",
	Long: `Generate AI assistant configuration files (CLAUDE.md, AGENTS.md, .cursor/rules/)
from existing analysis results. These files help AI coding assistants understand the project.`,
	RunE: runAIRules,
}

func init() {
	generateCmd.AddCommand(aiRulesCmd)
	aiRulesCmd.Flags().StringVar(&readmeRepoPath, "repo-path", ".", "Path to repository")
}

func runAIRules(cmd *cobra.Command, args []string) error {
	cliOverrides := map[string]interface{}{
		"repo_path": readmeRepoPath,
		"debug":     debugFlag,
	}

	cfg, err := config.LoadAIRulesConfig(readmeRepoPath, cliOverrides)
	if err != nil {
		return fmt.Errorf("failed to load configuration: %w", err)
	}

	// Initialize logger using helper
	logger, err := InitLogger(readmeRepoPath, debugFlag, verboseFlag)
	if err != nil {
		return err
	}
	defer func() { _ = logger.Sync() }()

	showProgress := !verboseFlag
	logger.Info("Starting AI rules generation",
		logging.String("repo_path", readmeRepoPath),
	)

	var progress *tui.SimpleProgress
	if showProgress {
		progress = tui.NewSimpleProgress("Gendocs Generate AI Rules")
		progress.Start()
		progress.Step("Loading analysis documents...")
		progress.Info(fmt.Sprintf("Repository: %s", readmeRepoPath))
		progress.Step("Generating CLAUDE.md...")
	}

	handler := handlers.NewAIRulesHandler(*cfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		return HandleCommandError(err, progress, showProgress)
	}

	if showProgress {
		progress.Success("CLAUDE.md generated successfully")
		progress.Done()
	} else {
		logger.Info("AI rules generation complete")
	}

	return nil
}

// exportCmd represents the generate export command
var exportCmd = &cobra.Command{
	Use:   "export",
	Short: "Export documentation to different formats",
	Long: `Export generated documentation to formats like HTML or JSON for easier sharing.

Supported formats:
  - html: Standalone HTML file with embedded CSS and syntax highlighting
  - json: Structured JSON with metadata and hierarchical content

Examples:
  # Export README.md to HTML
  gendocs generate export --repo-path . --format html --output docs.html

  # Export README.md to JSON
  gendocs generate export --repo-path . --format json --output docs.json

  # Export specific file
  gendocs generate export --repo-path . --input .ai/docs/code_structure.md --format html

  # Export with default output (README.md â†’ README.html or README.json)
  gendocs generate export --repo-path .
`,
	RunE: runExport,
}

var (
	exportFormat string
	exportOutput string
	exportInput  string
)

func init() {
	generateCmd.AddCommand(exportCmd)

	exportCmd.Flags().StringVar(&exportFormat, "format", "html", "Export format (html, json)")
	exportCmd.Flags().StringVar(&exportOutput, "output", "", "Output file path (default: input.html or input.json)")
	exportCmd.Flags().StringVar(&readmeRepoPath, "repo-path", ".", "Path to repository")
	exportCmd.Flags().StringVar(&exportInput, "input", "README.md", "Input markdown file")
}

func runExport(cmd *cobra.Command, args []string) error {
	inputPath := exportInput
	if !filepath.IsAbs(inputPath) {
		inputPath = filepath.Join(readmeRepoPath, inputPath)
	}

	if _, err := os.Stat(inputPath); err != nil {
		return fmt.Errorf("input file not found: %s", inputPath)
	}

	outputPath := exportOutput
	if outputPath == "" {
		ext := filepath.Ext(inputPath)
		baseName := strings.TrimSuffix(inputPath, ext)
		switch exportFormat {
		case "json":
			outputPath = baseName + ".json"
		case "html":
			outputPath = baseName + ".html"
		default:
			outputPath = baseName + ".html"
		}
	}

	if !filepath.IsAbs(outputPath) {
		outputPath = filepath.Join(readmeRepoPath, outputPath)
	}

	showProgress := !verboseFlag

	switch exportFormat {
	case "html":
		return exportToHTMLWithProgress(inputPath, outputPath, showProgress)
	case "json":
		return exportToJSONWithProgress(inputPath, outputPath, showProgress)
	default:
		return fmt.Errorf("unsupported format: %s (supported: html, json)", exportFormat)
	}
}

func exportToHTML(inputPath, outputPath string) error {
	return exportToHTMLWithProgress(inputPath, outputPath, false)
}

func exportToHTMLWithProgress(inputPath, outputPath string, showProgress bool) error {
	var progress *tui.SimpleProgress
	if showProgress {
		progress = tui.NewSimpleProgress("Gendocs Export")
		progress.Start()
		progress.Step(fmt.Sprintf("Exporting %s...", filepath.Base(inputPath)))
	} else {
		fmt.Printf("Exporting %s to %s...\n", inputPath, outputPath)
	}

	exporter, err := export.NewHTMLExporter()
	if err != nil {
		if showProgress {
			progress.Failed(err)
		}
		return fmt.Errorf("failed to create exporter: %w", err)
	}

	if err := exporter.ExportToHTML(inputPath, outputPath); err != nil {
		if showProgress {
			progress.Failed(err)
		}
		return fmt.Errorf("export failed: %w", err)
	}

	if showProgress {
		progress.Success(fmt.Sprintf("Exported to %s", outputPath))
		progress.Done()
	} else {
		fmt.Printf("âœ“ HTML exported to %s\n", outputPath)
	}

	return nil
}

func exportToJSONWithProgress(inputPath, outputPath string, showProgress bool) error {
	var progress *tui.SimpleProgress
	if showProgress {
		progress = tui.NewSimpleProgress("Gendocs Export")
		progress.Start()
		progress.Step(fmt.Sprintf("Exporting %s...", filepath.Base(inputPath)))
	} else {
		fmt.Printf("Exporting %s to %s...\n", inputPath, outputPath)
	}

	exporter, err := export.NewJSONExporter()
	if err != nil {
		if showProgress {
			progress.Failed(err)
		}
		return fmt.Errorf("failed to create exporter: %w", err)
	}

	if err := exporter.ExportToJSON(inputPath, outputPath); err != nil {
		if showProgress {
			progress.Failed(err)
		}
		return fmt.Errorf("export failed: %w", err)
	}

	if showProgress {
		progress.Success(fmt.Sprintf("Exported to %s", outputPath))
		progress.Done()
	} else {
		fmt.Printf("âœ“ JSON exported to %s\n", outputPath)
	}

	return nil
}
</file>
<file path="cmd/helpers.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/tui"
)

// CommandContext holds common resources used by CLI commands.
// It is the return value of initialization helpers and centralizes
// the logging, progress display, and repository path configuration.
type CommandContext struct {
	// Logger is the configured logger for the command
	Logger *logging.Logger

	// ShowProgress indicates whether to show progress UI (true) or verbose output (false)
	ShowProgress bool

	// RepoPath is the path to the repository being processed
	RepoPath string
}

// InitLogger creates a configured logger for CLI commands.
// It encapsulates the common logger initialization pattern:
//   - Calculates logDir based on repoPath (.ai/logs or repoPath/.ai/logs)
//   - Determines showProgress from verbose flag (showProgress = !verbose)
//   - Creates Config with appropriate settings
//   - Creates and returns the logger
//
// Parameters:
//   - repoPath: path to the repository being processed ("." for current directory)
//   - debug: enables caller information in logs
//   - verbose: when true, console output is enabled and showProgress is false
//
// Returns the configured logger and any error during initialization.
// The caller is responsible for calling logger.Sync() when done.
func InitLogger(repoPath string, debug bool, verbose bool) (*logging.Logger, error) {
	// Calculate log directory based on repo path
	logDir := ".ai/logs"
	if repoPath != "." {
		logDir = repoPath + "/.ai/logs"
	}

	// Determine showProgress from verbose flag
	// When verbose is true, we show console output instead of progress UI
	showProgress := !verbose

	// Create logger configuration
	logCfg := &logging.Config{
		LogDir:         logDir,
		FileLevel:      logging.LevelFromString("info"),
		ConsoleLevel:   logging.LevelFromString("debug"),
		EnableCaller:   debug,
		ConsoleEnabled: !showProgress,
	}

	// Create and return logger
	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		return nil, fmt.Errorf("failed to initialize logger: %w", err)
	}

	return logger, nil
}

// LLMDefaults holds default values for LLM configuration.
// These values are used when environment variables are not set.
type LLMDefaults struct {
	// Retries is the number of retry attempts for LLM API calls (default: 2)
	Retries int

	// Timeout is the timeout in seconds for LLM API calls (default: 180)
	Timeout int

	// MaxTokens is the maximum number of tokens for LLM responses (default: 8192)
	MaxTokens int

	// Temperature controls randomness in LLM responses (default: 0.0)
	Temperature float64
}

// LLMConfigFromEnv creates an LLMConfig by loading values from environment variables.
// It follows this precedence:
//  1. Prefixed environment variables (e.g., DOCUMENTER_LLM_PROVIDER, AI_RULES_LLM_MODEL)
//  2. ANALYZER_* fallback variables for Provider, Model, and APIKey
//  3. Provided default values for Retries, Timeout, MaxTokens, Temperature
//
// The prefix parameter should be the command-specific prefix (e.g., "DOCUMENTER", "AI_RULES").
// Environment variable pattern: {PREFIX}_LLM_{FIELD} where FIELD is PROVIDER, MODEL, API_KEY, or BASE_URL.
//
// Example usage:
//
//	cfg := LLMConfigFromEnv("DOCUMENTER", LLMDefaults{
//	    Retries:     2,
//	    Timeout:     180,
//	    MaxTokens:   8192,
//	    Temperature: 0.0,
//	})
func LLMConfigFromEnv(prefix string, defaults LLMDefaults) config.LLMConfig {
	cfg := config.LLMConfig{
		Provider:    os.Getenv(prefix + "_LLM_PROVIDER"),
		Model:       os.Getenv(prefix + "_LLM_MODEL"),
		APIKey:      os.Getenv(prefix + "_LLM_API_KEY"),
		BaseURL:     os.Getenv(prefix + "_LLM_BASE_URL"),
		Retries:     defaults.Retries,
		Timeout:     defaults.Timeout,
		MaxTokens:   defaults.MaxTokens,
		Temperature: defaults.Temperature,
	}

	// Fall back to ANALYZER_* environment variables for Provider, Model, and APIKey
	if cfg.Provider == "" {
		cfg.Provider = os.Getenv("ANALYZER_LLM_PROVIDER")
	}
	if cfg.Model == "" {
		cfg.Model = os.Getenv("ANALYZER_LLM_MODEL")
	}
	if cfg.APIKey == "" {
		cfg.APIKey = os.Getenv("ANALYZER_LLM_API_KEY")
	}

	return cfg
}

// HandleCommandError encapsulates the duplicated error handling pattern in CLI commands.
// It handles errors by:
//  1. Checking if the error is an *errors.AIDocGenError for user-friendly messages
//  2. Displaying the error via progress UI (if showProgress is true) or stderr
//  3. Returning the original error for proper exit code handling
//
// Parameters:
//   - err: the error to handle (if nil, returns nil immediately)
//   - progress: optional SimpleProgress for displaying errors in progress UI
//   - showProgress: if true, uses progress UI; if false, writes to stderr
//
// Returns the original error unchanged (allows chaining: return HandleCommandError(...))
func HandleCommandError(err error, progress *tui.SimpleProgress, showProgress bool) error {
	if err == nil {
		return nil
	}

	// Check if it's an AIDocGenError for better user messaging
	if docErr, ok := err.(*errors.AIDocGenError); ok {
		if showProgress && progress != nil {
			progress.Error(docErr.GetUserMessage())
			progress.Failed(nil)
		} else {
			fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
		}
		return docErr
	}

	// For other errors, show via progress or let it propagate
	if showProgress && progress != nil {
		progress.Failed(err)
	}
	return err
}
</file>
<file path="cmd/helpers_test.go">
package cmd

import (
	"errors"
	"os"
	"path/filepath"
	"testing"

	appErrors "github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/tui"
)

// TestInitLogger_CurrentDirectory tests logger initialization with current directory
func TestInitLogger_CurrentDirectory(t *testing.T) {
	// Create a temp directory to avoid polluting the actual repo
	tmpDir := t.TempDir()
	originalDir, _ := os.Getwd()
	_ = os.Chdir(tmpDir)
	defer func() { _ = os.Chdir(originalDir) }()

	logger, err := InitLogger(".", false, false)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}
	defer func() { _ = logger.Sync() }()

	// Verify log directory was created at .ai/logs
	if _, err := os.Stat(filepath.Join(tmpDir, ".ai", "logs")); os.IsNotExist(err) {
		t.Error("Expected .ai/logs directory to be created")
	}
}

// TestInitLogger_CustomRepoPath tests logger initialization with a custom repo path
func TestInitLogger_CustomRepoPath(t *testing.T) {
	tmpDir := t.TempDir()
	repoPath := filepath.Join(tmpDir, "my-repo")
	_ = os.MkdirAll(repoPath, 0755)

	logger, err := InitLogger(repoPath, false, false)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}
	defer func() { _ = logger.Sync() }()

	// Verify log directory was created at repoPath/.ai/logs
	logDir := filepath.Join(repoPath, ".ai", "logs")
	if _, err := os.Stat(logDir); os.IsNotExist(err) {
		t.Errorf("Expected %s directory to be created", logDir)
	}
}

// TestInitLogger_DebugFlag tests that debug flag enables caller info
func TestInitLogger_DebugFlag(t *testing.T) {
	tmpDir := t.TempDir()
	originalDir, _ := os.Getwd()
	_ = os.Chdir(tmpDir)
	defer func() { _ = os.Chdir(originalDir) }()

	// With debug = true
	logger, err := InitLogger(".", true, false)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}
	defer func() { _ = logger.Sync() }()

	// Logger should be created successfully with debug enabled
	// The actual caller info is internal to the logger; we just verify creation
	if logger == nil {
		t.Error("Expected logger to be created with debug flag")
	}
}

// TestInitLogger_VerboseFlag tests that verbose flag affects console output
func TestInitLogger_VerboseFlag(t *testing.T) {
	tmpDir := t.TempDir()
	originalDir, _ := os.Getwd()
	_ = os.Chdir(tmpDir)
	defer func() { _ = os.Chdir(originalDir) }()

	// With verbose = true (showProgress = false, console enabled)
	logger, err := InitLogger(".", false, true)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}
	defer func() { _ = logger.Sync() }()

	if logger == nil {
		t.Error("Expected logger to be created with verbose flag")
	}
}

// TestInitLogger_AllFlagCombinations tests various combinations of debug and verbose
func TestInitLogger_AllFlagCombinations(t *testing.T) {
	testCases := []struct {
		name    string
		debug   bool
		verbose bool
	}{
		{"debug=false,verbose=false", false, false},
		{"debug=false,verbose=true", false, true},
		{"debug=true,verbose=false", true, false},
		{"debug=true,verbose=true", true, true},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			tmpDir := t.TempDir()
			originalDir, _ := os.Getwd()
			_ = os.Chdir(tmpDir)
			defer func() { _ = os.Chdir(originalDir) }()

			logger, err := InitLogger(".", tc.debug, tc.verbose)
			if err != nil {
				t.Fatalf("Expected no error for %s, got %v", tc.name, err)
			}
			defer func() { _ = logger.Sync() }()

			if logger == nil {
				t.Errorf("Expected logger to be created for %s", tc.name)
			}
		})
	}
}

// TestLLMConfigFromEnv_PrefixedVariables tests loading config from prefixed env vars
func TestLLMConfigFromEnv_PrefixedVariables(t *testing.T) {
	// Save and clear environment
	os.Clearenv()
	defer os.Clearenv()

	// Set prefixed env vars
	_ = os.Setenv("DOCUMENTER_LLM_PROVIDER", "openai")
	_ = os.Setenv("DOCUMENTER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("DOCUMENTER_LLM_API_KEY", "doc-api-key")
	_ = os.Setenv("DOCUMENTER_LLM_BASE_URL", "https://custom.openai.com")

	defaults := LLMDefaults{
		Retries:     3,
		Timeout:     120,
		MaxTokens:   4096,
		Temperature: 0.5,
	}

	cfg := LLMConfigFromEnv("DOCUMENTER", defaults)

	if cfg.Provider != "openai" {
		t.Errorf("Expected provider 'openai', got '%s'", cfg.Provider)
	}
	if cfg.Model != "gpt-4" {
		t.Errorf("Expected model 'gpt-4', got '%s'", cfg.Model)
	}
	if cfg.APIKey != "doc-api-key" {
		t.Errorf("Expected API key 'doc-api-key', got '%s'", cfg.APIKey)
	}
	if cfg.BaseURL != "https://custom.openai.com" {
		t.Errorf("Expected base URL 'https://custom.openai.com', got '%s'", cfg.BaseURL)
	}
	if cfg.Retries != 3 {
		t.Errorf("Expected retries 3, got %d", cfg.Retries)
	}
	if cfg.Timeout != 120 {
		t.Errorf("Expected timeout 120, got %d", cfg.Timeout)
	}
	if cfg.MaxTokens != 4096 {
		t.Errorf("Expected max tokens 4096, got %d", cfg.MaxTokens)
	}
	if cfg.Temperature != 0.5 {
		t.Errorf("Expected temperature 0.5, got %f", cfg.Temperature)
	}
}

// TestLLMConfigFromEnv_FallbackToAnalyzer tests fallback to ANALYZER_* vars
func TestLLMConfigFromEnv_FallbackToAnalyzer(t *testing.T) {
	os.Clearenv()
	defer os.Clearenv()

	// Only set ANALYZER_* fallback vars (no DOCUMENTER_* vars)
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "anthropic")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "claude-3")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "analyzer-key")

	defaults := LLMDefaults{
		Retries:     2,
		Timeout:     180,
		MaxTokens:   8192,
		Temperature: 0.0,
	}

	cfg := LLMConfigFromEnv("DOCUMENTER", defaults)

	// Should fall back to ANALYZER_* values
	if cfg.Provider != "anthropic" {
		t.Errorf("Expected fallback provider 'anthropic', got '%s'", cfg.Provider)
	}
	if cfg.Model != "claude-3" {
		t.Errorf("Expected fallback model 'claude-3', got '%s'", cfg.Model)
	}
	if cfg.APIKey != "analyzer-key" {
		t.Errorf("Expected fallback API key 'analyzer-key', got '%s'", cfg.APIKey)
	}
}

// TestLLMConfigFromEnv_PrefixOverridesAnalyzer tests that prefixed vars override ANALYZER_*
func TestLLMConfigFromEnv_PrefixOverridesAnalyzer(t *testing.T) {
	os.Clearenv()
	defer os.Clearenv()

	// Set both prefixed and ANALYZER_* vars
	_ = os.Setenv("AI_RULES_LLM_PROVIDER", "gemini")
	_ = os.Setenv("AI_RULES_LLM_MODEL", "gemini-pro")
	_ = os.Setenv("AI_RULES_LLM_API_KEY", "ai-rules-key")
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "analyzer-key")

	defaults := LLMDefaults{}

	cfg := LLMConfigFromEnv("AI_RULES", defaults)

	// Prefixed vars should win
	if cfg.Provider != "gemini" {
		t.Errorf("Expected provider 'gemini', got '%s'", cfg.Provider)
	}
	if cfg.Model != "gemini-pro" {
		t.Errorf("Expected model 'gemini-pro', got '%s'", cfg.Model)
	}
	if cfg.APIKey != "ai-rules-key" {
		t.Errorf("Expected API key 'ai-rules-key', got '%s'", cfg.APIKey)
	}
}

// TestLLMConfigFromEnv_NoEnvVars tests behavior with no env vars set
func TestLLMConfigFromEnv_NoEnvVars(t *testing.T) {
	os.Clearenv()
	defer os.Clearenv()

	defaults := LLMDefaults{
		Retries:     5,
		Timeout:     300,
		MaxTokens:   16384,
		Temperature: 0.7,
	}

	cfg := LLMConfigFromEnv("TEST", defaults)

	// Should have empty strings for env-loaded values
	if cfg.Provider != "" {
		t.Errorf("Expected empty provider, got '%s'", cfg.Provider)
	}
	if cfg.Model != "" {
		t.Errorf("Expected empty model, got '%s'", cfg.Model)
	}
	if cfg.APIKey != "" {
		t.Errorf("Expected empty API key, got '%s'", cfg.APIKey)
	}
	if cfg.BaseURL != "" {
		t.Errorf("Expected empty base URL, got '%s'", cfg.BaseURL)
	}

	// Defaults should be applied
	if cfg.Retries != 5 {
		t.Errorf("Expected retries 5, got %d", cfg.Retries)
	}
	if cfg.Timeout != 300 {
		t.Errorf("Expected timeout 300, got %d", cfg.Timeout)
	}
	if cfg.MaxTokens != 16384 {
		t.Errorf("Expected max tokens 16384, got %d", cfg.MaxTokens)
	}
	if cfg.Temperature != 0.7 {
		t.Errorf("Expected temperature 0.7, got %f", cfg.Temperature)
	}
}

// TestLLMConfigFromEnv_PartialFallback tests partial fallback (some vars from prefix, some from ANALYZER_*)
func TestLLMConfigFromEnv_PartialFallback(t *testing.T) {
	os.Clearenv()
	defer os.Clearenv()

	// Set only provider from prefix, rest from ANALYZER_*
	_ = os.Setenv("DOCUMENTER_LLM_PROVIDER", "gemini")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "analyzer-key")

	defaults := LLMDefaults{}

	cfg := LLMConfigFromEnv("DOCUMENTER", defaults)

	if cfg.Provider != "gemini" {
		t.Errorf("Expected provider 'gemini' from prefix, got '%s'", cfg.Provider)
	}
	if cfg.Model != "gpt-4" {
		t.Errorf("Expected model 'gpt-4' from fallback, got '%s'", cfg.Model)
	}
	if cfg.APIKey != "analyzer-key" {
		t.Errorf("Expected API key 'analyzer-key' from fallback, got '%s'", cfg.APIKey)
	}
}

// TestLLMConfigFromEnv_DifferentPrefixes tests various prefix values
func TestLLMConfigFromEnv_DifferentPrefixes(t *testing.T) {
	testCases := []struct {
		prefix   string
		envKey   string
		expected string
	}{
		{"DOCUMENTER", "DOCUMENTER_LLM_PROVIDER", "provider1"},
		{"AI_RULES", "AI_RULES_LLM_PROVIDER", "provider2"},
		{"ANALYZER", "ANALYZER_LLM_PROVIDER", "provider3"},
		{"CUSTOM", "CUSTOM_LLM_PROVIDER", "provider4"},
	}

	for _, tc := range testCases {
		t.Run(tc.prefix, func(t *testing.T) {
			os.Clearenv()
			_ = os.Setenv(tc.envKey, tc.expected)

			cfg := LLMConfigFromEnv(tc.prefix, LLMDefaults{})

			if cfg.Provider != tc.expected {
				t.Errorf("Expected provider '%s', got '%s'", tc.expected, cfg.Provider)
			}
		})
	}
}

// TestHandleCommandError_NilError tests that nil error returns nil
func TestHandleCommandError_NilError(t *testing.T) {
	result := HandleCommandError(nil, nil, false)
	if result != nil {
		t.Errorf("Expected nil, got %v", result)
	}
}

// TestHandleCommandError_NilErrorWithProgress tests nil error with progress
func TestHandleCommandError_NilErrorWithProgress(t *testing.T) {
	progress := tui.NewSimpleProgress("Test")
	result := HandleCommandError(nil, progress, true)
	if result != nil {
		t.Errorf("Expected nil, got %v", result)
	}
}

// TestHandleCommandError_AIDocGenError_NoProgress tests AIDocGenError without progress UI
func TestHandleCommandError_AIDocGenError_NoProgress(t *testing.T) {
	// Create an AIDocGenError
	docErr := appErrors.NewError("test error message", appErrors.ExitGeneralError)

	// Redirect stderr to capture output
	oldStderr := os.Stderr
	r, w, _ := os.Pipe()
	os.Stderr = w

	result := HandleCommandError(docErr, nil, false)

	_ = w.Close()
	os.Stderr = oldStderr

	// Read captured output
	buf := make([]byte, 1024)
	n, _ := r.Read(buf)
	output := string(buf[:n])

	// Should return the same error
	if result != docErr {
		t.Errorf("Expected same error to be returned, got %v", result)
	}

	// Should print to stderr
	if len(output) == 0 {
		t.Error("Expected error message to be printed to stderr")
	}
}

// TestHandleCommandError_AIDocGenError_WithProgress tests AIDocGenError with progress UI
func TestHandleCommandError_AIDocGenError_WithProgress(t *testing.T) {
	docErr := appErrors.NewError("user-facing error", appErrors.ExitGeneralError)
	progress := tui.NewSimpleProgress("Test")

	// We can't easily capture progress output, but we can verify the function completes
	// without panicking and returns the correct error
	result := HandleCommandError(docErr, progress, true)

	if result != docErr {
		t.Errorf("Expected same error to be returned, got %v", result)
	}
}

// TestHandleCommandError_RegularError_NoProgress tests regular error without progress UI
func TestHandleCommandError_RegularError_NoProgress(t *testing.T) {
	regularErr := errors.New("regular error")

	// With showProgress=false and no progress, error should just be returned
	result := HandleCommandError(regularErr, nil, false)

	if result != regularErr {
		t.Errorf("Expected same error to be returned, got %v", result)
	}
}

// TestHandleCommandError_RegularError_WithProgress tests regular error with progress UI
func TestHandleCommandError_RegularError_WithProgress(t *testing.T) {
	regularErr := errors.New("regular error with progress")
	progress := tui.NewSimpleProgress("Test")

	result := HandleCommandError(regularErr, progress, true)

	if result != regularErr {
		t.Errorf("Expected same error to be returned, got %v", result)
	}
}

// TestHandleCommandError_ShowProgressFalse_WithNilProgress tests showProgress=false with nil progress
func TestHandleCommandError_ShowProgressFalse_WithNilProgress(t *testing.T) {
	regularErr := errors.New("some error")

	// Should not panic even with nil progress when showProgress is false
	result := HandleCommandError(regularErr, nil, false)

	if result != regularErr {
		t.Errorf("Expected same error to be returned, got %v", result)
	}
}

// TestHandleCommandError_ShowProgressTrue_WithNilProgress tests showProgress=true with nil progress
func TestHandleCommandError_ShowProgressTrue_WithNilProgress(t *testing.T) {
	regularErr := errors.New("some error")

	// Should not panic even with nil progress - the function should handle this
	result := HandleCommandError(regularErr, nil, true)

	if result != regularErr {
		t.Errorf("Expected same error to be returned, got %v", result)
	}
}

// TestCommandContext_Fields tests CommandContext struct fields
func TestCommandContext_Fields(t *testing.T) {
	ctx := CommandContext{
		Logger:       nil,
		ShowProgress: true,
		RepoPath:     "/test/path",
	}

	if !ctx.ShowProgress {
		t.Error("Expected ShowProgress to be true")
	}
	if ctx.RepoPath != "/test/path" {
		t.Errorf("Expected RepoPath '/test/path', got '%s'", ctx.RepoPath)
	}
}

// TestLLMDefaults_ZeroValues tests LLMDefaults with zero values
func TestLLMDefaults_ZeroValues(t *testing.T) {
	defaults := LLMDefaults{}

	if defaults.Retries != 0 {
		t.Errorf("Expected Retries 0, got %d", defaults.Retries)
	}
	if defaults.Timeout != 0 {
		t.Errorf("Expected Timeout 0, got %d", defaults.Timeout)
	}
	if defaults.MaxTokens != 0 {
		t.Errorf("Expected MaxTokens 0, got %d", defaults.MaxTokens)
	}
	if defaults.Temperature != 0.0 {
		t.Errorf("Expected Temperature 0.0, got %f", defaults.Temperature)
	}
}

// TestLLMDefaults_TypicalValues tests LLMDefaults with typical values
func TestLLMDefaults_TypicalValues(t *testing.T) {
	defaults := LLMDefaults{
		Retries:     2,
		Timeout:     180,
		MaxTokens:   8192,
		Temperature: 0.0,
	}

	if defaults.Retries != 2 {
		t.Errorf("Expected Retries 2, got %d", defaults.Retries)
	}
	if defaults.Timeout != 180 {
		t.Errorf("Expected Timeout 180, got %d", defaults.Timeout)
	}
	if defaults.MaxTokens != 8192 {
		t.Errorf("Expected MaxTokens 8192, got %d", defaults.MaxTokens)
	}
	if defaults.Temperature != 0.0 {
		t.Errorf("Expected Temperature 0.0, got %f", defaults.Temperature)
	}
}
</file>
<file path="cmd/root.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
)

var (
	debugFlag          bool
	verboseFlag        bool
	cacheStatsRepoPath string
)

// rootCmd represents the base command
var rootCmd = &cobra.Command{
	Use:   "gendocs",
	Short: "AI-powered code documentation generator",
	Long: `Generate comprehensive documentation for your codebase using AI.

Gendocs analyzes your codebase structure, dependencies, data flow, and APIs
to generate detailed documentation including README.md, AI assistant configs,
and more.`,
	Version: "2.0.0",
}

// Execute runs the root command
func Execute() {
	if err := rootCmd.Execute(); err != nil {
		fmt.Fprintf(os.Stderr, "Error: %v\n", err)
		os.Exit(1)
	}
}

func init() {
	// Persistent flags (available to all subcommands)
	rootCmd.PersistentFlags().BoolVar(&debugFlag, "debug", false, "Enable debug mode")
	rootCmd.PersistentFlags().BoolVarP(&verboseFlag, "verbose", "v", false, "Show detailed log output instead of progress UI")

	// Add cache-stats command
	rootCmd.AddCommand(cacheStatsCmd)
	cacheStatsCmd.Flags().StringVar(&cacheStatsRepoPath, "repo-path", ".", "Path to repository")

	// Add cache-clear command
	rootCmd.AddCommand(cacheClearCmd)
}

// cacheStatsCmd represents the cache-stats command
var cacheStatsCmd = &cobra.Command{
	Use:   "cache-stats",
	Short: "Display LLM cache statistics",
	Long: `Display statistics about the LLM response cache, including:
  - Total entries and expired entries
  - Cache hits, misses, and hit rate
  - Storage size and evictions

This command shows statistics from the disk cache file without running analysis.`,
	RunE: runCacheStats,
}

func runCacheStats(cmd *cobra.Command, args []string) error {
	displayCacheStats(cacheStatsRepoPath)
	return nil
}
</file>
<file path="docs/ARCHITECTURE.md">
# Gendocs Architecture

**Version:** 2.0 (Go Implementation)
**Last Updated:** 2025-12-23

---

## Table of Contents

1. [System Overview](#system-overview)
2. [Architectural Layers](#architectural-layers)
3. [Component Responsibilities](#component-responsibilities)
4. [Design Patterns](#design-patterns)
5. [Data Flow](#data-flow)
6. [Configuration System](#configuration-system)
7. [Error Handling Strategy](#error-handling-strategy)
8. [Concurrency Model](#concurrency-model)
9. [Testing Strategy](#testing-strategy)
10. [Extension Points](#extension-points)

---

## System Overview

Gendocs is a CLI tool that analyzes codebases and generates comprehensive documentation using Large Language Models (LLMs). The system employs a multi-agent architecture where specialized AI agents analyze different aspects of the code.

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLI Layer                            â”‚
â”‚              (Cobra Commands + Flag Parsing)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Handler Layer                           â”‚
â”‚         (Business Logic Orchestration + I/O)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Agent Layer                            â”‚
â”‚    (AnalyzerAgent + 5 Sub-Agents + 2 Generator Agents)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Layer     â”‚                  â”‚     Tool Layer        â”‚
â”‚  (3 Providers)  â”‚                  â”‚ (File Operations)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **7 AI Agents:** 1 orchestrator + 5 analyzers + 2 generators
- **3 LLM Providers:** OpenAI, Anthropic Claude, Google Gemini
- **2 Tools:** FileReadTool, ListFilesTool
- **5 Commands:** analyze, generate (readme, ai-rules), cronjob, config

---

## Architectural Layers

### 1. CLI Layer (`cmd/`)

**Responsibility:** Command parsing and user interaction

**Components:**
- `root.go` - Root command and global flags
- `analyze.go` - Code analysis command
- `generate.go` - Documentation generation commands
- `cronjob.go` - Automated GitLab processing
- `config.go` - Interactive configuration wizard

**Key Decisions:**
- Uses Cobra for command routing
- Minimal business logic (delegated to handlers)
- Validates flags but not business rules

### 2. Handler Layer (`internal/handlers/`)

**Responsibility:** Business logic orchestration

**Pattern:** Handler-Agent separation

```go
type Handler interface {
    Handle(ctx context.Context) error
}
```

**Components:**
- `AnalyzeHandler` - Orchestrates code analysis flow
- `READMEHandler` - Generates README.md
- `AIRulesHandler` - Generates CLAUDE.md/AGENTS.md
- `CronjobHandler` - Processes GitLab repositories in batch

**Responsibilities:**
1. Load and validate configuration
2. Initialize agents with proper dependencies
3. Coordinate agent execution
4. Write output files
5. Handle errors and provide user feedback

### 3. Agent Layer (`internal/agents/`)

**Responsibility:** AI-powered analysis and generation

**Architecture:** Orchestrator + Sub-Agents

```
AnalyzerAgent (Orchestrator)
    â”œâ”€ StructureAnalyzer
    â”œâ”€ DependencyAnalyzer
    â”œâ”€ DataFlowAnalyzer
    â”œâ”€ RequestFlowAnalyzer
    â””â”€ APIAnalyzer

DocumenterAgent (Standalone)

AIRulesGeneratorAgent (Standalone)
```

**Base Agent Structure:**

```go
type Agent interface {
    Run(ctx context.Context) (string, error)
    Name() string
}

type BaseAgent struct {
    name          string
    llmClient     llm.LLMClient
    tools         []tools.Tool
    promptManager *prompts.Manager
    logger        *logging.Logger
    systemPrompt  string
    maxRetries    int
    maxTokens     int
    temperature   float64
}
```

**Tool Calling Loop:**

```
1. Send prompt + tools to LLM
2. LLM responds with content or tool calls
3. If tool calls:
   a. Execute each tool
   b. Add results to conversation
   c. Return to step 1
4. If no tool calls, return content
```

### 4. LLM Layer (`internal/llm/`)

**Responsibility:** LLM provider abstraction

**Interface:**

```go
type LLMClient interface {
    GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error)
    SupportsTools() bool
    GetProvider() string
}
```

**Providers:**
- `OpenAIClient` - OpenAI API (GPT-4, GPT-3.5)
- `AnthropicClient` - Anthropic Claude API
- `GeminiClient` - Google Gemini API

**Common Features:**
- Retry logic with exponential backoff
- Tool calling support (function calling)
- Token usage tracking
- Error normalization

### 5. Tool Layer (`internal/tools/`)

**Responsibility:** Agent capabilities

**Interface:**

```go
type Tool interface {
    Name() string
    Description() string
    Parameters() map[string]interface{}
    Execute(ctx context.Context, params map[string]interface{}) (interface{}, error)
}
```

**Available Tools:**
- `FileReadTool` - Read file contents with pagination
- `ListFilesTool` - List files recursively

**Safety Features:**
- Path traversal prevention
- Retry logic for transient errors
- Parameter validation

---

## Component Responsibilities

### Configuration (`internal/config/`)

**Precedence Order (highest to lowest):**
1. CLI flags
2. Project `.ai/config.yaml`
3. Global `~/.gendocs.yaml`
4. Environment variables
5. Hardcoded defaults

**Example:**

```yaml
# .ai/config.yaml
analyzer:
  max_workers: 4
  llm:
    provider: anthropic
    model: claude-3-sonnet
    api_key: sk-ant-...
```

**Configuration Loading:**

```go
cfg, err := config.LoadAnalyzerConfig(repoPath, cliOverrides)
// cfg contains merged configuration with proper precedence
```

### Prompt Management (`internal/prompts/`)

**Responsibility:** Template-based prompt generation

**Storage:** YAML files in `prompts/` directory

```yaml
# prompts/analyzer.yaml
structure_analyzer_system: |
  You are a code structure analyst...

structure_analyzer_user: |
  Analyze the repository at {{.RepoPath}}
```

**Rendering:**

```go
prompt, err := promptManager.Render("structure_analyzer_user", map[string]interface{}{
    "RepoPath": "/path/to/repo",
})
```

### Logging (`internal/logging/`)

**Dual Output:**
- **Console:** Colored, human-readable (using zap)
- **File:** Structured JSON for debugging

**Log Levels:**
- DEBUG: Detailed execution flow
- INFO: Major operations
- WARN: Recoverable issues
- ERROR: Failures

**Usage:**

```go
logger.Info("Starting analysis",
    logging.String("repo", repoPath),
    logging.Int("workers", maxWorkers),
)
```

### Error Handling (`internal/errors/`)

**14 Error Types:**

```go
type AIDocGenError struct {
    ErrorType   string // e.g., "MissingEnvVar"
    Message     string // Technical message
    UserMessage string // User-friendly message
    Suggestion  string // How to fix
    Context     map[string]interface{}
}
```

**Examples:**
- `ConfigFileError` - Invalid YAML
- `MissingEnvVarError` - Required env var not set
- `LLMClientError` - LLM API failure
- `ToolExecutionError` - Tool failed
- `PathTraversalError` - Security violation

### Worker Pool (`internal/worker_pool/`)

**Purpose:** Concurrent agent execution

**Implementation:** Semaphore-based

```go
pool := worker_pool.NewWorkerPool(maxWorkers) // 0 = auto-detect CPUs

tasks := []Task{
    func(ctx) { return analyzeStructure() },
    func(ctx) { return analyzeDependencies() },
    func(ctx) { return analyzeDataFlow() },
}

results := pool.Run(ctx, tasks) // Blocks until all complete
```

**Features:**
- Automatic CPU detection (`runtime.NumCPU()`)
- Context-aware cancellation
- Maintains result order

---

## Design Patterns

### 1. Handler-Agent Pattern

**Problem:** Separate CLI concerns from AI orchestration

**Solution:**

```
CLI Command â†’ Handler â†’ Agent â†’ LLM
     â†“           â†“        â†“       â†“
  Flags     Config    Tools   API
```

**Benefits:**
- Testable agents (mock LLM client)
- Clean separation of concerns
- Reusable agents across commands

### 2. Factory Pattern

**Used In:** LLM clients, Agents

```go
func NewLLMClient(cfg config.LLMConfig) (llm.LLMClient, error) {
    switch cfg.Provider {
    case "openai":
        return NewOpenAIClient(cfg), nil
    case "anthropic":
        return NewAnthropicClient(cfg), nil
    case "gemini":
        return NewGeminiClient(cfg), nil
    }
}
```

**Benefits:**
- Decouples creation from usage
- Enables testing with mocks
- Centralized provider logic

### 3. Tool Interface Pattern

**Purpose:** Extensible agent capabilities

**Adding New Tool:**

```go
type GrepTool struct {
    BaseTool
}

func (g *GrepTool) Name() string { return "grep" }
func (g *GrepTool) Description() string { return "Search for patterns" }
func (g *GrepTool) Parameters() map[string]interface{} { ... }
func (g *GrepTool) Execute(ctx, params) (interface{}, error) { ... }

// Register with agent
agent.tools = append(agent.tools, NewGrepTool())
```

### 4. Strategy Pattern (LLM Providers)

**Problem:** Different LLM APIs have different formats

**Solution:** Common interface, provider-specific implementations

```go
// OpenAI format
{"messages": [{"role": "user", "content": "..."}]}

// Anthropic format
{"messages": [{"role": "user", "content": [{"type": "text", "text": "..."}]}]}

// Gemini format
{"contents": [{"parts": [{"text": "..."}], "role": "user"}]}
```

All converted to/from unified `CompletionRequest/Response`.

---

## Data Flow

### Analyze Command Flow

```
1. CLI: gendocs analyze --repo-path .
   â†“
2. cmd/analyze.go: Parse flags, build config
   â†“
3. handlers.AnalyzeHandler: Load configuration
   â†“
4. Create AnalyzerAgent with 5 sub-agents
   â†“
5. Run sub-agents in parallel (worker pool)
   â†“
   â”œâ”€ StructureAnalyzer
   â”‚   â”œâ”€ List files (tool)
   â”‚   â”œâ”€ Read key files (tool)
   â”‚   â””â”€ LLM analysis â†’ code_structure.md
   â”‚
   â”œâ”€ DependencyAnalyzer â†’ dependencies.md
   â”œâ”€ DataFlowAnalyzer â†’ data_flow.md
   â”œâ”€ RequestFlowAnalyzer â†’ request_flow.md
   â””â”€ APIAnalyzer â†’ api_analysis.md
   â†“
6. Collect results
   â†“
7. Write to .ai/docs/
   â†“
8. Log completion
```

### Tool Calling Flow

```
Agent:
  "I need to read main.go"
  ToolCall: {name: "read_file", args: {path: "main.go"}}
    â†“
Tool:
  - Validate path (no traversal)
  - Execute: os.ReadFile("main.go")
  - Return: {content: [...], line_count: 50}
    â†“
Agent:
  "The file contains..."`
  (continues analysis)
```

---

## Configuration System

### Multi-Source Loading

**Implementation:** Viper library

```go
type Loader struct {
    v *viper.Viper
}

func (l *Loader) LoadForAgent(repoPath, section string) (*viper.Viper, error) {
    // 1. Load ~/.gendocs.yaml
    l.loadGlobalConfig()

    // 2. Load .ai/config.yaml
    l.loadProjectConfig(repoPath)

    // 3. Merge with CLI overrides
    l.applyCLIOverrides(overrides)

    // 4. Substitute environment variables
    l.v.AutomaticEnv()

    return l.v, nil
}
```

### Validation

```go
func validateLLMConfig(cfg *LLMConfig) error {
    if cfg.APIKey == "" {
        return NewMissingEnvVarError("API_KEY", "Required for LLM provider")
    }

    validProviders := map[string]bool{
        "openai": true, "anthropic": true, "gemini": true,
    }

    if !validProviders[cfg.Provider] {
        return NewInvalidEnvVarError("PROVIDER", cfg.Provider, "Must be: openai, anthropic, or gemini")
    }

    return nil
}
```

---

## Error Handling Strategy

### Error Flow

```
Tool Error â†’ ModelRetryError (retryable)
    â†“
BaseAgent catches â†’ Formats for LLM
    â†“
LLM sees error â†’ Adjusts strategy or reports
    â†“
Final result includes error context
```

### Error Context

```go
err := &AIDocGenError{
    ErrorType: "LLMClientError",
    Message:   "API request failed: 429 Too Many Requests",
    UserMessage: `
Rate limit exceeded. You can:
1. Wait a few minutes and retry
2. Reduce --max-workers flag
3. Upgrade your API tier
    `,
    Suggestion: "Try: gendocs analyze --max-workers 2",
    Context: map[string]interface{}{
        "provider": "openai",
        "model":    "gpt-4",
        "attempt":  3,
    },
}
```

### User-Facing Errors

```bash
$ gendocs analyze
Error: Missing required configuration: ANALYZER_LLM_API_KEY

ANALYZER_LLM_API_KEY is required but not set. You can fix this by:

1. Setting the environment variable:
   export ANALYZER_LLM_API_KEY="your-key"

2. Adding to .ai/config.yaml:
   analyzer:
     llm:
       api_key: "your-key"

3. Running the configuration wizard:
   gendocs config
```

---

## Concurrency Model

### Worker Pool Implementation

```go
type WorkerPool struct {
    maxWorkers int
    semaphore  chan struct{}
}

func (wp *WorkerPool) Run(ctx context.Context, tasks []Task) []Result {
    results := make([]Result, len(tasks))
    var wg sync.WaitGroup

    for i, task := range tasks {
        wg.Add(1)
        go func(index int, t Task) {
            defer wg.Done()

            // Acquire semaphore (blocks if max workers reached)
            wp.semaphore <- struct{}{}
            defer func() { <-wp.semaphore }()

            // Execute task
            results[index] = t(ctx)
        }(i, task)
    }

    wg.Wait()
    return results
}
```

### Parallelism Strategy

**Parallel:**
- Sub-agent execution (5 analyzers run concurrently)
- File operations (if safe)

**Sequential:**
- LLM API calls per agent (tool calling loop)
- File writes (prevent corruption)

**Configurable:**
```bash
# Auto-detect CPUs (default)
gendocs analyze

# Limit for rate limits
gendocs analyze --max-workers 2

# Maximize throughput
gendocs analyze --max-workers 8
```

---

## Testing Strategy

### Test Types

1. **Unit Tests** (`*_test.go`)
   - LLM clients with mock HTTP servers
   - Tools with temp directories
   - Prompt manager with mock prompts
   - Config loader with temp files

2. **Integration Tests** (`*_integration_test.go`)
   - Agent flows with mock LLM clients
   - End-to-end analyze workflow
   - Tool calling sequences

3. **Build Tag Separation:**
   ```go
   // +build integration
   ```

   Run: `go test -tags integration ./...`

### Mock Infrastructure

**MockLLMClient:**

```go
mockClient := &MockLLMClient{
    Responses: []llm.CompletionResponse{
        {ToolCalls: ...}, // Response 1
        {Content: ...},   // Response 2
    },
}

agent.llmClient = mockClient
agent.Run(ctx) // Uses mock responses
```

**Test Helpers:**

```go
// Create temp git repo
repoPath := testing.CreateTempRepo(t, map[string]string{
    "main.go": "package main...",
    "go.mod":  "module test...",
})

// Assert file operations
testing.AssertFileExists(t, "README.md")
testing.AssertFileContains(t, "README.md", "## Features")
```

### Coverage Goals

- **Unit Tests:** 80%+ coverage
- **Integration Tests:** Critical paths
- **Manual Tests:** Real LLM API calls (pre-release)

---

## Extension Points

### Adding a New LLM Provider

1. **Implement Interface:**

```go
// internal/llm/newprovider.go
type NewProviderClient struct {
    *BaseLLMClient
    apiKey string
}

func (c *NewProviderClient) GenerateCompletion(...) (CompletionResponse, error) {
    // Transform request to provider format
    // Make API call
    // Transform response to unified format
}
```

2. **Register in Factory:**

```go
// internal/llm/factory.go
case "newprovider":
    return NewNewProviderClient(cfg, retryClient), nil
```

3. **Add Tests:**

```go
// internal/llm/newprovider_test.go
func TestNewProviderClient_GenerateCompletion_Success(t *testing.T) { ... }
```

### Adding a New Tool

1. **Implement Interface:**

```go
// internal/tools/newtool.go
type NewTool struct {
    BaseTool
}

func (n *NewTool) Name() string { return "new_tool" }
func (n *NewTool) Execute(ctx, params) (interface{}, error) {
    // Implement tool logic
}
```

2. **Register with Agent:**

```go
// internal/agents/analyzer.go
tools := []tools.Tool{
    tools.NewFileReadTool(3),
    tools.NewListFilesTool(3),
    tools.NewNewTool(3), // Add here
}
```

### Adding a New Sub-Agent

1. **Create Factory Function:**

```go
// internal/agents/factory.go
func CreateNewAnalyzer(...) (*SubAgent, error) {
    return NewSubAgent(SubAgentConfig{
        Name: "NewAnalyzer",
        PromptSuffix: "new_analyzer",
    }, ...)
}
```

2. **Add Prompts:**

```yaml
# prompts/analyzer.yaml
new_analyzer_system: |
  You are a specialized analyzer for...

new_analyzer_user: |
  Analyze {{.RepoPath}} for...
```

3. **Integrate in Orchestrator:**

```go
// internal/handlers/analyze.go
newAnalyzer, _ := CreateNewAnalyzer(...)
results := pool.Run(ctx, []Task{
    structureAnalyzer.Run,
    newAnalyzer.Run, // Add here
})
```

---

## Performance Considerations

### Bottlenecks

1. **LLM API Latency**
   - Mitigation: Parallel sub-agent execution
   - Configurable: `--max-workers`

2. **File I/O**
   - Mitigation: Efficient streaming for large files
   - Tool: FileReadTool supports pagination

3. **Memory**
   - Large codebases: Tools return bounded data
   - LLM context: Agents work on subsets

### Optimization Strategies

**Implemented:**
- Worker pool for parallelism
- HTTP keep-alive connections
- Retry with exponential backoff

**Future (Phase 4):**
- Incremental analysis cache
- Dependency graph-based invalidation
- Streaming responses from LLMs

---

## Security Considerations

### Path Traversal Prevention

```go
// tools/file_read.go
func validatePath(repoPath, filePath string) error {
    absRepo, _ := filepath.Abs(repoPath)
    absFile, _ := filepath.Abs(filepath.Join(repoPath, filePath))

    if !strings.HasPrefix(absFile, absRepo) {
        return &PathTraversalError{...}
    }
    return nil
}
```

### API Key Handling

- Never logged or displayed
- Loaded from secure sources (env vars, config files with restricted permissions)
- Not passed in CLI arguments (shell history risk)

### LLM Output Validation

- Markdown validator prevents broken output
- No shell command execution from LLM outputs
- Tools have strict parameter validation

---

## Deployment

### Binary Distribution

```bash
# Build for multiple platforms
GOOS=linux GOARCH=amd64 go build -o gendocs-linux-amd64
GOOS=darwin GOARCH=amd64 go build -o gendocs-darwin-amd64
GOOS=windows GOARCH=amd64 go build -o gendocs-windows-amd64.exe
```

### Docker (Future)

```dockerfile
FROM golang:1.22-alpine
WORKDIR /app
COPY . .
RUN go build -o gendocs
ENTRYPOINT ["./gendocs"]
```

---

## Future Architecture Evolution

### Planned Changes (Phase 4)

1. **Cache Layer:**
   ```
   Agent â†’ Cache Check â†’ (miss) â†’ LLM â†’ Cache Store
   ```

2. **GitHub Support:**
   ```
   GitProvider Interface
       â”œâ”€ GitLabProvider
       â””â”€ GitHubProvider
   ```

3. **Plugin System:**
   - **Rejected:** Dynamic plugins (security risk)
   - **Accepted:** Static tool registration

---

## Conclusion

Gendocs architecture prioritizes:
- **Modularity:** Clear layer separation
- **Testability:** Mockable dependencies
- **Extensibility:** Well-defined interfaces
- **Security:** Input validation and sandboxing
- **Performance:** Concurrent execution where safe

The Handler-Agent pattern provides clean separation between orchestration and AI logic, making the system maintainable and testable.

---

**Document Version:** 2.0
**Maintainers:** Gendocs Team
**Last Review:** 2025-12-23
</file>
<file path="docs/CONTRIBUTING.md">
# Contributing to Gendocs

Thank you for your interest in contributing to Gendocs! This document provides guidelines and instructions for contributing.

---

## Table of Contents

1. [Code of Conduct](#code-of-conduct)
2. [Getting Started](#getting-started)
3. [Development Setup](#development-setup)
4. [Development Workflow](#development-workflow)
5. [Testing Requirements](#testing-requirements)
6. [Code Style](#code-style)
7. [Commit Guidelines](#commit-guidelines)
8. [Pull Request Process](#pull-request-process)
9. [Adding Features](#adding-features)

---

## Code of Conduct

- Be respectful and constructive
- Focus on the issue, not the person
- Accept constructive criticism gracefully
- Prioritize project goals over personal preferences

---

## Getting Started

### Prerequisites

- **Go 1.22+** installed
- **Git** configured
- **Make** utility (optional but recommended)
- API key for at least one LLM provider (OpenAI, Anthropic, or Gemini)

### Fork and Clone

```bash
# Fork the repository on GitHub, then:
git clone https://github.com/YOUR_USERNAME/gendocs.git
cd gendocs

# Add upstream remote
git remote add upstream https://github.com/user/gendocs.git
```

---

## Development Setup

### 1. Install Dependencies

```bash
go mod download
go mod verify
```

### 2. Set Up Environment

```bash
# Copy example env file
cp .env.example .env

# Edit with your API keys
nano .env
```

Example `.env`:
```bash
ANALYZER_LLM_PROVIDER=openai
ANALYZER_LLM_MODEL=gpt-4
ANALYZER_LLM_API_KEY=sk-...

DOCUMENTER_LLM_PROVIDER=openai
DOCUMENTER_LLM_MODEL=gpt-4
DOCUMENTER_LLM_API_KEY=sk-...
```

### 3. Build

```bash
make build
# Or: go build -o gendocs .
```

### 4. Verify Installation

```bash
./gendocs --version
./gendocs --help
```

---

## Development Workflow

### 1. Create a Feature Branch

```bash
git checkout -b feature/my-new-feature
# or
git checkout -b fix/bug-description
```

**Branch Naming:**
- `feature/` - New features
- `fix/` - Bug fixes
- `docs/` - Documentation changes
- `refactor/` - Code refactoring
- `test/` - Test improvements

### 2. Make Changes

Follow these principles:
- **Small, focused changes** - One feature/fix per branch
- **Test as you go** - Write tests alongside code
- **Document as you go** - Update docs with changes

### 3. Run Tests Frequently

```bash
# Quick check (runs in ~5s)
make test-short

# Full test suite
make test

# With coverage
make test-coverage
```

### 4. Run Linters

```bash
# Install golangci-lint first:
# https://golangci-lint.run/usage/install/

make lint
```

### 5. Commit Changes

```bash
git add .
git commit -m "feat: add new feature"
# See Commit Guidelines below for message format
```

### 6. Keep Branch Updated

```bash
git fetch upstream
git rebase upstream/main
```

### 7. Push and Create PR

```bash
git push origin feature/my-new-feature
```

Then create a Pull Request on GitHub.

---

## Testing Requirements

### Minimum Requirements

All contributions **must** include tests:

1. **New Features** â†’ Unit tests + integration tests (if applicable)
2. **Bug Fixes** â†’ Test that reproduces the bug + fix
3. **Refactoring** â†’ Existing tests must pass
4. **Documentation** â†’ No tests required (but examples should work)

### Coverage Standards

- **New code:** 80%+ coverage required
- **Overall project:** Must not decrease below 60%

### Running Tests

```bash
# Unit tests only (fast)
make test-short

# All tests
make test

# Integration tests
go test -tags integration ./...

# Specific package
go test ./internal/llm/

# Specific test
go test -run TestOpenAIClient_GenerateCompletion_Success ./internal/llm/
```

### Writing Tests

See [TESTING.md](TESTING.md) for detailed guidelines.

**Quick Example:**

```go
func TestMyFunction_Success(t *testing.T) {
    // Arrange
    input := "test"

    // Act
    result, err := MyFunction(input)

    // Assert
    if err != nil {
        t.Fatalf("Expected no error, got %v", err)
    }

    if result != "expected" {
        t.Errorf("Expected 'expected', got '%s'", result)
    }
}
```

---

## Code Style

### Go Conventions

Follow standard Go conventions:

```bash
# Format code
go fmt ./...

# Run linter
golangci-lint run ./...
```

### Style Guidelines

1. **Package Comments**

```go
// Package llm provides abstractions for LLM providers.
//
// This package implements a common interface for OpenAI,
// Anthropic, and Gemini APIs.
package llm
```

2. **Function Comments**

```go
// GenerateCompletion sends a completion request to the LLM.
//
// The function handles tool calling loops automatically,
// executing tools and returning results to the LLM until
// a final response is generated.
func GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
    // ...
}
```

3. **Variable Names**

```go
// Good - descriptive
var analyzerConfig *config.AnalyzerConfig
var maxWorkers int

// Bad - abbreviations
var cfg *config.AnalyzerConfig
var mw int
```

4. **Error Handling**

```go
// Good - wrap errors with context
if err != nil {
    return fmt.Errorf("failed to load config: %w", err)
}

// Bad - lose context
if err != nil {
    return err
}
```

5. **Imports Organization**

```go
import (
    // Standard library
    "context"
    "fmt"
    "os"

    // External packages
    "github.com/spf13/cobra"

    // Internal packages
    "github.com/user/gendocs/internal/config"
    "github.com/user/gendocs/internal/llm"
)
```

---

## Commit Guidelines

### Commit Message Format

Use [Conventional Commits](https://www.conventionalcommits.org/):

```
<type>(<scope>): <subject>

<body>

<footer>
```

### Types

- `feat:` - New feature
- `fix:` - Bug fix
- `docs:` - Documentation changes
- `test:` - Test additions or fixes
- `refactor:` - Code refactoring
- `perf:` - Performance improvements
- `chore:` - Build/tooling changes

### Examples

```bash
# Feature
feat(llm): add support for Claude 3.5 Sonnet

Implements support for Anthropic's Claude 3.5 Sonnet model
with improved tool calling.

Closes #123

# Bug fix
fix(tools): prevent path traversal in FileReadTool

Adds validation to ensure file paths cannot escape the
repository directory.

# Documentation
docs(readme): update installation instructions

# Test
test(llm): add integration tests for retry logic

# Breaking change
feat(config)!: change config file format to YAML

BREAKING CHANGE: Configuration files now use YAML format
instead of JSON. Users must migrate their configs.
```

### Commit Best Practices

âœ… **DO:**
- Write descriptive commit messages
- Keep commits focused and atomic
- Reference issues in commit messages

âŒ **DON'T:**
- Commit unrelated changes together
- Use vague messages like "fix bug" or "update code"
- Commit commented-out code or debug statements

---

## Pull Request Process

### Before Creating PR

1. âœ… All tests pass (`make test`)
2. âœ… Linters pass (`make lint`)
3. âœ… Code formatted (`go fmt ./...`)
4. âœ… Branch rebased on latest main
5. âœ… Commit messages follow guidelines
6. âœ… Documentation updated (if applicable)

### PR Title

Use same format as commit messages:

```
feat(llm): add support for Claude 3.5 Sonnet
fix(config): handle missing API key gracefully
docs: add examples for custom prompts
```

### PR Description

Use this template:

```markdown
## Summary
Brief description of what this PR does

## Changes
- Change 1
- Change 2
- Change 3

## Testing
- [ ] Unit tests added/updated
- [ ] Integration tests added/updated
- [ ] Manual testing performed

## Checklist
- [ ] Tests pass
- [ ] Linters pass
- [ ] Documentation updated
- [ ] Breaking changes documented

## Related Issues
Closes #123
Relates to #456
```

### PR Review Process

1. **Automated Checks** run on every PR:
   - Tests must pass
   - Linters must pass
   - Coverage must not decrease

2. **Code Review** by maintainer:
   - Architecture fit
   - Code quality
   - Test coverage
   - Documentation

3. **Changes Requested** (if needed):
   - Address feedback
   - Push updates to same branch
   - Request re-review

4. **Merge** when approved:
   - Squash merge for clean history
   - Delete branch after merge

---

## Adding Features

### Adding a New LLM Provider

1. **Implement Interface**

```go
// internal/llm/newprovider.go
type NewProviderClient struct {
    *BaseLLMClient
    apiKey string
    model  string
}

func NewNewProviderClient(cfg config.LLMConfig, retryClient *RetryClient) *NewProviderClient {
    return &NewProviderClient{
        BaseLLMClient: NewBaseLLMClient(retryClient),
        apiKey:        cfg.APIKey,
        model:         cfg.Model,
    }
}

func (c *NewProviderClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
    // Transform request to provider format
    // Make API call
    // Transform response to unified format
}

func (c *NewProviderClient) SupportsTools() bool {
    return true
}

func (c *NewProviderClient) GetProvider() string {
    return "newprovider"
}
```

2. **Register in Factory**

```go
// internal/llm/factory.go
func (f *Factory) NewLLMClient(cfg config.LLMConfig) (LLMClient, error) {
    switch cfg.Provider {
    case "openai":
        return NewOpenAIClient(cfg, retryClient), nil
    case "anthropic":
        return NewAnthropicClient(cfg, retryClient), nil
    case "gemini":
        return NewGeminiClient(cfg, retryClient), nil
    case "newprovider": // Add here
        return NewNewProviderClient(cfg, retryClient), nil
    }
}
```

3. **Add Tests**

```go
// internal/llm/newprovider_test.go
func TestNewProviderClient_GenerateCompletion_Success(t *testing.T) { ... }
func TestNewProviderClient_GenerateCompletion_WithToolCalls(t *testing.T) { ... }
func TestNewProviderClient_GenerateCompletion_InvalidAPIKey(t *testing.T) { ... }
```

4. **Update Documentation**

- Add to `README.md` supported providers
- Add to `docs/ARCHITECTURE.md` LLM layer
- Add configuration example

### Adding a New Tool

1. **Implement Interface**

```go
// internal/tools/newtool.go
type NewTool struct {
    BaseTool
}

func NewNewTool(maxRetries int) *NewTool {
    return &NewTool{
        BaseTool: NewBaseTool(maxRetries),
    }
}

func (t *NewTool) Name() string {
    return "new_tool"
}

func (t *NewTool) Description() string {
    return "Description of what the tool does"
}

func (t *NewTool) Parameters() map[string]interface{} {
    return map[string]interface{}{
        "type": "object",
        "properties": map[string]interface{}{
            "param1": map[string]interface{}{
                "type": "string",
                "description": "Parameter description",
            },
        },
        "required": []string{"param1"},
    }
}

func (t *NewTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) {
    return t.RetryableExecute(ctx, func() (interface{}, error) {
        // Implement tool logic
    })
}
```

2. **Add Tests**

```go
// internal/tools/newtool_test.go
func TestNewTool_Execute_Success(t *testing.T) { ... }
func TestNewTool_Execute_InvalidParams(t *testing.T) { ... }
```

3. **Register with Agents**

```go
// internal/agents/analyzer.go
tools := []tools.Tool{
    tools.NewFileReadTool(3),
    tools.NewListFilesTool(3),
    tools.NewNewTool(3), // Add here
}
```

4. **Update Documentation**

### Adding a New Command

1. **Create Command File**

```go
// cmd/newcommand.go
var newCmd = &cobra.Command{
    Use:   "newcommand",
    Short: "Short description",
    Long:  `Long description`,
    RunE:  runNewCommand,
}

func init() {
    rootCmd.AddCommand(newCmd)
    newCmd.Flags().StringVar(&flagVar, "flag", "default", "Flag description")
}

func runNewCommand(cmd *cobra.Command, args []string) error {
    // Implementation
}
```

2. **Create Handler**

```go
// internal/handlers/newhandler.go
type NewHandler struct {
    config config.NewConfig
    logger *logging.Logger
}

func NewNewHandler(cfg config.NewConfig, logger *logging.Logger) *NewHandler {
    return &NewHandler{config: cfg, logger: logger}
}

func (h *NewHandler) Handle(ctx context.Context) error {
    // Implementation
}
```

3. **Add Tests**

4. **Update Documentation**

---

## Questions?

- **Issues:** https://github.com/user/gendocs/issues
- **Discussions:** https://github.com/user/gendocs/discussions
- **Email:** maintainers@gendocs.dev (if exists)

---

**Thank you for contributing!** ğŸ‰

Every contribution, no matter how small, helps make Gendocs better.

---

**Document Version:** 1.0
**Last Updated:** 2025-12-23
</file>
<file path="docs/EXPORT.md">
# Documentation Export

This guide explains how to export Gendocs-generated documentation to various formats for easier sharing and publishing.

## Quick Start

```bash
# Export README.md to HTML (default format)
gendocs generate export

# Export README.md to JSON
gendocs generate export --format json --output docs.json

# Export specific file to HTML
gendocs generate export --input .ai/docs/code_structure.md --output structure.html

# Generate and export in one command
gendocs generate readme --export-html
```

## Supported Formats

### HTML

Generate standalone HTML files with embedded CSS and syntax highlighting.

**Features:**
- Single-file output (no external dependencies)
- Embedded CSS for GitHub-style rendering
- Syntax highlighting for code blocks (50+ languages)
- Responsive design (mobile-friendly)
- Dark theme syntax highlighting (Monokai)
- Valid HTML5 output

**Usage:**
```bash
gendocs generate export [flags]
```

**Flags:**
- `--input string`: Input markdown file (default: "README.md")
- `--output string`: Output file path (default: input.html or input.json based on format)
- `--format string`: Export format - "html" or "json" (default: "html")
- `--repo-path string`: Path to repository (default: ".")

### JSON

Generate structured JSON with metadata and hierarchical content.

**Features:**
- Structured data format for programmatic access
- Metadata section with title, timestamps, generator info, word/character counts
- Hierarchical headings tree for navigation and table of contents
- All document elements in flat array (paragraphs, code blocks, lists, tables, blockquotes, links, images)
- Language information for code blocks
- Table column alignment information
- Task list checkbox states
- URL-safe heading IDs

**Usage:**
```bash
gendocs generate export --format json [flags]
```

**When to use JSON export:**
- **Search indexing**: Feed documentation into search engines (Elasticsearch, Algolia, Meilisearch)
- **Static site generators**: Process with custom templates (Hugo, Jekyll, Eleventy)
- **API documentation**: Generate API references from markdown
- **Documentation portals**: Integrate into existing platforms
- **Content analysis**: Analyze documentation structure and metrics
- **Content migration**: Convert between documentation systems
- **Custom processing**: Apply transformations or extract specific data

**Example output structure:**
```json
{
  "metadata": {
    "title": "Document Title",
    "generated_at": "2025-12-29T10:30:00Z",
    "generator": {
      "name": "Gendocs",
      "version": "1.0.0"
    },
    "source_file": "README.md",
    "word_count": 1234,
    "char_count": 5678
  },
  "content": {
    "headings": [
      {
        "id": "section-title",
        "level": 2,
        "text": "Section Title",
        "children": []
      }
    ],
    "elements": [
      {"type": "heading", "level": 1, "text": "Title"},
      {"type": "paragraph", "content": "Content..."},
      {"type": "code_block", "language": "go", "code": "..."}
    ]
  }
}
```

For detailed JSON structure documentation, see [JSON_FORMAT.md](JSON_FORMAT.md).

## Common Use Cases

### Export README for GitHub Pages

```bash
# Generate README and export to docs/index.html for GitHub Pages
gendocs generate readme --export-html
mv README.html docs/index.html
```

### Export All Analysis Documents

```bash
# Export each analysis document to HTML
gendocs generate export --input .ai/docs/code_structure.md --output docs/structure.html
gendocs generate export --input .ai/docs/dependencies.md --output docs/dependencies.html
gendocs generate export --input .ai/docs/data_flow.md --output docs/data-flow.html
gendocs generate export --input .ai/docs/request_flow.md --output docs/request-flow.html
gendocs generate export --input .ai/docs/api_documentation.md --output docs/api.html
```

### Export Documentation for Search Indexing

```bash
# Export README to JSON for search indexing
gendocs generate export --input README.md --output search-index.json --format json

# Export all docs to JSON for indexing
gendocs generate export --input .ai/docs/code_structure.md --output search/structure.json --format json
gendocs generate export --input .ai/docs/dependencies.md --output search/dependencies.json --format json
gendocs generate export --input .ai/docs/api_documentation.md --output search/api.json --format json
```

### Export for Static Site Generator

```bash
# Export to JSON for processing with Hugo/Jekyll/Eleventy
gendocs generate export --input README.md --output content/docs/index.json --format json

# Use with jq to extract specific data
jq '{title: .metadata.title, sections: [.content.headings[] | {id, text, level}]}' search-index.json
```

### Batch Export Script

Create a script to export all documentation:

```bash
#!/bin/bash
# export-docs.sh

FORMAT=${1:-html}  # Default to HTML if no format specified
OUTPUT_DIR="docs/$FORMAT"

echo "Exporting documentation to $FORMAT..."

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Export README
gendocs generate export --input README.md --output "$OUTPUT_DIR/index.$FORMAT" --format "$FORMAT"

# Export analysis documents
for file in .ai/docs/*.md; do
    if [ -f "$file" ]; then
        basename=$(basename "$file" .md)
        gendocs generate export --input "$file" --output "$OUTPUT_DIR/${basename}.$FORMAT" --format "$FORMAT"
        echo "  âœ“ Exported $basename to ${basename}.$FORMAT"
    fi
done

echo "All documentation exported to $OUTPUT_DIR/"
```

Usage:
```bash
# Export to HTML (default)
./export-docs.sh
# or
./export-docs.sh html

# Export to JSON
./export-docs.sh json
```

### Integrate with CI/CD

#### GitHub Actions

```yaml
name: Generate Documentation

on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.22'

      - name: Install Gendocs
        run: |
          git clone https://github.com/user/gendocs
          cd gendocs
          go build -o gendocs .
          sudo mv gendocs /usr/local/bin/

      - name: Export Documentation
        run: |
          # Export to HTML for GitHub Pages
          gendocs generate export --output docs/index.html

          # Optionally export to JSON for search indexing
          gendocs generate export --output docs/search-index.json --format json
        env:
          DOCUMENTER_LLM_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          DOCUMENTER_LLM_PROVIDER: openai
          DOCUMENTER_LLM_MODEL: gpt-4o

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs
```

#### GitLab CI

```yaml
pages:
  stage: deploy
  script:
    - go install github.com/user/gendocs@latest
    - gendocs generate export --output public/index.html
  artifacts:
    paths:
      - public
  only:
    - main
```

## Output Customization

Currently, the HTML exporter uses a fixed GitHub-style theme. Future versions may support:

- Custom CSS themes
- Light/dark mode toggle
- Configurable syntax highlighting themes
- Custom header/footer templates

## Technical Details

### HTML Structure

The exported HTML follows this structure:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Gendocs">
    <title><!-- Extracted from first H1 --></title>
    <style>
        <!-- Embedded CSS -->
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="generator-badge">Generated with Gendocs</div>
        </header>
        <main>
            <!-- Converted Markdown content -->
        </main>
        <footer>
            <p>Generated on YYYY-MM-DD HH:MM:SS by Gendocs</p>
        </footer>
    </div>
</body>
</html>
```

### Markdown Processing

The exporter uses [Goldmark](https://github.com/yuin/goldmark) with these extensions:

- **GitHub Flavored Markdown (GFM)**: Tables, strikethrough, task lists
- **Syntax Highlighting**: [Chroma](https://github.com/alecthomas/chroma) with Monokai theme
- **Raw HTML**: Preserves raw HTML in Markdown source

Supported GFM features:
- Tables with alignment
- Strikethrough (`~~text~~`)
- Task lists (`- [x] Done`, `- [ ] Todo`)
- Autolinks
- Hard line breaks

### Syntax Highlighting

Code blocks are highlighted using Chroma with support for 50+ languages:

- Go, Python, JavaScript, TypeScript, Rust, Java, C, C++, C#
- Ruby, PHP, Swift, Kotlin, Scala, Elixir, Erlang
- Shell (bash, zsh), PowerShell, SQL, YAML, JSON, XML
- HTML, CSS, SCSS, Markdown, Dockerfile
- And many more...

Example:

````markdown
```go
func main() {
    fmt.Println("Hello, World!")
}
```
````

Renders with syntax-aware coloring for keywords, strings, comments, etc.

## Troubleshooting

### Input file not found

**Error:** `input file not found: README.md`

**Solution:** Ensure the input file exists or provide the correct path:

```bash
# Use absolute path
gendocs generate export --input /full/path/to/file.md

# Or relative to repo-path
gendocs generate export --repo-path /path/to/repo --input README.md
```

### Invalid Markdown

**Issue:** Exported HTML looks broken or incomplete

**Solution:** Validate your Markdown syntax:

```bash
# Check for common issues:
# - Unclosed code blocks (```)
# - Malformed tables
# - Invalid HTML in Markdown

# Use a Markdown linter
npm install -g markdownlint-cli
markdownlint README.md
```

### Large file export slow

**Issue:** Exporting very large Markdown files (>1MB) is slow

**Solution:**
1. Split large documents into smaller sections
2. Use table of contents with links instead of one huge file
3. Consider pagination for very long documents

### Missing syntax highlighting

**Issue:** Code blocks appear without syntax highlighting

**Solution:** Ensure language is specified in code fence:

````markdown
<!-- Wrong: no language specified -->
```
func main() {}
```

<!-- Correct: language specified -->
```go
func main() {}
```
````

### JSON Export Issues

#### Empty JSON output

**Issue:** JSON file is generated but content is empty or missing elements

**Solution:** Ensure your markdown uses proper formatting:
```bash
# Check that headings use # syntax, not underlines
# Proper: ## Heading
# Improper: Heading\n=======

# Validate JSON structure
jq . docs.json
```

#### Large JSON file size

**Issue:** JSON output is much larger than source markdown

**Explanation:** This is expected. JSON files are typically 2-3x larger due to:
- Structural overhead (quotes, brackets, field names)
- Metadata fields
- Hierarchical heading structure
- Detailed element information

**Solution:** For very large documents, consider:
```bash
# Minify JSON for production
jq -c . docs.json > docs.min.json

# Extract only needed fields
jq '{metadata, headings: .content.headings}' docs.json
```

#### Missing fields in JSON

**Issue:** Expected fields like `word_count` are not present

**Solution:** Some fields are optional. Always check before accessing:
```javascript
// Good
const wordCount = doc.metadata.word_count || 0;

// Bad - may error if undefined
const wordCount = doc.metadata.word_count;
```

#### Processing JSON with special characters

**Issue:** JSON parsing fails with Unicode or special characters

**Solution:** Ensure proper encoding:
```bash
# Use jq for safe parsing
jq . docs.json

# In Python, use encoding parameter
with open('docs.json', 'r', encoding='utf-8') as f:
    doc = json.load(f)
```

## Future Enhancements

Planned features for future versions:

- **PDF Export**: Generate PDF from HTML using headless browser
- **Multi-page Sites**: Generate full documentation sites with navigation
- **Custom Themes**: Support for custom CSS themes
- **Table of Contents**: Automatic ToC generation for long documents
- **Search**: Client-side search functionality
- **Version History**: Compare documentation across versions

## See Also

- [README.md](../README.md) - Main project documentation
- [JSON Format Guide](JSON_FORMAT.md) - Detailed JSON export structure and usage examples
- [JSON Export Examples](../examples/json-export/) - Comprehensive JSON export examples with code samples
- [Custom Prompts](../examples/custom-prompts/) - Customize analysis behavior
- [PLAN.md](../PLAN.md) - Development roadmap

## Support

For issues or questions:
- Report bugs: https://github.com/user/gendocs/issues
- Feature requests: https://github.com/user/gendocs/discussions
</file>
<file path="docs/JSON_FORMAT.md">
# JSON Export Format Guide

## Overview

The JSON export format converts Markdown documentation into structured JSON data, enabling programmatic access to documentation content for further processing, indexing, or integration with other tools.

## When to Use JSON Export

JSON export is ideal for:

- **API Documentation**: Generate API reference documentation
- **Search Indexing**: Feed documentation into search engines (Elasticsearch, Algolia, etc.)
- **Static Site Generators**: Process documentation with custom templates
- **Documentation Portals**: Integrate into existing documentation platforms
- **Data Analysis**: Analyze documentation structure and content
- **Content Migration**: Convert documentation between systems
- **Custom Processing**: Apply custom transformations to documentation

## Quick Start

```bash
# Export to JSON
gendocs generate export --input README.md --output docs.json --format json

# Export with progress indicator
gendocs generate export --input README.md --output docs.json --format json --progress
```

## JSON Structure Overview

The JSON output consists of two main sections:

```json
{
  "metadata": {
    // Document metadata (title, timestamps, generator info)
  },
  "content": {
    "headings": [
      // Hierarchical heading tree for navigation
    ],
    "elements": [
      // Flat list of all document elements in order
    ]
  }
}
```

## Metadata Section

Contains information about the document and generation process.

### Fields

| Field | Type | Description |
|-------|------|-------------|
| `title` | string | Document title (from first H1 or "Documentation") |
| `generated_at` | string | ISO 8601 timestamp when JSON was generated |
| `generator.name` | string | Generator name ("Gendocs") |
| `generator.version` | string | Generator version |
| `generator.url` | string | Generator repository URL |
| `source_file` | string | Original markdown filename |
| `word_count` | number | Total word count (optional) |
| `char_count` | number | Total character count (optional) |

### Example

```json
{
  "metadata": {
    "title": "Getting Started with Gendocs",
    "generated_at": "2025-12-29T10:30:00Z",
    "generator": {
      "name": "Gendocs",
      "version": "1.0.0",
      "url": "https://github.com/user/gendocs"
    },
    "source_file": "README.md",
    "word_count": 1234,
    "char_count": 5678
  }
}
```

## Content Structure

### Headings Array

Hierarchical tree of document headings for navigation and table of contents generation.

**Structure:**

```json
{
  "headings": [
    {
      "id": "getting-started",
      "level": 1,
      "text": "Getting Started",
      "children": [
        {
          "id": "installation",
          "level": 2,
          "text": "Installation",
          "children": []
        }
      ]
    }
  ]
}
```

**Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Unique identifier (URL-safe slug) |
| `level` | number | Heading level (1-6) |
| `text` | string | Heading text (plain text, no markdown) |
| `children` | array | Nested child headings |

**Use Cases:**

- Generate table of contents
- Create navigation menus
- Build documentation site structure
- Enable anchor link navigation

### Elements Array

Flat list of all document elements in document order. Each element has a `type` field indicating its kind.

**Element Types:**

#### 1. Paragraph

```json
{
  "type": "paragraph",
  "content": "This is a paragraph with **bold** and *italic* text."
}
```

**Fields:**
- `type`: "paragraph"
- `content`: Text content with inline markdown formatting

#### 2. Heading

```json
{
  "type": "heading",
  "level": 2,
  "text": "Section Title"
}
```

**Fields:**
- `type`: "heading"
- `level`: Heading level (1-6)
- `text`: Heading text (plain)

#### 3. Code Block

```json
{
  "type": "code_block",
  "language": "go",
  "code": "func main() {\n    fmt.Println(\"Hello\")\n}",
  "lines": 3
}
```

**Fields:**
- `type`: "code_block"
- `language`: Language identifier (or empty if none)
- `code`: Code content
- `lines`: Number of lines

#### 4. List

```json
{
  "type": "list",
  "list_type": "unordered",
  "items": [
    {
      "content": "First item",
      "items": []
    }
  ]
}
```

**Fields:**
- `type`: "list"
- `list_type`: "unordered", "ordered", or "task"
- `start`: (optional) Starting number for ordered lists
- `items`: Array of list items (can be nested)

**List Item Fields:**
- `content`: Item text content
- `checked`: (optional) Boolean for task lists
- `items`: Nested sub-items

#### 5. Table

```json
{
  "type": "table",
  "header": [
    {"content": "Column 1", "alignment": "left"},
    {"content": "Column 2", "alignment": "center"}
  ],
  "rows": [
    [
      {"content": "Value 1"},
      {"content": "Value 2"}
    ]
  ]
}
```

**Fields:**
- `type`: "table"
- `header`: Array of header cells with alignment
- `rows`: Array of arrays (table rows)

**Cell Fields:**
- `content`: Cell text content
- `alignment`: "left", "center", "right", or null

#### 6. Blockquote

```json
{
  "type": "blockquote",
  "content": "This is a quoted text",
  "elements": []
}
```

**Fields:**
- `type`: "blockquote"
- `content`: Quote text content
- `elements`: Optional nested elements

#### 7. Thematic Break (Horizontal Rule)

```json
{
  "type": "thematic_break"
}
```

#### 8. Link

```json
{
  "type": "link",
  "url": "https://example.com",
  "title": "Link Title",
  "text": "Click here"
}
```

**Fields:**
- `type`: "link"
- `url`: Target URL
- `title`: Optional link title
- `text`: Link text

#### 9. Image

```json
{
  "type": "image",
  "url": "https://example.com/image.png",
  "title": "Image Title",
  "alt": "Alternative text"
}
```

**Fields:**
- `type`: "image"
- `url`: Image URL
- `title`: Optional image title
- `alt`: Alt text

## Complete Example

### Input Markdown

```markdown
# Getting Started

Welcome to **Gendocs**! This tool helps you generate documentation.

## Features

- Easy to use
- Powerful features
- Flexible configuration

### Installation

```bash
go install github.com/user/gendocs@latest
```

## Quick Start

1. Install Gendocs
2. Run `gendocs init`
3. Generate docs

| Option | Default | Description |
|:-------|:-------:|------------:|
| output | docs   | Output directory |
| format | html   | Export format |

> **Note:** This is important information.

[Visit GitHub](https://github.com/user/gendocs)
```

### Output JSON (simplified)

```json
{
  "metadata": {
    "title": "Getting Started",
    "generated_at": "2025-12-29T10:30:00Z",
    "generator": {
      "name": "Gendocs",
      "version": "1.0.0",
      "url": "https://github.com/user/gendocs"
    },
    "source_file": "README.md"
  },
  "content": {
    "headings": [
      {
        "id": "getting-started",
        "level": 1,
        "text": "Getting Started",
        "children": [
          {
            "id": "features",
            "level": 2,
            "text": "Features",
            "children": [
              {
                "id": "installation",
                "level": 3,
                "text": "Installation",
                "children": []
              }
            ]
          },
          {
            "id": "quick-start",
            "level": 2,
            "text": "Quick Start",
            "children": []
          }
        ]
      }
    ],
    "elements": [
      {"type": "heading", "level": 1, "text": "Getting Started"},
      {"type": "paragraph", "content": "Welcome to **Gendocs**! This tool helps you generate documentation."},
      {"type": "heading", "level": 2, "text": "Features"},
      {
        "type": "list",
        "list_type": "unordered",
        "items": [
          {"content": "Easy to use", "items": []},
          {"content": "Powerful features", "items": []},
          {"content": "Flexible configuration", "items": []}
        ]
      },
      {"type": "heading", "level": 3, "text": "Installation"},
      {
        "type": "code_block",
        "language": "bash",
        "code": "go install github.com/user/gendocs@latest\n",
        "lines": 1
      },
      {"type": "heading", "level": 2, "text": "Quick Start"},
      {
        "type": "list",
        "list_type": "ordered",
        "start": 1,
        "items": [
          {"content": "Install Gendocs", "items": []},
          {"content": "Run `gendocs init`", "items": []},
          {"content": "Generate docs", "items": []}
        ]
      },
      {
        "type": "table",
        "header": [
          {"content": "Option", "alignment": "left"},
          {"content": "Default", "alignment": "center"},
          {"content": "Description", "alignment": "right"}
        ],
        "rows": [
          [
            {"content": "output"},
            {"content": "docs"},
            {"content": "Output directory"}
          ],
          [
            {"content": "format"},
            {"content": "html"},
            {"content": "Export format"}
          ]
        ]
      },
      {
        "type": "blockquote",
        "content": "**Note:** This is important information.",
        "elements": []
      },
      {
        "type": "link",
        "url": "https://github.com/user/gendocs",
        "title": "",
        "text": "Visit GitHub"
      }
    ]
  }
}
```

## Usage Examples

### Generate Table of Contents

```javascript
const fs = require('fs');
const doc = JSON.parse(fs.readFileSync('docs.json', 'utf8'));

function generateTOC(headings, level = 0) {
  const indent = '  '.repeat(level);
  return headings.map(h => {
    const children = h.children.length > 0
      ? '\n' + generateTOC(h.children, level + 1)
      : '';
    return `${indent}- [${h.text}](#${h.id})${children}`;
  }).join('\n');
}

console.log(generateTOC(doc.content.headings));
```

### Extract All Code Blocks

```bash
# Extract all Go code blocks
jq '.content.elements[] | select(.type == "code_block" and .language == "go")' docs.json

# Extract code with language info
jq '.content.elements[] | select(.type == "code_block") | {language, lines: .lines}' docs.json
```

### Convert to HTML

```python
import json

def element_to_html(element):
    if element['type'] == 'paragraph':
        return f"<p>{element['content']}</p>"
    elif element['type'] == 'heading':
        return f"<h{element['level']}>{element['text']}</h{element['level']}>"
    elif element['type'] == 'code_block':
        return f"<pre><code>{element['code']}</code></pre>"
    # ... handle other types
    return ''

with open('docs.json') as f:
    doc = json.load(f)

html = '\n'.join(element_to_html(el) for el in doc['content']['elements'])
print(html)
```

### Index for Search

```javascript
const doc = JSON.parse(fs.readFileSync('docs.json', 'utf8'));

const index = {
  title: doc.metadata.title,
  url: doc.metadata.source_file,
  sections: doc.content.headings.map(h => ({
    title: h.text,
    anchor: h.id,
    level: h.level
  })),
  content: doc.content.elements
    .filter(el => el.type === 'paragraph')
    .map(el => el.content)
    .join(' ')
};

// Send to search engine
indexInSearchEngine(index);
```

### Count Words by Section

```python
import json

def count_words_in_section(elements, start_idx, end_idx):
    word_count = 0
    for el in elements[start_idx:end_idx]:
        if el['type'] == 'paragraph':
            word_count += len(el['content'].split())
    return word_count

with open('docs.json') as f:
    doc = json.load(f)

# Simple section counting based on heading positions
elements = doc['content']['elements']
heading_positions = [i for i, el in enumerate(elements) if el['type'] == 'heading']

for i, pos in enumerate(heading_positions):
    end_pos = heading_positions[i + 1] if i + 1 < len(heading_positions) else len(elements)
    heading = elements[pos]
    words = count_words_in_section(elements, pos + 1, end_pos)
    print(f"{heading['text']}: {words} words")
```

## Best Practices

### 1. Validate JSON Structure

Always validate the JSON output before processing:

```bash
# Validate JSON syntax
jq empty docs.json

# Check for required fields
jq '.metadata.title' docs.json
```

### 2. Handle Missing Fields

Some fields are optional. Always check before accessing:

```javascript
// Good
const wordCount = doc.metadata.word_count || 0;

// Bad - will error if undefined
const wordCount = doc.metadata.word_count;
```

### 3. Use Version Information

Check the generator version to ensure compatibility:

```javascript
if (doc.metadata.generator.version !== '1.0.0') {
  console.warn('Version mismatch, expected 1.0.0');
}
```

### 4. Preserve IDs

Heading IDs are URL-safe and unique. Use them for:
- Anchor links in HTML
- Fragment identifiers
- Reference keys

### 5. Handle Nested Structures

Lists and headings can be deeply nested. Use recursive functions:

```javascript
function processList(items, depth = 0) {
  items.forEach(item => {
    console.log(`${'  '.repeat(depth)}- ${item.content}`);
    if (item.items) {
      processList(item.items, depth + 1);
    }
  });
}
```

## Validation Rules

The JSON output follows these rules:

1. **Required Fields**: `metadata.title`, `metadata.generated_at`, `metadata.generator`
2. **Heading Levels**: Values are 1-6
3. **List Types**: Values are "unordered", "ordered", or "task"
4. **Cell Alignment**: Values are "left", "center", "right", or null
5. **Timestamp Format**: ISO 8601 (RFC 3339)
6. **Unique IDs**: Heading IDs are unique within a document

## Troubleshooting

### Empty headings array

**Issue**: No headings in output

**Cause**: Document has no markdown headings

**Solution**: Ensure document uses `#` for headings, not underlines

### Missing word count

**Issue**: `word_count` field not present

**Cause**: Word counting is optional and may not be calculated

**Solution**: Use `doc.metadata.word_count || 0` for safe access

### Malformed JSON

**Issue**: JSON parsing fails

**Cause**: Export process error or file corruption

**Solution**:
```bash
# Validate JSON
jq . docs.json

# Re-export
gendocs generate export --input README.md --output docs.json --format json
```

## Performance Considerations

- **File Size**: JSON files can be 2-3x larger than source markdown
- **Parsing**: Modern JSON parsers are fast (<100ms for typical docs)
- **Memory**: For very large documents (>10MB), consider streaming parsers

## Advanced Features

### Future Extensibility

The JSON structure is designed for forward compatibility:

- New fields can be added without breaking existing parsers
- New element types will be added with clear `type` identifiers
- Unknown element types can be safely ignored

### Custom Processing

You can transform the JSON to suit your needs:

```javascript
// Flatten heading hierarchy
const flatHeadings = [];
function flatten(headings) {
  headings.forEach(h => {
    flatHeadings.push({id: h.id, text: h.text, level: h.level});
    if (h.children) flatten(h.children);
  });
}
flatten(doc.content.headings);
```

## See Also

- [Export Guide](./EXPORT.md) - General export documentation
- [JSON Structure Design](./.auto-claude/specs/008-add-json-exporter-for-structured-documentation-dat/json_structure_design.md) - Detailed technical design
- [Example Output](./.auto-claude/specs/008-add-json-exporter-for-structured-documentation-dat/example_output.json) - Complete example
</file>
<file path="docs/TESTING.md">
# Testing Guide

**Project:** Gendocs
**Last Updated:** 2025-12-23

---

## Table of Contents

1. [Overview](#overview)
2. [Test Organization](#test-organization)
3. [Running Tests](#running-tests)
4. [Writing Unit Tests](#writing-unit-tests)
5. [Writing Integration Tests](#writing-integration-tests)
6. [Test Helpers](#test-helpers)
7. [Coverage Requirements](#coverage-requirements)
8. [Best Practices](#best-practices)

---

## Overview

Gendocs employs a comprehensive testing strategy with three test levels:

1. **Unit Tests** - Test individual components in isolation
2. **Integration Tests** - Test component interactions
3. **Manual Tests** - Real LLM API validation (pre-release)

### Current Status

| Component | Unit Tests | Integration Tests | Coverage |
|-----------|------------|-------------------|----------|
| LLM Clients | âœ… 31 tests | N/A | ~85% |
| Tools | âœ… 29 tests | âœ… Included | ~90% |
| Prompts | âœ… 17 tests | N/A | ~85% |
| Config | âœ… 20 tests | N/A | ~80% |
| Validation | âœ… 25 tests | N/A | ~95% |
| Agents | âœ… Basic | âœ… 6 tests | ~70% |
| **TOTAL** | **~147 tests** | **6 tests** | **~82%** |

---

## Test Organization

### File Structure

```
internal/
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ openai.go
â”‚   â”œâ”€â”€ openai_test.go          # Unit tests
â”‚   â”œâ”€â”€ anthropic.go
â”‚   â””â”€â”€ anthropic_test.go
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ analyzer.go
â”‚   â”œâ”€â”€ analyzer_test.go        # Unit tests
â”‚   â””â”€â”€ analyzer_integration_test.go  # Integration tests
â””â”€â”€ testing/
    â”œâ”€â”€ helpers.go               # Test utilities
    â””â”€â”€ fixtures.go              # Sample data
```

### Naming Conventions

- Unit tests: `*_test.go`
- Integration tests: `*_integration_test.go`
- Test functions: `Test<Component>_<Method>_<Scenario>`

**Examples:**
```go
func TestOpenAIClient_GenerateCompletion_Success(t *testing.T)
func TestFileReadTool_Execute_FileNotFound(t *testing.T)
func TestAnalyzerAgent_CompleteFlow(t *testing.T)  // Integration
```

### Build Tags

Integration tests use build tags:

```go
// +build integration

package agents

func TestAnalyzerAgent_CompleteFlow(t *testing.T) {
    if testing.Short() {
        t.Skip("Skipping integration test in short mode")
    }
    // ...
}
```

---

## Running Tests

### All Tests

```bash
# Run all tests (unit + integration)
make test

# With verbose output
make test-verbose

# With coverage report
make test-coverage
```

### Unit Tests Only

```bash
# Fast unit tests only (skip integration)
make test-short

# Or directly
go test -short -race ./...
```

### Integration Tests

```bash
# Run integration tests
go test -tags integration ./...

# Specific package
go test -tags integration ./internal/agents/
```

### Specific Tests

```bash
# Run single test
go test -run TestOpenAIClient_GenerateCompletion_Success ./internal/llm/

# Run test pattern
go test -run TestFileReadTool ./internal/tools/
```

### Coverage

```bash
# Generate coverage report
make test-coverage

# View HTML report
go tool cover -html=coverage/coverage.out

# Check coverage for specific package
go test -cover ./internal/llm/
```

---

## Writing Unit Tests

### Basic Structure

```go
package mypackage

import (
    "testing"
)

func TestMyFunction_Success(t *testing.T) {
    // Arrange
    input := "test"

    // Act
    result, err := MyFunction(input)

    // Assert
    if err != nil {
        t.Fatalf("Expected no error, got %v", err)
    }

    if result != "expected" {
        t.Errorf("Expected 'expected', got '%s'", result)
    }
}
```

### Table-Driven Tests

```go
func TestMyFunction_Various(t *testing.T) {
    tests := []struct {
        name    string
        input   string
        want    string
        wantErr bool
    }{
        {
            name:    "valid input",
            input:   "test",
            want:    "TEST",
            wantErr: false,
        },
        {
            name:    "empty input",
            input:   "",
            want:    "",
            wantErr: true,
        },
    }

    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            got, err := MyFunction(tt.input)

            if (err != nil) != tt.wantErr {
                t.Errorf("MyFunction() error = %v, wantErr %v", err, tt.wantErr)
                return
            }

            if got != tt.want {
                t.Errorf("MyFunction() = %v, want %v", got, tt.want)
            }
        })
    }
}
```

### Testing LLM Clients

```go
func TestOpenAIClient_GenerateCompletion_Success(t *testing.T) {
    // Setup mock HTTP server
    server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        // Validate request
        assert.Equal(t, "Bearer test-key", r.Header.Get("Authorization"))

        // Return mock response
        w.Write([]byte(`{
            "choices": [{
                "message": {"content": "test response"}
            }],
            "usage": {"prompt_tokens": 10, "completion_tokens": 5}
        }`))
    }))
    defer server.Close()

    // Create client
    client := NewOpenAIClient(config.LLMConfig{
        APIKey: "test-key",
        BaseURL: server.URL,
    }, nil)

    // Execute
    resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
        Messages: []Message{{Role: "user", Content: "test"}},
    })

    // Verify
    assert.NoError(t, err)
    assert.Equal(t, "test response", resp.Content)
}
```

### Testing Tools

```go
func TestFileReadTool_Execute_Success(t *testing.T) {
    // Create temp directory
    tmpDir := t.TempDir()
    testFile := filepath.Join(tmpDir, "test.txt")
    os.WriteFile(testFile, []byte("test content"), 0644)

    // Create tool
    tool := NewFileReadTool(3)

    // Execute
    result, err := tool.Execute(context.Background(), map[string]interface{}{
        "file_path": testFile,
    })

    // Verify
    assert.NoError(t, err)
    assert.Contains(t, result.(map[string]interface{})["content"], "test content")
}
```

---

## Writing Integration Tests

### Agent Flow Tests

```go
// +build integration

func TestAnalyzerAgent_CompleteFlow(t *testing.T) {
    if testing.Short() {
        t.Skip("Skipping integration test")
    }

    // 1. Create test repository
    repoPath := testing.CreateTempRepo(t, testing.SampleGoProject())

    // 2. Create mock LLM with predefined responses
    mockClient := &testing.MockLLMClient{
        Responses: []llm.CompletionResponse{
            // Tool call to list files
            {ToolCalls: []llm.ToolCall{{Name: "list_files", ...}}},
            // Tool call to read file
            {ToolCalls: []llm.ToolCall{{Name: "read_file", ...}}},
            // Final analysis
            {Content: "# Analysis\n\nThis is a Go project..."},
        },
    }

    // 3. Create agent with dependencies
    agent, _ := NewAnalyzerAgent(...)
    agent.llmClient = mockClient

    // 4. Execute
    result, err := agent.Run(context.Background())

    // 5. Verify
    assert.NoError(t, err)
    assert.Contains(t, result, "Analysis")
    assert.Equal(t, 3, mockClient.CallCount)
}
```

### Multi-Component Tests

```go
func TestHandlerToAgentFlow(t *testing.T) {
    if testing.Short() {
        t.Skip("Skipping integration test")
    }

    // Setup
    cfg := config.AnalyzerConfig{...}
    handler := handlers.NewAnalyzeHandler(cfg, logger)

    // Execute full flow
    err := handler.Handle(context.Background())

    // Verify outputs
    assert.NoError(t, err)
    testing.AssertFileExists(t, ".ai/docs/code_structure.md")
}
```

---

## Test Helpers

### Location

All test helpers are in `internal/testing/`:

```go
import testHelpers "github.com/user/gendocs/internal/testing"
```

### MockLLMClient

```go
// Create mock with responses
mockClient := testHelpers.NewMockLLMClient(
    llm.CompletionResponse{Content: "Response 1"},
    llm.CompletionResponse{Content: "Response 2"},
)

// Use in tests
agent.llmClient = mockClient
agent.Run(ctx)

// Verify calls
assert.Equal(t, 2, mockClient.CallCount)
assert.Equal(t, "user prompt", mockClient.LastRequest.Messages[0].Content)
```

### CreateTempRepo

```go
// Create temp git repository with files
repoPath := testHelpers.CreateTempRepo(t, map[string]string{
    "main.go": "package main\n...",
    "go.mod": "module test\n...",
    "internal/db.go": "package internal\n...",
})

// Cleanup is automatic (t.TempDir())
```

### Sample Fixtures

```go
// Predefined project structures
files := testHelpers.SampleGoProject()
files := testHelpers.SamplePythonProject()

// Sample outputs
output := testHelpers.SampleAnalysisOutput()
readme := testHelpers.SampleREADME()
```

### Assertions

```go
// File operations
testHelpers.AssertFileExists(t, "README.md")
testHelpers.AssertFileNotExists(t, "invalid.txt")
testHelpers.AssertFileContains(t, "README.md", "## Features")

// YAML helpers
testHelpers.CreateYAML(t, tmpDir, "config.yaml", map[string]string{
    "key": "value",
})
```

---

## Coverage Requirements

### Minimum Coverage Targets

- **Critical Components:** 90%+
  - LLM clients
  - Tools
  - Error handling

- **Business Logic:** 80%+
  - Agents
  - Handlers
  - Configuration

- **Infrastructure:** 70%+
  - Logging
  - Worker pool

### Measuring Coverage

```bash
# Full report
make test-coverage

# Per-package
go test -cover ./internal/llm/
# ok    github.com/user/gendocs/internal/llm    1.234s  coverage: 87.5% of statements

# Detailed function coverage
go test -coverprofile=coverage.out ./...
go tool cover -func=coverage.out
```

### Coverage Gates

CI blocks merges if coverage drops below 60%:

```yaml
# .github/workflows/test.yml
- name: Check coverage
  run: |
    go test -coverprofile=coverage.out ./...
    coverage=$(go tool cover -func=coverage.out | grep total | awk '{print $3}' | sed 's/%//')
    if (( $(echo "$coverage < 60" | bc -l) )); then
      echo "Coverage $coverage% is below 60%"
      exit 1
    fi
```

---

## Best Practices

### DO âœ…

1. **Use `t.Helper()` in Helper Functions**

```go
func assertNoError(t *testing.T, err error) {
    t.Helper() // Reports correct line number on failure
    if err != nil {
        t.Fatalf("Expected no error, got: %v", err)
    }
}
```

2. **Use `t.Cleanup()` for Deferred Cleanup**

```go
func TestWithCleanup(t *testing.T) {
    server := httptest.NewServer(...)
    t.Cleanup(func() { server.Close() }) // Runs even if test fails

    // Test code...
}
```

3. **Use Subtests for Organization**

```go
func TestMyFunction(t *testing.T) {
    t.Run("success case", func(t *testing.T) { ... })
    t.Run("error case", func(t *testing.T) { ... })
    t.Run("edge case", func(t *testing.T) { ... })
}
```

4. **Test Error Messages**

```go
if !strings.Contains(err.Error(), "expected phrase") {
    t.Errorf("Error should mention 'expected phrase', got: %v", err)
}
```

5. **Use Table Tests for Multiple Scenarios**

### DON'T âŒ

1. **Don't Use `t.Fatal` in Goroutines**

```go
// WRONG
go func() {
    t.Fatal("error") // Causes panic
}()

// RIGHT
go func() {
    if err != nil {
        t.Error("error")
    }
}()
```

2. **Don't Test Implementation Details**

```go
// WRONG - tests internal behavior
assert.Equal(t, 3, client.requestCount)

// RIGHT - tests observable behavior
resp, err := client.GenerateCompletion(...)
assert.NoError(t, err)
assert.NotEmpty(t, resp.Content)
```

3. **Don't Hardcode Paths**

```go
// WRONG
tmpFile := "/tmp/test.txt"

// RIGHT
tmpDir := t.TempDir()
tmpFile := filepath.Join(tmpDir, "test.txt")
```

4. **Don't Ignore Errors**

```go
// WRONG
result, _ := MyFunction()

// RIGHT
result, err := MyFunction()
if err != nil {
    t.Fatalf("Unexpected error: %v", err)
}
```

5. **Don't Share State Between Tests**

```go
// WRONG
var globalState = map[string]string{}

func TestA(t *testing.T) {
    globalState["key"] = "value" // Affects TestB
}

func TestB(t *testing.T) {
    // Depends on TestA running first
}

// RIGHT - each test is independent
func TestA(t *testing.T) {
    state := map[string]string{}
    state["key"] = "value"
}
```

---

## Debugging Tests

### Verbose Output

```bash
# See all test output
go test -v ./...

# See logs from test
go test -v -args -debug
```

### Running Single Test

```bash
# Isolate failing test
go test -run TestMyFunction_SpecificCase ./pkg/
```

### Print Debugging

```go
func TestDebug(t *testing.T) {
    t.Logf("Debug info: %+v", someVar)  // Only shows if test fails or -v
    fmt.Printf("Always prints: %v\n", someVar)  // Always visible
}
```

### Race Detector

```bash
# Detect race conditions
go test -race ./...
```

---

## CI/CD Integration

### GitHub Actions

```yaml
name: Test
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-go@v5
        with:
          go-version: '1.22'
      - run: make test-coverage
      - uses: codecov/codecov-action@v3
        with:
          file: ./coverage/coverage.out
```

### Local Pre-Commit

```bash
#!/bin/bash
# .git/hooks/pre-commit

make test-short || exit 1
make lint || exit 1
```

---

## Troubleshooting

### Tests Hang

- Check for missing `t.Parallel()` with shared resources
- Look for blocking operations without timeouts
- Verify context cancellation is handled

### Flaky Tests

- Race conditions: Run with `-race` flag
- Timing issues: Add explicit synchronization
- External dependencies: Mock or stub

### Coverage Issues

- Use `-coverprofile` to identify uncovered lines
- Check for unreachable code
- Verify test actually exercises the code path

---

## Resources

- [Go Testing Package](https://pkg.go.dev/testing)
- [Effective Go - Testing](https://go.dev/doc/effective_go#testing)
- [Testify Assertions](https://github.com/stretchr/testify) (if added)

---

**Document Version:** 1.0
**Maintainers:** Gendocs Team
**Last Review:** 2025-12-23
</file>
<file path="examples/custom-prompts/basic-override.yaml">
# Basic Custom Prompts Example
#
# This file demonstrates how to override system prompts with custom versions.
# Copy this file to your repository's .ai/prompts/ directory and modify as needed.
#
# Usage:
#   1. Create .ai/prompts/ directory in your repository root
#   2. Copy this file: cp examples/custom-prompts/basic-override.yaml .ai/prompts/
#   3. Modify the prompts below to match your project's needs
#   4. Run gendocs commands normally - custom prompts will be automatically loaded

# Override the structure analyzer to focus on specific architectural patterns
structure_analyzer_system: |
  You are an expert software architect analyzing code structure.

  Your analysis should focus on:
  1. **Architectural Patterns**: Identify design patterns (MVC, layered, hexagonal, etc.)
  2. **Module Organization**: How code is organized into packages/modules
  3. **Component Boundaries**: Clear separation of concerns
  4. **Code Organization**: Directory structure and file naming conventions

  Provide clear, actionable insights about the architecture.

# Override the dependency analyzer to highlight security-sensitive dependencies
dependency_analyzer_system: |
  You are a security-aware dependency analyst.

  When analyzing dependencies:
  1. Identify all external dependencies and their purposes
  2. **Security Focus**: Flag dependencies that handle sensitive data (crypto, auth, database)
  3. Version Information: Note if dependencies are outdated or deprecated
  4. Dependency Graph: Show how components depend on each other

  Highlight any security implications of the dependency choices.

# Override the documenter to use a specific tone and structure
documenter_system_prompt: |
  You are a technical writer creating clear, professional documentation.

  Documentation Guidelines:
  - **Clarity First**: Use simple, direct language
  - **Structure**: Always include Overview, Features, Usage, and Examples sections
  - **Code Examples**: Provide practical examples for all major features
  - **Audience**: Assume readers are developers with general programming knowledge

  Create documentation that helps developers quickly understand and use the project.
</file>
<file path="examples/custom-prompts/enterprise-docs.yaml">
# Enterprise Documentation Style Custom Prompts
#
# This example enforces enterprise documentation standards with formal tone,
# compliance considerations, and comprehensive documentation.
#
# Usage: Copy to .ai/prompts/ for enterprise projects

documenter_system_prompt: |
  You are a senior technical writer creating enterprise-grade documentation.

  Follow these standards:

  ## Style Guidelines
  - **Formal Tone**: Professional, authoritative language
  - **Completeness**: Cover all aspects thoroughly
  - **Accuracy**: Verify all technical claims
  - **Consistency**: Use standard terminology throughout

  ## Required Sections

  ### 1. Executive Summary
  - Business value and purpose
  - Target users and use cases
  - Key capabilities

  ### 2. Architecture Overview
  - System architecture with diagrams (describe in detail)
  - Component relationships
  - Technology stack
  - Integration points

  ### 3. Security & Compliance
  - Authentication and authorization
  - Data protection measures
  - Compliance requirements (GDPR, SOC2, HIPAA as applicable)
  - Audit logging

  ### 4. Installation & Configuration
  - System requirements
  - Step-by-step installation
  - Configuration options
  - Environment setup

  ### 5. API Documentation
  - All endpoints with examples
  - Request/response formats
  - Error codes and handling
  - Rate limiting and quotas

  ### 6. Operations Guide
  - Deployment procedures
  - Monitoring and alerting
  - Backup and recovery
  - Troubleshooting

  ### 7. Development Guide
  - Development environment setup
  - Coding standards
  - Testing requirements
  - CI/CD pipeline

  Target audience: Enterprise IT teams, compliance officers, and senior management.

ai_rules_system_prompt: |
  You are creating AI assistant rules for an enterprise codebase.

  Enforce these standards:

  ## Code Quality Standards
  - **Documentation**: All public APIs must have comprehensive comments
  - **Testing**: Minimum 80% code coverage required
  - **Security**: Follow OWASP top 10 guidelines
  - **Performance**: Profile before optimizing

  ## Enterprise Requirements
  - **Logging**: Use structured logging for audit trails
  - **Error Handling**: Never expose internal errors to users
  - **Configuration**: All config must be externalized
  - **Secrets Management**: Never commit secrets, use secret management service

  ## Compliance
  - **Data Privacy**: Mark all PII/PHI data handling
  - **Audit Logging**: Log all data access and modifications
  - **Access Control**: Implement role-based access control
  - **Data Retention**: Follow data retention policies

  ## Review Checklist
  Before submitting code:
  - [ ] All functions documented with purpose, parameters, and return values
  - [ ] Unit tests written and passing
  - [ ] Integration tests updated if needed
  - [ ] Security scan completed
  - [ ] Performance impact assessed
  - [ ] Compliance requirements verified

  Emphasize production-ready, maintainable, secure code.

structure_analyzer_system: |
  You are analyzing enterprise software architecture.

  Focus on:
  1. **Enterprise Patterns**: Identify enterprise patterns (SOA, microservices, monolithic)
  2. **Layering**: Presentation, business logic, data access separation
  3. **Modularity**: How well components are isolated and reusable
  4. **Scalability**: Architecture's ability to scale horizontally/vertically
  5. **Maintainability**: Code organization for long-term maintenance

  Evaluate against enterprise architecture best practices.

dependency_analyzer_system: |
  You are analyzing dependencies in enterprise software.

  Focus on:
  1. **License Compliance**: Identify license types of all dependencies
  2. **Security Vulnerabilities**: Flag known CVEs in dependencies
  3. **Enterprise Support**: Whether dependencies have enterprise support
  4. **Version Management**: Dependency version consistency across modules
  5. **Vendor Lock-in**: Identify dependencies that create vendor lock-in

  Highlight compliance and security risks.
</file>
<file path="examples/custom-prompts/microservices.yaml">
# Microservices Architecture Custom Prompts
#
# This example is tailored for analyzing microservices-based systems.
# It focuses on service boundaries, API contracts, and distributed system concerns.
#
# Usage: Copy to .ai/prompts/ in your microservices repository

structure_analyzer_system: |
  You are an expert in microservices architecture and distributed systems.

  When analyzing structure, focus on:
  1. **Service Boundaries**: Identify individual services and their responsibilities
  2. **Bounded Contexts**: Map services to domain-driven design bounded contexts
  3. **Service Organization**: How services are organized (by feature, by layer, etc.)
  4. **Shared Components**: Identify common libraries or shared infrastructure
  5. **Configuration Management**: How services are configured and deployed

  Evaluate whether service boundaries follow single responsibility principle.

dependency_analyzer_system: |
  You are analyzing dependencies in a microservices architecture.

  Focus on:
  1. **Service Dependencies**: Which services depend on other services
  2. **External Dependencies**: Third-party services (databases, message queues, caches)
  3. **Circular Dependencies**: Flag any circular service dependencies
  4. **Shared Libraries**: Common code shared across services
  5. **Infrastructure Dependencies**: Docker, Kubernetes, service mesh components

  Identify potential bottlenecks or single points of failure.

data_flow_analyzer_system: |
  You are analyzing data flow in a distributed microservices system.

  Focus on:
  1. **Data Ownership**: Which service owns which data
  2. **Data Flow Patterns**: Event-driven, request-response, pub/sub
  3. **Message Passing**: How services communicate (REST, gRPC, message queues)
  4. **Data Consistency**: How consistency is maintained across services
  5. **Event Sourcing**: If event sourcing or CQRS patterns are used

  Identify data flow bottlenecks and consistency concerns.

request_flow_analyzer_system: |
  You are analyzing request flows in a microservices architecture.

  Focus on:
  1. **API Gateway**: Entry points and routing logic
  2. **Service Orchestration**: How requests flow through multiple services
  3. **Request Patterns**: Synchronous vs asynchronous communication
  4. **Circuit Breakers**: Resilience patterns (retries, timeouts, fallbacks)
  5. **Tracing**: How requests can be traced across services

  Identify potential cascading failure scenarios.

api_analyzer_system: |
  You are analyzing APIs in a microservices system.

  Focus on:
  1. **API Contracts**: REST, gRPC, GraphQL endpoints
  2. **Versioning Strategy**: How API versions are managed
  3. **API Gateway**: Routing, rate limiting, authentication
  4. **Inter-service Communication**: Internal vs external APIs
  5. **API Documentation**: OpenAPI/Swagger specifications

  Evaluate API design quality and versioning strategy.

documenter_system_prompt: |
  You are documenting a microservices-based system.

  Structure your documentation as:

  # System Overview
  - High-level architecture diagram (describe the services)
  - Service responsibilities
  - Communication patterns

  # Services
  For each service:
  - Purpose and responsibility
  - API endpoints
  - Dependencies (internal and external)
  - Data ownership

  # Deployment
  - Infrastructure requirements
  - Service discovery mechanism
  - Configuration management

  # Development Guide
  - How to run services locally
  - Testing strategies
  - Debugging distributed traces

  Target audience: Backend engineers working with distributed systems.

ai_rules_system_prompt: |
  You are creating AI assistant rules for a microservices codebase.

  Include guidance for:
  1. **Service Boundaries**: How to respect service boundaries when making changes
  2. **API Contracts**: Never break existing API contracts without versioning
  3. **Distributed Tracing**: How to add tracing to new code
  4. **Testing**: Unit tests, integration tests, and contract tests
  5. **Configuration**: Use environment variables, not hardcoded config
  6. **Error Handling**: Proper error handling for network failures

  Emphasize distributed system best practices.
</file>
<file path="examples/custom-prompts/README.md">
# Custom Prompts Examples

This directory contains example custom prompt configurations that demonstrate how to tailor Gendocs' AI analysis to different use cases.

## Quick Start

1. **Choose an example** that matches your project type
2. **Copy to your project**:
   ```bash
   # From your project root
   mkdir -p .ai/prompts
   cp /path/to/gendocs/examples/custom-prompts/basic-override.yaml .ai/prompts/
   ```
3. **Customize** the prompts to match your specific needs
4. **Run Gendocs** - your custom prompts will be automatically loaded

## Available Examples

### 1. basic-override.yaml
**Best for:** Getting started with custom prompts

Simple examples of overriding:
- Structure analyzer (focus on architectural patterns)
- Dependency analyzer (security-aware)
- Documenter (clear, professional tone)

**Use this if:** You want to customize a few prompts without overwhelming changes.

### 2. microservices.yaml
**Best for:** Microservices and distributed systems

Comprehensive prompts tailored for:
- Service boundary analysis
- API contract evaluation
- Data flow in distributed systems
- Request tracing and resilience patterns

**Use this if:** Your project uses microservices, service-oriented architecture, or has multiple communicating services.

### 3. enterprise-docs.yaml
**Best for:** Enterprise software with compliance requirements

Enforces enterprise standards:
- Formal documentation style
- Security and compliance focus
- Comprehensive operations guides
- Audit trail requirements

**Use this if:** Your project requires enterprise-grade documentation, compliance certifications, or strict security standards.

## Customization Tips

### Mix and Match
You can combine prompts from different examples:

```bash
# Copy base template
cp examples/custom-prompts/basic-override.yaml .ai/prompts/my-prompts.yaml

# Edit to add specific overrides
vim .ai/prompts/my-prompts.yaml
```

### Partial Overrides
You don't need to override all prompts - only override what you need:

```yaml
# .ai/prompts/custom.yaml
# Only override the documenter, keep other prompts as default
documenter_system_prompt: |
  Your custom documenter prompt here...
```

### Multiple Files
The system loads all `.yaml` and `.yml` files in `.ai/prompts/`:

```
.ai/prompts/
â”œâ”€â”€ analyzers.yaml      # Custom analyzer prompts
â”œâ”€â”€ documentation.yaml  # Custom documenter prompts
â””â”€â”€ style-guide.yaml    # Project-specific style rules
```

If the same prompt appears in multiple files, the last one loaded wins.

## Prompt Template Variables

All prompts support Go `text/template` syntax for dynamic content:

```yaml
structure_analyzer_system: |
  Analyzing repository: {{.RepoPath}}
  Project: {{.ProjectName}}
  Language: {{.Language}}
```

Available variables depend on the context - check the system prompts in `prompts/` for examples.

## Testing Your Custom Prompts

1. **Verify Loading**: Run with verbose logging to see which prompts are loaded
   ```bash
   ./gendocs analyze --repo-path . --log-level debug
   ```

2. **Check Output**: Compare analysis results before and after customization

3. **Iterate**: Refine prompts based on the quality of generated documentation

## Troubleshooting

### "Missing required prompts" error
- Ensure you're overriding, not replacing the system prompts
- All required prompts must exist (either in system or project prompts)
- Check prompt names match exactly (case-sensitive)

### Custom prompts not being used
- Verify `.ai/prompts/` directory is in your repository root (not in `.ai/` of Gendocs)
- Check file extension is `.yaml` or `.yml`
- Ensure YAML syntax is valid: `yamllint .ai/prompts/*.yaml`

### Prompts from multiple files conflict
- Check which file is loaded last (alphabetical order)
- Rename files to control load order: `01-base.yaml`, `02-custom.yaml`

## Contributing

Have a useful custom prompt configuration? Consider contributing it:

1. Create a new example file following the existing format
2. Add comprehensive comments explaining the use case
3. Update this README with a description
4. Submit a pull request

## Support

- **Documentation**: See main README.md section on "Custom Prompts"
- **System Prompts Reference**: Check `prompts/` directory for available prompt names
- **Issues**: Report problems at https://github.com/user/gendocs/issues
</file>
<file path="examples/json-export/complete-example.json">
{
  "metadata": {
    "title": "Complete Example: JSON Export Features",
    "generated_at": "2025-12-29T10:30:00Z",
    "generator": {
      "name": "Gendocs",
      "version": "1.0.0",
      "url": "https://github.com/user/gendocs"
    },
    "source_file": "complete-example.md",
    "word_count": 387,
    "char_count": 2145
  },
  "content": {
    "headings": [
      {
        "id": "complete-example-json-export-features",
        "level": 1,
        "text": "Complete Example: JSON Export Features",
        "children": [
          {
            "id": "introduction",
            "level": 2,
            "text": "Introduction",
            "children": [
              {
                "id": "why-use-json-export",
                "level": 3,
                "text": "Why Use JSON Export?",
                "children": []
              }
            ]
          },
          {
            "id": "text-formatting",
            "level": 2,
            "text": "Text Formatting",
            "children": []
          },
          {
            "id": "lists",
            "level": 2,
            "text": "Lists",
            "children": [
              {
                "id": "unordered-list",
                "level": 3,
                "text": "Unordered List",
                "children": []
              },
              {
                "id": "ordered-list",
                "level": 3,
                "text": "Ordered List",
                "children": []
              },
              {
                "id": "task-list",
                "level": 3,
                "text": "Task List",
                "children": []
              }
            ]
          },
          {
            "id": "code-blocks",
            "level": 2,
            "text": "Code Blocks",
            "children": []
          },
          {
            "id": "tables",
            "level": 2,
            "text": "Tables",
            "children": []
          },
          {
            "id": "blockquotes",
            "level": 2,
            "text": "Blockquotes",
            "children": []
          },
          {
            "id": "links-and-images",
            "level": 2,
            "text": "Links and Images",
            "children": [
              {
                "id": "different-link-types",
                "level": 3,
                "text": "Different Link Types",
                "children": []
              },
              {
                "id": "images",
                "level": 3,
                "text": "Images",
                "children": []
              }
            ]
          },
          {
            "id": "horizontal-rules",
            "level": 2,
            "text": "Horizontal Rules",
            "children": []
          },
          {
            "id": "advanced-features",
            "level": 2,
            "text": "Advanced Features",
            "children": [
              {
                "id": "inline-formatting-combinations",
                "level": 3,
                "text": "Inline Formatting Combinations",
                "children": []
              },
              {
                "id": "special-characters",
                "level": 3,
                "text": "Special Characters",
                "children": []
              }
            ]
          },
          {
            "id": "deep-nesting",
            "level": 2,
            "text": "Deep Nesting",
            "children": []
          },
          {
            "id": "code-examples-with-special-cases",
            "level": 2,
            "text": "Code Examples with Special Cases",
            "children": []
          },
          {
            "id": "final-notes",
            "level": 2,
            "text": "Final Notes",
            "children": []
          }
        ]
      }
    ],
    "elements": [
      {
        "type": "heading",
        "level": 1,
        "text": "Complete Example: JSON Export Features"
      },
      {
        "type": "paragraph",
        "content": "This document demonstrates all the features supported by the JSON exporter."
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Introduction"
      },
      {
        "type": "paragraph",
        "content": "The JSON exporter converts **Markdown** documents into *structured* JSON format for programmatic processing."
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Why Use JSON Export?"
      },
      {
        "type": "list",
        "list_type": "unordered",
        "items": [
          {
            "content": "**Search indexing**: Feed documentation into search engines",
            "items": []
          },
          {
            "content": "**API integration**: Process docs with custom tools",
            "items": []
          },
          {
            "content": "**Data analysis**: Analyze documentation structure",
            "items": []
          },
          {
            "content": "**Content migration**: Convert between documentation systems",
            "items": []
          }
        ]
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Text Formatting"
      },
      {
        "type": "paragraph",
        "content": "This paragraph shows **bold text**, *italic text*, `inline code`, and [links](https://example.com)."
      },
      {
        "type": "paragraph",
        "content": "You can also combine **bold and *italic* together**, or use `code` in the middle of sentences."
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Lists"
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Unordered List"
      },
      {
        "type": "list",
        "list_type": "unordered",
        "items": [
          {
            "content": "First item",
            "items": []
          },
          {
            "content": "Second item",
            "items": [
              {
                "content": "Nested item",
                "items": []
              },
              {
                "content": "Another nested item",
                "items": []
              }
            ]
          },
          {
            "content": "Third item",
            "items": []
          }
        ]
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Ordered List"
      },
      {
        "type": "list",
        "list_type": "ordered",
        "start": 1,
        "items": [
          {
            "content": "First step",
            "items": []
          },
          {
            "content": "Second step",
            "items": [
              {
                "content": "Nested step 1",
                "items": []
              },
              {
                "content": "Nested step 2",
                "items": []
              }
            ]
          },
          {
            "content": "Third step",
            "items": []
          }
        ]
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Task List"
      },
      {
        "type": "list",
        "list_type": "task",
        "items": [
          {
            "content": "Completed task",
            "checked": true,
            "items": []
          },
          {
            "content": "Incomplete task",
            "checked": false,
            "items": []
          },
          {
            "content": "Another completed task",
            "checked": true,
            "items": [
              {
                "content": "Nested completed task",
                "checked": true,
                "items": []
              },
              {
                "content": "Nested incomplete task",
                "checked": false,
                "items": []
              }
            ]
          }
        ]
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Code Blocks"
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Go Example"
      },
      {
        "type": "code_block",
        "language": "go",
        "code": "func main() {\n    fmt.Println(\"Hello, World!\")\n}\n",
        "lines": 3
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Python Example"
      },
      {
        "type": "code_block",
        "language": "python",
        "code": "def hello():\n    print(\"Hello, World!\")\n",
        "lines": 2
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Bash Example"
      },
      {
        "type": "code_block",
        "language": "bash",
        "code": "echo \"Hello, World!\"\n",
        "lines": 1
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Without Language"
      },
      {
        "type": "code_block",
        "language": "",
        "code": "This is a code block without language specification.\n",
        "lines": 1
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Tables"
      },
      {
        "type": "table",
        "header": [
          {
            "content": "Feature",
            "alignment": "left"
          },
          {
            "content": "Status",
            "alignment": "center"
          },
          {
            "content": "Priority",
            "alignment": "center"
          },
          {
            "content": "Notes",
            "alignment": "right"
          }
        ],
        "rows": [
          [
            {
              "content": "JSON Export"
            },
            {
              "content": "âœ… Done"
            },
            {
              "content": "High"
            },
            {
              "content": "Initial implementation"
            }
          ],
          [
            {
              "content": "HTML Export"
            },
            {
              "content": "âœ… Done"
            },
            {
              "content": "High"
            },
            {
              "content": "Production ready"
            }
          ],
          [
            {
              "content": "PDF Export"
            },
            {
              "content": "ğŸš§ WIP"
            },
            {
              "content": "Medium"
            },
            {
              "content": "Planned for v2.0"
            }
          ]
        ]
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Blockquotes"
      },
      {
        "type": "blockquote",
        "content": "This is a simple blockquote.\n\nIt can span multiple lines.",
        "elements": []
      },
      {
        "type": "blockquote",
        "content": "**Note:** Blockquotes can contain formatting like **bold** and `code`.",
        "elements": []
      },
      {
        "type": "blockquote",
        "content": "Nested blockquotes are supported:",
        "elements": []
      },
      {
        "type": "blockquote",
        "content": "This is nested inside another blockquote.",
        "elements": []
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Links and Images"
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Different Link Types"
      },
      {
        "type": "paragraph",
        "content": "- Inline link: [GitHub](https://github.com)\n- Reference link: [GitLab][1]\n- Autolink: https://example.com"
      },
      {
        "type": "paragraph",
        "content": "[1]: https://gitlab.com"
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Images"
      },
      {
        "type": "paragraph",
        "content": "Inline image with alt text:"
      },
      {
        "type": "image",
        "url": "https://example.com/image.png",
        "title": "",
        "alt": "Example Image"
      },
      {
        "type": "paragraph",
        "content": "Image with title:"
      },
      {
        "type": "image",
        "url": "https://example.com/screenshot.png",
        "title": "Screenshot of the code",
        "alt": "Code Screenshot"
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Horizontal Rules"
      },
      {
        "type": "thematic_break"
      },
      {
        "type": "paragraph",
        "content": "Above is a thematic break (horizontal rule)."
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Advanced Features"
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Inline Formatting Combinations"
      },
      {
        "type": "paragraph",
        "content": "You can use **bold with `code` inside**, or *italic with [links](https://example.com) embedded*, or even ***all three together***."
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Special Characters"
      },
      {
        "type": "paragraph",
        "content": "The exporter handles special characters:"
      },
      {
        "type": "list",
        "list_type": "unordered",
        "items": [
          {
            "content": "Unicode: cafÃ©, naÃ¯ve, æ—¥æœ¬èª",
            "items": []
          },
          {
            "content": "Math: E = mcÂ², âˆ‘, âˆ«",
            "items": []
          },
          {
            "content": "Currency: $100, â‚¬50, Â£30",
            "items": []
          },
          {
            "content": "Symbols: Â©, Â®, â„¢, â¤ï¸",
            "items": []
          }
        ]
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Deep Nesting"
      },
      {
        "type": "paragraph",
        "content": "Lists can be deeply nested:"
      },
      {
        "type": "list",
        "list_type": "unordered",
        "items": [
          {
            "content": "Level 1",
            "items": [
              {
                "content": "Level 2",
                "items": [
                  {
                    "content": "Level 3",
                    "items": [
                      {
                        "content": "Level 4",
                        "items": []
                      }
                    ]
                  },
                  {
                    "content": "Back to Level 3",
                    "items": []
                  }
                ]
              },
              {
                "content": "Back to Level 2",
                "items": []
              }
            ]
          }
        ]
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Code Examples with Special Cases"
      },
      {
        "type": "heading",
        "level": 3,
        "text": "String with quotes"
      },
      {
        "type": "code_block",
        "language": "javascript",
        "code": "const message = \"He said, \\\"Hello!\\\"\";\n",
        "lines": 1
      },
      {
        "type": "heading",
        "level": 3,
        "text": "Multiline string"
      },
      {
        "type": "code_block",
        "language": "python",
        "code": "text = \"\"\"\nThis is a\nmultiline string\nin Python\n\"\"\"\n",
        "lines": 5
      },
      {
        "type": "heading",
        "level": 2,
        "text": "Final Notes"
      },
      {
        "type": "paragraph",
        "content": "The JSON export preserves:"
      },
      {
        "type": "list",
        "list_type": "unordered",
        "items": [
          {
            "content": "Document structure and hierarchy",
            "items": []
          },
          {
            "content": "All markdown element types",
            "items": []
          },
          {
            "content": "Metadata (title, timestamps, word counts)",
            "items": []
          },
          {
            "content": "Heading hierarchy for navigation",
            "items": []
          }
        ]
      },
      {
        "type": "paragraph",
        "content": "For more information, see the [JSON Format Documentation](./JSON_FORMAT.md)."
      }
    ]
  }
}
</file>
<file path="examples/json-export/complete-example.md">
# Complete Example: JSON Export Features

This document demonstrates all the features supported by the JSON exporter.

## Introduction

The JSON exporter converts **Markdown** documents into *structured* JSON format for programmatic processing.

### Why Use JSON Export?

- **Search indexing**: Feed documentation into search engines
- **API integration**: Process docs with custom tools
- **Data analysis**: Analyze documentation structure
- **Content migration**: Convert between documentation systems

## Text Formatting

This paragraph shows **bold text**, *italic text*, `inline code`, and [links](https://example.com).

You can also combine **bold and *italic* together**, or use `code` in the middle of sentences.

## Lists

### Unordered List

- First item
- Second item
  - Nested item
  - Another nested item
- Third item

### Ordered List

1. First step
2. Second step
   1. Nested step 1
   2. Nested step 2
3. Third step

### Task List

- [x] Completed task
- [ ] Incomplete task
- [x] Another completed task
  - [x] Nested completed task
  - [ ] Nested incomplete task

## Code Blocks

### Go Example

```go
func main() {
    fmt.Println("Hello, World!")
}
```

### Python Example

```python
def hello():
    print("Hello, World!")
```

### Bash Example

```bash
echo "Hello, World!"
```

### Without Language

```
This is a code block without language specification.
```

## Tables

| Feature | Status | Priority | Notes |
|:--------|:------:|--------:|-------|
| JSON Export | âœ… Done | High | Initial implementation |
| HTML Export | âœ… Done | High | Production ready |
| PDF Export | ğŸš§ WIP | Medium | Planned for v2.0 |

## Blockquotes

> This is a simple blockquote.
>
> It can span multiple lines.

> **Note:** Blockquotes can contain formatting like **bold** and `code`.

> Nested blockquotes are supported:
>
>> This is nested inside another blockquote.

## Links and Images

### Different Link Types

- Inline link: [GitHub](https://github.com)
- Reference link: [GitLab][1]
- Autolink: https://example.com

[1]: https://gitlab.com

### Images

Inline image with alt text:

![Example Image](https://example.com/image.png)

Image with title:

![Code Screenshot](https://example.com/screenshot.png "Screenshot of the code")

## Horizontal Rules

---

Above is a thematic break (horizontal rule).

## Advanced Features

### Inline Formatting Combinations

You can use **bold with `code` inside**, or *italic with [links](https://example.com) embedded*, or even ***all three together***.

### Special Characters

The exporter handles special characters:
- Unicode: cafÃ©, naÃ¯ve, æ—¥æœ¬èª
- Math: E = mcÂ², âˆ‘, âˆ«
- Currency: $100, â‚¬50, Â£30
- Symbols: Â©, Â®, â„¢, â¤ï¸

## Deep Nesting

Lists can be deeply nested:

- Level 1
  - Level 2
    - Level 3
      - Level 4
    - Back to Level 3
  - Back to Level 2

## Code Examples with Special Cases

### String with quotes

```javascript
const message = "He said, \"Hello!\"";
```

### Multiline string

```python
text = """
This is a
multiline string
in Python
"""
```

## Final Notes

The JSON export preserves:
- Document structure and hierarchy
- All markdown element types
- Metadata (title, timestamps, word counts)
- Heading hierarchy for navigation

For more information, see the [JSON Format Documentation](./JSON_FORMAT.md).
</file>
<file path="examples/json-export/README.md">
# JSON Export Examples

This directory contains example markdown files and their corresponding JSON output to demonstrate the JSON export feature.

## Files

### complete-example.md / complete-example.json

A comprehensive example demonstrating all features supported by the JSON exporter:

**Features demonstrated:**
- All heading levels (H1-H6) with hierarchical structure
- Text formatting (bold, italic, inline code)
- Unordered, ordered, and task lists with nesting
- Code blocks with different language syntax highlighting
- Tables with column alignment
- Blockquotes (single and nested)
- Links (inline, reference, autolinks)
- Images with alt text and titles
- Horizontal rules (thematic breaks)
- Special characters (Unicode, math symbols, emojis)
- Deep nesting in lists
- Complex inline formatting combinations

## How to Use These Examples

### Generate JSON from Markdown

```bash
# Navigate to the project root
cd /path/to/gendocs

# Export the example markdown to JSON
gendocs generate export \
  --input examples/json-export/complete-example.md \
  --output examples/json-export/output.json \
  --format json
```

### Compare Output

You can compare your generated JSON with the provided example:

```bash
# Using jq for pretty printing
jq . examples/json-export/complete-example.json

# Or use a diff tool
diff <(jq . examples/json-export/complete-example.json) <(jq . your-output.json)
```

## Understanding the JSON Structure

The JSON output consists of two main sections:

### 1. Metadata

Contains document information:
- `title`: Document title (from first H1)
- `generated_at`: ISO 8601 timestamp
- `generator`: Name, version, and URL of the generator
- `source_file`: Original markdown filename
- `word_count`: Total word count (optional)
- `char_count`: Total character count (optional)

### 2. Content

Contains the document content in two forms:

#### Headings Array
Hierarchical tree structure for navigation:
- `id`: URL-safe unique identifier
- `level`: Heading level (1-6)
- `text`: Heading text (plain)
- `children`: Nested child headings

#### Elements Array
Flat list of all document elements in order:
- Each element has a `type` field
- Types include: `paragraph`, `heading`, `code_block`, `list`, `table`, `blockquote`, `thematic_break`, `link`, `image`
- Type-specific fields for each element

## Practical Usage Examples

### Extract All Code Blocks

```bash
# Extract Go code blocks
jq '.content.elements[] | select(.type == "code_block" and .language == "go")' \
  examples/json-export/complete-example.json
```

### Generate Table of Contents

```javascript
const fs = require('fs');
const doc = JSON.parse(fs.readFileSync('examples/json-export/complete-example.json', 'utf8'));

function generateTOC(headings, level = 0) {
  const indent = '  '.repeat(level);
  return headings.map(h => {
    const link = `${indent}- [${h.text}](#${h.id})`;
    const children = h.children.length > 0
      ? '\n' + generateTOC(h.children, level + 1)
      : '';
    return link + children;
  }).join('\n');
}

console.log(generateTOC(doc.content.headings));
```

### Count Words by Section

```python
import json

with open('examples/json-export/complete-example.json') as f:
    doc = json.load(f)

def count_words(elements, start, end):
    count = 0
    for el in elements[start:end]:
        if el['type'] == 'paragraph':
            count += len(el['content'].split())
    return count

# Count words in each section
elements = doc['content']['elements']
heading_indices = [i for i, el in enumerate(elements) if el['type'] == 'heading']

for i, idx in enumerate(heading_indices):
    end_idx = heading_indices[i + 1] if i + 1 < len(heading_indices) else len(elements)
    heading = elements[idx]
    words = count_words(elements, idx + 1, end_idx)
    print(f"{heading['text']}: {words} words")
```

### Convert to HTML

```python
import json

def element_to_html(element):
    type_map = {
        'paragraph': lambda e: f"<p>{e['content']}</p>",
        'heading': lambda e: f"<h{e['level']}>{e['text']}</h{e['level']}>",
        'code_block': lambda e: f'<pre><code class="language-{e["language"]}">{e["code"]}</code></pre>',
        'thematic_break': lambda e: '<hr>',
    }
    return type_map.get(element['type'], lambda e: '')(element)

with open('examples/json-export/complete-example.json') as f:
    doc = json.load(f)

html = '\n'.join(element_to_html(el) for el in doc['content']['elements'])
print(html)
```

## Validation

You can validate the JSON structure:

```bash
# Check if JSON is valid
jq empty examples/json-export/complete-example.json

# Verify required fields
jq '.metadata.title' examples/json-export/complete-example.json
jq '.metadata.generated_at' examples/json-export/complete-example.json
jq '.content.headings | length' examples/json-export/complete-example.json
jq '.content.elements | length' examples/json-export/complete-example.json
```

## More Information

- [JSON Format Guide](../../docs/JSON_FORMAT.md) - Complete JSON structure documentation
- [Export Guide](../../docs/EXPORT.md) - General export documentation
- [Implementation Plan](../../.auto-claude/specs/008-add-json-exporter-for-structured-documentation-dat/implementation_plan.json) - Technical implementation details

## Tips for Working with JSON Export

1. **Use jq for CLI Operations**: jq is powerful for filtering and transforming JSON
2. **Validate First**: Always validate JSON before processing
3. **Handle Optional Fields**: Some fields like `word_count` are optional
4. **Preserve IDs**: Heading IDs are URL-safe and unique - use them for anchor links
5. **Consider File Size**: JSON files are typically 2-3x larger than source markdown
6. **Use Version Info**: Check `generator.version` for compatibility

## Troubleshooting

### JSON Parsing Errors

```bash
# Validate JSON syntax
jq . examples/json-export/complete-example.json
```

### Missing Fields

Some fields are optional. Always check before accessing:

```javascript
// Good
const wordCount = doc.metadata.word_count || 0;

// Bad - may error if undefined
const wordCount = doc.metadata.word_count;
```

### Empty Headings Array

This is normal for documents without markdown headings (using `#`). Ensure your markdown uses `#` for headings, not underlines.

## Contributing

If you find issues with the JSON export or have suggestions for improvements, please:
1. Check the [implementation plan](../../.auto-claude/specs/008-add-json-exporter-for-structured-documentation-dat/implementation_plan.json)
2. Review the [JSON format documentation](../../docs/JSON_FORMAT.md)
3. Open an issue with example input/output that demonstrates the problem
</file>
<file path="internal/agents/ai_rules_generator.go">
package agents

import (
	"context"
	"fmt"
	"os"
	"path/filepath"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// AIRulesGeneratorAgent generates AI assistant config files
type AIRulesGeneratorAgent struct {
	config        config.AIRulesConfig
	promptManager *prompts.Manager
	logger        *logging.Logger
}

// NewAIRulesGeneratorAgent creates a new AI rules generator agent
func NewAIRulesGeneratorAgent(cfg config.AIRulesConfig, promptManager *prompts.Manager, logger *logging.Logger) *AIRulesGeneratorAgent {
	return &AIRulesGeneratorAgent{
		config:        cfg,
		promptManager: promptManager,
		logger:        logger,
	}
}

// Run generates AI rules files
func (aa *AIRulesGeneratorAgent) Run(ctx context.Context) error {
	// Pre-load all analysis documents
	analysisFiles := []string{
		"structure_analysis.md",
		"dependency_analysis.md",
		"data_flow_analysis.md",
		"request_flow_analysis.md",
		"api_analysis.md",
	}

	analysisContent := make(map[string]string)
	docsDir := filepath.Join(aa.config.RepoPath, ".ai/docs")

	for _, filename := range analysisFiles {
		filePath := filepath.Join(docsDir, filename)
		content, err := os.ReadFile(filePath)
		if err != nil {
			aa.logger.Warn(fmt.Sprintf("Could not read %s: %v", filename, err))
			continue
		}
		analysisContent[filename] = string(content)
	}

	// Setup LLM response caches
	memoryCache, diskCache, cacheCleanup, err := setupCaches(aa.config.LLM, aa.logger)
	if err != nil {
		aa.logger.Warn(fmt.Sprintf("Failed to setup LLM cache: %v (caching disabled)", err))
		cacheCleanup = func() {} // No-op cleanup
	}
	defer cacheCleanup()

	// Create LLM factory with cache support
	retryClient := llm.NewRetryClient(llm.DefaultRetryConfig())
	factory := llm.NewFactory(retryClient, memoryCache, diskCache, aa.config.LLM.Cache.IsEnabled(), aa.config.LLM.Cache.GetTTL())

	// For now, generate CLAUDE.md
	agent, err := CreateAIRulesGeneratorAgent(aa.config.LLM, aa.config.RepoPath, factory, aa.promptManager, aa.logger)
	if err != nil {
		return fmt.Errorf("failed to create AI rules agent: %w", err)
	}

	// Render user prompt with analysis content embedded
	promptData := map[string]interface{}{
		"RepoPath":        aa.config.RepoPath,
		"AnalysisContent": analysisContent,
	}
	userPrompt, err := aa.promptManager.Render("ai_rules_user", promptData)
	if err != nil {
		return fmt.Errorf("failed to render prompt: %w", err)
	}

	// Run agent with custom user prompt
	output, err := agent.RunOnce(ctx, userPrompt)
	if err != nil {
		return fmt.Errorf("AI rules agent failed: %w", err)
	}

	// Save to CLAUDE.md
	outputPath := filepath.Join(aa.config.RepoPath, "CLAUDE.md")
	if err := agent.SaveOutput(output, outputPath); err != nil {
		return err
	}

	aa.logger.Info(fmt.Sprintf("CLAUDE.md generated at %s", outputPath))
	return nil
}
</file>
<file path="internal/agents/analyzer.go">
package agents

import (
	"context"
	"fmt"
	"path/filepath"

	"github.com/user/gendocs/internal/cache"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	"github.com/user/gendocs/internal/worker_pool"
)

type ProgressReporter interface {
	AddTask(id, name, description string)
	StartTask(id string)
	CompleteTask(id string)
	FailTask(id string, err error)
	SkipTask(id string)
}

// AnalyzerAgent orchestrates all sub-agents for code analysis
type AnalyzerAgent struct {
	config        config.AnalyzerConfig
	llmFactory    *llm.Factory
	promptManager *prompts.Manager
	logger        *logging.Logger
	workerPool    *worker_pool.WorkerPool

	progress     ProgressReporter
	cacheCleanup func() // Cleanup function for LLM cache
}

// NewAnalyzerAgent creates a new analyzer agent
func NewAnalyzerAgent(cfg config.AnalyzerConfig, promptManager *prompts.Manager, logger *logging.Logger) *AnalyzerAgent {
	// Create retry client
	retryClient := llm.NewRetryClient(llm.DefaultRetryConfig())

	// Setup LLM response caches
	memoryCache, diskCache, cacheCleanup, err := setupCaches(cfg.LLM, logger)
	if err != nil {
		logger.Warn(fmt.Sprintf("Failed to setup LLM cache: %v (caching disabled)", err))
		cacheCleanup = func() {} // No-op cleanup
	}

	// Create LLM factory with cache support
	factory := llm.NewFactory(retryClient, memoryCache, diskCache, cfg.LLM.Cache.IsEnabled(), cfg.LLM.Cache.GetTTL())

	return &AnalyzerAgent{
		config:        cfg,
		llmFactory:    factory,
		promptManager: promptManager,
		logger:        logger,
		workerPool:    worker_pool.NewWorkerPool(cfg.MaxWorkers),
		cacheCleanup:  cacheCleanup,
	}
}

func (aa *AnalyzerAgent) SetProgressReporter(p ProgressReporter) {
	aa.progress = p
}

// Run executes all sub-agents concurrently
func (aa *AnalyzerAgent) Run(ctx context.Context) (*AnalysisResult, error) {
	// Ensure cache cleanup runs on exit
	defer aa.cacheCleanup()

	aa.logger.Info("Starting analysis",
		logging.String("repo_path", aa.config.RepoPath),
		logging.Int("max_workers", aa.config.MaxWorkers),
	)

	// Load cache and detect changes (unless force mode)
	var analysisCache *cache.AnalysisCache
	var changeReport *cache.ChangeReport
	var currentFiles map[string]cache.FileInfo
	var scanErr error
	var scanMetrics cache.ScanMetrics

	// Always load/create cache (needed for selective hashing)
	analysisCache, _ = cache.LoadCache(aa.config.RepoPath)
	if analysisCache == nil {
		analysisCache = cache.NewCache()
	}

	// Always scan files for cache update (with cache for selective hashing and metrics tracking)
	currentFiles, scanErr = cache.ScanFiles(aa.config.RepoPath, nil, analysisCache, &scanMetrics, aa.config.GetMaxHashWorkers())
	if scanErr != nil {
		aa.logger.Warn(fmt.Sprintf("Failed to scan files: %v", scanErr))
	}

	// Log scan metrics to show optimization effectiveness
	aa.logger.Debug("File scan metrics",
		logging.Int("total_files", scanMetrics.TotalFiles),
		logging.Int("cached_files", scanMetrics.CachedFiles),
		logging.Int("hashed_files", scanMetrics.HashedFiles),
	)
	if scanMetrics.TotalFiles > 0 {
		cacheHitRate := float64(scanMetrics.CachedFiles) / float64(scanMetrics.TotalFiles) * 100
		aa.logger.Debug(fmt.Sprintf("Cache hit rate: %.1f%% (%d/%d files reused cached hashes)",
			cacheHitRate, scanMetrics.CachedFiles, scanMetrics.TotalFiles))
	}

	if !aa.config.Force && scanErr == nil {
		// Detect changes
		changeReport = analysisCache.DetectChanges(aa.config.RepoPath, currentFiles)

		if !changeReport.HasChanges {
			aa.logger.Info("No changes detected since last analysis",
				logging.String("last_analysis", analysisCache.LastAnalysis.Format("2006-01-02 15:04:05")),
			)
			return &AnalysisResult{
				Successful: []string{"No changes - using cached results"},
				Failed:     []FailedAnalysis{},
			}, nil
		}

		aa.logger.Info("Incremental analysis",
			logging.Int("new_files", len(changeReport.NewFiles)),
			logging.Int("modified_files", len(changeReport.ModifiedFiles)),
			logging.Int("deleted_files", len(changeReport.DeletedFiles)),
			logging.Int("agents_to_run", len(changeReport.AgentsToRun)),
			logging.Int("agents_to_skip", len(changeReport.AgentsToSkip)),
		)

		if len(changeReport.AgentsToSkip) > 0 {
			aa.logger.Info(fmt.Sprintf("Skipping unchanged agents: %v", changeReport.AgentsToSkip))
		}
	} else {
		aa.logger.Info("Force mode enabled - running full analysis")
	}

	// Use the existing factory
	factory := aa.llmFactory

	// Build task list based on configuration and change report
	var tasks []worker_pool.Task
	var outputPaths []string
	var agentNames []string

	docsDir := filepath.Join(aa.config.RepoPath, ".ai", "docs")

	// Helper to check if agent should run
	shouldRunAgent := func(agentName string) bool {
		if aa.config.Force || changeReport == nil {
			return true
		}
		for _, a := range changeReport.AgentsToRun {
			if a == agentName {
				return true
			}
		}
		return false
	}

	agentDisplayNames := map[string]string{
		"structure_analyzer":    "Structure Analysis",
		"dependency_analyzer":   "Dependency Analysis",
		"data_flow_analyzer":    "Data Flow Analysis",
		"request_flow_analyzer": "Request Flow Analysis",
		"api_analyzer":          "API Analysis",
	}

	if !aa.config.ExcludeStructure && shouldRunAgent("structure_analyzer") {
		task, outputPath := aa.createTaskWithProgress(ctx, factory, "structure_analyzer", CreateStructureAnalyzer,
			filepath.Join(docsDir, "structure_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
		agentNames = append(agentNames, "structure_analyzer")
		if aa.progress != nil {
			aa.progress.AddTask("structure_analyzer", agentDisplayNames["structure_analyzer"], "Analyzing code structure")
		}
	}

	if !aa.config.ExcludeDeps && shouldRunAgent("dependency_analyzer") {
		task, outputPath := aa.createTaskWithProgress(ctx, factory, "dependency_analyzer", CreateDependencyAnalyzer,
			filepath.Join(docsDir, "dependency_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
		agentNames = append(agentNames, "dependency_analyzer")
		if aa.progress != nil {
			aa.progress.AddTask("dependency_analyzer", agentDisplayNames["dependency_analyzer"], "Analyzing dependencies")
		}
	}

	if !aa.config.ExcludeDataFlow && shouldRunAgent("data_flow_analyzer") {
		task, outputPath := aa.createTaskWithProgress(ctx, factory, "data_flow_analyzer", CreateDataFlowAnalyzer,
			filepath.Join(docsDir, "data_flow_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
		agentNames = append(agentNames, "data_flow_analyzer")
		if aa.progress != nil {
			aa.progress.AddTask("data_flow_analyzer", agentDisplayNames["data_flow_analyzer"], "Analyzing data flow")
		}
	}

	if !aa.config.ExcludeReqFlow && shouldRunAgent("request_flow_analyzer") {
		task, outputPath := aa.createTaskWithProgress(ctx, factory, "request_flow_analyzer", CreateRequestFlowAnalyzer,
			filepath.Join(docsDir, "request_flow_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
		agentNames = append(agentNames, "request_flow_analyzer")
		if aa.progress != nil {
			aa.progress.AddTask("request_flow_analyzer", agentDisplayNames["request_flow_analyzer"], "Analyzing request flow")
		}
	}

	if !aa.config.ExcludeAPI && shouldRunAgent("api_analyzer") {
		task, outputPath := aa.createTaskWithProgress(ctx, factory, "api_analyzer", CreateAPIAnalyzer,
			filepath.Join(docsDir, "api_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
		agentNames = append(agentNames, "api_analyzer")
		if aa.progress != nil {
			aa.progress.AddTask("api_analyzer", agentDisplayNames["api_analyzer"], "Analyzing APIs")
		}
	}

	if changeReport != nil && aa.progress != nil {
		for _, skipped := range changeReport.AgentsToSkip {
			if displayName, ok := agentDisplayNames[skipped]; ok {
				aa.progress.AddTask(skipped, displayName, "")
				aa.progress.SkipTask(skipped)
			}
		}
	}

	if len(tasks) == 0 {
		if changeReport != nil && len(changeReport.AgentsToSkip) > 0 {
			aa.logger.Info("All required agents already up-to-date")
			return &AnalysisResult{
				Successful: changeReport.AgentsToSkip,
				Failed:     []FailedAnalysis{},
			}, nil
		}
		return nil, fmt.Errorf("no analysis tasks to run (all agents excluded)")
	}

	aa.logger.Info(fmt.Sprintf("Running %d analysis tasks concurrently", len(tasks)))

	// Execute all tasks concurrently
	results := aa.workerPool.Run(ctx, tasks)

	// Process results
	analysisResult := aa.processResults(outputPaths, results)

	// Update cache with results
	if analysisCache != nil && len(currentFiles) > 0 {
		agentResults := make(map[string]bool)
		for i, name := range agentNames {
			agentResults[name] = results[i].Error == nil
		}
		// Also mark skipped agents as successful (they were already cached)
		if changeReport != nil {
			for _, skipped := range changeReport.AgentsToSkip {
				agentResults[skipped] = true
			}
		}
		// In force mode, mark all agents as successful
		if aa.config.Force {
			for _, name := range []string{"structure_analyzer", "dependency_analyzer", "data_flow_analyzer", "request_flow_analyzer", "api_analyzer"} {
				if _, exists := agentResults[name]; !exists {
					agentResults[name] = true
				}
			}
		}

		analysisCache.UpdateAfterAnalysis(aa.config.RepoPath, currentFiles, agentResults)
		if err := analysisCache.Save(aa.config.RepoPath); err != nil {
			aa.logger.Warn(fmt.Sprintf("Failed to save cache: %v", err))
		} else {
			aa.logger.Info("Analysis cache updated")
		}
	}

	return analysisResult, nil
}

func (aa *AnalyzerAgent) createTaskWithProgress(ctx context.Context, factory *llm.Factory, name string, creator AgentCreator, outputPath string) (worker_pool.Task, string) {
	task := func(ctx context.Context) (interface{}, error) {
		if aa.progress != nil {
			aa.progress.StartTask(name)
		}

		aa.logger.Info(fmt.Sprintf("Creating %s", name))

		// Create agent
		agent, err := creator(aa.config.LLM, aa.config.RepoPath, factory, aa.promptManager, aa.logger)
		if err != nil {
			if aa.progress != nil {
				aa.progress.FailTask(name, err)
			}
			return nil, fmt.Errorf("failed to create %s: %w", name, err)
		}

		// Run agent
		output, err := agent.Run(ctx)
		if err != nil {
			if aa.progress != nil {
				aa.progress.FailTask(name, err)
			}
			return nil, fmt.Errorf("%s failed: %w", name, err)
		}

		// Save output
		if err := agent.SaveOutput(output, outputPath); err != nil {
			if aa.progress != nil {
				aa.progress.FailTask(name, err)
			}
			return nil, fmt.Errorf("failed to save %s output: %w", name, err)
		}

		if aa.progress != nil {
			aa.progress.CompleteTask(name)
		}

		aa.logger.Info(fmt.Sprintf("%s completed successfully", name))
		return output, nil
	}

	return task, outputPath
}

// processResults processes worker pool results
func (aa *AnalyzerAgent) processResults(outputPaths []string, results []worker_pool.Result) *AnalysisResult {
	result := &AnalysisResult{
		Successful: []string{},
		Failed:     []FailedAnalysis{},
	}

	for i, r := range results {
		// Get agent name from output path
		name := filepath.Base(outputPaths[i])
		name = name[:len(name)-11] // Remove "_analysis.md"

		if r.Error != nil {
			result.Failed = append(result.Failed, FailedAnalysis{
				Name:  name,
				Error: r.Error,
			})
			aa.logger.Error(fmt.Sprintf("%s failed", name), logging.Error(r.Error))
		} else {
			result.Successful = append(result.Successful, name)
			aa.logger.Info(fmt.Sprintf("%s succeeded", name))
		}
	}

	aa.logger.Info(fmt.Sprintf("Analysis complete: %d/%d successful",
		len(result.Successful), len(result.Successful)+len(result.Failed)))

	return result
}
</file>
<file path="internal/agents/analyzer_integration_test.go">
//go:build integration
// +build integration

package agents

import (
	"context"
	"os"
	"path/filepath"
	"testing"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	testHelpers "github.com/user/gendocs/internal/testing"
	"github.com/user/gendocs/internal/tools"
)

// TestAnalyzerAgent_CompleteFlow tests the complete analyzer workflow
// Skip with: go test -short
func TestAnalyzerAgent_CompleteFlow(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	// Create test repository
	repoPath := testHelpers.CreateTempRepo(t, testHelpers.SampleGoProject())

	// Create mock LLM client with predefined responses
	mockClient := &testHelpers.MockLLMClient{
		Responses: []llm.CompletionResponse{
			// Response 1: Request to list files
			{
				Content: "I need to list the files in the repository.",
				ToolCalls: []llm.ToolCall{
					{
						Name: "list_files",
						Arguments: map[string]interface{}{
							"directory": ".",
						},
					},
				},
				Usage: llm.TokenUsage{InputTokens: 100, OutputTokens: 20},
			},
			// Response 2: Request to read main.go
			{
				Content: "Let me read the main file.",
				ToolCalls: []llm.ToolCall{
					{
						Name: "read_file",
						Arguments: map[string]interface{}{
							"file_path": "main.go",
						},
					},
				},
				Usage: llm.TokenUsage{InputTokens: 150, OutputTokens: 25},
			},
			// Response 3: Request to read go.mod
			{
				Content: "Let me check the dependencies.",
				ToolCalls: []llm.ToolCall{
					{
						Name: "read_file",
						Arguments: map[string]interface{}{
							"file_path": "go.mod",
						},
					},
				},
				Usage: llm.TokenUsage{InputTokens: 200, OutputTokens: 30},
			},
			// Response 4: Final analysis
			{
				Content: testHelpers.SampleAnalysisOutput(),
				Usage:   llm.TokenUsage{InputTokens: 500, OutputTokens: 300},
			},
		},
	}

	// Create logger
	logCfg := &logging.Config{
		LogDir:       t.TempDir(),
		FileLevel:    logging.LevelInfo,
		ConsoleLevel: logging.LevelInfo,
		EnableCaller: false,
	}
	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		t.Fatalf("Failed to create logger: %v", err)
	}
	defer logger.Sync()

	// Create tools
	fileReadTool := tools.NewFileReadTool(3)
	listFilesTool := tools.NewListFilesTool(3)

	// Create prompt manager
	// For integration tests, we use simple mock prompts
	promptManager := prompts.NewManagerFromMap(map[string]string{
		"analyzer_system": "You are a code analyzer.",
		"analyzer_user":   "Analyze the repository at {{.RepoPath}}",
	})

	// Create sub-agent (simulating AnalyzerAgent behavior)
	subAgent, err := NewSubAgent(SubAgentConfig{
		Name:         "TestAnalyzer",
		LLMConfig:    config.LLMConfig{},
		RepoPath:     repoPath,
		PromptSuffix: "analyzer",
	}, &llm.Factory{}, promptManager, logger)

	if err != nil {
		t.Fatalf("Failed to create sub-agent: %v", err)
	}

	// Override the LLM client with our mock
	subAgent.BaseAgent.llmClient = mockClient

	// Add tools
	subAgent.BaseAgent.tools = []tools.Tool{fileReadTool, listFilesTool}

	// Run the analyzer
	ctx := context.Background()
	result, err := subAgent.Run(ctx)

	// Verify execution
	if err != nil {
		t.Fatalf("Expected no error, got: %v", err)
	}

	// Verify result contains expected content
	if result == "" {
		t.Fatal("Expected non-empty result")
	}

	if len(result) < 100 {
		t.Errorf("Expected result to be substantial, got %d characters", len(result))
	}

	// Verify tool calls happened
	if mockClient.CallCount != 4 {
		t.Errorf("Expected 4 LLM calls (3 tool requests + 1 final), got %d", mockClient.CallCount)
	}

	// Verify result contains analysis content
	expectedPhrases := []string{"Code Structure", "Components", "Architecture"}
	for _, phrase := range expectedPhrases {
		if !containsString(result, phrase) {
			t.Errorf("Expected result to contain '%s'", phrase)
		}
	}
}

// TestSubAgent_ToolCalling tests that sub-agents can call tools correctly
func TestSubAgent_ToolCalling(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	// Create test repository
	repoPath := testHelpers.CreateTempRepo(t, map[string]string{
		"test.txt": "test content",
	})

	// Mock client that requests file read
	mockClient := &testHelpers.MockLLMClient{
		Responses: []llm.CompletionResponse{
			{
				Content: "Reading file",
				ToolCalls: []llm.ToolCall{
					{
						Name: "read_file",
						Arguments: map[string]interface{}{
							"file_path": "test.txt",
						},
					},
				},
			},
			{
				Content: "File contains: test content",
			},
		},
	}

	// Minimal setup
	logger, _ := logging.NewLogger(&logging.Config{
		LogDir:       t.TempDir(),
		FileLevel:    logging.LevelInfo,
		ConsoleLevel: logging.LevelError,
	})
	defer logger.Sync()

	promptManager := prompts.NewManagerFromMap(map[string]string{
		"test_system": "Test system",
		"test_user":   "Test user",
	})

	subAgent, err := NewSubAgent(SubAgentConfig{
		Name:         "TestAgent",
		LLMConfig:    config.LLMConfig{},
		RepoPath:     repoPath,
		PromptSuffix: "test",
	}, &llm.Factory{}, promptManager, logger)

	if err != nil {
		t.Fatalf("Failed to create agent: %v", err)
	}

	subAgent.BaseAgent.llmClient = mockClient
	subAgent.BaseAgent.tools = []tools.Tool{tools.NewFileReadTool(3)}

	// Run
	result, err := subAgent.Run(context.Background())
	if err != nil {
		t.Fatalf("Expected no error, got: %v", err)
	}

	// Verify tool was called
	if mockClient.CallCount != 2 {
		t.Errorf("Expected 2 calls, got %d", mockClient.CallCount)
	}

	// Verify result mentions the file content
	if !containsString(result, "test content") {
		t.Error("Expected result to mention file content")
	}
}

// TestSubAgent_ErrorHandling tests error handling in agent execution
func TestSubAgent_ErrorHandling(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	repoPath := t.TempDir()

	// Mock client that triggers tool error
	mockClient := &testHelpers.MockLLMClient{
		Responses: []llm.CompletionResponse{
			{
				Content: "Reading non-existent file",
				ToolCalls: []llm.ToolCall{
					{
						Name: "read_file",
						Arguments: map[string]interface{}{
							"file_path": "/nonexistent/file.txt",
						},
					},
				},
			},
			{
				Content: "I encountered an error reading the file.",
			},
		},
	}

	logger, _ := logging.NewLogger(&logging.Config{
		LogDir:       t.TempDir(),
		FileLevel:    logging.LevelError,
		ConsoleLevel: logging.LevelError,
	})
	defer logger.Sync()

	promptManager := prompts.NewManagerFromMap(map[string]string{
		"test_system": "Test",
		"test_user":   "Test",
	})

	subAgent, _ := NewSubAgent(SubAgentConfig{
		Name:         "ErrorTest",
		LLMConfig:    config.LLMConfig{},
		RepoPath:     repoPath,
		PromptSuffix: "test",
	}, &llm.Factory{}, promptManager, logger)

	subAgent.BaseAgent.llmClient = mockClient
	subAgent.BaseAgent.tools = []tools.Tool{tools.NewFileReadTool(3)}

	// Run - should handle tool error gracefully
	result, err := subAgent.Run(context.Background())

	// Agent should not crash, but report the error to LLM
	if err != nil {
		t.Fatalf("Agent should handle tool errors gracefully, got: %v", err)
	}

	// Result should still be returned (LLM's response after seeing error)
	if result == "" {
		t.Error("Expected result even after tool error")
	}
}

// TestSubAgent_ContextCancellation tests context cancellation
func TestSubAgent_ContextCancellation(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	repoPath := t.TempDir()

	// Mock client that never returns (simulated)
	mockClient := &testHelpers.MockLLMClient{
		Responses: []llm.CompletionResponse{
			{Content: "Response"},
		},
	}

	logger, _ := logging.NewLogger(&logging.Config{
		LogDir:       t.TempDir(),
		FileLevel:    logging.LevelError,
		ConsoleLevel: logging.LevelError,
	})
	defer logger.Sync()

	promptManager := prompts.NewManagerFromMap(map[string]string{
		"test_system": "Test",
		"test_user":   "Test",
	})

	subAgent, _ := NewSubAgent(SubAgentConfig{
		Name:         "CancelTest",
		LLMConfig:    config.LLMConfig{},
		RepoPath:     repoPath,
		PromptSuffix: "test",
	}, &llm.Factory{}, promptManager, logger)

	subAgent.BaseAgent.llmClient = mockClient

	// Create canceled context
	ctx, cancel := context.WithCancel(context.Background())
	cancel() // Cancel immediately

	// Run should respect context
	_, err := subAgent.Run(ctx)

	// Should get context canceled error
	if err == nil {
		t.Error("Expected error for canceled context")
	}
}

// TestSubAgent_MultipleToolCalls tests multiple sequential tool calls
func TestSubAgent_MultipleToolCalls(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	repoPath := testHelpers.CreateTempRepo(t, map[string]string{
		"file1.txt": "content 1",
		"file2.txt": "content 2",
		"file3.txt": "content 3",
	})

	mockClient := &testHelpers.MockLLMClient{
		Responses: []llm.CompletionResponse{
			{ToolCalls: []llm.ToolCall{{Name: "list_files", Arguments: map[string]interface{}{"directory": "."}}}},
			{ToolCalls: []llm.ToolCall{{Name: "read_file", Arguments: map[string]interface{}{"file_path": "file1.txt"}}}},
			{ToolCalls: []llm.ToolCall{{Name: "read_file", Arguments: map[string]interface{}{"file_path": "file2.txt"}}}},
			{ToolCalls: []llm.ToolCall{{Name: "read_file", Arguments: map[string]interface{}{"file_path": "file3.txt"}}}},
			{Content: "All files read successfully"},
		},
	}

	logger, _ := logging.NewLogger(&logging.Config{
		LogDir:       t.TempDir(),
		FileLevel:    logging.LevelError,
		ConsoleLevel: logging.LevelError,
	})
	defer logger.Sync()

	promptManager := prompts.NewManagerFromMap(map[string]string{
		"test_system": "Test",
		"test_user":   "Test",
	})

	subAgent, _ := NewSubAgent(SubAgentConfig{
		Name:         "MultiToolTest",
		LLMConfig:    config.LLMConfig{},
		RepoPath:     repoPath,
		PromptSuffix: "test",
	}, &llm.Factory{}, promptManager, logger)

	subAgent.BaseAgent.llmClient = mockClient
	subAgent.BaseAgent.tools = []tools.Tool{
		tools.NewFileReadTool(3),
		tools.NewListFilesTool(3),
	}

	result, err := subAgent.Run(context.Background())
	if err != nil {
		t.Fatalf("Expected no error, got: %v", err)
	}

	// Should have called LLM 5 times (4 tool calls + 1 final)
	if mockClient.CallCount != 5 {
		t.Errorf("Expected 5 LLM calls, got %d", mockClient.CallCount)
	}

	if !containsString(result, "successfully") {
		t.Error("Expected success message in result")
	}
}

// Helper function
func containsString(haystack, needle string) bool {
	return len(haystack) >= len(needle) &&
		(haystack == needle || len(needle) == 0 || findSubstring(haystack, needle))
}

func findSubstring(s, substr string) bool {
	for i := 0; i <= len(s)-len(substr); i++ {
		if s[i:i+len(substr)] == substr {
			return true
		}
	}
	return false
}
</file>
<file path="internal/agents/base.go">
package agents

import (
	"context"
	"encoding/json"
	"fmt"

	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	"github.com/user/gendocs/internal/tools"
)

// Context management constants
const (
	// MaxConversationTokens is the maximum estimated tokens allowed in conversation history
	MaxConversationTokens = 100000

	// MaxToolResponseTokens is the maximum tokens for a single tool response
	MaxToolResponseTokens = 15000

	// TokenEstimateRatio is the approximate characters per token (for estimation)
	TokenEstimateRatio = 4
)

// Agent is the interface that all agents must implement
type Agent interface {
	// Run executes the agent and returns the generated output
	Run(ctx context.Context) (string, error)

	// Name returns the agent name
	Name() string
}

// BaseAgent provides common functionality for all agents
type BaseAgent struct {
	name          string
	llmClient     llm.LLMClient
	tools         []tools.Tool
	promptManager *prompts.Manager
	logger        *logging.Logger
	systemPrompt  string
	maxRetries    int
	maxTokens     int
	temperature   float64
}

// NewBaseAgent creates a new base agent
func NewBaseAgent(
	name string,
	llmClient llm.LLMClient,
	tools []tools.Tool,
	promptManager *prompts.Manager,
	logger *logging.Logger,
	systemPrompt string,
	maxRetries int,
) *BaseAgent {
	return &BaseAgent{
		name:          name,
		llmClient:     llmClient,
		tools:         tools,
		promptManager: promptManager,
		logger:        logger,
		systemPrompt:  systemPrompt,
		maxRetries:    maxRetries,
		maxTokens:     8192,
		temperature:   0.0,
	}
}

// SetMaxTokens sets the maximum tokens for LLM responses
func (ba *BaseAgent) SetMaxTokens(maxTokens int) {
	ba.maxTokens = maxTokens
}

// SetTemperature sets the temperature for LLM responses
func (ba *BaseAgent) SetTemperature(temperature float64) {
	ba.temperature = temperature
}

// RunOnce executes the agent once with the given user prompt
func (ba *BaseAgent) RunOnce(ctx context.Context, userPrompt string) (string, error) {
	// Initialize conversation history with the user prompt
	// This ensures the prompt is preserved across all iterations
	conversationHistory := []llm.Message{
		{Role: "user", Content: userPrompt},
	}

	// Maximum iterations to prevent infinite loops
	const maxIterations = 100
	iterations := 0

	// Tool calling loop
	for {
		iterations++
		if iterations > maxIterations {
			ba.logger.Warn("Maximum iterations reached, forcing completion",
				logging.String("agent", ba.name),
				logging.Int("iterations", iterations),
			)
			return "", fmt.Errorf("agent exceeded maximum iterations (%d)", maxIterations)
		}

		// Trim conversation history to prevent context overflow
		conversationHistory = trimConversationHistory(conversationHistory, MaxConversationTokens)

		// Log current context size
		currentTokens := estimateHistoryTokens(conversationHistory)
		ba.logger.Info("Calling LLM",
			logging.String("agent", ba.name),
			logging.Int("tool_count", len(ba.tools)),
			logging.Int("history_messages", len(conversationHistory)),
			logging.Int("estimated_tokens", currentTokens),
		)

		req := llm.CompletionRequest{
			SystemPrompt: ba.systemPrompt,
			Messages:     conversationHistory,
			Tools:        ba.convertTools(),
			MaxTokens:    ba.maxTokens,
			Temperature:  ba.temperature,
		}

		// Call LLM
		resp, err := ba.llmClient.GenerateCompletion(ctx, req)
		if err != nil {
			return "", fmt.Errorf("LLM call failed: %w", err)
		}

		ba.logger.Info("LLM response received",
			logging.String("agent", ba.name),
			logging.Int("input_tokens", resp.Usage.InputTokens),
			logging.Int("output_tokens", resp.Usage.OutputTokens),
			logging.Int("tool_calls", len(resp.ToolCalls)),
		)

		// If no tool calls, return content
		if len(resp.ToolCalls) == 0 {
			return resp.Content, nil
		}

		// Add assistant response to conversation history (including tool calls)
		conversationHistory = append(conversationHistory, llm.Message{
			Role:      "assistant",
			Content:   resp.Content,
			ToolCalls: resp.ToolCalls,
		})

		// Execute tool calls
		for _, toolCall := range resp.ToolCalls {
			tool := ba.findTool(toolCall.Name)
			if tool == nil {
				ba.logger.Warn("Tool not found", logging.String("tool", toolCall.Name))
				// Add error response
				conversationHistory = append(conversationHistory, llm.Message{
					Role:    "tool",
					Content: fmt.Sprintf("Error: Tool '%s' not found", toolCall.Name),
					ToolID:  toolCall.Name,
				})
				continue
			}

			ba.logger.Info("Executing tool",
				logging.String("tool", tool.Name()),
				logging.String("agent", ba.name),
			)

			// Execute tool
			result, err := tool.Execute(ctx, toolCall.Arguments)
			if err != nil {
				ba.logger.Error("Tool execution failed",
					logging.String("tool", tool.Name()),
					logging.Error(err),
				)
				conversationHistory = append(conversationHistory, llm.Message{
					Role:    "tool",
					Content: fmt.Sprintf("Error: %v", err),
					ToolID:  toolCall.Name,
				})
			} else {
				// Format and truncate tool response
				formattedResult := formatToolResult(result)
				truncatedResult := truncateToolResponse(formattedResult, MaxToolResponseTokens)

				conversationHistory = append(conversationHistory, llm.Message{
					Role:    "tool",
					Content: truncatedResult,
					ToolID:  toolCall.Name,
				})
			}
		}

		// Continue loop to get final response from LLM
	}
}

// convertTools converts agent tools to LLM tool definitions
func (ba *BaseAgent) convertTools() []llm.ToolDefinition {
	var toolDefs []llm.ToolDefinition
	for _, tool := range ba.tools {
		toolDefs = append(toolDefs, llm.ToolDefinition{
			Name:        tool.Name(),
			Description: tool.Description(),
			Parameters:  tool.Parameters(),
		})
	}
	return toolDefs
}

// findTool finds a tool by name
func (ba *BaseAgent) findTool(name string) tools.Tool {
	for _, tool := range ba.tools {
		if tool.Name() == name {
			return tool
		}
	}
	return nil
}

// Name returns the agent name
func (ba *BaseAgent) Name() string {
	return ba.name
}

// estimateTokens estimates the number of tokens in a string
func estimateTokens(text string) int {
	return len(text) / TokenEstimateRatio
}

// estimateHistoryTokens estimates total tokens in conversation history
func estimateHistoryTokens(history []llm.Message) int {
	total := 0
	for _, msg := range history {
		total += estimateTokens(msg.Content)
	}
	return total
}

// trimConversationHistory keeps conversation history within token limits
// It removes older messages while preserving the most recent context
func trimConversationHistory(history []llm.Message, maxTokens int) []llm.Message {
	if len(history) == 0 {
		return history
	}

	totalTokens := estimateHistoryTokens(history)

	// If within limits, return as is
	if totalTokens <= maxTokens {
		return history
	}

	// Remove older messages from the beginning, keeping at least the last 4 messages
	// (typically: assistant response, tool result, assistant response, tool result)
	minKeep := 4
	if len(history) < minKeep {
		minKeep = len(history)
	}

	trimmed := history
	for len(trimmed) > minKeep && estimateHistoryTokens(trimmed) > maxTokens {
		trimmed = trimmed[1:]
	}

	// If still too large, truncate individual messages
	if estimateHistoryTokens(trimmed) > maxTokens {
		for i := range trimmed {
			if trimmed[i].Role == "tool" && len(trimmed[i].Content) > MaxToolResponseTokens*TokenEstimateRatio {
				// Truncate tool responses that are too large
				maxChars := MaxToolResponseTokens * TokenEstimateRatio
				trimmed[i].Content = trimmed[i].Content[:maxChars] + "\n[TRUNCATED - response exceeded token limit]"
			}
		}
	}

	return trimmed
}

// truncateToolResponse truncates a tool response if it exceeds the limit
func truncateToolResponse(response string, maxTokens int) string {
	maxChars := maxTokens * TokenEstimateRatio
	if len(response) <= maxChars {
		return response
	}

	return response[:maxChars] + "\n\n[TRUNCATED - Tool response exceeded " + fmt.Sprintf("%d", maxTokens) + " token limit]"
}

// formatToolResult formats a tool result for inclusion in conversation history
func formatToolResult(result interface{}) string {
	// Try to marshal as JSON for cleaner output
	jsonBytes, err := json.Marshal(result)
	if err != nil {
		return fmt.Sprintf("%v", result)
	}
	return string(jsonBytes)
}
</file>
<file path="internal/agents/documenter.go">
package agents

import (
	"context"
	"fmt"
	"os"
	"path/filepath"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// DocumenterAgent generates README.md
type DocumenterAgent struct {
	config        config.DocumenterConfig
	promptManager *prompts.Manager
	logger        *logging.Logger
}

// NewDocumenterAgent creates a new documenter agent
func NewDocumenterAgent(cfg config.DocumenterConfig, promptManager *prompts.Manager, logger *logging.Logger) *DocumenterAgent {
	return &DocumenterAgent{
		config:        cfg,
		promptManager: promptManager,
		logger:        logger,
	}
}

// Run generates the README
func (da *DocumenterAgent) Run(ctx context.Context) error {
	// Pre-load all analysis documents
	analysisFiles := []string{
		"structure_analysis.md",
		"dependency_analysis.md",
		"data_flow_analysis.md",
		"request_flow_analysis.md",
		"api_analysis.md",
	}

	analysisContent := make(map[string]string)
	docsDir := filepath.Join(da.config.RepoPath, ".ai/docs")

	for _, filename := range analysisFiles {
		filePath := filepath.Join(docsDir, filename)
		content, err := os.ReadFile(filePath)
		if err != nil {
			da.logger.Warn(fmt.Sprintf("Could not read %s: %v", filename, err))
			continue
		}
		analysisContent[filename] = string(content)
	}

	// Setup LLM response caches
	memoryCache, diskCache, cacheCleanup, err := setupCaches(da.config.LLM, da.logger)
	if err != nil {
		da.logger.Warn(fmt.Sprintf("Failed to setup LLM cache: %v (caching disabled)", err))
		cacheCleanup = func() {} // No-op cleanup
	}
	defer cacheCleanup()

	// Create LLM factory with cache support
	retryClient := llm.NewRetryClient(llm.DefaultRetryConfig())
	factory := llm.NewFactory(retryClient, memoryCache, diskCache, da.config.LLM.Cache.IsEnabled(), da.config.LLM.Cache.GetTTL())

	// Create documenter agent
	agent, err := CreateDocumenterAgent(da.config.LLM, da.config.RepoPath, factory, da.promptManager, da.logger)
	if err != nil {
		return fmt.Errorf("failed to create documenter agent: %w", err)
	}

	// Render user prompt with analysis content embedded
	promptData := map[string]interface{}{
		"RepoPath":        da.config.RepoPath,
		"AnalysisContent": analysisContent,
	}
	userPrompt, err := da.promptManager.Render("documenter_user", promptData)
	if err != nil {
		return fmt.Errorf("failed to render prompt: %w", err)
	}

	// Run agent with custom user prompt
	output, err := agent.RunOnce(ctx, userPrompt)
	if err != nil {
		return fmt.Errorf("documenter agent failed: %w", err)
	}

	// Save to README.md
	outputPath := filepath.Join(da.config.RepoPath, "README.md")
	if err := agent.SaveOutput(output, outputPath); err != nil {
		return err
	}

	da.logger.Info(fmt.Sprintf("README.md generated at %s", outputPath))
	return nil
}
</file>
<file path="internal/agents/factory.go">
package agents

import (
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// CreateStructureAnalyzer creates the structure analyzer sub-agent
func CreateStructureAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "StructureAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "structure_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateDependencyAnalyzer creates the dependency analyzer sub-agent
func CreateDependencyAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "DependencyAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "dependency_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateDataFlowAnalyzer creates the data flow analyzer sub-agent
func CreateDataFlowAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "DataFlowAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "data_flow_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateRequestFlowAnalyzer creates the request flow analyzer sub-agent
func CreateRequestFlowAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "RequestFlowAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "request_flow_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateAPIAnalyzer creates the API analyzer sub-agent
func CreateAPIAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "APIAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "api_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateDocumenterAgent creates the documenter agent (README generator)
func CreateDocumenterAgent(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "DocumenterAgent",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "documenter",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateAIRulesGeneratorAgent creates the AI rules generator agent
func CreateAIRulesGeneratorAgent(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "AIRulesGeneratorAgent",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "ai_rules",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}
</file>
<file path="internal/agents/sub_agents.go">
package agents

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/llmcache"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	"github.com/user/gendocs/internal/tools"
)

// SubAgentConfig holds configuration for sub-agents
type SubAgentConfig struct {
	Name         string
	LLMConfig    config.LLMConfig
	RepoPath     string
	PromptSuffix string // e.g., "structure_analyzer"
}

// SubAgent is a specialized analysis agent
type SubAgent struct {
	*BaseAgent
	config SubAgentConfig
}

// NewSubAgent creates a new sub-agent
func NewSubAgent(cfg SubAgentConfig, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	// Create LLM client
	llmClient, err := llmFactory.CreateClient(cfg.LLMConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create LLM client: %w", err)
	}

	// Create tools
	toolList := []tools.Tool{
		tools.NewFileReadTool(2),
		tools.NewListFilesTool(2),
	}

	// Load system prompt
	systemPrompt, err := promptManager.Get(cfg.PromptSuffix + "_system")
	if err != nil {
		return nil, fmt.Errorf("failed to load system prompt: %w", err)
	}

	baseAgent := NewBaseAgent(
		cfg.Name,
		llmClient,
		toolList,
		promptManager,
		logger,
		systemPrompt,
		cfg.LLMConfig.GetRetries(),
	)

	return &SubAgent{
		BaseAgent: baseAgent,
		config:    cfg,
	}, nil
}

// setupCaches initializes LLM response caches based on configuration
// Returns memory cache, disk cache, cleanup function, and error
// The cleanup function should be called when done to stop auto-save
func setupCaches(llmCfg config.LLMConfig, logger *logging.Logger) (*llmcache.LRUCache, *llmcache.DiskCache, func(), error) {
	// Check if caching is enabled
	if !llmCfg.Cache.IsEnabled() {
		return nil, nil, func() {}, nil
	}

	// Create memory cache
	memoryCache := llmcache.NewLRUCache(llmCfg.Cache.GetMaxSize())
	memoryCache.SetLogger(logger.Named("llmcache.memory"))

	// Create disk cache
	diskCache := llmcache.NewDiskCache(
		llmCfg.Cache.GetCachePath(),
		llmCfg.Cache.GetTTL(),
		100*1024*1024, // 100MB max disk size
	)
	diskCache.SetLogger(logger.Named("llmcache.disk"))

	// Load existing disk cache
	if err := diskCache.Load(); err != nil {
		logger.Warn(fmt.Sprintf("Failed to load disk cache: %v (starting with empty cache)", err))
	}

	// Start auto-save (every 5 minutes)
	diskCache.StartAutoSave(5 * time.Minute)

	// Create cleanup function
	cleanup := func() {
		diskCache.Stop()
	}

	logger.Info(fmt.Sprintf("LLM response caching enabled (max_size=%d, ttl=%s, path=%s)",
		llmCfg.Cache.GetMaxSize(),
		llmCfg.Cache.GetTTL(),
		llmCfg.Cache.GetCachePath()))

	return memoryCache, diskCache, cleanup, nil
}

// Run executes the sub-agent
func (sa *SubAgent) Run(ctx context.Context) (string, error) {
	// Render user prompt with variables
	userPrompt, err := sa.promptManager.Render(sa.config.PromptSuffix+"_user", map[string]interface{}{
		"RepoPath": sa.config.RepoPath,
	})
	if err != nil {
		return "", fmt.Errorf("failed to render user prompt: %w", err)
	}

	// Run with retry logic
	var lastErr error
	for attempt := 0; attempt < sa.maxRetries; attempt++ {
		sa.logger.Info(fmt.Sprintf("Running sub-agent %s (attempt %d/%d)", sa.config.Name, attempt+1, sa.maxRetries))

		result, err := sa.RunOnce(ctx, userPrompt)
		if err == nil {
			sa.logger.Info(fmt.Sprintf("Sub-agent %s completed successfully", sa.config.Name))
			return result, nil
		}

		lastErr = err
		sa.logger.Warn(fmt.Sprintf("Sub-agent %s attempt %d failed: %v", sa.config.Name, attempt+1, err))
	}

	return "", fmt.Errorf("sub-agent %s failed after %d retries: %w", sa.config.Name, sa.maxRetries, lastErr)
}

// SaveOutput saves the agent output to a file
func (sa *SubAgent) SaveOutput(output, outputPath string) error {
	// Clean the output to remove unwanted preambles and code fences
	cleanedOutput := cleanLLMOutput(output)

	// Ensure directory exists
	dir := filepath.Dir(outputPath)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create directory: %w", err)
	}

	// Write output
	if err := os.WriteFile(outputPath, []byte(cleanedOutput), 0644); err != nil {
		return fmt.Errorf("failed to write output: %w", err)
	}

	sa.logger.Info(fmt.Sprintf("Output saved to %s", outputPath))
	return nil
}

// cleanLLMOutput removes common LLM output artifacts like markdown code fences and preambles
func cleanLLMOutput(output string) string {
	lines := strings.Split(output, "\n")

	// Find the start of actual markdown content
	startIdx := -1

	// First, try to find markdown code fence
	for i, line := range lines {
		trimmed := strings.TrimSpace(line)
		if trimmed == "```markdown" {
			// Start after the code fence
			startIdx = i + 1
			break
		}
	}

	// If no code fence found, look for markdown heading (# Something)
	if startIdx == -1 {
		for i, line := range lines {
			trimmed := strings.TrimSpace(line)
			// Look for any markdown heading (# or ## or ###, etc.)
			if strings.HasPrefix(trimmed, "#") && len(trimmed) > 1 && trimmed[1] != '`' {
				startIdx = i
				break
			}
		}
	}

	// If still no markdown found, look for common preamble patterns to skip
	if startIdx == -1 {
		for i, line := range lines {
			trimmed := strings.TrimSpace(line)
			lower := strings.ToLower(trimmed)
			// Skip lines that look like preambles or tool outputs
			if strings.HasPrefix(lower, "okay,") ||
				strings.HasPrefix(lower, "here's") ||
				strings.HasPrefix(lower, "here is") ||
				strings.Contains(trimmed, "```tool_outputs") ||
				strings.Contains(trimmed, "{\"read_file_response\"") ||
				(strings.HasPrefix(trimmed, "*") && strings.Contains(trimmed, "**")) {
				continue
			}
			// Found a line that doesn't match preamble patterns
			// If this line starts a list or paragraph, it might be the start of content
			if trimmed != "" && !strings.HasPrefix(trimmed, "```") {
				startIdx = i
				break
			}
		}
	}

	// If no markdown heading found, return original output
	if startIdx == -1 {
		return output
	}

	// Take everything from the first heading onward
	relevantLines := lines[startIdx:]

	// Remove trailing code fence if present
	endIdx := len(relevantLines)
	for i := len(relevantLines) - 1; i >= 0; i-- {
		trimmed := strings.TrimSpace(relevantLines[i])
		if trimmed == "" {
			continue
		}
		if trimmed == "```" {
			endIdx = i
		} else {
			break
		}
	}

	return strings.Join(relevantLines[:endIdx], "\n")
}
</file>
<file path="internal/agents/types.go">
package agents

import (
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// AnalysisResult represents the result of an analysis
type AnalysisResult struct {
	Successful []string
	Failed     []FailedAnalysis
}

// FailedAnalysis represents a failed analysis
type FailedAnalysis struct {
	Name  string
	Error error
}

// AgentCreator is a function that creates an agent
type AgentCreator func(llmCfg config.LLMConfig, repoPath string, factory *llm.Factory, promptMgr *prompts.Manager, logger *logging.Logger) (*SubAgent, error)
</file>
<file path="internal/cache/BENCHMARKS.md">
# File Scanning Benchmarks

This document describes the benchmarks for measuring the performance improvements from selective hashing and parallel processing.

## Running the Benchmarks

Run all benchmarks:
```bash
go test -bench=BenchmarkScanFiles -benchmem ./internal/cache/
```

Run specific benchmark:
```bash
go test -bench=BenchmarkScanFiles_WithCacheParallel -benchmem ./internal/cache/
```

Run with verbose output and multiple iterations:
```bash
go test -bench=BenchmarkScanFiles -benchmem -benchtime=5x ./internal/cache/
```

Compare benchmark results across runs:
```bash
go test -bench=BenchmarkScanFiles -benchmem ./internal/cache/ > bench1.txt
# Make changes...
go test -bench=BenchmarkScanFiles -benchmem ./internal/cache/ > bench2.txt
benchstat bench1.txt bench2.txt
```

## Benchmark Descriptions

### BenchmarkScanFiles_NoCacheSequential
**Baseline**: No cache, sequential hashing (single worker)
- Represents worst-case scenario before any optimizations
- All files are hashed one at a time
- Measures: Sequential hashing performance without caching

### BenchmarkScanFiles_WithCacheSequential
**Selective hashing only**: With cache, sequential hashing (single worker)
- All files use cached hashes (simulating unchanged repository)
- Measures: Benefit of selective hashing alone
- Expected: Much faster than baseline if all files are cached

### BenchmarkScanFiles_WithCacheParallel
**Combined optimizations**: With cache, parallel hashing (auto workers)
- All files use cached hashes
- Multiple workers process files in parallel
- Measures: Combined benefit of both optimizations
- Expected: Fastest for incremental scans

### BenchmarkScanFiles_PartialCacheParallel
**Realistic incremental scan**: Partial cache (20% changed), parallel hashing
- Simulates typical scenario where some files changed
- 20% of files need rehashing, 80% use cache
- Measures: Performance in real-world usage
- Expected: Significant improvement over full rehash

### BenchmarkScanFiles_NoCacheParallel
**Parallelism only**: No cache, parallel hashing
- All files need hashing, but done in parallel
- Measures: Benefit of parallel processing alone
- Expected: Faster than sequential, slower than cached scenarios

### BenchmarkScanFiles_LargeRepository
**Large dataset**: 500 files, with cache, parallel hashing
- Tests scalability on larger repositories
- Measures: Performance as repository size grows
- Expected: Consistent performance regardless of size (if cached)

### BenchmarkScanFiles_WithStats
**Detailed metrics**: Reports throughput statistics
- Files per second
- MB per second
- Operation latency in milliseconds
- Useful for understanding absolute performance characteristics

### BenchmarkParallelHashWorkers
**Worker scaling**: Tests different worker counts (1, 2, 4, 8)
- Helps identify optimal worker count for your system
- Measures: Scaling efficiency
- Expected: Diminishing returns after CPU count

## Expected Results

For a typical project with 1000 source files:

| Scenario | Expected Speedup |
|----------|-----------------|
| No cache, sequential | 1x (baseline) |
| No cache, parallel (4 workers) | 2-3x |
| With cache, sequential (all cached) | 10-50x |
| With cache, parallel (all cached) | 20-100x |
| Partial cache (20% changed) | 3-10x |

*Actual results depend on:
- File sizes and count
- CPU core count
- Disk I/O speed
- Cache hit rate (percentage of unchanged files)*

## Interpreting Results

### Key Metrics

1. **ns/op**: Nanoseconds per operation (lower is better)
   - Total time to scan the repository
   - Includes file walking, cache lookups, and hashing

2. **B/op**: Bytes per operation (memory allocations)
   - Lower is better
   - Should be consistent across different configurations

3. **allocs/op**: Allocations per operation
   - Lower is better
   - Indicates GC pressure

4. **files/sec**: Files processed per second
   - Higher is better
   - Throughput metric for file scanning

5. **MB/sec**: Megabytes processed per second
   - Higher is better
   - Accounts for file sizes, not just count

### Performance Goals

The implementation aims to achieve:
- **3-5x faster** for incremental scans with some changes
- **10-100x faster** for incremental scans with no changes
- **2-3x faster** for full scans on multi-core systems

## Customization

To customize benchmarks for your use case:

1. **Change file count**: Modify `numFiles` parameter in benchmark calls
2. **Change change percentage**: Modify `changePercent` in `BenchmarkScanFiles_PartialCacheParallel`
3. **Change worker counts**: Modify `workerCounts` array in `BenchmarkParallelHashWorkers`
4. **Change file content**: Modify `filePatterns` in `setupBenchmarkRepo`
</file>
<file path="internal/cache/cache.go">
// Package cache provides file scanning and caching functionality for code analysis.
//
// The cache package implements an optimized file scanning system that uses two key optimizations:
//
//  1. Selective Hashing: Files are only re-hashed if their modification time (mtime) and size
//     have changed since the last scan. This significantly reduces I/O and CPU overhead for
//     incremental analysis where most files haven't changed.
//
//  2. Parallel Hashing: When files do need hashing, they are processed concurrently using a
//     worker pool pattern. This takes advantage of multi-core CPUs to speed up the CPU-bound
//     hash computation.
//
// The combination of these optimizations can provide 3-5x speedup for incremental scans on
// large repositories with many unchanged files.
package cache

import (
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"io"
	"os"
	"os/exec"
	"path/filepath"
	"runtime"
	"strings"
	"sync"
	"time"
)

// CacheVersion is the current cache format version
const CacheVersion = 1

// CacheFileName is the name of the cache file
const CacheFileName = ".ai/analysis_cache.json"

// AnalysisCache holds information about previous analysis runs.
//
// The cache stores file metadata (hash, modification time, size) to enable selective hashing.
// On subsequent scans, files with matching mtime and size can skip re-computation of their SHA256 hash.
// This is particularly effective for incremental analysis where only a small subset of files change.
type AnalysisCache struct {
	Version      int                    `json:"version"`
	LastAnalysis time.Time              `json:"last_analysis"`
	GitCommit    string                 `json:"git_commit"`
	Files        map[string]FileInfo    `json:"files"`
	Agents       map[string]AgentStatus `json:"agents"`
}

// FileInfo holds metadata about a file for change detection.
//
// The combination of Hash, Modified (mtime), and Size allows us to detect file changes
// without re-reading file contents. A file is considered unchanged if both Modified and Size
// match the cached values, allowing us to skip expensive hash computation.
type FileInfo struct {
	Hash     string    `json:"hash"`     // SHA256 hash of file contents
	Modified time.Time `json:"modified"` // File modification time (mtime)
	Size     int64     `json:"size"`     // File size in bytes
}

// AgentStatus holds information about an agent's last run.
//
// This is used to track which agents have successfully completed and which need to run
// when changes are detected.
type AgentStatus struct {
	LastRun       time.Time `json:"last_run"`
	Success       bool      `json:"success"`
	FilesAnalyzed []string  `json:"files_analyzed,omitempty"`
}

// ScanMetrics holds statistics about file scanning operations.
//
// These metrics help track the effectiveness of selective hashing optimization:
// - CachedFiles: Files that skipped hashing due to cache hits (mtime+size match)
// - HashedFiles: Files that required new hash computation (cache misses)
// - A high cache hit rate (CachedFiles / TotalFiles) indicates effective optimization
type ScanMetrics struct {
	TotalFiles  int // Total number of files scanned
	CachedFiles int // Number of files that reused cached hashes (cache hits)
	HashedFiles int // Number of files that required new hash computation (cache misses)
}

// hashFileJob represents a file hashing job in the parallel processing system.
//
// This struct holds the file path information needed to compute a SHA256 hash.
// Jobs are distributed to worker goroutines via a buffered channel.
type hashFileJob struct {
	relPath  string // Relative path of the file to hash (used as the result map key)
	fullPath string // Absolute path of the file to hash (used for file I/O)
}

// hashFileResult holds the result of hashing a single file.
//
// Workers send results through this struct to a results channel, allowing the main
// goroutine to collect all hash computations without blocking.
type hashFileResult struct {
	relPath string // Relative path of the file (for mapping back to the file list)
	hash    string // Computed SHA256 hash (empty if error occurred)
	err     error  // Error if hashing failed (nil on success)
}

// DefaultMaxHashWorkers is the default maximum number of parallel hash workers.
//
// This constant provides a safety cap to prevent overwhelming the filesystem with
// too many concurrent I/O operations. Even on systems with many CPU cores, we limit
// parallelism to avoid excessive disk contention.
const DefaultMaxHashWorkers = 8

// getMaxHashWorkers returns the optimal number of hash workers based on CPU count and configured limit.
//
// This function implements a smart worker count strategy:
// - If maxHashWorkers is 0: Use CPU count, capped at DefaultMaxHashWorkers (auto-detect mode)
// - If maxHashWorkers > 0: Use configured value, capped at DefaultMaxHashWorkers (explicit mode)
//
// The cap ensures we don't overwhelm the filesystem with too many parallel reads,
// which could actually degrade performance due to I/O contention.
//
// Parameters:
//   - maxHashWorkers: Configured maximum (0 for auto-detect, >0 for explicit)
//
// Returns:
//   - The optimal number of workers to use (capped at DefaultMaxHashWorkers)
func getMaxHashWorkers(maxHashWorkers int) int {
	numCPU := runtime.NumCPU()

	// If explicit configuration is provided, use it (with safety cap)
	if maxHashWorkers > 0 {
		if maxHashWorkers > DefaultMaxHashWorkers {
			return DefaultMaxHashWorkers
		}
		return maxHashWorkers
	}

	// Default: use CPU count, capped at DefaultMaxHashWorkers
	if numCPU > DefaultMaxHashWorkers {
		return DefaultMaxHashWorkers
	}
	return numCPU
}

// parallelHashFiles computes hashes for multiple files concurrently using a worker pool pattern.
//
// This function implements the parallel hashing optimization that significantly speeds up
// file scanning on multi-core systems. The worker pool design ensures:
//
// 1. Concurrent Execution: Multiple goroutines compute hashes in parallel, utilizing all CPU cores
// 2. Bounded Parallelism: Worker count is capped to avoid overwhelming the filesystem with I/O
// 3. Clean Shutdown: Uses sync.WaitGroup to ensure all workers complete before returning
// 4. Non-Blocking: Uses buffered channels for job distribution and result collection
//
// Architecture:
//   - Worker Pool: Fixed number of goroutines that process jobs from a shared queue
//   - Job Queue: Buffered channel holding hashFileJob objects (one per file to hash)
//   - Results Channel: Buffered channel collecting hashFileResult objects as workers finish
//
// Workflow:
//  1. Create worker pool (determined by getMaxHashWorkers())
//  2. Dispatch all jobs to the job queue
//  3. Workers pull jobs, compute hashes, send results
//  4. Close job queue and wait for all workers to finish
//  5. Collect results from the results channel into a map
//
// Parameters:
//   - jobs: Slice of file hashing jobs to process
//   - maxHashWorkers: Maximum number of workers (0 = auto-detect CPU count, capped at 8)
//
// Returns:
//   - Map of relative file paths to their SHA256 hashes (excludes files that errored)
func parallelHashFiles(jobs []hashFileJob, maxHashWorkers int) map[string]string {
	if len(jobs) == 0 {
		return make(map[string]string)
	}

	numWorkers := getMaxHashWorkers(maxHashWorkers)
	results := make(map[string]string)
	resultsChan := make(chan hashFileResult, len(jobs))
	var wg sync.WaitGroup

	// Create worker pool
	jobQueue := make(chan hashFileJob, len(jobs))

	// Start workers
	for i := 0; i < numWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for job := range jobQueue {
				hash, err := HashFile(job.fullPath)
				resultsChan <- hashFileResult{
					relPath: job.relPath,
					hash:    hash,
					err:     err,
				}
			}
		}()
	}

	// Dispatch jobs
	for _, job := range jobs {
		jobQueue <- job
	}
	close(jobQueue)

	// Wait for all workers to finish
	go func() {
		wg.Wait()
		close(resultsChan)
	}()

	// Collect results
	for result := range resultsChan {
		if result.err == nil {
			results[result.relPath] = result.hash
		}
	}

	return results
}

// ChangeReport describes what changed since last analysis
type ChangeReport struct {
	HasChanges       bool
	NewFiles         []string
	ModifiedFiles    []string
	DeletedFiles     []string
	AgentsToRun      []string
	AgentsToSkip     []string
	Reason           string
	IsFirstRun       bool
	GitCommitChanged bool
}

// AgentFilePatterns maps agents to file patterns they care about
var AgentFilePatterns = map[string][]string{
	"structure_analyzer": {
		"*.go", "*.py", "*.js", "*.ts", "*.jsx", "*.tsx",
		"*.java", "*.rs", "*.c", "*.cpp", "*.h", "*.hpp",
		"go.mod", "package.json", "Cargo.toml", "pom.xml",
	},
	"dependency_analyzer": {
		"go.mod", "go.sum", "package.json", "package-lock.json",
		"yarn.lock", "pnpm-lock.yaml", "Cargo.toml", "Cargo.lock",
		"requirements.txt", "pyproject.toml", "pom.xml", "build.gradle",
	},
	"data_flow_analyzer": {
		"*.go", "*.py", "*.js", "*.ts", "*.jsx", "*.tsx",
		"*.java", "*.rs",
	},
	"request_flow_analyzer": {
		"*handler*.go", "*controller*.go", "*route*.go", "*api*.go",
		"*handler*.py", "*view*.py", "*route*.py",
		"*controller*.js", "*route*.js", "*api*.js",
		"*controller*.ts", "*route*.ts", "*api*.ts",
	},
	"api_analyzer": {
		"*handler*.go", "*controller*.go", "*route*.go", "*api*.go",
		"*handler*.py", "*view*.py", "*route*.py",
		"*controller*.js", "*route*.js", "*api*.js",
		"*controller*.ts", "*route*.ts", "*api*.ts",
		"openapi*.yaml", "swagger*.yaml", "*.proto",
	},
}

// NewCache creates a new empty cache
func NewCache() *AnalysisCache {
	return &AnalysisCache{
		Version: CacheVersion,
		Files:   make(map[string]FileInfo),
		Agents:  make(map[string]AgentStatus),
	}
}

// LoadCache loads the cache from disk
func LoadCache(repoPath string) (*AnalysisCache, error) {
	cachePath := filepath.Join(repoPath, CacheFileName)

	data, err := os.ReadFile(cachePath)
	if err != nil {
		if os.IsNotExist(err) {
			return NewCache(), nil
		}
		return nil, fmt.Errorf("failed to read cache: %w", err)
	}

	var cache AnalysisCache
	if err := json.Unmarshal(data, &cache); err != nil {
		// Cache is corrupted, return fresh cache
		return NewCache(), nil
	}

	// Check version compatibility
	if cache.Version != CacheVersion {
		// Version mismatch, return fresh cache
		return NewCache(), nil
	}

	return &cache, nil
}

// Save saves the cache to disk
func (c *AnalysisCache) Save(repoPath string) error {
	cachePath := filepath.Join(repoPath, CacheFileName)

	// Ensure directory exists
	dir := filepath.Dir(cachePath)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create cache directory: %w", err)
	}

	// Use json.Marshal instead of json.MarshalIndent for better performance
	// The cache file is machine-readable, so pretty-printing adds unnecessary
	// file size and encoding overhead. json.Unmarshal is indentation-agnostic.
	data, err := json.Marshal(c)
	if err != nil {
		return fmt.Errorf("failed to marshal cache: %w", err)
	}

	if err := os.WriteFile(cachePath, data, 0644); err != nil {
		return fmt.Errorf("failed to write cache: %w", err)
	}

	return nil
}

// GetCurrentGitCommit returns the current git commit hash
func GetCurrentGitCommit(repoPath string) string {
	cmd := exec.Command("git", "rev-parse", "--short", "HEAD")
	cmd.Dir = repoPath
	output, err := cmd.Output()
	if err != nil {
		return ""
	}
	return strings.TrimSpace(string(output))
}

// HashFile calculates SHA256 hash of a file
func HashFile(path string) (string, error) {
	file, err := os.Open(path)
	if err != nil {
		return "", err
	}
	defer func() { _ = file.Close() }()

	hasher := sha256.New()
	if _, err := io.Copy(hasher, file); err != nil {
		return "", err
	}

	return hex.EncodeToString(hasher.Sum(nil)), nil
}

// ScanFiles scans repository files and returns their information using selective hashing and parallel processing.
//
// This function is the core of the optimized file scanning system, combining two key optimizations:
//
// 1. SELECTIVE HASHING (Cache Hit Detection):
//   - For each file, compare its mtime and size against the cached values
//   - If both match: Skip hash computation, reuse cached hash (cache HIT)
//   - If either differs: Mark file for hashing (cache MISS)
//   - This dramatically reduces I/O for incremental scans where most files are unchanged
//
// 2. PARALLEL HASHING (Worker Pool):
//   - All files marked as cache misses are hashed concurrently
//   - Uses a worker pool pattern with bounded parallelism
//   - Separates I/O-bound directory walking from CPU-bound hash computation
//
// Three-Phase Architecture:
//
//	Phase 1 - File Discovery (I/O Bound):
//	  - Walk the directory tree using filepath.Walk()
//	  - Collect file metadata (path, mtime, size) for all files
//	  - No hash computation yet, just metadata gathering
//	  - This allows us to make cache hit/miss decisions for all files upfront
//
//	Phase 2 - Cache Classification (In-Memory):
//	  - For each file, check if cached metadata exists
//	  - Compare mtime and size to determine cache hit vs miss
//	  - Separate files into two groups:
//	    * Cached: Reuse hash, update metrics
//	    * Needs Hashing: Add to parallel hashing batch
//	  - Build initial results map with cached hashes and placeholders
//
//	Phase 3 - Parallel Hashing (CPU Bound):
//	  - Batch hash all "needs hashing" files using parallelHashFiles()
//	  - Worker pool computes multiple hashes concurrently
//	  - Update the results map with computed hashes
//	  - This is where the CPU-intensive work happens in parallel
//
// Performance Characteristics:
//   - Best Case (all files cached): O(n) directory walk, no hash computations
//   - Worst Case (all files changed): O(n) directory walk + parallel hash computation
//   - Typical Case (mix): O(n) walk + parallel hash for subset of files
//   - Parallel hashing provides near-linear speedup with CPU cores
//
// Parameters:
//   - repoPath: Root directory of the repository to scan
//   - ignorePatterns: File/directory patterns to skip (e.g., ".git", "node_modules")
//   - cache: Optional analysis cache (nil = no cache, compute all hashes)
//   - metrics: Optional metrics tracker (nil = don't track statistics)
//   - maxHashWorkers: Maximum parallel hash workers (0 = auto-detect CPU count, capped at 8)
//
// Returns:
//   - map[string]FileInfo: Map of relative file paths to their metadata (hash, mtime, size)
//   - error: Error if directory walk fails
//
// Example Usage:
//
//	// With cache and metrics
//	cache, _ := LoadCache(repoPath)
//	var metrics ScanMetrics
//	files, err := ScanFiles(repoPath, nil, cache, &metrics, 0)
//	fmt.Printf("Cache hit rate: %.1f%%\n",
//	    float64(metrics.CachedFiles)/float64(metrics.TotalFiles)*100)
//
//	// Without cache (backward compatible)
//	files, err := ScanFiles(repoPath, nil, nil, nil, 0)
func ScanFiles(repoPath string, ignorePatterns []string, cache *AnalysisCache, metrics *ScanMetrics, maxHashWorkers int) (map[string]FileInfo, error) {
	// Phase 1: Walk directory tree and collect file metadata (no hashing yet)
	//
	// This phase is I/O bound as we read directory entries and file metadata from disk.
	// We deliberately avoid any hash computation here to minimize I/O overhead.
	// The fileMetadata struct allows us to collect all info needed for cache decisions.
	type fileMetadata struct {
		relPath  string
		fullPath string
		modTime  time.Time
		size     int64
	}

	var filesToProcess []fileMetadata
	err := filepath.Walk(repoPath, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			if os.IsPermission(err) {
				return nil
			}
			return err
		}

		// Get relative path
		relPath, err := filepath.Rel(repoPath, path)
		if err != nil {
			return nil
		}

		// Skip directories and apply ignore patterns
		if info.IsDir() {
			if shouldIgnore(relPath, info.Name(), ignorePatterns) {
				return filepath.SkipDir
			}
			return nil
		}

		// Skip ignored files
		if shouldIgnore(relPath, info.Name(), ignorePatterns) {
			return nil
		}

		// Skip binary files (quick check by extension)
		if isBinaryExtension(filepath.Ext(path)) {
			return nil
		}

		// Track total files
		if metrics != nil {
			metrics.TotalFiles++
		}

		// Collect file metadata for processing
		filesToProcess = append(filesToProcess, fileMetadata{
			relPath:  relPath,
			fullPath: path,
			modTime:  info.ModTime(),
			size:     info.Size(),
		})

		return nil
	})

	if err != nil {
		return nil, err
	}

	// Phase 2: Separate files into cached and needs-hashing groups
	//
	// This phase examines each file discovered in Phase 1 and determines whether we can
	// reuse the cached hash or need to compute a new one. The decision is based on:
	//   1. Does the file exist in the cache?
	//   2. If yes, do both mtime AND size match exactly?
	//
	// Files that pass both tests are cache hits and skip hash computation.
	// Files that fail either test are cache misses and are added to the parallel hashing batch.
	//
	// We build the results map incrementally:
	//   - Cache hits: Add immediately with their cached hash
	//   - Cache misses: Add now with empty hash, will be filled in Phase 3
	files := make(map[string]FileInfo)
	var filesToHash []hashFileJob

	for _, meta := range filesToProcess {
		// Check if we can reuse cached hash
		//
		// Cache hit condition (both must be true):
		//   1. File exists in cache
		//   2. Cached mtime equals current mtime (using Equal() for time.Time comparison)
		//   3. Cached size equals current size
		//
		// We require BOTH mtime and size to match because:
		//   - mtime alone can have false positives (e.g., file touched without modification)
		//   - size alone can have false positives (e.g., file changed but same size)
		//   - Combined, they provide a very reliable change detection heuristic
		var hash string
		cached := false
		if cache != nil {
			if cachedFile, exists := cache.Files[meta.relPath]; exists {
				// If mtime and size match, reuse the cached hash
				if cachedFile.Modified.Equal(meta.modTime) && cachedFile.Size == meta.size {
					hash = cachedFile.Hash
					cached = true
				}
			}
		}

		// Track metrics for cached files
		if metrics != nil && cached {
			metrics.CachedFiles++
		}

		// If not cached, add to parallel hashing batch
		//
		// Files that fail the cache hit test are collected into a batch for parallel hashing.
		// This batching allows us to compute multiple hashes concurrently, significantly
		// speeding up the process on multi-core systems.
		if !cached {
			filesToHash = append(filesToHash, hashFileJob{
				relPath:  meta.relPath,
				fullPath: meta.fullPath,
			})
		}

		// Initialize file entry (hash will be filled in after parallel hashing)
		//
		// We build the final results map in Phase 2, but cache misses get an empty hash
		// that will be filled in during Phase 3. This allows us to have a single results
		// map that gets progressively filled in, rather than merging multiple maps.
		files[meta.relPath] = FileInfo{
			Hash:     hash,
			Modified: meta.modTime,
			Size:     meta.size,
		}
	}

	// Phase 3: Batch hash all files that need it in parallel
	//
	// This is the CPU-intensive phase where we compute SHA256 hashes for all cache misses.
	// The parallelHashFiles() function implements a worker pool that:
	//   - Spawns multiple worker goroutines (limited by maxHashWorkers)
	//   - Distributes file hashing jobs across workers
	//   - Collects results into a map as workers complete
	//
	// After parallel hashing completes, we update the files map with the computed hashes.
	// For any hash that failed (error), we leave it as empty string - that file will be
	// treated as missing/errored in subsequent analysis.
	//
	// The batching approach ensures that we maximize parallelism while avoiding
	// overwhelming the filesystem with too many concurrent reads.
	if len(filesToHash) > 0 {
		hashResults := parallelHashFiles(filesToHash, maxHashWorkers)

		// Update file entries with computed hashes
		for relPath, hash := range hashResults {
			if file, exists := files[relPath]; exists {
				file.Hash = hash
				files[relPath] = file
			}

			// Track metrics for hashed files
			if metrics != nil {
				metrics.HashedFiles++
			}
		}
	}

	return files, nil
}

// DetectChanges compares current files with cached files
func (c *AnalysisCache) DetectChanges(repoPath string, currentFiles map[string]FileInfo) *ChangeReport {
	report := &ChangeReport{
		NewFiles:      []string{},
		ModifiedFiles: []string{},
		DeletedFiles:  []string{},
		AgentsToRun:   []string{},
		AgentsToSkip:  []string{},
	}

	// Check if this is first run
	if len(c.Files) == 0 || c.LastAnalysis.IsZero() {
		report.IsFirstRun = true
		report.HasChanges = true
		report.Reason = "First analysis run"
		report.AgentsToRun = getAllAgents()
		for path := range currentFiles {
			report.NewFiles = append(report.NewFiles, path)
		}
		return report
	}

	// Check git commit
	currentCommit := GetCurrentGitCommit(repoPath)
	if currentCommit != "" && c.GitCommit != "" && currentCommit != c.GitCommit {
		report.GitCommitChanged = true
	}

	// Find new and modified files
	for path, info := range currentFiles {
		cached, exists := c.Files[path]
		if !exists {
			report.NewFiles = append(report.NewFiles, path)
		} else if cached.Hash != info.Hash {
			report.ModifiedFiles = append(report.ModifiedFiles, path)
		}
	}

	// Find deleted files
	for path := range c.Files {
		if _, exists := currentFiles[path]; !exists {
			report.DeletedFiles = append(report.DeletedFiles, path)
		}
	}

	// Determine which agents need to run
	changedFiles := append(report.NewFiles, report.ModifiedFiles...)
	changedFiles = append(changedFiles, report.DeletedFiles...)

	report.HasChanges = len(changedFiles) > 0

	if !report.HasChanges {
		report.Reason = "No files changed since last analysis"
		report.AgentsToSkip = getAllAgents()
		return report
	}

	// Determine which agents are affected by the changes
	for agent, patterns := range AgentFilePatterns {
		if agentNeedsRun(changedFiles, patterns, c.Agents[agent]) {
			report.AgentsToRun = append(report.AgentsToRun, agent)
		} else {
			report.AgentsToSkip = append(report.AgentsToSkip, agent)
		}
	}

	// If no specific agents matched, run all (safety fallback)
	if len(report.AgentsToRun) == 0 && report.HasChanges {
		report.AgentsToRun = getAllAgents()
		report.AgentsToSkip = []string{}
		report.Reason = "Changes detected but no specific agent patterns matched"
	} else {
		report.Reason = fmt.Sprintf("%d files changed, %d agents need re-run",
			len(changedFiles), len(report.AgentsToRun))
	}

	return report
}

// UpdateAfterAnalysis updates the cache after a successful analysis
func (c *AnalysisCache) UpdateAfterAnalysis(repoPath string, currentFiles map[string]FileInfo, agentResults map[string]bool) {
	c.LastAnalysis = time.Now()
	c.GitCommit = GetCurrentGitCommit(repoPath)
	c.Files = currentFiles

	for agent, success := range agentResults {
		c.Agents[agent] = AgentStatus{
			LastRun: time.Now(),
			Success: success,
		}
	}
}

// Helper functions

func getAllAgents() []string {
	return []string{
		"structure_analyzer",
		"dependency_analyzer",
		"data_flow_analyzer",
		"request_flow_analyzer",
		"api_analyzer",
	}
}

func agentNeedsRun(changedFiles []string, patterns []string, lastStatus AgentStatus) bool {
	// If agent never ran successfully, it needs to run
	if lastStatus.LastRun.IsZero() || !lastStatus.Success {
		return true
	}

	// Check if any changed file matches agent's patterns
	for _, file := range changedFiles {
		for _, pattern := range patterns {
			if matchPattern(file, pattern) {
				return true
			}
		}
	}

	return false
}

func matchPattern(filename, pattern string) bool {
	// Handle patterns like "*handler*.go", "*_test.go", etc.
	if strings.Contains(pattern, "*") {
		// Simple glob matching
		pattern = strings.ToLower(pattern)
		filename = strings.ToLower(filepath.Base(filename))

		// Handle *.ext patterns (e.g., "*.go")
		if strings.HasPrefix(pattern, "*.") && !strings.Contains(pattern[1:], "*") {
			ext := strings.TrimPrefix(pattern, "*")
			return strings.HasSuffix(filename, ext)
		}

		// Handle *keyword*.ext patterns (e.g., "*handler*.go")
		if strings.HasPrefix(pattern, "*") && strings.HasSuffix(pattern, "*") && strings.Contains(pattern[1:], "*") {
			// Extract the middle part
			middle := strings.Trim(pattern, "*")
			return strings.Contains(filename, middle)
		}

		// Handle *suffix patterns (e.g., "*_test.go", "_test.go")
		if strings.HasPrefix(pattern, "*") && !strings.HasSuffix(pattern, "*") {
			suffix := strings.TrimPrefix(pattern, "*")
			return strings.HasSuffix(filename, suffix)
		}

		// Handle prefix* patterns (e.g., "test_*")
		if strings.HasSuffix(pattern, "*") && !strings.HasPrefix(pattern, "*") {
			prefix := strings.TrimSuffix(pattern, "*")
			return strings.HasPrefix(filename, prefix)
		}

		// Handle *keyword* patterns (e.g., "*test*")
		if strings.HasPrefix(pattern, "*") && strings.HasSuffix(pattern, "*") {
			keyword := strings.Trim(pattern, "*")
			return strings.Contains(filename, keyword)
		}
	}

	// Exact match (for files like go.mod)
	return strings.EqualFold(filepath.Base(filename), pattern)
}

func shouldIgnore(relPath, name string, patterns []string) bool {
	// Default ignore patterns
	defaultIgnore := []string{
		".git", "node_modules", "vendor", ".venv", "venv",
		"__pycache__", "dist", "build", ".ai",
	}

	for _, pattern := range append(defaultIgnore, patterns...) {
		// Exact name match
		if name == pattern {
			return true
		}

		// Directory prefix match (e.g., ".git/objects")
		if strings.HasPrefix(relPath, pattern+"/") {
			return true
		}

		// Glob pattern matching for filenames
		if strings.Contains(pattern, "*") {
			// Check if the filename matches the glob pattern
			if matchPattern(name, pattern) {
				return true
			}
		}
	}

	return false
}

var binaryExts = map[string]bool{
	".exe": true, ".dll": true, ".so": true, ".dylib": true,
	".bin": true, ".o": true, ".a": true, ".obj": true,
	".png": true, ".jpg": true, ".jpeg": true, ".gif": true,
	".ico": true, ".bmp": true, ".webp": true,
	".mp3": true, ".mp4": true, ".avi": true, ".mov": true,
	".zip": true, ".tar": true, ".gz": true, ".rar": true,
	".pdf": true, ".doc": true, ".docx": true,
	".woff": true, ".woff2": true, ".ttf": true, ".eot": true,
}

func isBinaryExtension(ext string) bool {
	return binaryExts[strings.ToLower(ext)]
}
</file>
<file path="internal/cache/cache_bench_test.go">
package cache

import (
	"fmt"
	"os"
	"path/filepath"
	"testing"
	"time"
)

// Benchmark setup helper that creates a realistic test repository
func setupBenchmarkRepo(b *testing.B, numFiles int) (string, map[string]FileInfo) {
	b.Helper()

	tmpDir := b.TempDir()

	// Create a realistic file structure similar to a typical project
	// Source files with various extensions and sizes
	createdFiles := make(map[string]FileInfo)

	// Create directories
	dirs := []string{
		"cmd/server",
		"internal/handlers",
		"internal/models",
		"internal/utils",
		"pkg/api",
		"pkg/config",
		"web/assets",
		"scripts",
	}

	for _, dir := range dirs {
		if err := os.MkdirAll(filepath.Join(tmpDir, dir), 0755); err != nil {
			b.Fatalf("Failed to create directory %s: %v", dir, err)
		}
	}

	// Create files with realistic content patterns
	filePatterns := []struct {
		path    string
		content string
		count   int
	}{
		{
			path: "cmd/server/main.go",
			content: `package main

import (
	"fmt"
	"net/http"
)

func main() {
	http.HandleFunc("/", handler)
	http.ListenAndServe(":8080", nil)
}

func handler(w http.ResponseWriter, r *http.Request) {
	fmt.Fprintf(w, "Hello, World!")
}
`,
			count: 1,
		},
		{
			path: "internal/handlers/user.go",
			content: `package handlers

type User struct {
	ID    int    ` + "`json:\"id\"`" + `
	Name  string ` + "`json:\"name\"`" + `
	Email string ` + "`json:\"email\"`" + `
}

func GetUser(id int) (*User, error) {
	// Database query logic
	return &User{ID: id, Name: "John Doe", Email: "john@example.com"}, nil
}

func CreateUser(u *User) error {
	// Create user logic
	return nil
}

func UpdateUser(u *User) error {
	// Update user logic
	return nil
}

func DeleteUser(id int) error {
	// Delete user logic
	return nil
}
`,
			count: 5,
		},
		{
			path: "internal/models/models.go",
			content: `package models

type Model struct {
	ID        int       ` + "`json:\"id\"`" + `
	CreatedAt time.Time ` + "`json:\"created_at\"`" + `
	UpdatedAt time.Time ` + "`json:\"updated_at\"`" + `
}

func (m *Model) Validate() error {
	if m.ID < 0 {
		return fmt.Errorf("invalid ID")
	}
	return nil
}
`,
			count: 3,
		},
		{
			path: "pkg/api/client.go",
			content: `package api

import "net/http"

type Client struct {
	BaseURL    string
	HTTPClient *http.Client
}

func NewClient(baseURL string) *Client {
	return &Client{
		BaseURL:    baseURL,
		HTTPClient: &http.Client{Timeout: 30 * time.Second},
	}
}

func (c *Client) Get(endpoint string) (*http.Response, error) {
	return c.HTTPClient.Get(c.BaseURL + endpoint)
}

func (c *Client) Post(endpoint string, body interface{}) (*http.Response, error) {
	return c.HTTPClient.Post(c.BaseURL+endpoint, "application/json", nil)
}
`,
			count: 4,
		},
		{
			path: "pkg/config/config.go",
			content: `package config

type Config struct {
	DatabaseURL string
	ServerPort  int
	Debug       bool
	LogLevel    string
}

func Load() (*Config, error) {
	return &Config{
		DatabaseURL: "postgres://localhost/db",
		ServerPort:  8080,
		Debug:       false,
		LogLevel:    "info",
	}, nil
}
`,
			count: 2,
		},
		{
			path: "scripts/build.sh",
			content: `#!/bin/bash
set -e
echo "Building application..."
go build -o bin/app ./cmd/server
echo "Build complete!"
`,
			count: 3,
		},
		{
			path: "README.md",
			content: `# Project README

## Overview
This is a sample project.

## Installation
` + "```bash" + `
go install ./cmd/server
` + "```" + `

## Usage
Run the server with:
` + "```bash" + `
server --port 8080
` + "```" + `
`,
			count: 1,
		},
	}

	// Create files to reach target count
	fileCount := 0
	for _, pattern := range filePatterns {
		for i := 0; i < pattern.count; i++ {
			if fileCount >= numFiles {
				break
			}

			var filePath string
			if i == 0 {
				filePath = pattern.path
			} else {
				// Create variations of the file
				ext := filepath.Ext(pattern.path)
				base := pattern.path[:len(pattern.path)-len(ext)]
				filePath = fmt.Sprintf("%s_%d%s", base, i, ext)
			}

			fullPath := filepath.Join(tmpDir, filePath)
			if err := os.WriteFile(fullPath, []byte(pattern.content), 0644); err != nil {
				b.Fatalf("Failed to create file %s: %v", filePath, err)
			}

			// Store file info for cache
			info, err := os.Stat(fullPath)
			if err != nil {
				b.Fatalf("Failed to stat file %s: %v", filePath, err)
			}

			hash, err := HashFile(fullPath)
			if err != nil {
				b.Fatalf("Failed to hash file %s: %v", filePath, err)
			}

			createdFiles[filePath] = FileInfo{
				Hash:     hash,
				Modified: info.ModTime(),
				Size:     info.Size(),
			}

			fileCount++
		}
		if fileCount >= numFiles {
			break
		}
	}

	return tmpDir, createdFiles
}

// BenchmarkScanFiles_NoCacheSequential benchmarks baseline performance:
// - No cache (all files need hashing)
// - Sequential hashing (single worker)
// This represents the worst-case scenario before optimizations
func BenchmarkScanFiles_NoCacheSequential(b *testing.B) {
	repoPath, _ := setupBenchmarkRepo(b, 100)

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		metrics := &ScanMetrics{}
		_, err := ScanFiles(repoPath, []string{}, nil, metrics, 1) // maxHashWorkers=1 for sequential
		if err != nil {
			b.Fatalf("ScanFiles failed: %v", err)
		}
	}
}

// BenchmarkScanFiles_WithCacheSequential benchmarks selective hashing only:
// - With cache (unchanged files use cached hashes)
// - Sequential hashing (single worker)
// This measures the benefit of selective hashing without parallelism
func BenchmarkScanFiles_WithCacheSequential(b *testing.B) {
	repoPath, cachedFiles := setupBenchmarkRepo(b, 100)

	// Create cache with all files (simulating previous scan)
	cache := NewCache()
	cache.Files = cachedFiles

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		metrics := &ScanMetrics{}
		_, err := ScanFiles(repoPath, []string{}, cache, metrics, 1) // maxHashWorkers=1 for sequential
		if err != nil {
			b.Fatalf("ScanFiles failed: %v", err)
		}

		// Verify selective hashing is working (all files should be cached)
		if metrics.CachedFiles != len(cachedFiles) {
			b.Errorf("Expected %d cached files, got %d", len(cachedFiles), metrics.CachedFiles)
		}
		if metrics.HashedFiles != 0 {
			b.Errorf("Expected 0 hashed files, got %d", metrics.HashedFiles)
		}
	}
}

// BenchmarkScanFiles_WithCacheParallel benchmarks selective hashing + parallel processing:
// - With cache (unchanged files use cached hashes)
// - Parallel hashing (default worker count)
// This measures the combined benefit of both optimizations
func BenchmarkScanFiles_WithCacheParallel(b *testing.B) {
	repoPath, cachedFiles := setupBenchmarkRepo(b, 100)

	// Create cache with all files (simulating previous scan)
	cache := NewCache()
	cache.Files = cachedFiles

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		metrics := &ScanMetrics{}
		_, err := ScanFiles(repoPath, []string{}, cache, metrics, 0) // maxHashWorkers=0 for auto-detect
		if err != nil {
			b.Fatalf("ScanFiles failed: %v", err)
		}

		// Verify selective hashing is working (all files should be cached)
		if metrics.CachedFiles != len(cachedFiles) {
			b.Errorf("Expected %d cached files, got %d", len(cachedFiles), metrics.CachedFiles)
		}
		if metrics.HashedFiles != 0 {
			b.Errorf("Expected 0 hashed files, got %d", metrics.HashedFiles)
		}
	}
}

// BenchmarkScanFiles_PartialCacheParallel benchmarks realistic incremental scan:
// - With partial cache (some files changed, need rehashing)
// - Parallel hashing (default worker count)
// This simulates a typical incremental scan where some files have changed
func BenchmarkScanFiles_PartialCacheParallel(b *testing.B) {
	repoPath, cachedFiles := setupBenchmarkRepo(b, 100)

	// Modify 20% of files to simulate changes
	changedFiles := make(map[string]bool)
	fileCount := 0
	changePercent := 0.2
	numChanged := int(float64(len(cachedFiles)) * changePercent)

	for path := range cachedFiles {
		if fileCount >= numChanged {
			break
		}
		changedFiles[path] = true
		fileCount++
	}

	// Create cache with old modtime for changed files
	cache := NewCache()
	for path, info := range cachedFiles {
		if changedFiles[path] {
			// Set modtime to 1 hour ago to trigger rehashing
			cache.Files[path] = FileInfo{
				Hash:     info.Hash,
				Modified: info.Modified.Add(-1 * time.Hour),
				Size:     info.Size,
			}
		} else {
			cache.Files[path] = info
		}
	}

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		metrics := &ScanMetrics{}
		_, err := ScanFiles(repoPath, []string{}, cache, metrics, 0) // maxHashWorkers=0 for auto-detect
		if err != nil {
			b.Fatalf("ScanFiles failed: %v", err)
		}

		// Verify selective hashing is working correctly
		expectedCached := len(cachedFiles) - numChanged
		if metrics.CachedFiles != expectedCached {
			b.Errorf("Expected %d cached files, got %d", expectedCached, metrics.CachedFiles)
		}
		if metrics.HashedFiles != numChanged {
			b.Errorf("Expected %d hashed files, got %d", numChanged, metrics.HashedFiles)
		}
	}
}

// BenchmarkScanFiles_NoCacheParallel benchmarks parallel hashing without cache:
// - No cache (all files need hashing)
// - Parallel hashing (default worker count)
// This measures the benefit of parallel processing alone
func BenchmarkScanFiles_NoCacheParallel(b *testing.B) {
	repoPath, _ := setupBenchmarkRepo(b, 100)

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		metrics := &ScanMetrics{}
		_, err := ScanFiles(repoPath, []string{}, nil, metrics, 0) // maxHashWorkers=0 for auto-detect
		if err != nil {
			b.Fatalf("ScanFiles failed: %v", err)
		}

		// Verify all files were hashed
		if metrics.HashedFiles != metrics.TotalFiles {
			b.Errorf("Expected %d hashed files, got %d", metrics.TotalFiles, metrics.HashedFiles)
		}
	}
}

// BenchmarkScanFiles_LargeRepository benchmarks performance on larger repository
func BenchmarkScanFiles_LargeRepository(b *testing.B) {
	// Test with larger file count to simulate big projects
	repoPath, cachedFiles := setupBenchmarkRepo(b, 500)

	// Create cache with all files
	cache := NewCache()
	cache.Files = cachedFiles

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		metrics := &ScanMetrics{}
		_, err := ScanFiles(repoPath, []string{}, cache, metrics, 0)
		if err != nil {
			b.Fatalf("ScanFiles failed: %v", err)
		}
	}
}

// Benchmark helper to calculate throughput statistics
func BenchmarkScanFiles_WithStats(b *testing.B) {
	repoPath, cachedFiles := setupBenchmarkRepo(b, 100)

	// Calculate total size
	var totalSize int64
	for _, info := range cachedFiles {
		totalSize += info.Size
	}

	// Create cache
	cache := NewCache()
	cache.Files = cachedFiles

	b.ResetTimer()

	// Run benchmark and collect metrics
	var totalOps int
	var totalDuration time.Duration

	for i := 0; i < b.N; i++ {
		start := time.Now()
		metrics := &ScanMetrics{}
		_, err := ScanFiles(repoPath, []string{}, cache, metrics, 0)
		if err != nil {
			b.Fatalf("ScanFiles failed: %v", err)
		}
		duration := time.Since(start)

		totalOps++
		totalDuration += duration

		// Report throughput statistics
		filesPerSec := float64(metrics.TotalFiles) / duration.Seconds()
		mbPerSec := (float64(totalSize) / (1024 * 1024)) / duration.Seconds()

		b.ReportMetric(filesPerSec, "files/sec")
		b.ReportMetric(mbPerSec, "MB/sec")
	}

	// Report average metrics
	avgDuration := totalDuration / time.Duration(totalOps)
	b.ReportMetric(float64(avgDuration.Milliseconds()), "ms/op")
}

// BenchmarkParallelHashWorkers benchmarks different worker counts
func BenchmarkParallelHashWorkers(b *testing.B) {
	repoPath, _ := setupBenchmarkRepo(b, 100)

	workerCounts := []int{1, 2, 4, 8}

	for _, workers := range workerCounts {
		b.Run(fmt.Sprintf("workers=%d", workers), func(b *testing.B) {
			b.ResetTimer()
			for i := 0; i < b.N; i++ {
				metrics := &ScanMetrics{}
				_, err := ScanFiles(repoPath, []string{}, nil, metrics, workers)
				if err != nil {
					b.Fatalf("ScanFiles failed: %v", err)
				}
			}
		})
	}
}
</file>
<file path="internal/cache/cache_integration_test.go">
package cache

import (
	"os"
	"path/filepath"
	"testing"
	"time"
)

// TestIntegration_FullScanCycle verifies the complete workflow: load cache -> scan -> detect changes -> save
func TestIntegration_FullScanCycle(t *testing.T) {
	// Setup: Create temporary repository directory
	tmpDir := t.TempDir()

	// Create initial test files
	files := map[string]string{
		"main.go":   "package main\n\nfunc main() {}\n",
		"utils.go":  "package main\n\nfunc Utils() {}\n",
		"README.md": "# Test Project\n",
		"go.mod":    "module test\n\ngo 1.21\n",
	}

	for name, content := range files {
		path := filepath.Join(tmpDir, name)
		if err := os.WriteFile(path, []byte(content), 0644); err != nil {
			t.Fatalf("Failed to create %s: %v", name, err)
		}
	}

	// First scan: no cache exists
	cache1, err := LoadCache(tmpDir)
	if err != nil {
		t.Fatalf("LoadCache failed: %v", err)
	}

	if cache1.Version != CacheVersion {
		t.Errorf("Expected cache version %d, got %d", CacheVersion, cache1.Version)
	}

	if len(cache1.Files) != 0 {
		t.Errorf("Expected empty cache on first load, got %d files", len(cache1.Files))
	}

	// Scan files
	metrics1 := &ScanMetrics{}
	currentFiles1, err := ScanFiles(tmpDir, []string{}, cache1, metrics1, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify first scan results
	if metrics1.TotalFiles != 4 {
		t.Errorf("Expected 4 total files, got %d", metrics1.TotalFiles)
	}

	if metrics1.CachedFiles != 0 {
		t.Errorf("Expected 0 cached files on first scan, got %d", metrics1.CachedFiles)
	}

	if metrics1.HashedFiles != 4 {
		t.Errorf("Expected 4 hashed files on first scan, got %d", metrics1.HashedFiles)
	}

	// Detect changes (should detect all as new on first run)
	report1 := cache1.DetectChanges(tmpDir, currentFiles1)
	if !report1.IsFirstRun {
		t.Error("Expected IsFirstRun to be true")
	}

	if !report1.HasChanges {
		t.Error("Expected HasChanges to be true on first run")
	}

	if len(report1.NewFiles) != 4 {
		t.Errorf("Expected 4 new files, got %d", len(report1.NewFiles))
	}

	// Update cache after analysis
	agentResults1 := map[string]bool{
		"structure_analyzer":    true,
		"dependency_analyzer":   true,
		"data_flow_analyzer":    true,
		"request_flow_analyzer": true,
		"api_analyzer":          true,
	}
	cache1.UpdateAfterAnalysis(tmpDir, currentFiles1, agentResults1)

	// Save cache
	if err := cache1.Save(tmpDir); err != nil {
		t.Fatalf("Save failed: %v", err)
	}

	// Verify cache file was created
	cachePath := filepath.Join(tmpDir, CacheFileName)
	if _, err := os.Stat(cachePath); os.IsNotExist(err) {
		t.Fatal("Cache file was not created")
	}

	// Second scan: load existing cache
	cache2, err := LoadCache(tmpDir)
	if err != nil {
		t.Fatalf("LoadCache failed: %v", err)
	}

	// Verify cache was loaded correctly
	if len(cache2.Files) != 4 {
		t.Errorf("Expected 4 files in loaded cache, got %d", len(cache2.Files))
	}

	if cache2.LastAnalysis.IsZero() {
		t.Errorf("Expected LastAnalysis to be set")
	}

	// Scan files again (should use cache)
	metrics2 := &ScanMetrics{}
	currentFiles2, err := ScanFiles(tmpDir, []string{}, cache2, metrics2, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify cache was used
	if metrics2.TotalFiles != 4 {
		t.Errorf("Expected 4 total files, got %d", metrics2.TotalFiles)
	}

	if metrics2.CachedFiles != 4 {
		t.Errorf("Expected 4 cached files, got %d", metrics2.CachedFiles)
	}

	if metrics2.HashedFiles != 0 {
		t.Errorf("Expected 0 hashed files (all cached), got %d", metrics2.HashedFiles)
	}

	// Detect changes (should detect no changes)
	report2 := cache2.DetectChanges(tmpDir, currentFiles2)
	if report2.IsFirstRun {
		t.Error("Expected IsFirstRun to be false on second run")
	}

	if report2.HasChanges {
		t.Error("Expected HasChanges to be false when nothing changed")
	}

	if len(report2.NewFiles) != 0 {
		t.Errorf("Expected 0 new files, got %d", len(report2.NewFiles))
	}

	if len(report2.ModifiedFiles) != 0 {
		t.Errorf("Expected 0 modified files, got %d", len(report2.ModifiedFiles))
	}

	// Verify file hashes match between scans
	for path, file1 := range currentFiles1 {
		file2, exists := currentFiles2[path]
		if !exists {
			t.Errorf("File %s missing from second scan", path)
			continue
		}

		if file1.Hash != file2.Hash {
			t.Errorf("Hash mismatch for %s: %s != %s", path, file1.Hash, file2.Hash)
		}
	}
}

// TestIntegration_IncrementalScanWithChanges verifies incremental scanning when files are modified
func TestIntegration_IncrementalScanWithChanges(t *testing.T) {
	// Setup: Create temporary repository directory
	tmpDir := t.TempDir()

	// Create initial files
	initialFiles := map[string]string{
		"main.go":   "package main\n\nfunc main() {}\n",
		"utils.go":  "package main\n\nfunc Utils() {}\n",
		"README.md": "# Test Project\n",
	}

	for name, content := range initialFiles {
		path := filepath.Join(tmpDir, name)
		if err := os.WriteFile(path, []byte(content), 0644); err != nil {
			t.Fatalf("Failed to create %s: %v", name, err)
		}
	}

	// First scan: create initial cache
	cache1 := NewCache()
	metrics1 := &ScanMetrics{}
	currentFiles1, err := ScanFiles(tmpDir, []string{}, cache1, metrics1, 0)
	if err != nil {
		t.Fatalf("First ScanFiles failed: %v", err)
	}

	cache1.UpdateAfterAnalysis(tmpDir, currentFiles1, map[string]bool{"structure_analyzer": true})
	if err := cache1.Save(tmpDir); err != nil {
		t.Fatalf("Save failed: %v", err)
	}

	// Wait a bit to ensure mtime changes
	time.Sleep(10 * time.Millisecond)

	// Modify some files and add new ones
	// Note: We don't rewrite utils.go because WriteFile would change its mtime,
	// making the cache detect it as modified even though content is the same
	modifiedFiles := map[string]string{
		"main.go":   "package main\n\nfunc main() {\n\tprintln(\"changed\")\n}\n", // Modified
		"README.md": "# Test Project\n\nUpdated documentation\n",                  // Modified
		"new.go":    "package main\n\nfunc New() {}\n",                            // New file
	}

	for name, content := range modifiedFiles {
		path := filepath.Join(tmpDir, name)
		if err := os.WriteFile(path, []byte(content), 0644); err != nil {
			t.Fatalf("Failed to write %s: %v", name, err)
		}
	}

	// Second scan: load cache and detect changes
	cache2, err := LoadCache(tmpDir)
	if err != nil {
		t.Fatalf("LoadCache failed: %v", err)
	}

	metrics2 := &ScanMetrics{}
	currentFiles2, err := ScanFiles(tmpDir, []string{}, cache2, metrics2, 0)
	if err != nil {
		t.Fatalf("Second ScanFiles failed: %v", err)
	}

	// Verify scan metrics
	if metrics2.TotalFiles != 4 {
		t.Errorf("Expected 4 total files, got %d", metrics2.TotalFiles)
	}

	if metrics2.CachedFiles != 1 {
		t.Errorf("Expected 1 cached file (utils.go), got %d", metrics2.CachedFiles)
	}

	if metrics2.HashedFiles != 3 {
		t.Errorf("Expected 3 hashed files (main.go, README.md, new.go), got %d", metrics2.HashedFiles)
	}

	// Detect changes
	report2 := cache2.DetectChanges(tmpDir, currentFiles2)
	if !report2.HasChanges {
		t.Error("Expected HasChanges to be true when files changed")
	}

	if len(report2.NewFiles) != 1 {
		t.Errorf("Expected 1 new file (new.go), got %d", len(report2.NewFiles))
	}

	if report2.NewFiles[0] != "new.go" {
		t.Errorf("Expected new.go to be new, got %s", report2.NewFiles[0])
	}

	if len(report2.ModifiedFiles) != 2 {
		t.Errorf("Expected 2 modified files (main.go, README.md), got %d", len(report2.ModifiedFiles))
	}

	// Verify modified files are detected (order may vary)
	modifiedMap := make(map[string]bool)
	for _, file := range report2.ModifiedFiles {
		modifiedMap[file] = true
	}

	if !modifiedMap["main.go"] {
		t.Error("Expected main.go to be in modified files")
	}

	if !modifiedMap["README.md"] {
		t.Error("Expected README.md to be in modified files")
	}

	if modifiedMap["utils.go"] {
		t.Error("Expected utils.go to NOT be in modified files (unchanged)")
	}

	if len(report2.DeletedFiles) != 0 {
		t.Errorf("Expected 0 deleted files, got %d", len(report2.DeletedFiles))
	}
}

// TestIntegration_IncrementalScanWithDeletions verifies handling of deleted files
func TestIntegration_IncrementalScanWithDeletions(t *testing.T) {
	// Setup: Create temporary repository directory
	tmpDir := t.TempDir()

	// Create initial files
	initialFiles := map[string]string{
		"main.go":   "package main\n\nfunc main() {}\n",
		"utils.go":  "package main\n\nfunc Utils() {}\n",
		"config.go": "package main\n\nfunc Config() {}\n",
	}

	for name, content := range initialFiles {
		path := filepath.Join(tmpDir, name)
		if err := os.WriteFile(path, []byte(content), 0644); err != nil {
			t.Fatalf("Failed to create %s: %v", name, err)
		}
	}

	// First scan: create initial cache
	cache1 := NewCache()
	metrics1 := &ScanMetrics{}
	currentFiles1, err := ScanFiles(tmpDir, []string{}, cache1, metrics1, 0)
	if err != nil {
		t.Fatalf("First ScanFiles failed: %v", err)
	}

	cache1.UpdateAfterAnalysis(tmpDir, currentFiles1, map[string]bool{"structure_analyzer": true})
	if err := cache1.Save(tmpDir); err != nil {
		t.Fatalf("Save failed: %v", err)
	}

	// Delete some files
	_ = os.Remove(filepath.Join(tmpDir, "utils.go"))
	_ = os.Remove(filepath.Join(tmpDir, "config.go"))

	// Modify remaining file
	time.Sleep(10 * time.Millisecond)
	_ = os.WriteFile(filepath.Join(tmpDir, "main.go"), []byte("package main\n\nfunc main() {\n}\n"), 0644)

	// Second scan: load cache and detect deletions
	cache2, err := LoadCache(tmpDir)
	if err != nil {
		t.Fatalf("LoadCache failed: %v", err)
	}

	metrics2 := &ScanMetrics{}
	currentFiles2, err := ScanFiles(tmpDir, []string{}, cache2, metrics2, 0)
	if err != nil {
		t.Fatalf("Second ScanFiles failed: %v", err)
	}

	// Verify only main.go is in current files
	if len(currentFiles2) != 1 {
		t.Errorf("Expected 1 current file, got %d", len(currentFiles2))
	}

	if _, exists := currentFiles2["main.go"]; !exists {
		t.Error("Expected main.go to be in current files")
	}

	// Detect changes
	report2 := cache2.DetectChanges(tmpDir, currentFiles2)
	if !report2.HasChanges {
		t.Error("Expected HasChanges to be true when files deleted")
	}

	if len(report2.DeletedFiles) != 2 {
		t.Errorf("Expected 2 deleted files, got %d", len(report2.DeletedFiles))
	}

	// Verify deleted files (order may vary)
	deletedMap := make(map[string]bool)
	for _, file := range report2.DeletedFiles {
		deletedMap[file] = true
	}

	if !deletedMap["utils.go"] {
		t.Error("Expected utils.go to be in deleted files")
	}

	if !deletedMap["config.go"] {
		t.Error("Expected config.go to be in deleted files")
	}

	if deletedMap["main.go"] {
		t.Error("Expected main.go to NOT be in deleted files")
	}
}

// TestIntegration_CachePersistenceAcrossMultipleCycles verifies cache persistence across many scan cycles
func TestIntegration_CachePersistenceAcrossMultipleCycles(t *testing.T) {
	// Setup: Create temporary repository directory
	tmpDir := t.TempDir()

	// Create initial files
	files := map[string]string{
		"main.go": "package main\n\nfunc main() {}\n",
	}

	for name, content := range files {
		path := filepath.Join(tmpDir, name)
		if err := os.WriteFile(path, []byte(content), 0644); err != nil {
			t.Fatalf("Failed to create %s: %v", name, err)
		}
	}

	// Perform multiple scan cycles
	numCycles := 5
	var prevHash string

	for i := 0; i < numCycles; i++ {
		// Load cache
		cache, err := LoadCache(tmpDir)
		if err != nil {
			t.Fatalf("Cycle %d: LoadCache failed: %v", i, err)
		}

		// Scan files
		metrics := &ScanMetrics{}
		currentFiles, err := ScanFiles(tmpDir, []string{}, cache, metrics, 0)
		if err != nil {
			t.Fatalf("Cycle %d: ScanFiles failed: %v", i, err)
		}

		// On first cycle, all files should be hashed
		if i == 0 {
			if metrics.HashedFiles != 1 {
				t.Errorf("Cycle %d: Expected 1 hashed file, got %d", i, metrics.HashedFiles)
			}
			if metrics.CachedFiles != 0 {
				t.Errorf("Cycle %d: Expected 0 cached files, got %d", i, metrics.CachedFiles)
			}
		} else {
			// On subsequent cycles, all files should be cached
			if metrics.HashedFiles != 0 {
				t.Errorf("Cycle %d: Expected 0 hashed files, got %d", i, metrics.HashedFiles)
			}
			if metrics.CachedFiles != 1 {
				t.Errorf("Cycle %d: Expected 1 cached file, got %d", i, metrics.CachedFiles)
			}
		}

		// Update and save cache
		cache.UpdateAfterAnalysis(tmpDir, currentFiles, map[string]bool{"structure_analyzer": true})
		if err := cache.Save(tmpDir); err != nil {
			t.Fatalf("Cycle %d: Save failed: %v", i, err)
		}

		// Verify hash remains consistent across cycles
		currentHash := currentFiles["main.go"].Hash
		if prevHash != "" && currentHash != prevHash {
			t.Errorf("Cycle %d: Hash changed unexpectedly: %s -> %s", i, prevHash, currentHash)
		}
		prevHash = currentHash

		// Small delay to ensure mtime changes if we were to modify files
		time.Sleep(5 * time.Millisecond)
	}

	// Final verification: load cache one more time and verify contents
	finalCache, err := LoadCache(tmpDir)
	if err != nil {
		t.Fatalf("Final LoadCache failed: %v", err)
	}

	if len(finalCache.Files) != 1 {
		t.Errorf("Expected 1 file in final cache, got %d", len(finalCache.Files))
	}

	if finalCache.Files["main.go"].Hash != prevHash {
		t.Errorf("Final cache hash mismatch: %s != %s", finalCache.Files["main.go"].Hash, prevHash)
	}

	if len(finalCache.Agents) != 1 {
		t.Errorf("Expected 1 agent in final cache, got %d", len(finalCache.Agents))
	}
}

// TestIntegration_CacheWithIgnorePatterns verifies cache works correctly with ignore patterns
func TestIntegration_CacheWithIgnorePatterns(t *testing.T) {
	// Setup: Create temporary repository directory
	tmpDir := t.TempDir()

	// Create files including some that should be ignored
	files := map[string]string{
		"main.go":         "package main\n\nfunc main() {}\n",
		"vendor/lib.go":   "package vendor\n",
		"test_test.go":    "package test\n",
		"README.md":       "# Test\n",
		".ai/config.yaml": "config: true\n",
	}

	for name, content := range files {
		path := filepath.Join(tmpDir, name)
		if err := os.MkdirAll(filepath.Dir(path), 0755); err != nil {
			t.Fatalf("Failed to create directory: %v", err)
		}
		if err := os.WriteFile(path, []byte(content), 0644); err != nil {
			t.Fatalf("Failed to create %s: %v", name, err)
		}
	}

	// First scan with ignore patterns
	ignorePatterns := []string{"vendor", "*_test.go"}
	cache1 := NewCache()
	metrics1 := &ScanMetrics{}
	currentFiles1, err := ScanFiles(tmpDir, ignorePatterns, cache1, metrics1, 0)
	if err != nil {
		t.Fatalf("First ScanFiles failed: %v", err)
	}

	// Verify only non-ignored files were scanned
	if metrics1.TotalFiles != 2 {
		t.Errorf("Expected 2 total files (main.go, README.md), got %d", metrics1.TotalFiles)
	}

	if len(currentFiles1) != 2 {
		t.Errorf("Expected 2 files in results, got %d", len(currentFiles1))
	}

	// Save cache
	cache1.UpdateAfterAnalysis(tmpDir, currentFiles1, map[string]bool{"structure_analyzer": true})
	if err := cache1.Save(tmpDir); err != nil {
		t.Fatalf("Save failed: %v", err)
	}

	// Second scan with same ignore patterns
	cache2, err := LoadCache(tmpDir)
	if err != nil {
		t.Fatalf("LoadCache failed: %v", err)
	}

	metrics2 := &ScanMetrics{}
	_, err = ScanFiles(tmpDir, ignorePatterns, cache2, metrics2, 0)
	if err != nil {
		t.Fatalf("Second ScanFiles failed: %v", err)
	}

	// Verify cache was used correctly
	if metrics2.CachedFiles != 2 {
		t.Errorf("Expected 2 cached files, got %d", metrics2.CachedFiles)
	}

	if metrics2.HashedFiles != 0 {
		t.Errorf("Expected 0 hashed files, got %d", metrics2.HashedFiles)
	}

	// Verify ignored files are not in cache
	if _, exists := cache2.Files["vendor/lib.go"]; exists {
		t.Error("vendor/lib.go should not be in cache (ignored)")
	}

	if _, exists := cache2.Files["test_test.go"]; exists {
		t.Error("test_test.go should not be in cache (ignored)")
	}

	if _, exists := cache2.Files[".ai/config.yaml"]; exists {
		t.Error(".ai/config.yaml should not be in cache (ignored)")
	}
}

// TestIntegration_CacheCorruptionHandling verifies handling of corrupted cache files
func TestIntegration_CacheCorruptionHandling(t *testing.T) {
	// Setup: Create temporary repository directory
	tmpDir := t.TempDir()

	// Create a corrupted cache file
	cachePath := filepath.Join(tmpDir, CacheFileName)
	if err := os.MkdirAll(filepath.Dir(cachePath), 0755); err != nil {
		t.Fatalf("Failed to create cache directory: %v", err)
	}

	// Write invalid JSON
	corruptedData := []byte("{ invalid json content")
	if err := os.WriteFile(cachePath, corruptedData, 0644); err != nil {
		t.Fatalf("Failed to write corrupted cache: %v", err)
	}

	// Load cache should handle corruption gracefully and return new cache
	cache, err := LoadCache(tmpDir)
	if err != nil {
		t.Fatalf("LoadCache should not fail on corrupted cache, got: %v", err)
	}

	// Should return a fresh cache
	if cache == nil {
		t.Fatal("Expected non-nil cache after corruption")
	}

	if len(cache.Files) != 0 {
		t.Errorf("Expected empty cache after corruption, got %d files", len(cache.Files))
	}

	if cache.Version != CacheVersion {
		t.Errorf("Expected cache version %d after corruption, got %d", CacheVersion, cache.Version)
	}

	// Verify we can still scan files normally
	testFile := filepath.Join(tmpDir, "test.go")
	if err := os.WriteFile(testFile, []byte("package main\n"), 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	metrics := &ScanMetrics{}
	files, err := ScanFiles(tmpDir, []string{}, cache, metrics, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	if len(files) != 1 {
		t.Errorf("Expected 1 file, got %d", len(files))
	}

	if metrics.HashedFiles != 1 {
		t.Errorf("Expected 1 hashed file, got %d", metrics.HashedFiles)
	}
}

// TestIntegration_VersionMismatchHandling verifies handling of cache version mismatches
func TestIntegration_VersionMismatchHandling(t *testing.T) {
	// Setup: Create temporary repository directory
	tmpDir := t.TempDir()

	// Create cache file with different version
	cachePath := filepath.Join(tmpDir, CacheFileName)
	if err := os.MkdirAll(filepath.Dir(cachePath), 0755); err != nil {
		t.Fatalf("Failed to create cache directory: %v", err)
	}

	// oldCache struct is intentionally unused - we use raw JSON instead to simulate
	// a cache file with a different version that we don't know how to deserialize

	data := []byte(`{"version":999,"last_analysis":"0001-01-01T00:00:00Z","git_commit":"","files":{},"agents":{}}`)
	if err := os.WriteFile(cachePath, data, 0644); err != nil {
		t.Fatalf("Failed to write old cache: %v", err)
	}

	// Load cache should handle version mismatch and return new cache
	cache, err := LoadCache(tmpDir)
	if err != nil {
		t.Fatalf("LoadCache should not fail on version mismatch, got: %v", err)
	}

	// Should return a fresh cache with current version
	if cache.Version != CacheVersion {
		t.Errorf("Expected cache version %d, got %d", CacheVersion, cache.Version)
	}

	if len(cache.Files) != 0 {
		t.Errorf("Expected empty cache after version mismatch, got %d files", len(cache.Files))
	}
}
</file>
<file path="internal/cache/cache_test.go">
package cache

import (
	"os"
	"path/filepath"
	"testing"
	"time"
)

// TestScanFiles_CacheHit verifies that files with unchanged mtime and size reuse cached hashes
func TestScanFiles_CacheHit(t *testing.T) {
	// Setup: Create temporary directory with test files
	tmpDir := t.TempDir()

	// Create test file with known content
	testFile := filepath.Join(tmpDir, "test.go")
	content := []byte("package main\n\nfunc main() {}\n")
	if err := os.WriteFile(testFile, content, 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	// Get file metadata
	info, err := os.Stat(testFile)
	if err != nil {
		t.Fatalf("Failed to stat test file: %v", err)
	}
	modTime := info.ModTime()
	fileSize := info.Size()

	// Compute expected hash
	expectedHash, err := HashFile(testFile)
	if err != nil {
		t.Fatalf("Failed to compute hash: %v", err)
	}

	// Create cache with the file entry
	cache := NewCache()
	cache.Files["test.go"] = FileInfo{
		Hash:     expectedHash,
		Modified: modTime,
		Size:     fileSize,
	}

	// Scan with cache
	metrics := &ScanMetrics{}
	files, err := ScanFiles(tmpDir, []string{}, cache, metrics, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify file info matches
	fileInfo, exists := files["test.go"]
	if !exists {
		t.Fatal("File not found in scan results")
	}

	if fileInfo.Hash != expectedHash {
		t.Errorf("Expected hash %s, got %s", expectedHash, fileInfo.Hash)
	}

	if !fileInfo.Modified.Equal(modTime) {
		t.Errorf("Expected modTime %v, got %v", modTime, fileInfo.Modified)
	}

	if fileInfo.Size != fileSize {
		t.Errorf("Expected size %d, got %d", fileSize, fileInfo.Size)
	}

	// Verify cache was used (file should be cached, not hashed)
	if metrics.CachedFiles != 1 {
		t.Errorf("Expected 1 cached file, got %d", metrics.CachedFiles)
	}

	if metrics.HashedFiles != 0 {
		t.Errorf("Expected 0 hashed files, got %d", metrics.HashedFiles)
	}

	if metrics.TotalFiles != 1 {
		t.Errorf("Expected 1 total file, got %d", metrics.TotalFiles)
	}
}

// TestScanFiles_CacheMissDifferentMTime verifies that files with different mtime get rehashed
func TestScanFiles_CacheMissDifferentMTime(t *testing.T) {
	// Setup: Create temporary directory with test file
	tmpDir := t.TempDir()

	testFile := filepath.Join(tmpDir, "test.go")
	content := []byte("package main\n\nfunc main() {}\n")
	if err := os.WriteFile(testFile, content, 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	// Get file metadata
	info, err := os.Stat(testFile)
	if err != nil {
		t.Fatalf("Failed to stat test file: %v", err)
	}
	fileSize := info.Size()

	// Compute current hash
	expectedHash, err := HashFile(testFile)
	if err != nil {
		t.Fatalf("Failed to compute hash: %v", err)
	}

	// Create cache with OLD modtime (1 hour in the past)
	oldModTime := info.ModTime().Add(-1 * time.Hour)
	cache := NewCache()
	cache.Files["test.go"] = FileInfo{
		Hash:     "old-hash",
		Modified: oldModTime,
		Size:     fileSize,
	}

	// Scan with cache
	metrics := &ScanMetrics{}
	files, err := ScanFiles(tmpDir, []string{}, cache, metrics, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify file was rehashed
	fileInfo, exists := files["test.go"]
	if !exists {
		t.Fatal("File not found in scan results")
	}

	if fileInfo.Hash != expectedHash {
		t.Errorf("Expected hash %s, got %s", expectedHash, fileInfo.Hash)
	}

	if fileInfo.Modified.Equal(oldModTime) {
		t.Error("Expected modTime to be updated to current time")
	}

	// Verify cache was NOT used (file should be rehashed)
	if metrics.CachedFiles != 0 {
		t.Errorf("Expected 0 cached files, got %d", metrics.CachedFiles)
	}

	if metrics.HashedFiles != 1 {
		t.Errorf("Expected 1 hashed file, got %d", metrics.HashedFiles)
	}
}

// TestScanFiles_CacheMissDifferentSize verifies that files with different size get rehashed
func TestScanFiles_CacheMissDifferentSize(t *testing.T) {
	// Setup: Create temporary directory with test file
	tmpDir := t.TempDir()

	testFile := filepath.Join(tmpDir, "test.go")
	content := []byte("package main\n\nfunc main() {}\n")
	if err := os.WriteFile(testFile, content, 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	// Get file metadata
	info, err := os.Stat(testFile)
	if err != nil {
		t.Fatalf("Failed to stat test file: %v", err)
	}
	modTime := info.ModTime()

	// Compute current hash
	expectedHash, err := HashFile(testFile)
	if err != nil {
		t.Fatalf("Failed to compute hash: %v", err)
	}

	// Create cache with WRONG size (different from actual)
	wrongSize := info.Size() + 100
	cache := NewCache()
	cache.Files["test.go"] = FileInfo{
		Hash:     "old-hash",
		Modified: modTime,
		Size:     wrongSize,
	}

	// Scan with cache
	metrics := &ScanMetrics{}
	files, err := ScanFiles(tmpDir, []string{}, cache, metrics, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify file was rehashed
	fileInfo, exists := files["test.go"]
	if !exists {
		t.Fatal("File not found in scan results")
	}

	if fileInfo.Hash != expectedHash {
		t.Errorf("Expected hash %s, got %s", expectedHash, fileInfo.Hash)
	}

	if fileInfo.Size != info.Size() {
		t.Errorf("Expected size %d, got %d", info.Size(), fileInfo.Size)
	}

	// Verify cache was NOT used (file should be rehashed)
	if metrics.CachedFiles != 0 {
		t.Errorf("Expected 0 cached files, got %d", metrics.CachedFiles)
	}

	if metrics.HashedFiles != 1 {
		t.Errorf("Expected 1 hashed file, got %d", metrics.HashedFiles)
	}
}

// TestScanFiles_NewFiles verifies that files not in cache get hashed
func TestScanFiles_NewFiles(t *testing.T) {
	// Setup: Create temporary directory with test file
	tmpDir := t.TempDir()

	testFile := filepath.Join(tmpDir, "test.go")
	content := []byte("package main\n\nfunc main() {}\n")
	if err := os.WriteFile(testFile, content, 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	// Compute expected hash
	expectedHash, err := HashFile(testFile)
	if err != nil {
		t.Fatalf("Failed to compute hash: %v", err)
	}

	// Create empty cache (no files)
	cache := NewCache()

	// Scan with cache
	metrics := &ScanMetrics{}
	files, err := ScanFiles(tmpDir, []string{}, cache, metrics, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify file was hashed
	fileInfo, exists := files["test.go"]
	if !exists {
		t.Fatal("File not found in scan results")
	}

	if fileInfo.Hash != expectedHash {
		t.Errorf("Expected hash %s, got %s", expectedHash, fileInfo.Hash)
	}

	// Verify file was hashed (not cached)
	if metrics.CachedFiles != 0 {
		t.Errorf("Expected 0 cached files, got %d", metrics.CachedFiles)
	}

	if metrics.HashedFiles != 1 {
		t.Errorf("Expected 1 hashed file, got %d", metrics.HashedFiles)
	}
}

// TestScanFiles_DeletedFiles verifies that files in cache but not on disk are handled correctly
func TestScanFiles_DeletedFiles(t *testing.T) {
	// Setup: Create temporary directory
	tmpDir := t.TempDir()

	// Create cache with a file that doesn't exist on disk
	cache := NewCache()
	cache.Files["deleted.go"] = FileInfo{
		Hash:     "some-hash",
		Modified: time.Now(),
		Size:     100,
	}

	// Scan with cache (empty directory)
	metrics := &ScanMetrics{}
	files, err := ScanFiles(tmpDir, []string{}, cache, metrics, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify deleted file is not in scan results
	if _, exists := files["deleted.go"]; exists {
		t.Error("Deleted file should not be in scan results")
	}

	// Verify no files were processed
	if metrics.TotalFiles != 0 {
		t.Errorf("Expected 0 total files, got %d", metrics.TotalFiles)
	}

	if metrics.CachedFiles != 0 {
		t.Errorf("Expected 0 cached files, got %d", metrics.CachedFiles)
	}

	if metrics.HashedFiles != 0 {
		t.Errorf("Expected 0 hashed files, got %d", metrics.HashedFiles)
	}
}

// TestScanFiles_MultipleFilesMixedCache verifies selective hashing with multiple files
func TestScanFiles_MultipleFilesMixedCache(t *testing.T) {
	// Setup: Create temporary directory with multiple test files
	tmpDir := t.TempDir()

	// Create three test files
	files := map[string]string{
		"cached.go":  "package cached\n",
		"changed.go": "package changed\n",
		"newfile.go": "package newfile\n",
	}

	expectedHashes := make(map[string]string)
	for name, content := range files {
		path := filepath.Join(tmpDir, name)
		if err := os.WriteFile(path, []byte(content), 0644); err != nil {
			t.Fatalf("Failed to create %s: %v", name, err)
		}

		hash, err := HashFile(path)
		if err != nil {
			t.Fatalf("Failed to hash %s: %v", name, err)
		}
		expectedHashes[name] = hash
	}

	// Get file metadata
	cachedInfo, _ := os.Stat(filepath.Join(tmpDir, "cached.go"))
	changedInfo, _ := os.Stat(filepath.Join(tmpDir, "changed.go"))

	// Create cache:
	// - cached.go: same mtime and size (should use cache)
	// - changed.go: different mtime (should rehash)
	// - newfile.go: not in cache (should hash)
	cache := NewCache()
	cache.Files["cached.go"] = FileInfo{
		Hash:     expectedHashes["cached.go"],
		Modified: cachedInfo.ModTime(),
		Size:     cachedInfo.Size(),
	}
	cache.Files["changed.go"] = FileInfo{
		Hash:     "old-hash",
		Modified: changedInfo.ModTime().Add(-1 * time.Hour),
		Size:     changedInfo.Size(),
	}
	// newfile.go is not in cache

	// Scan with cache
	metrics := &ScanMetrics{}
	scanResults, err := ScanFiles(tmpDir, []string{}, cache, metrics, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify all files are in results
	if len(scanResults) != 3 {
		t.Errorf("Expected 3 files, got %d", len(scanResults))
	}

	// Verify cached.go used cached hash
	if scanResults["cached.go"].Hash != expectedHashes["cached.go"] {
		t.Errorf("cached.go: expected hash %s, got %s",
			expectedHashes["cached.go"], scanResults["cached.go"].Hash)
	}

	// Verify changed.go was rehashed
	if scanResults["changed.go"].Hash != expectedHashes["changed.go"] {
		t.Errorf("changed.go: expected hash %s, got %s",
			expectedHashes["changed.go"], scanResults["changed.go"].Hash)
	}

	// Verify newfile.go was hashed
	if scanResults["newfile.go"].Hash != expectedHashes["newfile.go"] {
		t.Errorf("newfile.go: expected hash %s, got %s",
			expectedHashes["newfile.go"], scanResults["newfile.go"].Hash)
	}

	// Verify metrics
	if metrics.TotalFiles != 3 {
		t.Errorf("Expected 3 total files, got %d", metrics.TotalFiles)
	}

	if metrics.CachedFiles != 1 {
		t.Errorf("Expected 1 cached file, got %d", metrics.CachedFiles)
	}

	if metrics.HashedFiles != 2 {
		t.Errorf("Expected 2 hashed files, got %d", metrics.HashedFiles)
	}
}

// TestScanFiles_NoCache verifies that ScanFiles works correctly without cache
func TestScanFiles_NoCache(t *testing.T) {
	// Setup: Create temporary directory with test file
	tmpDir := t.TempDir()

	testFile := filepath.Join(tmpDir, "test.go")
	content := []byte("package main\n\nfunc main() {}\n")
	if err := os.WriteFile(testFile, content, 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	// Compute expected hash
	expectedHash, err := HashFile(testFile)
	if err != nil {
		t.Fatalf("Failed to compute hash: %v", err)
	}

	// Scan WITHOUT cache (pass nil)
	files, err := ScanFiles(tmpDir, []string{}, nil, nil, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify file was hashed
	fileInfo, exists := files["test.go"]
	if !exists {
		t.Fatal("File not found in scan results")
	}

	if fileInfo.Hash != expectedHash {
		t.Errorf("Expected hash %s, got %s", expectedHash, fileInfo.Hash)
	}
}

// TestScanFiles_WithIgnorePatterns verifies that ignore patterns are respected
func TestScanFiles_WithIgnorePatterns(t *testing.T) {
	// Setup: Create temporary directory with test files
	tmpDir := t.TempDir()

	// Create files
	files := map[string]string{
		"main.go":       "package main\n",
		"vendor/lib.go": "package vendor\n",
		"test_test.go":  "package test\n",
	}

	for name, content := range files {
		path := filepath.Join(tmpDir, name)
		if err := os.MkdirAll(filepath.Dir(path), 0755); err != nil {
			t.Fatalf("Failed to create directory: %v", err)
		}
		if err := os.WriteFile(path, []byte(content), 0644); err != nil {
			t.Fatalf("Failed to create %s: %v", name, err)
		}
	}

	// Scan with ignore patterns
	ignorePatterns := []string{"vendor", "*_test.go"}
	scannedFiles, err := ScanFiles(tmpDir, ignorePatterns, nil, nil, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	// Verify only main.go is in results
	if len(scannedFiles) != 1 {
		t.Errorf("Expected 1 file, got %d", len(scannedFiles))
	}

	if _, exists := scannedFiles["main.go"]; !exists {
		t.Error("main.go should be in results")
	}

	if _, exists := scannedFiles["vendor/lib.go"]; exists {
		t.Error("vendor/lib.go should be ignored")
	}

	if _, exists := scannedFiles["test_test.go"]; exists {
		t.Error("test_test.go should be ignored")
	}
}

// TestScanFiles_MetricsNil verifies that ScanFiles works when metrics is nil
func TestScanFiles_MetricsNil(t *testing.T) {
	// Setup: Create temporary directory with test file
	tmpDir := t.TempDir()

	testFile := filepath.Join(tmpDir, "test.go")
	content := []byte("package main\n\nfunc main() {}\n")
	if err := os.WriteFile(testFile, content, 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	// Create cache
	cache := NewCache()

	// Scan with nil metrics (should not panic)
	files, err := ScanFiles(tmpDir, []string{}, cache, nil, 0)
	if err != nil {
		t.Fatalf("ScanFiles failed: %v", err)
	}

	if len(files) != 1 {
		t.Errorf("Expected 1 file, got %d", len(files))
	}
}

// TestHashFile verifies that HashFile computes correct SHA256 hash
func TestHashFile(t *testing.T) {
	// Setup: Create temporary file with known content
	tmpDir := t.TempDir()
	testFile := filepath.Join(tmpDir, "test.txt")
	content := []byte("Hello, World!\n")

	if err := os.WriteFile(testFile, content, 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	// Expected SHA256 hash of "Hello, World!\n"
	expectedHash := "c98c24b677eff44860afea6f493bbaec5bb1c4cbb209c6fc2bbb47f66ff2ad31"

	hash, err := HashFile(testFile)
	if err != nil {
		t.Fatalf("HashFile failed: %v", err)
	}

	if hash != expectedHash {
		t.Errorf("Expected hash %s, got %s", expectedHash, hash)
	}
}

// TestHashFile_NonExistent verifies that HashFile handles non-existent files
func TestHashFile_NonExistent(t *testing.T) {
	nonExistentFile := "/tmp/this-file-does-not-exist-12345.txt"

	_, err := HashFile(nonExistentFile)
	if err == nil {
		t.Error("Expected error for non-existent file, got nil")
	}
}
</file>
<file path="internal/config/loader.go">
package config

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/joho/godotenv"
	"github.com/spf13/viper"
	"github.com/user/gendocs/internal/errors"
)

// Loader handles loading configuration from multiple sources
type Loader struct {
	v *viper.Viper
}

// NewLoader creates a new configuration loader
func NewLoader() *Loader {
	// Load .env file if exists
	_ = godotenv.Load()

	v := viper.New()
	v.SetConfigType("yaml")
	v.AutomaticEnv()
	v.SetEnvPrefix("GENDOCS")
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))

	return &Loader{v: v}
}

// LoadForAgent loads configuration for a specific agent section
// Precedence: CLI > .ai/config.yaml > ~/.gendocs.yaml > Environment > Defaults
func (l *Loader) LoadForAgent(repoPath, section string, cliOverrides map[string]interface{}) (*viper.Viper, error) {
	// 1. Load defaults (set via struct defaults)

	// 2. Load from ~/.gendocs.yaml (global user config)
	if err := l.loadGlobalConfig(); err != nil {
		return nil, err
	}

	// 3. Load from .ai/config.yaml (project-specific config)
	if err := l.loadProjectConfig(repoPath); err != nil {
		return nil, err
	}

	// 4. Apply CLI overrides
	if err := l.applyCLIOverrides(cliOverrides); err != nil {
		return nil, err
	}

	return l.v, nil
}

// loadGlobalConfig loads configuration from ~/.gendocs.yaml
func (l *Loader) loadGlobalConfig() error {
	homeDir, err := os.UserHomeDir()
	if err != nil {
		return nil // Not a fatal error
	}

	globalConfig := filepath.Join(homeDir, ".gendocs.yaml")
	if _, err := os.Stat(globalConfig); err != nil {
		return nil // File doesn't exist, skip
	}

	l.v.SetConfigFile(globalConfig)
	if err := l.v.ReadInConfig(); err != nil {
		return errors.NewConfigFileError(globalConfig, err)
	}

	return nil
}

// loadProjectConfig loads configuration from .ai/config.yaml
func (l *Loader) loadProjectConfig(repoPath string) error {
	if repoPath == "" {
		repoPath = "."
	}

	configPath := filepath.Join(repoPath, ".ai", "config.yaml")
	if _, err := os.Stat(configPath); err != nil {
		return nil // File doesn't exist, skip
	}

	l.v.SetConfigFile(configPath)
	if err := l.v.MergeInConfig(); err != nil {
		return errors.NewConfigFileError(configPath, err)
	}

	return nil
}

// applyCLIOverrides applies CLI flag overrides
func (l *Loader) applyCLIOverrides(overrides map[string]interface{}) error {
	for key, value := range overrides {
		// Only set if value is not nil/zero
		if value != nil {
			l.v.Set(key, value)
		}
	}
	return nil
}

// GetEnvVar gets an environment variable, returning an error if not set
func GetEnvVar(name, description string) (string, error) {
	value := os.Getenv(name)
	if value == "" {
		return "", errors.NewMissingEnvVarError(name, description)
	}
	return value, nil
}

// GetEnvVarOrDefault gets an environment variable with a default value
func GetEnvVarOrDefault(name, defaultValue string) string {
	value := os.Getenv(name)
	if value == "" {
		return defaultValue
	}
	return value
}

// MergeConfigs merges multiple configuration sources with precedence
// Precedence order (highest to lowest): cli, project, global, env, defaults
func MergeConfigs(repoPath string, section string, defaults interface{}, cliOverrides map[string]interface{}) (map[string]interface{}, error) {
	loader := NewLoader()

	// Load all config sources
	v, err := loader.LoadForAgent(repoPath, section, cliOverrides)
	if err != nil {
		return nil, err
	}

	// Get the section-specific config
	var sectionConfig map[string]interface{}
	if section != "" {
		sectionConfig = v.GetStringMap(section)
	} else {
		// Get all settings if no section specified
		sectionConfig = v.AllSettings()
	}

	for key, value := range cliOverrides {
		if value != nil {
			setNested(sectionConfig, key, value)
		}
	}

	return sectionConfig, nil
}

// LoadAnalyzerConfig loads and validates analyzer configuration
func LoadAnalyzerConfig(repoPath string, cliOverrides map[string]interface{}) (*AnalyzerConfig, error) {
	configMap, err := MergeConfigs(repoPath, "analyzer", &AnalyzerConfig{}, cliOverrides)
	if err != nil {
		return nil, err
	}

	// Create config from map
	cfg := &AnalyzerConfig{
		BaseConfig: BaseConfig{
			RepoPath: getString(configMap, "repo_path", "."),
			Debug:    getBool(configMap, "debug", false),
		},
		MaxWorkers: getInt(configMap, "max_workers", 0),
	}

	// Load LLM config from environment or config
	cfg.LLM = LLMConfig{
		Provider:    getString(configMap, "llm.provider", getEnvOrDefault("ANALYZER_LLM_PROVIDER", "openai")),
		Model:       getString(configMap, "llm.model", getEnvOrDefault("ANALYZER_LLM_MODEL", "gpt-4o")),
		APIKey:      getString(configMap, "llm.api_key", getEnvOrDefault("ANALYZER_LLM_API_KEY", "")),
		BaseURL:     getString(configMap, "llm.base_url", getEnvOrDefault("ANALYZER_LLM_BASE_URL", "")),
		Retries:     getInt(configMap, "llm.retries", getEnvIntOrDefault("ANALYZER_AGENT_RETRIES", 2)),
		Timeout:     getInt(configMap, "llm.timeout", getEnvIntOrDefault("ANALYZER_LLM_TIMEOUT", 180)),
		MaxTokens:   getInt(configMap, "llm.max_tokens", getEnvIntOrDefault("ANALYZER_LLM_MAX_TOKENS", 8192)),
		Temperature: getFloat64(configMap, "llm.temperature", getEnvFloatOrDefault("ANALYZER_LLM_TEMPERATURE", 0.0)),
	}

	cfg.ExcludeStructure = getBool(configMap, "exclude_code_structure", false)
	cfg.ExcludeDataFlow = getBool(configMap, "exclude_data_flow", false)
	cfg.ExcludeDeps = getBool(configMap, "exclude_dependencies", false)
	cfg.ExcludeReqFlow = getBool(configMap, "exclude_request_flow", false)
	cfg.ExcludeAPI = getBool(configMap, "exclude_api_analysis", false)
	cfg.MaxHashWorkers = getInt(configMap, "max_hash_workers", 0)
	cfg.Force = getBool(configMap, "force", false)

	// Validate required fields
	if err := validateLLMConfig(&cfg.LLM, "ANALYZER"); err != nil {
		return nil, err
	}

	return cfg, nil
}

func LoadDocumenterConfig(repoPath string, cliOverrides map[string]interface{}) (*DocumenterConfig, error) {
	configMap, err := MergeConfigs(repoPath, "documenter", &DocumenterConfig{}, cliOverrides)
	if err != nil {
		return nil, err
	}

	cfg := &DocumenterConfig{
		BaseConfig: BaseConfig{
			RepoPath: getString(configMap, "repo_path", "."),
			Debug:    getBool(configMap, "debug", false),
		},
	}

	cfg.LLM = LLMConfig{
		Provider:    getString(configMap, "llm.provider", getEnvWithFallback("DOCUMENTER_LLM_PROVIDER", "ANALYZER_LLM_PROVIDER", "openai")),
		Model:       getString(configMap, "llm.model", getEnvWithFallback("DOCUMENTER_LLM_MODEL", "ANALYZER_LLM_MODEL", "gpt-4o")),
		APIKey:      getString(configMap, "llm.api_key", getEnvWithFallback("DOCUMENTER_LLM_API_KEY", "ANALYZER_LLM_API_KEY", "")),
		BaseURL:     getString(configMap, "llm.base_url", getEnvOrDefault("DOCUMENTER_LLM_BASE_URL", "")),
		Retries:     getInt(configMap, "llm.retries", getEnvIntOrDefault("DOCUMENTER_AGENT_RETRIES", 2)),
		Timeout:     getInt(configMap, "llm.timeout", getEnvIntOrDefault("DOCUMENTER_LLM_TIMEOUT", 180)),
		MaxTokens:   getInt(configMap, "llm.max_tokens", getEnvIntOrDefault("DOCUMENTER_LLM_MAX_TOKENS", 8192)),
		Temperature: getFloat64(configMap, "llm.temperature", getEnvFloatOrDefault("DOCUMENTER_LLM_TEMPERATURE", 0.0)),
	}

	if err := validateLLMConfig(&cfg.LLM, "DOCUMENTER"); err != nil {
		return nil, err
	}

	return cfg, nil
}

func LoadAIRulesConfig(repoPath string, cliOverrides map[string]interface{}) (*AIRulesConfig, error) {
	configMap, err := MergeConfigs(repoPath, "ai_rules", &AIRulesConfig{}, cliOverrides)
	if err != nil {
		return nil, err
	}

	cfg := &AIRulesConfig{
		BaseConfig: BaseConfig{
			RepoPath: getString(configMap, "repo_path", "."),
			Debug:    getBool(configMap, "debug", false),
		},
	}

	cfg.LLM = LLMConfig{
		Provider:    getString(configMap, "llm.provider", getEnvWithFallback("AI_RULES_LLM_PROVIDER", "ANALYZER_LLM_PROVIDER", "openai")),
		Model:       getString(configMap, "llm.model", getEnvWithFallback("AI_RULES_LLM_MODEL", "ANALYZER_LLM_MODEL", "gpt-4o")),
		APIKey:      getString(configMap, "llm.api_key", getEnvWithFallback("AI_RULES_LLM_API_KEY", "ANALYZER_LLM_API_KEY", "")),
		BaseURL:     getString(configMap, "llm.base_url", getEnvOrDefault("AI_RULES_LLM_BASE_URL", "")),
		Retries:     getInt(configMap, "llm.retries", getEnvIntOrDefault("AI_RULES_AGENT_RETRIES", 2)),
		Timeout:     getInt(configMap, "llm.timeout", getEnvIntOrDefault("AI_RULES_LLM_TIMEOUT", 240)),
		MaxTokens:   getInt(configMap, "llm.max_tokens", getEnvIntOrDefault("AI_RULES_LLM_MAX_TOKENS", 8192)),
		Temperature: getFloat64(configMap, "llm.temperature", getEnvFloatOrDefault("AI_RULES_LLM_TEMPERATURE", 0.0)),
	}

	cfg.MaxTokensMarkdown = getInt(configMap, "max_tokens_markdown", 0)
	cfg.MaxTokensCursor = getInt(configMap, "max_tokens_cursor", 0)

	if err := validateLLMConfig(&cfg.LLM, "AI_RULES"); err != nil {
		return nil, err
	}

	return cfg, nil
}

func setNested(m map[string]interface{}, dottedKey string, value interface{}) {
	parts := strings.Split(dottedKey, ".")
	if len(parts) == 1 {
		m[dottedKey] = value
		return
	}

	current := m
	for i := 0; i < len(parts)-1; i++ {
		part := parts[i]
		if next, ok := current[part].(map[string]interface{}); ok {
			current = next
		} else {
			newMap := make(map[string]interface{})
			current[part] = newMap
			current = newMap
		}
	}
	current[parts[len(parts)-1]] = value
}

// Helper functions for type-safe config access

func getString(m map[string]interface{}, key, defaultValue string) string {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	if str, ok := val.(string); ok {
		return str
	}
	return defaultValue
}

func getInt(m map[string]interface{}, key string, defaultValue int) int {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	switch v := val.(type) {
	case int:
		return v
	case float64:
		return int(v)
	case string:
		// Try to parse string as int
		var i int
		if _, err := fmt.Sscanf(v, "%d", &i); err == nil {
			return i
		}
	}
	return defaultValue
}

func getBool(m map[string]interface{}, key string, defaultValue bool) bool {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	if b, ok := val.(bool); ok {
		return b
	}
	return defaultValue
}

func getFloat64(m map[string]interface{}, key string, defaultValue float64) float64 {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	if f, ok := val.(float64); ok {
		return f
	}
	return defaultValue
}

func getEnvOrDefault(key, defaultValue string) string {
	if val := os.Getenv(key); val != "" {
		return val
	}
	return defaultValue
}

func getEnvWithFallback(primaryKey, fallbackKey, defaultValue string) string {
	if val := os.Getenv(primaryKey); val != "" {
		return val
	}
	if val := os.Getenv(fallbackKey); val != "" {
		return val
	}
	return defaultValue
}

func getEnvIntOrDefault(key string, defaultValue int) int {
	if val := os.Getenv(key); val != "" {
		var i int
		if _, err := fmt.Sscanf(val, "%d", &i); err == nil {
			return i
		}
	}
	return defaultValue
}

func getEnvFloatOrDefault(key string, defaultValue float64) float64 {
	if val := os.Getenv(key); val != "" {
		var f float64
		if _, err := fmt.Sscanf(val, "%f", &f); err == nil {
			return f
		}
	}
	return defaultValue
}

// validateLLMConfig validates LLM configuration
func validateLLMConfig(cfg *LLMConfig, prefix string) error {
	if cfg.APIKey == "" {
		return errors.NewMissingEnvVarError(prefix+"_LLM_API_KEY", "API key for LLM provider")
	}

	validProviders := map[string]bool{
		"openai":    true,
		"anthropic": true,
		"gemini":    true,
	}

	if !validProviders[cfg.Provider] {
		return errors.NewInvalidEnvVarError(prefix+"_LLM_PROVIDER", cfg.Provider, "Must be one of: openai, anthropic, gemini")
	}

	return nil
}
</file>
<file path="internal/config/loader_test.go">
package config

import (
	"os"
	"path/filepath"
	"testing"
)

func TestLoadAnalyzerConfig_DefaultValues(t *testing.T) {
	// Setup: Clean environment
	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")

	cfg, err := LoadAnalyzerConfig(".", map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if cfg.LLM.Provider != "openai" {
		t.Errorf("Expected provider 'openai', got '%s'", cfg.LLM.Provider)
	}

	if cfg.LLM.Model != "gpt-4" {
		t.Errorf("Expected model 'gpt-4', got '%s'", cfg.LLM.Model)
	}

	if cfg.LLM.APIKey != "test-key" {
		t.Errorf("Expected API key 'test-key', got '%s'", cfg.LLM.APIKey)
	}

	// Default max_workers should be 0 (auto-detect)
	if cfg.MaxWorkers != 0 {
		t.Errorf("Expected max_workers 0, got %d", cfg.MaxWorkers)
	}
}

func TestLoadAnalyzerConfig_CLIOverridesAll(t *testing.T) {
	// Setup: Create temp directory with config files
	tmpDir := t.TempDir()

	// Create project config
	projectConfig := filepath.Join(tmpDir, ".ai", "config.yaml")
	_ = os.MkdirAll(filepath.Dir(projectConfig), 0755)
	projectConfigContent := `
analyzer:
  max_workers: 4
  llm:
    provider: anthropic
    model: claude-3
`
	_ = os.WriteFile(projectConfig, []byte(projectConfigContent), 0644)

	// Setup environment
	os.Clearenv()
	_ = os.Setenv("ANALYZER_MAX_WORKERS", "8")
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")

	// CLI overrides should win
	cliOverrides := map[string]interface{}{
		"max_workers":  16,
		"llm.provider": "gemini",
		"llm.model":    "gemini-pro",
		"llm.api_key":  "cli-key",
	}

	cfg, err := LoadAnalyzerConfig(tmpDir, cliOverrides)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// CLI should override everything
	if cfg.MaxWorkers != 16 {
		t.Errorf("Expected max_workers 16 (CLI), got %d", cfg.MaxWorkers)
	}

	// Note: The config system has complex precedence, test what actually works
	if cfg.LLM.Provider != "gemini" && cfg.LLM.Provider != "openai" {
		t.Logf("Provider precedence: expected 'gemini' (CLI) or 'openai' (env), got '%s'", cfg.LLM.Provider)
	}
}

func TestLoadAnalyzerConfig_ProjectOverridesGlobal(t *testing.T) {
	tmpDir := t.TempDir()

	// Create global config
	homeDir := t.TempDir()
	_ = os.Setenv("HOME", homeDir)
	globalConfig := filepath.Join(homeDir, ".gendocs.yaml")
	globalConfigContent := `
analyzer:
  max_workers: 2
`
	_ = os.WriteFile(globalConfig, []byte(globalConfigContent), 0644)

	// Create project config
	projectConfig := filepath.Join(tmpDir, ".ai", "config.yaml")
	_ = os.MkdirAll(filepath.Dir(projectConfig), 0755)
	projectConfigContent := `
analyzer:
  max_workers: 4
`
	_ = os.WriteFile(projectConfig, []byte(projectConfigContent), 0644)

	// Setup minimal environment
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")

	cfg, err := LoadAnalyzerConfig(tmpDir, map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Project config should override global
	// Note: Actual precedence may vary based on viper implementation
	if cfg.MaxWorkers == 2 {
		t.Log("Global config took precedence (unexpected)")
	}
}

func TestLoadAnalyzerConfig_MissingAPIKey(t *testing.T) {
	// Setup: No API key
	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	// No API key set

	_, err := LoadAnalyzerConfig(".", map[string]interface{}{})
	if err == nil {
		t.Fatal("Expected error for missing API key, got nil")
	}

	// Error should mention API key
	if !containsString(err.Error(), "API_KEY") && !containsString(err.Error(), "api_key") {
		t.Errorf("Expected error to mention API key, got: %v", err)
	}
}

func TestLoadAnalyzerConfig_InvalidProvider(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "invalid-provider")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "some-model")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")

	_, err := LoadAnalyzerConfig(".", map[string]interface{}{})
	if err == nil {
		t.Fatal("Expected error for invalid provider, got nil")
	}
}

func TestLoadAnalyzerConfig_ExclusionFlags(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")

	cliOverrides := map[string]interface{}{
		"exclude_code_structure": true,
		"exclude_data_flow":      true,
		"exclude_dependencies":   false,
		"exclude_request_flow":   true,
		"exclude_api_analysis":   false,
	}

	cfg, err := LoadAnalyzerConfig(".", cliOverrides)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if !cfg.ExcludeStructure {
		t.Error("Expected ExcludeStructure to be true")
	}

	if !cfg.ExcludeDataFlow {
		t.Error("Expected ExcludeDataFlow to be true")
	}

	if cfg.ExcludeDeps {
		t.Error("Expected ExcludeDeps to be false")
	}

	if !cfg.ExcludeReqFlow {
		t.Error("Expected ExcludeReqFlow to be true")
	}

	if cfg.ExcludeAPI {
		t.Error("Expected ExcludeAPI to be false")
	}
}

func TestLoadAnalyzerConfig_YAMLParsing(t *testing.T) {
	tmpDir := t.TempDir()

	// Create project config with nested structure
	projectConfig := filepath.Join(tmpDir, ".ai", "config.yaml")
	_ = os.MkdirAll(filepath.Dir(projectConfig), 0755)
	projectConfigContent := `
analyzer:
  max_workers: 8
  exclude_code_structure: true
  exclude_data_flow: false
  llm:
    provider: anthropic
    model: claude-3-sonnet
    api_key: yaml-key
    base_url: https://api.anthropic.com
    retries: 3
    timeout: 240
    max_tokens: 16384
    temperature: 0.5
`
	_ = os.WriteFile(projectConfig, []byte(projectConfigContent), 0644)

	// Minimal env setup
	os.Clearenv()

	cfg, err := LoadAnalyzerConfig(tmpDir, map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Check values loaded from YAML
	if cfg.LLM.Provider != "anthropic" {
		t.Errorf("Expected provider 'anthropic', got '%s'", cfg.LLM.Provider)
	}

	if cfg.LLM.Model != "claude-3-sonnet" {
		t.Errorf("Expected model 'claude-3-sonnet', got '%s'", cfg.LLM.Model)
	}

	if cfg.LLM.APIKey != "yaml-key" {
		t.Errorf("Expected API key 'yaml-key', got '%s'", cfg.LLM.APIKey)
	}

	if cfg.MaxWorkers != 8 {
		t.Errorf("Expected max_workers 8, got %d", cfg.MaxWorkers)
	}

	if !cfg.ExcludeStructure {
		t.Error("Expected ExcludeStructure to be true")
	}

	if cfg.ExcludeDataFlow {
		t.Error("Expected ExcludeDataFlow to be false")
	}
}

func TestLoadAnalyzerConfig_InvalidYAML(t *testing.T) {
	tmpDir := t.TempDir()

	// Create invalid YAML
	projectConfig := filepath.Join(tmpDir, ".ai", "config.yaml")
	_ = os.MkdirAll(filepath.Dir(projectConfig), 0755)
	invalidYAML := `
analyzer:
  this is not: valid: yaml: syntax
`
	_ = os.WriteFile(projectConfig, []byte(invalidYAML), 0644)

	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")

	_, err := LoadAnalyzerConfig(tmpDir, map[string]interface{}{})
	// May or may not error depending on viper's YAML parser tolerance
	_ = err
}

func TestGetEnvVar_Success(t *testing.T) {
	_ = os.Setenv("TEST_VAR", "test-value")
	defer func() { _ = os.Unsetenv("TEST_VAR") }()

	value, err := GetEnvVar("TEST_VAR", "Test variable")
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if value != "test-value" {
		t.Errorf("Expected 'test-value', got '%s'", value)
	}
}

func TestGetEnvVar_Missing(t *testing.T) {
	_ = os.Unsetenv("MISSING_VAR")

	_, err := GetEnvVar("MISSING_VAR", "Missing variable")
	if err == nil {
		t.Fatal("Expected error for missing env var, got nil")
	}
}

func TestGetEnvVarOrDefault_WithValue(t *testing.T) {
	_ = os.Setenv("TEST_VAR", "actual-value")
	defer func() { _ = os.Unsetenv("TEST_VAR") }()

	value := GetEnvVarOrDefault("TEST_VAR", "default-value")
	if value != "actual-value" {
		t.Errorf("Expected 'actual-value', got '%s'", value)
	}
}

func TestGetEnvVarOrDefault_WithoutValue(t *testing.T) {
	_ = os.Unsetenv("MISSING_VAR")

	value := GetEnvVarOrDefault("MISSING_VAR", "default-value")
	if value != "default-value" {
		t.Errorf("Expected 'default-value', got '%s'", value)
	}
}

func TestLoadAnalyzerConfig_BaseURL(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")
	_ = os.Setenv("ANALYZER_LLM_BASE_URL", "https://custom.openai.com")

	cfg, err := LoadAnalyzerConfig(".", map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if cfg.LLM.BaseURL != "https://custom.openai.com" {
		t.Errorf("Expected base URL 'https://custom.openai.com', got '%s'", cfg.LLM.BaseURL)
	}
}

func TestLoadAnalyzerConfig_AllProviders(t *testing.T) {
	providers := []string{"openai", "anthropic", "gemini"}

	for _, provider := range providers {
		t.Run(provider, func(t *testing.T) {
			os.Clearenv()
			_ = os.Setenv("ANALYZER_LLM_PROVIDER", provider)
			_ = os.Setenv("ANALYZER_LLM_MODEL", "test-model")
			_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")

			cfg, err := LoadAnalyzerConfig(".", map[string]interface{}{})
			if err != nil {
				t.Fatalf("Expected no error for provider '%s', got %v", provider, err)
			}

			if cfg.LLM.Provider != provider {
				t.Errorf("Expected provider '%s', got '%s'", provider, cfg.LLM.Provider)
			}
		})
	}
}

func TestLoadAnalyzerConfig_RepoPath(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")

	testPath := "/custom/repo/path"
	cliOverrides := map[string]interface{}{
		"repo_path": testPath,
	}

	cfg, err := LoadAnalyzerConfig(".", cliOverrides)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if cfg.RepoPath != testPath {
		t.Errorf("Expected repo path '%s', got '%s'", testPath, cfg.RepoPath)
	}
}

func TestLoadAnalyzerConfig_DebugFlag(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "openai")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "test-key")

	cliOverrides := map[string]interface{}{
		"debug": true,
	}

	cfg, err := LoadAnalyzerConfig(".", cliOverrides)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if !cfg.Debug {
		t.Error("Expected Debug to be true")
	}
}

func TestLoadDocumenterConfig_DefaultValues(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("DOCUMENTER_LLM_PROVIDER", "openai")
	_ = os.Setenv("DOCUMENTER_LLM_MODEL", "gpt-4")
	_ = os.Setenv("DOCUMENTER_LLM_API_KEY", "test-key")

	cfg, err := LoadDocumenterConfig(".", map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if cfg.LLM.Provider != "openai" {
		t.Errorf("Expected provider 'openai', got '%s'", cfg.LLM.Provider)
	}

	if cfg.LLM.Model != "gpt-4" {
		t.Errorf("Expected model 'gpt-4', got '%s'", cfg.LLM.Model)
	}

	if cfg.LLM.APIKey != "test-key" {
		t.Errorf("Expected API key 'test-key', got '%s'", cfg.LLM.APIKey)
	}
}

func TestLoadDocumenterConfig_FallbackToAnalyzer(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "anthropic")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "claude-3")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "analyzer-key")

	cfg, err := LoadDocumenterConfig(".", map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if cfg.LLM.Provider != "anthropic" {
		t.Errorf("Expected provider 'anthropic' (fallback), got '%s'", cfg.LLM.Provider)
	}

	if cfg.LLM.Model != "claude-3" {
		t.Errorf("Expected model 'claude-3' (fallback), got '%s'", cfg.LLM.Model)
	}

	if cfg.LLM.APIKey != "analyzer-key" {
		t.Errorf("Expected API key 'analyzer-key' (fallback), got '%s'", cfg.LLM.APIKey)
	}
}

func TestLoadDocumenterConfig_YAMLParsing(t *testing.T) {
	tmpDir := t.TempDir()

	projectConfig := filepath.Join(tmpDir, ".ai", "config.yaml")
	_ = os.MkdirAll(filepath.Dir(projectConfig), 0755)
	projectConfigContent := `
documenter:
  llm:
    provider: anthropic
    model: claude-3-sonnet
    api_key: yaml-doc-key
    timeout: 200
`
	_ = os.WriteFile(projectConfig, []byte(projectConfigContent), 0644)

	os.Clearenv()

	cfg, err := LoadDocumenterConfig(tmpDir, map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if cfg.LLM.Provider != "anthropic" {
		t.Errorf("Expected provider 'anthropic', got '%s'", cfg.LLM.Provider)
	}

	if cfg.LLM.Model != "claude-3-sonnet" {
		t.Errorf("Expected model 'claude-3-sonnet', got '%s'", cfg.LLM.Model)
	}

	if cfg.LLM.APIKey != "yaml-doc-key" {
		t.Errorf("Expected API key 'yaml-doc-key', got '%s'", cfg.LLM.APIKey)
	}

	if cfg.LLM.Timeout != 200 {
		t.Errorf("Expected timeout 200, got %d", cfg.LLM.Timeout)
	}
}

func TestLoadDocumenterConfig_MissingAPIKey(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("DOCUMENTER_LLM_PROVIDER", "openai")
	_ = os.Setenv("DOCUMENTER_LLM_MODEL", "gpt-4")

	_, err := LoadDocumenterConfig(".", map[string]interface{}{})
	if err == nil {
		t.Fatal("Expected error for missing API key, got nil")
	}
}

func TestLoadAIRulesConfig_DefaultValues(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("AI_RULES_LLM_PROVIDER", "openai")
	_ = os.Setenv("AI_RULES_LLM_MODEL", "gpt-4")
	_ = os.Setenv("AI_RULES_LLM_API_KEY", "test-key")

	cfg, err := LoadAIRulesConfig(".", map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if cfg.LLM.Provider != "openai" {
		t.Errorf("Expected provider 'openai', got '%s'", cfg.LLM.Provider)
	}

	if cfg.LLM.Model != "gpt-4" {
		t.Errorf("Expected model 'gpt-4', got '%s'", cfg.LLM.Model)
	}

	if cfg.LLM.Timeout != 240 {
		t.Errorf("Expected default timeout 240, got %d", cfg.LLM.Timeout)
	}
}

func TestLoadAIRulesConfig_FallbackToAnalyzer(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("ANALYZER_LLM_PROVIDER", "gemini")
	_ = os.Setenv("ANALYZER_LLM_MODEL", "gemini-pro")
	_ = os.Setenv("ANALYZER_LLM_API_KEY", "analyzer-key")

	cfg, err := LoadAIRulesConfig(".", map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if cfg.LLM.Provider != "gemini" {
		t.Errorf("Expected provider 'gemini' (fallback), got '%s'", cfg.LLM.Provider)
	}

	if cfg.LLM.Model != "gemini-pro" {
		t.Errorf("Expected model 'gemini-pro' (fallback), got '%s'", cfg.LLM.Model)
	}

	if cfg.LLM.APIKey != "analyzer-key" {
		t.Errorf("Expected API key 'analyzer-key' (fallback), got '%s'", cfg.LLM.APIKey)
	}
}

func TestLoadAIRulesConfig_YAMLParsing(t *testing.T) {
	tmpDir := t.TempDir()

	projectConfig := filepath.Join(tmpDir, ".ai", "config.yaml")
	_ = os.MkdirAll(filepath.Dir(projectConfig), 0755)
	projectConfigContent := `
ai_rules:
  llm:
    provider: openai
    model: gpt-4o
    api_key: yaml-ai-key
  max_tokens_markdown: 16000
  max_tokens_cursor: 8000
`
	_ = os.WriteFile(projectConfig, []byte(projectConfigContent), 0644)

	os.Clearenv()

	cfg, err := LoadAIRulesConfig(tmpDir, map[string]interface{}{})
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if cfg.LLM.Provider != "openai" {
		t.Errorf("Expected provider 'openai', got '%s'", cfg.LLM.Provider)
	}

	if cfg.LLM.Model != "gpt-4o" {
		t.Errorf("Expected model 'gpt-4o', got '%s'", cfg.LLM.Model)
	}

	if cfg.LLM.APIKey != "yaml-ai-key" {
		t.Errorf("Expected API key 'yaml-ai-key', got '%s'", cfg.LLM.APIKey)
	}

	if cfg.MaxTokensMarkdown != 16000 {
		t.Errorf("Expected max_tokens_markdown 16000, got %d", cfg.MaxTokensMarkdown)
	}

	if cfg.MaxTokensCursor != 8000 {
		t.Errorf("Expected max_tokens_cursor 8000, got %d", cfg.MaxTokensCursor)
	}
}

func TestLoadAIRulesConfig_MissingAPIKey(t *testing.T) {
	os.Clearenv()
	_ = os.Setenv("AI_RULES_LLM_PROVIDER", "openai")
	_ = os.Setenv("AI_RULES_LLM_MODEL", "gpt-4")

	_, err := LoadAIRulesConfig(".", map[string]interface{}{})
	if err == nil {
		t.Fatal("Expected error for missing API key, got nil")
	}
}

func TestSetNested_SimpleKey(t *testing.T) {
	m := make(map[string]interface{})
	setNested(m, "key", "value")

	if m["key"] != "value" {
		t.Errorf("Expected 'value', got '%v'", m["key"])
	}
}

func TestSetNested_DottedKey(t *testing.T) {
	m := make(map[string]interface{})
	setNested(m, "llm.provider", "openai")

	llmMap, ok := m["llm"].(map[string]interface{})
	if !ok {
		t.Fatal("Expected nested map at 'llm'")
	}

	if llmMap["provider"] != "openai" {
		t.Errorf("Expected 'openai', got '%v'", llmMap["provider"])
	}
}

func TestSetNested_DeepKey(t *testing.T) {
	m := make(map[string]interface{})
	setNested(m, "a.b.c.d", "deep-value")

	aMap := m["a"].(map[string]interface{})
	bMap := aMap["b"].(map[string]interface{})
	cMap := bMap["c"].(map[string]interface{})

	if cMap["d"] != "deep-value" {
		t.Errorf("Expected 'deep-value', got '%v'", cMap["d"])
	}
}

// Helper function
func containsString(haystack, needle string) bool {
	return len(haystack) >= len(needle) &&
		(haystack == needle || len(needle) == 0 || findSubstring(haystack, needle))
}

func findSubstring(s, substr string) bool {
	for i := 0; i <= len(s)-len(substr); i++ {
		if s[i:i+len(substr)] == substr {
			return true
		}
	}
	return false
}
</file>
<file path="internal/config/models.go">
package config

import (
	"time"
)

// BaseConfig holds common configuration for all handlers
type BaseConfig struct {
	RepoPath string `mapstructure:"repo_path"`
	Debug    bool   `mapstructure:"debug"`
}

// LLMConfig holds LLM provider configuration
type LLMConfig struct {
	Provider    string         `mapstructure:"provider"` // openai, anthropic, gemini
	Model       string         `mapstructure:"model"`
	APIKey      string         `mapstructure:"api_key"`
	BaseURL     string         `mapstructure:"base_url"` // Optional, for OpenAI-compatible APIs
	Retries     int            `mapstructure:"retries"`
	Timeout     int            `mapstructure:"timeout"` // Timeout in seconds
	MaxTokens   int            `mapstructure:"max_tokens"`
	Temperature float64        `mapstructure:"temperature"`
	Cache       LLMCacheConfig `mapstructure:"cache"` // Cache configuration
}

// LLMCacheConfig holds LLM response cache configuration
type LLMCacheConfig struct {
	Enabled   bool   `mapstructure:"enabled"`    // Enable/disable caching
	MaxSize   int    `mapstructure:"max_size"`   // Maximum number of entries in memory cache
	TTL       int    `mapstructure:"ttl"`        // Time-to-live for cache entries in days
	CachePath string `mapstructure:"cache_path"` // Path to disk cache file
}

// GeminiConfig holds Gemini-specific configuration
type GeminiConfig struct {
	UseVertexAI bool   `mapstructure:"use_vertex_ai"`
	ProjectID   string `mapstructure:"project_id"`
	Location    string `mapstructure:"location"`
}

// RetryConfig holds HTTP retry configuration
type RetryConfig struct {
	MaxAttempts       int `mapstructure:"max_attempts"`         // Default: 5
	Multiplier        int `mapstructure:"multiplier"`           // Default: 1
	MaxWaitPerAttempt int `mapstructure:"max_wait_per_attempt"` // Default: 60 seconds
	MaxTotalWait      int `mapstructure:"max_total_wait"`       // Default: 300 seconds
}

// AnalyzerConfig holds configuration for the analyze command
type AnalyzerConfig struct {
	BaseConfig
	LLM              LLMConfig   `mapstructure:"llm"`
	ExcludeStructure bool        `mapstructure:"exclude_code_structure"`
	ExcludeDataFlow  bool        `mapstructure:"exclude_data_flow"`
	ExcludeDeps      bool        `mapstructure:"exclude_dependencies"`
	ExcludeReqFlow   bool        `mapstructure:"exclude_request_flow"`
	ExcludeAPI       bool        `mapstructure:"exclude_api_analysis"`
	MaxWorkers       int         `mapstructure:"max_workers"`
	MaxHashWorkers   int         `mapstructure:"max_hash_workers"`
	RetryConfig      RetryConfig `mapstructure:"retry"`
	Force            bool        `mapstructure:"force"`       // Force full re-analysis, ignore cache
	Incremental      bool        `mapstructure:"incremental"` // Enable incremental analysis (default: true)
}

// DocumenterConfig holds configuration for readme generation
type DocumenterConfig struct {
	BaseConfig
	LLM         LLMConfig   `mapstructure:"llm"`
	RetryConfig RetryConfig `mapstructure:"retry"`
}

// AIRulesConfig holds configuration for AI rules generation
type AIRulesConfig struct {
	BaseConfig
	LLM               LLMConfig   `mapstructure:"llm"`
	RetryConfig       RetryConfig `mapstructure:"retry"`
	MaxTokensMarkdown int         `mapstructure:"max_tokens_markdown"`
	MaxTokensCursor   int         `mapstructure:"max_tokens_cursor"`
}

// CronjobConfig holds configuration for cronjob command
type CronjobConfig struct {
	MaxDaysSinceLastCommit int    `mapstructure:"max_days_since_last_commit"`
	WorkingPath            string `mapstructure:"working_path"`
	GroupProjectID         int    `mapstructure:"group_project_id"`
}

// GitLabConfig holds GitLab integration configuration
type GitLabConfig struct {
	APIURL       string `mapstructure:"api_url"`
	UserName     string `mapstructure:"user_name"`
	UserUsername string `mapstructure:"user_username"`
	UserEmail    string `mapstructure:"user_email"`
	OAuthToken   string `mapstructure:"oauth_token"`
}

// LoggingConfig holds logging configuration
type LoggingConfig struct {
	LogDir       string `mapstructure:"log_dir"`
	FileLevel    string `mapstructure:"file_level"`    // debug, info, warn, error
	ConsoleLevel string `mapstructure:"console_level"` // debug, info, warn, error
}

// GlobalConfig holds top-level configuration from .ai/config.yaml
type GlobalConfig struct {
	Analyzer   AnalyzerConfig   `mapstructure:"analyzer"`
	Documenter DocumenterConfig `mapstructure:"documenter"`
	AIRules    AIRulesConfig    `mapstructure:"ai_rules"`
	Cronjob    CronjobConfig    `mapstructure:"cronjob"`
	GitLab     GitLabConfig     `mapstructure:"gitlab"`
	Gemini     GeminiConfig     `mapstructure:"gemini"`
	Logging    LoggingConfig    `mapstructure:"logging"`
}

// GetTimeout returns the timeout as a time.Duration
func (c *LLMConfig) GetTimeout() time.Duration {
	if c.Timeout == 0 {
		return 180 * time.Second // Default timeout
	}
	return time.Duration(c.Timeout) * time.Second
}

// GetMaxTokens returns the max tokens with a default
func (c *LLMConfig) GetMaxTokens() int {
	if c.MaxTokens == 0 {
		return 8192 // Default max tokens
	}
	return c.MaxTokens
}

// GetTemperature returns the temperature with a default
func (c *LLMConfig) GetTemperature() float64 {
	if c.Temperature == 0 {
		return 0.0 // Default temperature for deterministic output
	}
	return c.Temperature
}

// GetRetries returns the retry count with a default
func (c *LLMConfig) GetRetries() int {
	if c.Retries == 0 {
		return 2 // Default retries
	}
	return c.Retries
}

// IsEnabled returns whether caching is enabled
func (c *LLMCacheConfig) IsEnabled() bool {
	return c.Enabled
}

// GetMaxSize returns the maximum cache size with a default
func (c *LLMCacheConfig) GetMaxSize() int {
	if c.MaxSize == 0 {
		return 1000 // Default max entries
	}
	return c.MaxSize
}

// GetTTL returns the TTL as a time.Duration with a default
func (c *LLMCacheConfig) GetTTL() time.Duration {
	if c.TTL == 0 {
		return 7 * 24 * time.Hour // Default 7 days
	}
	return time.Duration(c.TTL) * 24 * time.Hour
}

// GetCachePath returns the cache file path with a default
func (c *LLMCacheConfig) GetCachePath() string {
	if c.CachePath == "" {
		return ".ai/llm_cache.json" // Default cache path
	}
	return c.CachePath
}

// GetMaxHashWorkers returns the max hash workers with a default (0 = use CPU count with max of 8)
func (c *AnalyzerConfig) GetMaxHashWorkers() int {
	return c.MaxHashWorkers
}
</file>
<file path="internal/errors/agent.go">
package errors

import (
	"fmt"
)

// AgentError is the base error for all agent-related errors
type AgentError struct {
	*AIDocGenError
}

// NewAgentError creates a new agent error
func NewAgentError(message string) *AgentError {
	return &AgentError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitAgentError,
		},
	}
}

// LLMConnectionError is raised when connection to LLM provider fails
type LLMConnectionError struct {
	*AIDocGenError
}

// NewLLMConnectionError creates a new LLM connection error
func NewLLMConnectionError(provider string, cause error) *LLMConnectionError {
	return &LLMConnectionError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to connect to LLM provider: %s", provider),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "LLM API Call",
				Component: "LLM Client",
				Details: map[string]interface{}{
					"provider": provider,
				},
				Suggestions: []string{
					"Check your internet connection",
					"Verify the API endpoint is accessible",
					"Check if the API key is valid",
					"Try again later (service may be unavailable)",
				},
				Recoverable: true,
			},
			ExitCode: ExitLLMError,
		},
	}
}

// LLMResponseError is raised when LLM response is invalid or cannot be parsed
type LLMResponseError struct {
	*AIDocGenError
}

// NewLLMResponseError creates a new LLM response error
func NewLLMResponseError(provider, reason string) *LLMResponseError {
	return &LLMResponseError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Invalid response from LLM provider: %s", provider),
			Context: &ErrorContext{
				Operation: "Parsing LLM Response",
				Component: "LLM Client",
				Details: map[string]interface{}{
					"provider": provider,
					"reason":   reason,
				},
				Suggestions: []string{
					"Check if the model name is correct",
					"Try a different model",
					"Report this issue if it persists",
				},
				Recoverable: true,
			},
			ExitCode: ExitLLMError,
		},
	}
}

// AgentTimeoutError is raised when an agent execution times out
type AgentTimeoutError struct {
	*AIDocGenError
}

// NewAgentTimeoutError creates a new agent timeout error
func NewAgentTimeoutError(agentName string, timeoutSeconds int) *AgentTimeoutError {
	return &AgentTimeoutError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Agent '%s' timed out after %d seconds", agentName, timeoutSeconds),
			Context: &ErrorContext{
				Operation: "Agent Execution",
				Component: agentName,
				Details: map[string]interface{}{
					"timeout_seconds": timeoutSeconds,
				},
				Suggestions: []string{
					"Increase the timeout via LLM_TIMEOUT environment variable",
					"Try reducing the size of the codebase",
					"Try a faster model",
				},
				Recoverable: false,
			},
			ExitCode: ExitAgentError,
		},
	}
}

// ToolExecutionError is raised when a tool execution fails
type ToolExecutionError struct {
	*AIDocGenError
}

// NewToolExecutionError creates a new tool execution error
func NewToolExecutionError(toolName string, cause error) *ToolExecutionError {
	return &ToolExecutionError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Tool '%s' execution failed", toolName),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Tool Execution",
				Component: toolName,
				Details: map[string]interface{}{
					"tool": toolName,
				},
				Suggestions: []string{
					"Check if the file/directory exists",
					"Verify file permissions",
					"Check the error details above",
				},
				Recoverable: true,
			},
			ExitCode: ExitAgentError,
		},
	}
}
</file>
<file path="internal/errors/base.go">
package errors

import (
	"fmt"
)

// AIDocGenError is the base error type for all application errors
type AIDocGenError struct {
	Message  string        // Human-readable error message
	Context  *ErrorContext // Rich error context
	Cause    error         // Underlying error (for wrapping)
	ExitCode ExitCode      // Exit code for CLI
}

// Error returns the error message with cause if present
func (e *AIDocGenError) Error() string {
	if e.Cause != nil {
		return fmt.Sprintf("%s: %v", e.Message, e.Cause)
	}
	return e.Message
}

// Unwrap returns the underlying cause
func (e *AIDocGenError) Unwrap() error {
	return e.Cause
}

// GetUserMessage returns a user-friendly error message with context
func (e *AIDocGenError) GetUserMessage() string {
	msg := fmt.Sprintf("ERROR: %s", e.Message)

	if e.Cause != nil {
		msg += fmt.Sprintf("\nCause: %v", e.Cause)
	}

	if e.Context != nil {
		msg += e.Context.Format()
	}

	return msg
}

// NewError creates a new AIDocGenError with the given message and exit code
func NewError(message string, exitCode ExitCode) *AIDocGenError {
	return &AIDocGenError{
		Message:  message,
		ExitCode: exitCode,
	}
}

// WrapError wraps an existing error with additional context
func WrapError(cause error, message string, exitCode ExitCode) *AIDocGenError {
	return &AIDocGenError{
		Message:  message,
		Cause:    cause,
		ExitCode: exitCode,
	}
}

// WrapErrorWithContext wraps an error with full context
func WrapErrorWithContext(cause error, message string, exitCode ExitCode, context *ErrorContext) *AIDocGenError {
	return &AIDocGenError{
		Message:  message,
		Context:  context,
		Cause:    cause,
		ExitCode: exitCode,
	}
}
</file>
<file path="internal/errors/config.go">
package errors

import (
	"fmt"
)

// ConfigurationError is raised when configuration is invalid or missing
type ConfigurationError struct {
	*AIDocGenError
}

// NewConfigurationError creates a new configuration error
func NewConfigurationError(message string) *ConfigurationError {
	return &ConfigurationError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitConfigError,
		},
	}
}

// MissingEnvVarError is raised when a required environment variable is not set
type MissingEnvVarError struct {
	*AIDocGenError
}

// NewMissingEnvVarError creates a new missing environment variable error
func NewMissingEnvVarError(varName, description string) *MissingEnvVarError {
	// Convert environment variable name to YAML key format for suggestions
	yamlKey := convertEnvToYAMLKey(varName)

	return &MissingEnvVarError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Required environment variable '%s' is not set", varName),
			Context: &ErrorContext{
				Operation: "Loading configuration",
				Component: "Environment",
				Details: map[string]interface{}{
					"variable":    varName,
					"description": description,
				},
				Suggestions: []string{
					"Run 'gendocs config' to set up configuration interactively",
					fmt.Sprintf("Export the variable: export %s='your-value'", varName),
					fmt.Sprintf("Add to .ai/config.yaml under analyzer.llm.%s", yamlKey),
					"Check .env.example for required variables",
				},
				Recoverable: false,
			},
			ExitCode: ExitConfigError,
		},
	}
}

// convertEnvToYAMLKey converts environment variable name to YAML key format
// Example: ANALYZER_LLM_API_KEY -> api_key
func convertEnvToYAMLKey(envVar string) string {
	parts := splitAndLower(envVar, "_")
	if len(parts) > 2 {
		// Take everything after the first two parts (ANALYZER_LLM)
		return joinParts(parts[2:], "_")
	}
	return joinParts(parts, "_")
}

func splitAndLower(s, sep string) []string {
	parts := make([]string, 0)
	current := ""
	for _, char := range s {
		if string(char) == sep {
			if current != "" {
				parts = append(parts, toLower(current))
				current = ""
			}
		} else {
			current += string(char)
		}
	}
	if current != "" {
		parts = append(parts, toLower(current))
	}
	return parts
}

func joinParts(parts []string, sep string) string {
	if len(parts) == 0 {
		return ""
	}
	result := parts[0]
	for i := 1; i < len(parts); i++ {
		result += sep + parts[i]
	}
	return result
}

func toLower(s string) string {
	result := ""
	for _, char := range s {
		if char >= 'A' && char <= 'Z' {
			result += string(char + 32)
		} else {
			result += string(char)
		}
	}
	return result
}

// InvalidEnvVarError is raised when an environment variable has an invalid value
type InvalidEnvVarError struct {
	*AIDocGenError
}

// NewInvalidEnvVarError creates a new invalid environment variable error
func NewInvalidEnvVarError(varName, value, reason string) *InvalidEnvVarError {
	return &InvalidEnvVarError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Environment variable '%s' has an invalid value", varName),
			Context: &ErrorContext{
				Operation: "Validating configuration",
				Component: "Environment",
				Details: map[string]interface{}{
					"variable": varName,
					"value":    value,
					"reason":   reason,
				},
				Suggestions: []string{
					fmt.Sprintf("Check the value of %s in your .env file", varName),
					"Refer to the documentation for valid values",
				},
				Recoverable: false,
			},
			ExitCode: ExitConfigError,
		},
	}
}

// ConfigFileError is raised when a configuration file cannot be read or parsed
type ConfigFileError struct {
	*AIDocGenError
}

// NewConfigFileError creates a new config file error
func NewConfigFileError(filePath string, cause error) *ConfigFileError {
	return &ConfigFileError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to load configuration file: %s", filePath),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Loading configuration",
				Component: "Config File",
				Details: map[string]interface{}{
					"file_path": filePath,
				},
				Suggestions: []string{
					"Check that the file exists and is readable",
					"Validate YAML syntax",
					"Check file permissions",
				},
				Recoverable: false,
			},
			ExitCode: ExitConfigError,
		},
	}
}
</file>
<file path="internal/errors/context.go">
package errors

import (
	"fmt"
	"strings"
)

// ErrorContext provides rich error information for user-friendly error messages
type ErrorContext struct {
	Operation   string                 // The operation that failed
	Component   string                 // The component that failed
	Details     map[string]interface{} // Additional details about the error
	Suggestions []string               // Actionable suggestions for the user
	Recoverable bool                   // Whether the error is recoverable
	RetryCount  int                    // Current retry count
	MaxRetries  int                    // Maximum retries allowed
}

// Format returns a formatted string representation of the error context
func (ec *ErrorContext) Format() string {
	var sb strings.Builder

	if ec.Operation != "" || ec.Component != "" {
		sb.WriteString("\nWhat happened:\n")
		if ec.Operation != "" && ec.Component != "" {
			sb.WriteString(fmt.Sprintf("  %s failed in %s.\n", ec.Operation, ec.Component))
		} else if ec.Operation != "" {
			sb.WriteString(fmt.Sprintf("  %s failed.\n", ec.Operation))
		} else if ec.Component != "" {
			sb.WriteString(fmt.Sprintf("  Failure in %s.\n", ec.Component))
		}
	}

	if len(ec.Details) > 0 {
		sb.WriteString("\nDetails:\n")
		for key, value := range ec.Details {
			sb.WriteString(fmt.Sprintf("  - %s: %v\n", key, value))
		}
	}

	if len(ec.Suggestions) > 0 {
		sb.WriteString("\nWhat you can do:\n")
		for i, suggestion := range ec.Suggestions {
			sb.WriteString(fmt.Sprintf("  %d. %s\n", i+1, suggestion))
		}
	}

	if ec.Recoverable {
		sb.WriteString(fmt.Sprintf("\nRecoverable: Yes (retry %d/%d)\n", ec.RetryCount, ec.MaxRetries))
	}

	return sb.String()
}
</file>
<file path="internal/errors/exit_codes.go">
package errors

type ExitCode int

const (
	ExitSuccess         ExitCode = 0
	ExitGeneralError    ExitCode = 1
	ExitConfigError     ExitCode = 2
	ExitValidationError ExitCode = 3
	ExitLLMError        ExitCode = 4
	ExitAgentError      ExitCode = 5
	ExitIOError         ExitCode = 6
	ExitGitLabError     ExitCode = 7
	ExitPartialSuccess  ExitCode = 10
)

func (e ExitCode) Int() int {
	return int(e)
}
</file>
<file path="internal/errors/gitlab.go">
package errors

import (
	"fmt"
)

// GitLabError is the base error for all GitLab-related errors
type GitLabError struct {
	*AIDocGenError
}

// NewGitLabError creates a new GitLab error
func NewGitLabError(message string) *GitLabError {
	return &GitLabError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitGitLabError,
		},
	}
}

// GitLabAuthError is raised when GitLab authentication fails
type GitLabAuthError struct {
	*AIDocGenError
}

// NewGitLabAuthError creates a new GitLab authentication error
func NewGitLabAuthError(cause error) *GitLabAuthError {
	return &GitLabAuthError{
		AIDocGenError: &AIDocGenError{
			Message: "GitLab authentication failed",
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "GitLab Authentication",
				Component: "GitLab Client",
				Suggestions: []string{
					"Verify GITLAB_OAUTH_TOKEN is set correctly",
					"Check if the token has not expired",
					"Ensure token has required permissions (api, read_repository)",
					"Generate a new token at GitLab user settings > access tokens",
				},
				Recoverable: false,
			},
			ExitCode: ExitGitLabError,
		},
	}
}

// GitLabAPIError is raised when GitLab API call fails
type GitLabAPIError struct {
	*AIDocGenError
}

// NewGitLabAPIError creates a new GitLab API error
func NewGitLabAPIError(operation, statusCode string, cause error) *GitLabAPIError {
	return &GitLabAPIError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("GitLab API error during %s", operation),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "GitLab API Call",
				Component: "GitLab Client",
				Details: map[string]interface{}{
					"operation":   operation,
					"status_code": statusCode,
				},
				Suggestions: []string{
					"Check GitLab API URL is correct",
					"Verify the project/group exists",
					"Check API rate limits",
					"Try again later",
				},
				Recoverable: true,
			},
			ExitCode: ExitGitLabError,
		},
	}
}

// GitCloneError is raised when git clone fails
type GitCloneError struct {
	*AIDocGenError
}

// NewGitCloneError creates a new git clone error
func NewGitCloneError(repoURL, reason string, cause error) *GitCloneError {
	return &GitCloneError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to clone repository: %s", repoURL),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Git Clone",
				Component: "Git",
				Details: map[string]interface{}{
					"repo_url": repoURL,
					"reason":   reason,
				},
				Suggestions: []string{
					"Check if the repository URL is correct",
					"Verify you have access to the repository",
					"Check git is installed and accessible",
					"Ensure sufficient disk space",
				},
				Recoverable: false,
			},
			ExitCode: ExitGitLabError,
		},
	}
}
</file>
<file path="internal/errors/handler.go">
package errors

import (
	"fmt"
)

// HandlerError is the base error for all handler-related errors
type HandlerError struct {
	*AIDocGenError
}

// NewHandlerError creates a new handler error
func NewHandlerError(message string) *HandlerError {
	return &HandlerError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitGeneralError,
		},
	}
}

// AnalysisError is raised when analysis fails
type AnalysisError struct {
	*AIDocGenError
}

// NewAnalysisError creates a new analysis error
func NewAnalysisError(reason string, cause error) *AnalysisError {
	return &AnalysisError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Analysis failed: %s", reason),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Codebase Analysis",
				Component: "AnalyzerAgent",
				Suggestions: []string{
					"Check if the repository path is valid",
					"Verify LLM configuration",
					"Try with --debug flag for more information",
					"Check if any agents were excluded",
				},
				Recoverable: false,
			},
			ExitCode: ExitAgentError,
		},
	}
}

// DocumentationError is raised when documentation generation fails
type DocumentationError struct {
	*AIDocGenError
}

// NewDocumentationError creates a new documentation error
func NewDocumentationError(docType string, cause error) *DocumentationError {
	return &DocumentationError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to generate %s documentation", docType),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Documentation Generation",
				Component: docType + "Agent",
				Suggestions: []string{
					"Ensure analysis has been run first",
					"Check that analysis files exist in .ai/docs/",
					"Verify LLM configuration",
				},
				Recoverable: false,
			},
			ExitCode: ExitAgentError,
		},
	}
}

// CronjobError is raised when cronjob execution fails
type CronjobError struct {
	*AIDocGenError
}

// NewCronjobError creates a new cronjob error
func NewCronjobError(reason string, cause error) *CronjobError {
	return &CronjobError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Cronjob failed: %s", reason),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "GitLab Cronjob",
				Component: "CronjobHandler",
				Suggestions: []string{
					"Check GitLab API credentials",
					"Verify group project ID",
					"Check GitLab API URL",
					"Review cronjob logs",
				},
				Recoverable: false,
			},
			ExitCode: ExitGeneralError,
		},
	}
}
</file>
<file path="internal/errors/validation.go">
package errors

import (
	"fmt"
)

// ValidationError is the base error for all validation-related errors
type ValidationError struct {
	*AIDocGenError
}

// NewValidationError creates a new validation error
func NewValidationError(message string) *ValidationError {
	return &ValidationError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitValidationError,
		},
	}
}

// MissingFileError is raised when a required file is not found
type MissingFileError struct {
	*AIDocGenError
}

// NewMissingFileError creates a new missing file error
func NewMissingFileError(filePath string) *MissingFileError {
	return &MissingFileError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Required file not found: %s", filePath),
			Context: &ErrorContext{
				Operation: "File Validation",
				Component: "Filesystem",
				Details: map[string]interface{}{
					"file_path": filePath,
				},
				Suggestions: []string{
					"Check that the file exists",
					"Verify the file path is correct",
					"Run analysis first to generate the file",
				},
				Recoverable: false,
			},
			ExitCode: ExitValidationError,
		},
	}
}

// InvalidPathError is raised when a path is invalid
type InvalidPathError struct {
	*AIDocGenError
}

// NewInvalidPathError creates a new invalid path error
func NewInvalidPathError(path string, reason string) *InvalidPathError {
	return &InvalidPathError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Invalid path: %s", path),
			Context: &ErrorContext{
				Operation: "Path Validation",
				Component: "Filesystem",
				Details: map[string]interface{}{
					"path":   path,
					"reason": reason,
				},
				Suggestions: []string{
					"Check that the path exists",
					"Verify the path is a valid directory",
					"Use an absolute path if relative path fails",
				},
				Recoverable: false,
			},
			ExitCode: ExitValidationError,
		},
	}
}

// OutputValidationError is raised when output validation fails
type OutputValidationError struct {
	*AIDocGenError
}

// NewOutputValidationError creates a new output validation error
func NewOutputValidationError(missingFiles []string) *OutputValidationError {
	return &OutputValidationError{
		AIDocGenError: &AIDocGenError{
			Message: "Output validation failed: expected files were not generated",
			Context: &ErrorContext{
				Operation: "Output Validation",
				Component: "Validation",
				Details: map[string]interface{}{
					"missing_files": missingFiles,
				},
				Suggestions: []string{
					"Check if LLM API calls succeeded",
					"Review error logs for individual agent failures",
					"Try running with --debug flag for more details",
				},
				Recoverable: false,
			},
			ExitCode: ExitValidationError,
		},
	}
}
</file>
<file path="internal/export/html.go">
package export

import (
	"bytes"
	"fmt"
	"html/template"
	"os"
	"strings"
	"time"

	"github.com/yuin/goldmark"
	highlighting "github.com/yuin/goldmark-highlighting/v2"
	"github.com/yuin/goldmark/extension"
	"github.com/yuin/goldmark/renderer/html"
)

// HTMLExporter converts Markdown to standalone HTML documents
type HTMLExporter struct {
	markdown     goldmark.Markdown
	htmlTemplate *template.Template
}

// HTMLDocument represents the data for HTML template rendering
type HTMLDocument struct {
	Title   string
	Content template.HTML
	CSS     template.CSS // Use template.CSS to mark as safe
}

// NewHTMLExporter creates a new HTML exporter with Goldmark configured
func NewHTMLExporter() (*HTMLExporter, error) {
	// Configure Goldmark with GitHub Flavored Markdown and syntax highlighting
	md := goldmark.New(
		goldmark.WithExtensions(
			extension.GFM,
			extension.Table,
			extension.Strikethrough,
			extension.TaskList,
			highlighting.NewHighlighting(
				highlighting.WithStyle("monokai"),
			),
		),
		goldmark.WithRendererOptions(
			html.WithHardWraps(),
			html.WithXHTML(),
			html.WithUnsafe(), // Allow raw HTML in Markdown
		),
	)

	// Load HTML template
	tmpl, err := loadHTMLTemplate()
	if err != nil {
		return nil, fmt.Errorf("failed to load HTML template: %w", err)
	}

	return &HTMLExporter{
		markdown:     md,
		htmlTemplate: tmpl,
	}, nil
}

// ExportToHTML converts a Markdown file to a standalone HTML file
func (e *HTMLExporter) ExportToHTML(markdownPath, outputPath string) error {
	// Read Markdown file
	mdContent, err := os.ReadFile(markdownPath)
	if err != nil {
		return fmt.Errorf("failed to read markdown: %w", err)
	}

	// Convert Markdown to HTML
	var buf bytes.Buffer
	if err := e.markdown.Convert(mdContent, &buf); err != nil {
		return fmt.Errorf("failed to convert markdown: %w", err)
	}

	// Extract title from first H1
	title := extractTitle(string(mdContent))

	// Render full HTML document
	doc := HTMLDocument{
		Title:   title,
		Content: template.HTML(buf.String()),
		CSS:     template.CSS(getDefaultCSS()), // Mark CSS as safe
	}

	var htmlBuf bytes.Buffer
	if err := e.htmlTemplate.Execute(&htmlBuf, doc); err != nil {
		return fmt.Errorf("failed to execute template: %w", err)
	}

	// Write output file
	if err := os.WriteFile(outputPath, htmlBuf.Bytes(), 0644); err != nil {
		return fmt.Errorf("failed to write HTML: %w", err)
	}

	return nil
}

// loadHTMLTemplate loads the HTML template with custom functions
func loadHTMLTemplate() (*template.Template, error) {
	const tmpl = `<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Gendocs">
    <title>{{.Title}}</title>
    <style>
        {{.CSS}}
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="generator-badge">Generated with Gendocs</div>
        </header>
        <main>
            {{.Content}}
        </main>
        <footer>
            <p>Generated on {{now}} by <a href="https://github.com/user/gendocs">Gendocs</a></p>
        </footer>
    </div>
</body>
</html>`

	return template.New("html").Funcs(template.FuncMap{
		"now": func() string {
			return time.Now().Format("2006-01-02 15:04:05")
		},
	}).Parse(tmpl)
}

// extractTitle extracts the first H1 heading from Markdown content
func extractTitle(markdown string) string {
	lines := strings.Split(markdown, "\n")
	for _, line := range lines {
		trimmed := strings.TrimSpace(line)
		if strings.HasPrefix(trimmed, "# ") {
			return strings.TrimPrefix(trimmed, "# ")
		}
	}
	return "Documentation"
}

// getDefaultCSS returns GitHub-style CSS for the HTML document
func getDefaultCSS() string {
	return `
        * {
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292f;
            background-color: #ffffff;
            margin: 0;
            padding: 0;
        }

        .container {
            max-width: 980px;
            margin: 0 auto;
            padding: 45px;
        }

        header {
            border-bottom: 1px solid #d0d7de;
            margin-bottom: 30px;
            padding-bottom: 10px;
        }

        .generator-badge {
            font-size: 12px;
            color: #57606a;
            text-align: right;
        }

        main {
            margin-bottom: 60px;
        }

        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
        }

        h1 {
            font-size: 2em;
            border-bottom: 1px solid #d0d7de;
            padding-bottom: 0.3em;
        }

        h2 {
            font-size: 1.5em;
            border-bottom: 1px solid #d0d7de;
            padding-bottom: 0.3em;
        }

        code {
            background-color: rgba(175, 184, 193, 0.2);
            border-radius: 6px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: ui-monospace, SFMono-Regular, 'SF Mono', Menlo, Consolas, monospace;
        }

        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
        }

        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }

        table {
            border-collapse: collapse;
            border-spacing: 0;
            width: 100%;
            margin-bottom: 16px;
        }

        table th {
            font-weight: 600;
            background-color: #f6f8fa;
        }

        table th, table td {
            padding: 6px 13px;
            border: 1px solid #d0d7de;
        }

        table tr:nth-child(2n) {
            background-color: #f6f8fa;
        }

        a {
            color: #0969da;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        blockquote {
            padding: 0 1em;
            color: #57606a;
            border-left: 0.25em solid #d0d7de;
            margin: 0 0 16px;
        }

        ul, ol {
            padding-left: 2em;
            margin-top: 0;
            margin-bottom: 16px;
        }

        li + li {
            margin-top: 0.25em;
        }

        footer {
            border-top: 1px solid #d0d7de;
            padding-top: 20px;
            text-align: center;
            font-size: 14px;
            color: #57606a;
        }

        @media (max-width: 768px) {
            .container {
                padding: 15px;
            }

            h1 {
                font-size: 1.6em;
            }

            h2 {
                font-size: 1.3em;
            }
        }
    `
}
</file>
<file path="internal/export/html_test.go">
package export

import (
	"os"
	"path/filepath"
	"strings"
	"testing"
)

func TestNewHTMLExporter_Success(t *testing.T) {
	exporter, err := NewHTMLExporter()
	if err != nil {
		t.Fatalf("Expected no error creating exporter, got %v", err)
	}

	if exporter == nil {
		t.Fatal("Expected exporter to be non-nil")
	}

	if exporter.markdown == nil {
		t.Error("Expected markdown renderer to be initialized")
	}

	if exporter.htmlTemplate == nil {
		t.Error("Expected HTML template to be initialized")
	}
}

func TestHTMLExporter_ExportToHTML_Success(t *testing.T) {
	exporter, err := NewHTMLExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	// Create temp directory and files
	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "test.md")
	htmlFile := filepath.Join(tmpDir, "test.html")

	markdown := `# Test Document

This is a **test** with code:

` + "```go\nfunc main() {\n    fmt.Println(\"Hello\")\n}\n```" + `

## Section 2

- Item 1
- Item 2
- Item 3

### Subsection

| Column 1 | Column 2 |
|----------|----------|
| Value 1  | Value 2  |
| Value 3  | Value 4  |

> This is a blockquote

[Link text](https://example.com)
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write test markdown: %v", err)
	}

	// Export to HTML
	err = exporter.ExportToHTML(mdFile, htmlFile)
	if err != nil {
		t.Fatalf("Expected no error exporting, got %v", err)
	}

	// Read generated HTML
	html, err := os.ReadFile(htmlFile)
	if err != nil {
		t.Fatalf("Failed to read generated HTML: %v", err)
	}

	htmlStr := string(html)

	// Verify HTML structure
	if !strings.Contains(htmlStr, "<!DOCTYPE html>") {
		t.Error("Expected DOCTYPE declaration")
	}

	if !strings.Contains(htmlStr, "<html lang=\"en\">") {
		t.Error("Expected html lang attribute")
	}

	if !strings.Contains(htmlStr, "<title>Test Document</title>") {
		t.Error("Expected title to be 'Test Document'")
	}

	// Verify content conversion
	if !strings.Contains(htmlStr, "<h1>Test Document</h1>") {
		t.Error("Expected H1 heading")
	}

	if !strings.Contains(htmlStr, "<h2>Section 2</h2>") {
		t.Error("Expected H2 heading")
	}

	if !strings.Contains(htmlStr, "<strong>test</strong>") {
		t.Error("Expected bold text")
	}

	if !strings.Contains(htmlStr, "<code") {
		t.Error("Expected code blocks")
	}

	// Check for code content (syntax highlighting splits into spans)
	if !strings.Contains(htmlStr, "func") || !strings.Contains(htmlStr, "main") {
		t.Error("Expected code content with 'func' and 'main'")
	}

	// Verify list
	if !strings.Contains(htmlStr, "<li>Item 1</li>") {
		t.Error("Expected list items")
	}

	// Verify table
	if !strings.Contains(htmlStr, "<table>") {
		t.Error("Expected table")
	}

	if !strings.Contains(htmlStr, "<th>Column 1</th>") {
		t.Error("Expected table headers")
	}

	if !strings.Contains(htmlStr, "<td>Value 1</td>") {
		t.Error("Expected table data")
	}

	// Verify blockquote
	if !strings.Contains(htmlStr, "<blockquote>") {
		t.Error("Expected blockquote")
	}

	// Verify link
	if !strings.Contains(htmlStr, "<a href=\"https://example.com\">Link text</a>") {
		t.Error("Expected link")
	}

	// Verify CSS is embedded
	if !strings.Contains(htmlStr, "font-family:") {
		t.Error("Expected embedded CSS")
	}

	// Verify footer
	if !strings.Contains(htmlStr, "Generated with Gendocs") {
		t.Error("Expected generator badge")
	}

	if !strings.Contains(htmlStr, "Generated on") {
		t.Error("Expected generation timestamp")
	}
}

func TestHTMLExporter_ExportToHTML_FileNotFound(t *testing.T) {
	exporter, err := NewHTMLExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "nonexistent.md")
	htmlFile := filepath.Join(tmpDir, "output.html")

	err = exporter.ExportToHTML(mdFile, htmlFile)
	if err == nil {
		t.Fatal("Expected error for nonexistent file, got nil")
	}

	if !strings.Contains(err.Error(), "failed to read markdown") {
		t.Errorf("Expected 'failed to read markdown' error, got: %v", err)
	}
}

func TestHTMLExporter_ExportToHTML_InvalidOutputPath(t *testing.T) {
	exporter, err := NewHTMLExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "test.md")

	markdown := "# Test\n\nContent"
	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write test markdown: %v", err)
	}

	// Try to write to a directory that doesn't exist
	invalidPath := filepath.Join(tmpDir, "nonexistent", "subdir", "output.html")

	err = exporter.ExportToHTML(mdFile, invalidPath)
	if err == nil {
		t.Fatal("Expected error for invalid output path, got nil")
	}
}

func TestExtractTitle(t *testing.T) {
	tests := []struct {
		name     string
		markdown string
		expected string
	}{
		{
			name:     "First line H1",
			markdown: "# My Title\n\nContent",
			expected: "My Title",
		},
		{
			name:     "H1 with whitespace",
			markdown: "  # Title with Spaces  \n\nContent",
			expected: "Title with Spaces",
		},
		{
			name:     "H1 not first line",
			markdown: "Some content\n# Title\n\nMore content",
			expected: "Title",
		},
		{
			name:     "No H1",
			markdown: "## H2 Only\n\nContent",
			expected: "Documentation",
		},
		{
			name:     "Empty markdown",
			markdown: "",
			expected: "Documentation",
		},
		{
			name:     "Only whitespace",
			markdown: "   \n\n   \n",
			expected: "Documentation",
		},
		{
			name:     "H1 with special characters",
			markdown: "# Title with **bold** and `code`\n",
			expected: "Title with **bold** and `code`",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			result := extractTitle(tt.markdown)
			if result != tt.expected {
				t.Errorf("Expected '%s', got '%s'", tt.expected, result)
			}
		})
	}
}

func TestGetDefaultCSS(t *testing.T) {
	css := getDefaultCSS()

	// Verify CSS contains key styles
	requiredStyles := []string{
		"font-family:",
		".container",
		"max-width:",
		"pre {",
		"code {",
		"table",
		"@media",
		"blockquote",
	}

	for _, style := range requiredStyles {
		if !strings.Contains(css, style) {
			t.Errorf("Expected CSS to contain '%s'", style)
		}
	}
}

func TestHTMLExporter_ComplexMarkdown(t *testing.T) {
	exporter, err := NewHTMLExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "complex.md")
	htmlFile := filepath.Join(tmpDir, "complex.html")

	// Test with GitHub Flavored Markdown extensions
	markdown := `# Complex Document

## Task Lists

- [x] Completed task
- [ ] Incomplete task

## Strikethrough

~~This text is crossed out~~

## Tables with alignment

| Left | Center | Right |
|:-----|:------:|------:|
| L1   | C1     | R1    |
| L2   | C2     | R2    |

## Multiple code blocks

` + "```python\ndef hello():\n    print(\"Hello\")\n```" + `

` + "```javascript\nfunction hello() {\n    console.log(\"Hello\");\n}\n```" + `

## Nested lists

1. First
   - Sub 1
   - Sub 2
2. Second
   1. Sub A
   2. Sub B

## Inline elements

**Bold**, *italic*, ***bold italic***, ~~strikethrough~~, ` + "`code`" + `
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToHTML(mdFile, htmlFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	html, err := os.ReadFile(htmlFile)
	if err != nil {
		t.Fatalf("Failed to read HTML: %v", err)
	}

	htmlStr := string(html)

	// Verify task lists
	if !strings.Contains(htmlStr, "type=\"checkbox\"") {
		t.Error("Expected task list checkboxes")
	}

	// Verify strikethrough
	if !strings.Contains(htmlStr, "<del>") || !strings.Contains(htmlStr, "crossed out") {
		t.Error("Expected strikethrough")
	}

	// Verify tables
	if !strings.Contains(htmlStr, "<table>") {
		t.Error("Expected table")
	}

	// Verify multiple code blocks (syntax highlighting splits tokens)
	hasPython := strings.Contains(htmlStr, "def") && strings.Contains(htmlStr, "hello") && strings.Contains(htmlStr, "print")
	hasJS := strings.Contains(htmlStr, "function") && strings.Contains(htmlStr, "console")

	if !hasPython || !hasJS {
		t.Error("Expected both Python and JavaScript code blocks")
	}
}

func TestHTMLExporter_EmptyMarkdown(t *testing.T) {
	exporter, err := NewHTMLExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "empty.md")
	htmlFile := filepath.Join(tmpDir, "empty.html")

	err = os.WriteFile(mdFile, []byte(""), 0644)
	if err != nil {
		t.Fatalf("Failed to write empty file: %v", err)
	}

	err = exporter.ExportToHTML(mdFile, htmlFile)
	if err != nil {
		t.Fatalf("Expected success with empty file, got error: %v", err)
	}

	html, err := os.ReadFile(htmlFile)
	if err != nil {
		t.Fatalf("Failed to read HTML: %v", err)
	}

	htmlStr := string(html)

	// Should still have valid HTML structure
	if !strings.Contains(htmlStr, "<!DOCTYPE html>") {
		t.Error("Expected valid HTML document")
	}

	// Should use default title
	if !strings.Contains(htmlStr, "<title>Documentation</title>") {
		t.Error("Expected default title 'Documentation'")
	}
}
</file>
<file path="internal/export/json.go">
package export

import (
	"encoding/json"
	"fmt"
	"os"
	"strings"
	"time"

	"github.com/yuin/goldmark"
	highlighting "github.com/yuin/goldmark-highlighting/v2"
	"github.com/yuin/goldmark/ast"
	"github.com/yuin/goldmark/extension"
	east "github.com/yuin/goldmark/extension/ast"
	"github.com/yuin/goldmark/parser"
	"github.com/yuin/goldmark/renderer/html"
	"github.com/yuin/goldmark/text"
)

// JSONDocument represents the complete JSON output structure
type JSONDocument struct {
	Metadata Metadata       `json:"metadata"`
	Content  ContentSection `json:"content"`
}

// Metadata contains document metadata
type Metadata struct {
	Title       string    `json:"title"`
	GeneratedAt time.Time `json:"generated_at"`
	Generator   Generator `json:"generator"`
	SourceFile  string    `json:"source_file"`
	WordCount   int       `json:"word_count,omitempty"`
	CharCount   int       `json:"char_count,omitempty"`
}

// Generator information
type Generator struct {
	Name    string `json:"name"`
	Version string `json:"version"`
	URL     string `json:"url"`
}

// ContentSection contains the document content
type ContentSection struct {
	Headings []Heading `json:"headings"`
	Elements []Element `json:"elements"`
}

// Heading represents a heading in the hierarchy
type Heading struct {
	ID       string    `json:"id"`
	Level    int       `json:"level"`
	Text     string    `json:"text"`
	Children []Heading `json:"children"`
}

// headingNode is used for building the hierarchy during AST traversal
type headingTreeNode struct {
	heading  *Heading
	parent   int   // index in the heading slice
	children []int // indices of children
}

// Element is a wrapper for different element types
type Element map[string]interface{}

// ParagraphElement represents a paragraph
type ParagraphElement struct {
	Type    string `json:"type"`
	Content string `json:"content"`
}

// HeadingElement represents a heading element in the elements array
type HeadingElement struct {
	Type  string `json:"type"`
	Level int    `json:"level"`
	Text  string `json:"text"`
}

// CodeBlockElement represents a code block
type CodeBlockElement struct {
	Type     string `json:"type"`
	Language string `json:"language"`
	Code     string `json:"code"`
	Lines    int    `json:"lines"`
}

// ListElement represents a list (unordered, ordered, or task)
type ListElement struct {
	Type     string     `json:"type"`
	ListType string     `json:"list_type"`       // "unordered", "ordered", "task"
	Start    int        `json:"start,omitempty"` // Starting number for ordered lists
	Items    []ListItem `json:"items"`
}

// ListItem represents a single list item (can be nested)
type ListItem struct {
	Content string     `json:"content"`
	Checked *bool      `json:"checked,omitempty"` // For task lists (nil if not a task)
	Items   []ListItem `json:"items"`             // Nested sub-items
}

// TableElement represents a table
type TableElement struct {
	Type   string        `json:"type"`
	Header []TableCell   `json:"header"`
	Rows   [][]TableCell `json:"rows"`
}

// TableCell represents a single table cell
type TableCell struct {
	Content   string `json:"text"`
	Alignment string `json:"alignment,omitempty"` // "left", "center", "right", or omitted
}

// BlockquoteElement represents a blockquote
type BlockquoteElement struct {
	Type     string    `json:"type"`
	Content  string    `json:"content"`
	Elements []Element `json:"elements,omitempty"` // For complex nested blockquotes
}

// ThematicBreakElement represents a horizontal rule
type ThematicBreakElement struct {
	Type string `json:"type"`
}

// LinkElement represents a link reference
type LinkElement struct {
	Type  string `json:"type"`
	URL   string `json:"url"`
	Title string `json:"title,omitempty"`
	Text  string `json:"text"`
}

// ImageElement represents an image
type ImageElement struct {
	Type  string `json:"type"`
	URL   string `json:"url"`
	Title string `json:"title,omitempty"`
	Alt   string `json:"alt"`
}

// JSONExporter converts Markdown to structured JSON documents
type JSONExporter struct {
	source   []byte
	markdown goldmark.Markdown
}

// NewJSONExporter creates a new JSON exporter with Goldmark configured
func NewJSONExporter() (*JSONExporter, error) {
	// Configure Goldmark with GitHub Flavored Markdown and syntax highlighting
	md := goldmark.New(
		goldmark.WithExtensions(
			extension.GFM,
			extension.Table,
			extension.Strikethrough,
			extension.TaskList,
			highlighting.NewHighlighting(
				highlighting.WithStyle("monokai"),
			),
		),
		goldmark.WithRendererOptions(
			html.WithHardWraps(),
			html.WithXHTML(),
			html.WithUnsafe(), // Allow raw HTML in Markdown
		),
		goldmark.WithParserOptions(
			parser.WithAutoHeadingID(),
		),
	)

	return &JSONExporter{
		markdown: md,
	}, nil
}

// ExportToJSON converts a Markdown file to a structured JSON file
func (e *JSONExporter) ExportToJSON(markdownPath, outputPath string) error {
	// Read Markdown file
	mdContent, err := os.ReadFile(markdownPath)
	if err != nil {
		return fmt.Errorf("failed to read markdown: %w", err)
	}
	e.source = mdContent

	// Parse markdown to AST
	context := parser.NewContext()
	doc := e.markdown.Parser().Parse(
		text.NewReader(mdContent),
		parser.WithContext(context),
	)

	// Extract metadata
	title := extractJSONTitle(string(mdContent))
	wordCount, charCount := countText(mdContent)

	// Build JSON document
	jsonDoc, err := e.buildJSONDocument(
		doc,
		string(mdContent),
		markdownPath,
		title,
		wordCount,
		charCount,
	)
	if err != nil {
		return fmt.Errorf("failed to build JSON document: %w", err)
	}

	// Marshal to JSON with indentation
	jsonData, err := marshalJSON(jsonDoc)
	if err != nil {
		return fmt.Errorf("failed to marshal JSON: %w", err)
	}

	// Write output file
	if err := os.WriteFile(outputPath, jsonData, 0644); err != nil {
		return fmt.Errorf("failed to write JSON: %w", err)
	}

	return nil
}

// buildJSONDocument constructs the JSON document structure from the AST
func (e *JSONExporter) buildJSONDocument(
	doc ast.Node,
	markdownContent string,
	sourceFile string,
	title string,
	wordCount int,
	charCount int,
) (*JSONDocument, error) {
	// Build metadata
	metadata := Metadata{
		Title:       title,
		GeneratedAt: time.Now(),
		Generator: Generator{
			Name:    "Gendocs",
			Version: "1.0.0",
			URL:     "https://github.com/user/gendocs",
		},
		SourceFile: sourceFile,
		WordCount:  wordCount,
		CharCount:  charCount,
	}

	// Extract content from AST
	headings, elements := e.traverseAST(doc)

	content := ContentSection{
		Headings: headings,
		Elements: elements,
	}

	return &JSONDocument{
		Metadata: metadata,
		Content:  content,
	}, nil
}

// traverseAST walks through the AST and extracts headings and elements
func (e *JSONExporter) traverseAST(doc ast.Node) ([]Heading, []Element) {
	var elements []Element
	var headingNodes []headingTreeNode
	var headingStack []int // Stack of heading indices

	// Walk the AST
	for child := doc.FirstChild(); child != nil; child = child.NextSibling() {
		e.processNode(child, &headingNodes, &headingStack, &elements)
	}

	// Build the final heading hierarchy
	headings := e.buildHeadingHierarchy(headingNodes)

	return headings, elements
}

// processNode processes a single AST node
func (e *JSONExporter) processNode(
	node ast.Node,
	headingNodes *[]headingTreeNode,
	headingStack *[]int,
	elements *[]Element,
) {
	switch n := node.(type) {
	case *ast.Heading:
		e.processHeading(n, headingNodes, headingStack, elements)
	case *ast.Paragraph:
		e.processParagraph(n, elements)
	case *ast.FencedCodeBlock:
		e.processFencedCodeBlock(n, elements)
	case *ast.CodeBlock:
		e.processCodeBlock(n, elements)
	case *ast.List:
		e.processList(n, elements)
	case *east.Table:
		e.processTable(n, elements)
	case *ast.Blockquote:
		e.processBlockquote(n, elements)
	case *ast.ThematicBreak:
		e.processThematicBreak(elements)
	case *ast.Text:
		// Text nodes are handled within their parent containers
		return
	case *ast.String:
		// String nodes are handled within their parent containers
		return
	}

	// Extract inline elements (links, images) from paragraphs only
	// This avoids duplication when walking the entire tree
	if _, isParagraph := node.(*ast.Paragraph); isParagraph {
		e.extractInlineElements(node, elements)
	}
}

// processHeading processes a heading node
func (e *JSONExporter) processHeading(
	headingNode *ast.Heading,
	headingNodes *[]headingTreeNode,
	headingStack *[]int,
	elements *[]Element,
) {
	// Extract heading text
	text := e.extractText(headingNode)

	// Create heading element for elements array
	headingElem := HeadingElement{
		Type:  "heading",
		Level: headingNode.Level,
		Text:  text,
	}
	*elements = append(*elements, map[string]interface{}{
		"type":  headingElem.Type,
		"level": headingElem.Level,
		"text":  headingElem.Text,
	})

	// Create heading for hierarchy
	newHeading := Heading{
		ID:    e.generateID(text),
		Level: headingNode.Level,
		Text:  text,
	}

	nodeIdx := len(*headingNodes)
	parentIdx := -1

	// Pop headings that are at the same level or higher (less nested)
	for len(*headingStack) > 0 {
		topIdx := (*headingStack)[len(*headingStack)-1]
		if (*headingNodes)[topIdx].heading.Level >= headingNode.Level {
			*headingStack = (*headingStack)[:len(*headingStack)-1]
		} else {
			break
		}
	}

	// Find parent
	if len(*headingStack) > 0 {
		parentIdx = (*headingStack)[len(*headingStack)-1]
	}

	newNode := headingTreeNode{
		heading:  &newHeading,
		parent:   parentIdx,
		children: []int{},
	}

	*headingNodes = append(*headingNodes, newNode)
	*headingStack = append(*headingStack, nodeIdx)

	// Add this node as a child of its parent
	if parentIdx >= 0 {
		(*headingNodes)[parentIdx].children = append((*headingNodes)[parentIdx].children, nodeIdx)
	}
}

// processParagraph processes a paragraph node
func (e *JSONExporter) processParagraph(para *ast.Paragraph, elements *[]Element) {
	content := e.extractText(para)

	// Only add non-empty paragraphs
	if strings.TrimSpace(content) != "" {
		paraElem := ParagraphElement{
			Type:    "paragraph",
			Content: content,
		}
		*elements = append(*elements, map[string]interface{}{
			"type":    paraElem.Type,
			"content": paraElem.Content,
		})
	}
}

// processFencedCodeBlock processes a fenced code block
func (e *JSONExporter) processFencedCodeBlock(code *ast.FencedCodeBlock, elements *[]Element) {
	// Extract language
	language := string(code.Language(e.source))

	// Extract code content using the Text() method
	codeStr := string(code.Text(e.source)) //nolint:staticcheck // TODO: migrate to code.Lines

	lines := strings.Count(codeStr, "\n") + 1
	if strings.TrimSpace(codeStr) == "" {
		lines = 0
	}

	codeElem := CodeBlockElement{
		Type:     "code_block",
		Language: language,
		Code:     codeStr,
		Lines:    lines,
	}

	*elements = append(*elements, map[string]interface{}{
		"type":     codeElem.Type,
		"language": codeElem.Language,
		"code":     codeElem.Code,
		"lines":    codeElem.Lines,
	})
}

// processCodeBlock processes an indented code block
func (e *JSONExporter) processCodeBlock(code *ast.CodeBlock, elements *[]Element) {
	// Extract code content using the Text() method
	codeStr := string(code.Text(e.source)) //nolint:staticcheck // TODO: migrate to code.Lines

	lines := strings.Count(codeStr, "\n") + 1
	if strings.TrimSpace(codeStr) == "" {
		lines = 0
	}

	codeElem := CodeBlockElement{
		Type:     "code_block",
		Language: "", // Indented code blocks have no language
		Code:     codeStr,
		Lines:    lines,
	}

	*elements = append(*elements, map[string]interface{}{
		"type":     codeElem.Type,
		"language": codeElem.Language,
		"code":     codeElem.Code,
		"lines":    codeElem.Lines,
	})
}

// processList processes a list node
func (e *JSONExporter) processList(list *ast.List, elements *[]Element) {
	// Determine list type
	listType := "unordered"
	start := 0

	if list.IsOrdered() {
		listType = "ordered"
		start = list.Start
	}

	// Check if it's a task list by checking ALL items
	// Task lists have a TaskCheckBox in their items
	hasTaskCheckBox := false
	for itemChild := list.FirstChild(); itemChild != nil; itemChild = itemChild.NextSibling() {
		if item, ok := itemChild.(*ast.ListItem); ok {
			// Check all children for TaskCheckBox (could be direct child or in TextBlock)
			for child := item.FirstChild(); child != nil; child = child.NextSibling() {
				if _, ok := child.(*east.TaskCheckBox); ok {
					hasTaskCheckBox = true
					break
				}
				// Also check inside TextBlock
				if textBlock, ok := child.(*ast.TextBlock); ok {
					for gc := textBlock.FirstChild(); gc != nil; gc = gc.NextSibling() {
						if _, ok := gc.(*east.TaskCheckBox); ok {
							hasTaskCheckBox = true
							break
						}
					}
				}
			}
			if hasTaskCheckBox {
				break
			}
		}
	}
	if hasTaskCheckBox {
		listType = "task"
	}

	// Extract list items
	items := e.extractListItems(list)

	listElem := ListElement{
		Type:     "list",
		ListType: listType,
		Start:    start,
		Items:    items,
	}

	elemData := map[string]interface{}{
		"type":      listElem.Type,
		"list_type": listElem.ListType,
		"items":     listElem.Items,
	}
	if start > 0 {
		elemData["start"] = start
	}

	*elements = append(*elements, elemData)
}

// extractListItems recursively extracts list items
func (e *JSONExporter) extractListItems(list *ast.List) []ListItem {
	var items []ListItem

	for item := list.FirstChild(); item != nil; item = item.NextSibling() {
		listItem, ok := item.(*ast.ListItem)
		if !ok {
			continue
		}

		// Extract content (text before any nested list)
		content := e.extractListItemContent(listItem)

		// Check for task checkbox (may be inside TextBlock)
		var checked *bool
		if listItem.FirstChild() != nil {
			// TaskCheckBox might be a direct child (old behavior)
			if checkbox, ok := listItem.FirstChild().(*east.TaskCheckBox); ok {
				isChecked := checkbox.IsChecked
				checked = &isChecked
			} else if textBlock, ok := listItem.FirstChild().(*ast.TextBlock); ok {
				// Or it might be inside a TextBlock
				if textBlock.FirstChild() != nil {
					if checkbox, ok := textBlock.FirstChild().(*east.TaskCheckBox); ok {
						isChecked := checkbox.IsChecked
						checked = &isChecked
					}
				}
			}
		}

		// Extract nested lists
		var nestedItems []ListItem
		for child := listItem.FirstChild(); child != nil; child = child.NextSibling() {
			if nestedList, ok := child.(*ast.List); ok {
				nestedItems = e.extractListItems(nestedList)
				break
			}
			// Also check inside TextBlock for nested lists
			if textBlock, ok := child.(*ast.TextBlock); ok {
				for gc := textBlock.FirstChild(); gc != nil; gc = gc.NextSibling() {
					if nestedList, ok := gc.(*ast.List); ok {
						nestedItems = e.extractListItems(nestedList)
						break
					}
				}
			}
		}

		items = append(items, ListItem{
			Content: content,
			Checked: checked,
			Items:   nestedItems,
		})
	}

	return items
}

// extractListItemContent extracts text content from a list item
func (e *JSONExporter) extractListItemContent(item *ast.ListItem) string {
	var content strings.Builder

	for child := item.FirstChild(); child != nil; child = child.NextSibling() {
		// Skip task checkboxes and nested lists
		if _, ok := child.(*east.TaskCheckBox); ok {
			continue
		}
		if _, ok := child.(*ast.List); ok {
			break
		}
		// Handle TextBlocks (which contain the actual text content)
		if textBlock, ok := child.(*ast.TextBlock); ok {
			// Extract text from all children of TextBlock, skipping TaskCheckBox
			for gc := textBlock.FirstChild(); gc != nil; gc = gc.NextSibling() {
				if _, ok := gc.(*east.TaskCheckBox); ok {
					continue
				}
				// Handle Text nodes directly
				if textNode, ok := gc.(*ast.Text); ok {
					content.WriteString(string(textNode.Segment.Value(e.source)))
				} else if strNode, ok := gc.(*ast.String); ok {
					content.WriteString(string(strNode.Value))
				} else {
					content.WriteString(e.extractNodeText(gc))
				}
			}
		} else {
			// For other node types, use the standard extraction
			content.WriteString(e.extractNodeText(child))
		}
	}

	return strings.TrimSpace(content.String())
}

// processTable processes a table node
func (e *JSONExporter) processTable(table *east.Table, elements *[]Element) {
	var header []TableCell
	var rows [][]TableCell

	// Process table children - TableHeader comes first, then TableRows
	for child := table.FirstChild(); child != nil; child = child.NextSibling() {
		switch n := child.(type) {
		case *east.TableHeader:
			// Extract header row from TableHeader
			header = e.extractTableHeader(n, table)
		case *east.TableRow:
			// Extract data row from TableRow
			rowData := e.extractTableRow(n, table)
			rows = append(rows, rowData)
		}
	}

	tableElem := TableElement{
		Type:   "table",
		Header: header,
		Rows:   rows,
	}

	*elements = append(*elements, map[string]interface{}{
		"type":   tableElem.Type,
		"header": tableElem.Header,
		"rows":   tableElem.Rows,
	})
}

// extractTableHeader extracts the header row from a TableHeader node
func (e *JSONExporter) extractTableHeader(header *east.TableHeader, table *east.Table) []TableCell {
	var cells []TableCell
	colIndex := 0

	for cell := header.FirstChild(); cell != nil; cell = cell.NextSibling() {
		tableCell, ok := cell.(*east.TableCell)
		if !ok {
			continue
		}

		content := e.extractText(tableCell)

		// Determine alignment
		var alignment string
		if table.Alignments != nil && colIndex < len(table.Alignments) {
			switch table.Alignments[colIndex] {
			case east.AlignLeft:
				alignment = "left"
			case east.AlignCenter:
				alignment = "center"
			case east.AlignRight:
				alignment = "right"
			default:
				alignment = "default"
			}
		}

		cells = append(cells, TableCell{
			Content:   strings.TrimSpace(content),
			Alignment: alignment,
		})
		colIndex++
	}

	return cells
}

// extractTableRow extracts a single table row
func (e *JSONExporter) extractTableRow(row *east.TableRow, table *east.Table) []TableCell {
	var cells []TableCell
	colIndex := 0

	for cell := row.FirstChild(); cell != nil; cell = cell.NextSibling() {
		tableCell, ok := cell.(*east.TableCell)
		if !ok {
			continue
		}

		content := e.extractText(tableCell)

		// Determine alignment
		var alignment string
		if table.Alignments != nil && colIndex < len(table.Alignments) {
			switch table.Alignments[colIndex] {
			case east.AlignLeft:
				alignment = "left"
			case east.AlignCenter:
				alignment = "center"
			case east.AlignRight:
				alignment = "right"
			default:
				alignment = ""
			}
		}

		cellData := TableCell{
			Content: content,
		}
		if alignment != "" {
			cellData.Alignment = alignment
		}

		cells = append(cells, cellData)
		colIndex++
	}

	return cells
}

// processBlockquote processes a blockquote node
func (e *JSONExporter) processBlockquote(blockquote *ast.Blockquote, elements *[]Element) {
	content := e.extractText(blockquote)

	blockquoteElem := BlockquoteElement{
		Type:     "blockquote",
		Content:  content,
		Elements: []Element{}, // Could be extended to include nested elements
	}

	*elements = append(*elements, map[string]interface{}{
		"type":     blockquoteElem.Type,
		"content":  blockquoteElem.Content,
		"elements": blockquoteElem.Elements,
	})
}

// processThematicBreak processes a horizontal rule
func (e *JSONExporter) processThematicBreak(elements *[]Element) {
	breakElem := ThematicBreakElement{
		Type: "thematic_break",
	}

	*elements = append(*elements, map[string]interface{}{
		"type": breakElem.Type,
	})
}

// extractInlineElements extracts links and images as separate elements
func (e *JSONExporter) extractInlineElements(node ast.Node, elements *[]Element) {
	// Walk through children to find links and images
	for child := node.FirstChild(); child != nil; child = child.NextSibling() {
		e.extractInlineElements(child, elements)

		switch n := child.(type) {
		case *ast.Link:
			linkElem := LinkElement{
				Type:  "link",
				URL:   string(n.Destination),
				Title: string(n.Title),
				Text:  e.extractText(n),
			}

			*elements = append(*elements, map[string]interface{}{
				"type":  linkElem.Type,
				"url":   linkElem.URL,
				"title": linkElem.Title,
				"text":  linkElem.Text,
			})

		case *ast.Image:
			imageElem := ImageElement{
				Type:  "image",
				URL:   string(n.Destination),
				Title: string(n.Title),
				Alt:   e.extractText(n),
			}

			*elements = append(*elements, map[string]interface{}{
				"type":  imageElem.Type,
				"url":   imageElem.URL,
				"title": imageElem.Title,
				"alt":   imageElem.Alt,
			})
		}
	}
}

// extractText extracts plain text from a node
func (e *JSONExporter) extractText(node ast.Node) string {
	return e.extractNodeText(node)
}

// extractNodeText recursively extracts text from a node
func (e *JSONExporter) extractNodeText(node ast.Node) string {
	var text strings.Builder

	for child := node.FirstChild(); child != nil; child = child.NextSibling() {
		switch n := child.(type) {
		case *ast.Text:
			text.WriteString(string(n.Segment.Value(e.source)))
		case *ast.String:
			text.WriteString(string(n.Value))
		case *ast.Emphasis, *ast.CodeSpan, *ast.Link, *ast.Image:
			// Recursively extract text from inline elements
			text.WriteString(e.extractNodeText(n))
		default:
			// Recursively process other node types
			text.WriteString(e.extractNodeText(n))
		}
	}

	return text.String()
}

// generateID generates a unique ID from text
func (e *JSONExporter) generateID(text string) string {
	// Simple slugification
	slug := strings.ToLower(strings.TrimSpace(text))
	slug = strings.ReplaceAll(slug, " ", "-")
	slug = strings.ReplaceAll(slug, "_", "-")

	// Remove non-alphanumeric characters (except hyphens)
	var result strings.Builder
	for _, r := range slug {
		if (r >= 'a' && r <= 'z') || (r >= '0' && r <= '9') || r == '-' {
			result.WriteRune(r)
		}
	}

	id := result.String()

	// Collapse multiple consecutive hyphens into one
	for strings.Contains(id, "--") {
		id = strings.ReplaceAll(id, "--", "-")
	}

	// Trim leading/trailing hyphens
	id = strings.Trim(id, "-")

	if id == "" {
		return "heading"
	}

	return id
}

// marshalJSON converts the JSONDocument to JSON bytes with indentation
func marshalJSON(doc *JSONDocument) ([]byte, error) {
	data, err := json.MarshalIndent(doc, "", "  ")
	if err != nil {
		return nil, err
	}
	return data, nil
}

// extractTitle extracts the first H1 heading from Markdown content
func extractJSONTitle(markdown string) string {
	lines := strings.Split(markdown, "\n")
	for _, line := range lines {
		trimmed := strings.TrimSpace(line)
		if strings.HasPrefix(trimmed, "# ") {
			return strings.TrimPrefix(trimmed, "# ")
		}
	}
	return "Documentation"
}

// countText counts words and characters in markdown content
func countText(content []byte) (wordCount, charCount int) {
	text := string(content)
	charCount = len(text)

	// Simple word count - split by whitespace
	words := strings.Fields(text)
	wordCount = len(words)

	return wordCount, charCount
}

// buildHeadingHierarchy constructs the hierarchical heading structure
func (e *JSONExporter) buildHeadingHierarchy(nodes []headingTreeNode) []Heading {
	var roots []Heading

	// Build hierarchy from nodes
	for _, node := range nodes {
		if node.parent < 0 {
			// This is a root level heading
			roots = append(roots, *e.buildHeadingNode(&node, nodes))
		}
	}

	return roots
}

// buildHeadingNode recursively builds a heading with its children
func (e *JSONExporter) buildHeadingNode(node *headingTreeNode, allNodes []headingTreeNode) *Heading {
	result := node.heading

	// Build children
	for _, childIdx := range node.children {
		if childIdx < len(allNodes) {
			childNode := e.buildHeadingNode(&allNodes[childIdx], allNodes)
			result.Children = append(result.Children, *childNode)
		}
	}

	return result
}
</file>
<file path="internal/export/json_test.go">
package export

import (
	"encoding/json"
	"os"
	"path/filepath"
	"strings"
	"testing"
)

func TestNewJSONExporter_Success(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Expected no error creating exporter, got %v", err)
	}

	if exporter == nil {
		t.Fatal("Expected exporter to be non-nil")
	}

	if exporter.markdown == nil {
		t.Error("Expected markdown parser to be initialized")
	}
}

func TestJSONExporter_ExportToJSON_Success(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	// Create temp directory and files
	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "test.md")
	jsonFile := filepath.Join(tmpDir, "test.json")

	markdown := `# Test Document

This is a **test** with code.

` + "```go\nfunc main() {\n    fmt.Println(\"Hello\")\n}\n```" + `

## Section 2

- Item 1
- Item 2
- Item 3

### Subsection

| Column 1 | Column 2 |
|----------|----------|
| Value 1  | Value 2  |
| Value 3  | Value 4  |

> This is a blockquote

[Link text](https://example.com)
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write test markdown: %v", err)
	}

	// Export to JSON
	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Expected no error exporting, got %v", err)
	}

	// Read generated JSON
	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read generated JSON: %v", err)
	}

	// Verify JSON is valid
	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Verify metadata
	if doc.Metadata.Title != "Test Document" {
		t.Errorf("Expected title 'Test Document', got '%s'", doc.Metadata.Title)
	}

	if doc.Metadata.Generator.Name != "Gendocs" {
		t.Errorf("Expected generator name 'Gendocs', got '%s'", doc.Metadata.Generator.Name)
	}

	if doc.Metadata.SourceFile != mdFile {
		t.Errorf("Expected source file '%s', got '%s'", mdFile, doc.Metadata.SourceFile)
	}

	if doc.Metadata.WordCount == 0 {
		t.Error("Expected word count to be greater than 0")
	}

	if doc.Metadata.CharCount == 0 {
		t.Error("Expected char count to be greater than 0")
	}

	// Verify headings
	if len(doc.Content.Headings) == 0 {
		t.Fatal("Expected at least one heading")
	}

	if doc.Content.Headings[0].Text != "Test Document" {
		t.Errorf("Expected first heading 'Test Document', got '%s'", doc.Content.Headings[0].Text)
	}

	if doc.Content.Headings[0].Level != 1 {
		t.Errorf("Expected first heading level 1, got %d", doc.Content.Headings[0].Level)
	}

	// Verify elements
	if len(doc.Content.Elements) == 0 {
		t.Fatal("Expected at least one element")
	}

	// Check for heading element
	foundHeading := false
	foundParagraphWithTest := false
	foundCodeBlock := false
	foundList := false
	foundTable := false
	foundBlockquote := false
	foundLink := false

	for _, elem := range doc.Content.Elements {
		elemType, ok := elem["type"].(string)
		if !ok {
			continue
		}

		switch elemType {
		case "heading":
			foundHeading = true
		case "paragraph":
			content, ok := elem["content"].(string)
			if !ok {
				t.Error("Expected paragraph to have content")
			}
			// Track if we found a paragraph with "test"
			if strings.Contains(content, "test") {
				foundParagraphWithTest = true
			}
		case "code_block":
			foundCodeBlock = true
			language, ok := elem["language"].(string)
			if !ok {
				t.Error("Expected code_block to have language")
			}
			if language != "go" {
				t.Errorf("Expected language 'go', got '%s'", language)
			}
			code, ok := elem["code"].(string)
			if !ok {
				t.Error("Expected code_block to have code")
			}
			if !strings.Contains(code, "func main") {
				t.Error("Expected code to contain 'func main'")
			}
		case "list":
			foundList = true
			listType, ok := elem["list_type"].(string)
			if !ok {
				t.Error("Expected list to have list_type")
			}
			if listType != "unordered" {
				t.Errorf("Expected list_type 'unordered', got '%s'", listType)
			}
		case "table":
			foundTable = true
			header, ok := elem["header"].([]interface{})
			if !ok {
				t.Error("Expected table to have header")
			}
			if len(header) != 2 {
				t.Errorf("Expected 2 header columns, got %d", len(header))
			}
		case "blockquote":
			foundBlockquote = true
			content, ok := elem["content"].(string)
			if !ok {
				t.Error("Expected blockquote to have content")
			}
			if !strings.Contains(content, "blockquote") {
				t.Error("Expected blockquote to contain 'blockquote'")
			}
		case "link":
			foundLink = true
			url, ok := elem["url"].(string)
			if !ok {
				t.Error("Expected link to have url")
			}
			if url != "https://example.com" {
				t.Errorf("Expected url 'https://example.com', got '%s'", url)
			}
		}
	}

	if !foundHeading {
		t.Error("Expected to find heading element")
	}
	if !foundParagraphWithTest {
		t.Error("Expected to find paragraph element with 'test'")
	}
	if !foundCodeBlock {
		t.Error("Expected to find code_block element")
	}
	if !foundList {
		t.Error("Expected to find list element")
	}
	if !foundTable {
		t.Error("Expected to find table element")
	}
	if !foundBlockquote {
		t.Error("Expected to find blockquote element")
	}
	if !foundLink {
		t.Error("Expected to find link element")
	}
}

func TestJSONExporter_ExportToJSON_FileNotFound(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "nonexistent.md")
	jsonFile := filepath.Join(tmpDir, "output.json")

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err == nil {
		t.Fatal("Expected error for nonexistent file, got nil")
	}

	if !strings.Contains(err.Error(), "failed to read markdown") {
		t.Errorf("Expected 'failed to read markdown' error, got: %v", err)
	}
}

func TestJSONExporter_ExportToJSON_InvalidOutputPath(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "test.md")

	markdown := "# Test\n\nContent"
	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write test markdown: %v", err)
	}

	// Try to write to a directory that doesn't exist
	invalidPath := filepath.Join(tmpDir, "nonexistent", "subdir", "output.json")

	err = exporter.ExportToJSON(mdFile, invalidPath)
	if err == nil {
		t.Fatal("Expected error for invalid output path, got nil")
	}

	if !strings.Contains(err.Error(), "failed to write JSON") {
		t.Errorf("Expected 'failed to write JSON' error, got: %v", err)
	}
}

func TestJSONExtractTitle(t *testing.T) {
	tests := []struct {
		name     string
		markdown string
		expected string
	}{
		{
			name:     "First line H1",
			markdown: "# My Title\n\nContent",
			expected: "My Title",
		},
		{
			name:     "H1 with whitespace",
			markdown: "  # Title with Spaces  \n\nContent",
			expected: "Title with Spaces",
		},
		{
			name:     "H1 not first line",
			markdown: "Some content\n# Title\n\nMore content",
			expected: "Title",
		},
		{
			name:     "No H1",
			markdown: "## H2 Only\n\nContent",
			expected: "Documentation",
		},
		{
			name:     "Empty markdown",
			markdown: "",
			expected: "Documentation",
		},
		{
			name:     "Only whitespace",
			markdown: "   \n\n   \n",
			expected: "Documentation",
		},
		{
			name:     "H1 with special characters",
			markdown: "# Title with **bold** and `code`\n",
			expected: "Title with **bold** and `code`",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			result := extractJSONTitle(tt.markdown)
			if result != tt.expected {
				t.Errorf("Expected '%s', got '%s'", tt.expected, result)
			}
		})
	}
}

func TestCountText(t *testing.T) {
	tests := []struct {
		name          string
		content       string
		expectedWords int
		expectedChars int
	}{
		{
			name:          "Simple text",
			content:       "Hello world",
			expectedWords: 2,
			expectedChars: 11,
		},
		{
			name:          "Empty string",
			content:       "",
			expectedWords: 0,
			expectedChars: 0,
		},
		{
			name:          "Multiple spaces",
			content:       "word1  word2    word3",
			expectedWords: 3,
			expectedChars: 21,
		},
		{
			name:          "Newlines",
			content:       "line1\nline2\nline3",
			expectedWords: 3,
			expectedChars: 17,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			words, chars := countText([]byte(tt.content))
			if words != tt.expectedWords {
				t.Errorf("Expected %d words, got %d", tt.expectedWords, words)
			}
			if chars != tt.expectedChars {
				t.Errorf("Expected %d chars, got %d", tt.expectedChars, chars)
			}
		})
	}
}

func TestJSONExporter_ComplexMarkdown(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "complex.md")
	jsonFile := filepath.Join(tmpDir, "complex.json")

	// Test with GitHub Flavored Markdown extensions
	markdown := `# Complex Document

## Task Lists

- [x] Completed task
- [ ] Incomplete task

## Strikethrough

~~This text is crossed out~~

## Tables with alignment

| Left | Center | Right |
|:-----|:------:|------:|
| L1   | C1     | R1    |
| L2   | C2     | R2    |

## Multiple code blocks

` + "```python\ndef hello():\n    print(\"Hello\")\n```" + `

` + "```javascript\nfunction hello() {\n    console.log(\"Hello\");\n}\n```" + `

## Nested lists

1. First
   - Sub 1
   - Sub 2
2. Second
   1. Sub A
   2. Sub B

## Inline elements

**Bold**, *italic*, ***bold italic***, ~~strikethrough~~, ` + "`code`" + `
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Verify task list
	foundTaskList := false
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "list" {
			if listType, ok := elem["list_type"].(string); ok && listType == "task" {
				foundTaskList = true
				items, ok := elem["items"].([]interface{})
				if !ok {
					t.Error("Expected task list to have items")
					continue
				}
				if len(items) != 2 {
					t.Errorf("Expected 2 task items, got %d", len(items))
				}
			}
		}
	}

	if !foundTaskList {
		t.Error("Expected to find task list")
	}

	// Verify multiple code blocks
	codeBlocks := 0
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "code_block" {
			codeBlocks++
			language, ok := elem["language"].(string)
			if !ok {
				continue
			}
			if language != "python" && language != "javascript" {
				t.Errorf("Unexpected code language: %s", language)
			}
		}
	}

	if codeBlocks != 2 {
		t.Errorf("Expected 2 code blocks, got %d", codeBlocks)
	}

	// Verify table alignment
	foundAlignedTable := false
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "table" {
			header, ok := elem["header"].([]interface{})
			if !ok {
				continue
			}
			// Check first column has left alignment
			if len(header) > 0 {
				firstCol, ok := header[0].(map[string]interface{})
				if ok {
					if alignment, ok := firstCol["alignment"].(string); ok && alignment == "left" {
						foundAlignedTable = true
					}
				}
			}
		}
	}

	if !foundAlignedTable {
		t.Error("Expected to find table with alignment info")
	}
}

func TestJSONExporter_EmptyMarkdown(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "empty.md")
	jsonFile := filepath.Join(tmpDir, "empty.json")

	err = os.WriteFile(mdFile, []byte(""), 0644)
	if err != nil {
		t.Fatalf("Failed to write empty file: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Expected success with empty file, got error: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Should have default title
	if doc.Metadata.Title != "Documentation" {
		t.Errorf("Expected default title 'Documentation', got '%s'", doc.Metadata.Title)
	}

	// Should have valid JSON structure even if empty
	if len(doc.Content.Headings) != 0 {
		t.Errorf("Expected no headings, got %d", len(doc.Content.Headings))
	}

	if len(doc.Content.Elements) != 0 {
		t.Errorf("Expected no elements, got %d", len(doc.Content.Elements))
	}
}

func TestJSONExporter_HeadingHierarchy(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "hierarchy.md")
	jsonFile := filepath.Join(tmpDir, "hierarchy.json")

	markdown := `# Level 1

Content 1

## Level 2

Content 2

### Level 3

Content 3

## Another Level 2

Content 4

# Another Level 1

Content 5
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Should have 2 root level headings
	if len(doc.Content.Headings) != 2 {
		t.Errorf("Expected 2 root headings, got %d", len(doc.Content.Headings))
	}

	// First heading should have 2 children (both Level 2 headings)
	firstHeading := doc.Content.Headings[0]
	if firstHeading.Level != 1 {
		t.Errorf("Expected first heading level 1, got %d", firstHeading.Level)
	}
	if len(firstHeading.Children) != 2 {
		t.Errorf("Expected first heading to have 2 children, got %d", len(firstHeading.Children))
	}

	// Level 2 should have 1 child (Level 3)
	if len(firstHeading.Children) > 0 {
		level2Heading := firstHeading.Children[0]
		if level2Heading.Level != 2 {
			t.Errorf("Expected level 2 heading, got level %d", level2Heading.Level)
		}
		if len(level2Heading.Children) != 1 {
			t.Errorf("Expected level 2 to have 1 child, got %d", len(level2Heading.Children))
		}

		// Level 3 should have no children
		if len(level2Heading.Children) > 0 {
			level3Heading := level2Heading.Children[0]
			if level3Heading.Level != 3 {
				t.Errorf("Expected level 3 heading, got level %d", level3Heading.Level)
			}
			if len(level3Heading.Children) != 0 {
				t.Errorf("Expected level 3 to have no children, got %d", len(level3Heading.Children))
			}
		}
	}
}

func TestJSONExporter_OrderedList(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "ordered.md")
	jsonFile := filepath.Join(tmpDir, "ordered.json")

	markdown := `# Ordered List Test

1. First item
2. Second item
3. Third item

## Starting at 5

5. Fifth item
6. Sixth item
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Find ordered lists
	orderedLists := 0
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "list" {
			if listType, ok := elem["list_type"].(string); ok && listType == "ordered" {
				orderedLists++

				items, ok := elem["items"].([]interface{})
				if !ok {
					t.Error("Expected ordered list to have items")
					continue
				}

				// First list should have 3 items
				if orderedLists == 1 && len(items) != 3 {
					t.Errorf("Expected first ordered list to have 3 items, got %d", len(items))
				}
				// Second list should have 2 items
				if orderedLists == 2 && len(items) != 2 {
					t.Errorf("Expected second ordered list to have 2 items, got %d", len(items))
				}

				// Check start number for second list
				if orderedLists == 2 {
					start, ok := elem["start"].(float64)
					if !ok {
						t.Error("Expected ordered list to have start number")
					} else if int(start) != 5 {
						t.Errorf("Expected start number 5, got %v", start)
					}
				}
			}
		}
	}

	if orderedLists != 2 {
		t.Errorf("Expected 2 ordered lists, got %d", orderedLists)
	}
}

func TestJSONExporter_CodeBlockWithoutLanguage(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "indented.md")
	jsonFile := filepath.Join(tmpDir, "indented.json")

	// Indented code block (no language)
	markdown := `# Indented Code Block

    This is an indented
    code block with
    no language specified
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Find code block without language
	foundCodeBlock := false
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "code_block" {
			language, ok := elem["language"].(string)
			if !ok {
				t.Error("Expected code_block to have language field")
				continue
			}
			if language == "" {
				foundCodeBlock = true
				code, ok := elem["code"].(string)
				if !ok {
					t.Error("Expected code_block to have code")
					continue
				}
				if !strings.Contains(code, "indented") {
					t.Error("Expected code to contain 'indented'")
				}
			}
		}
	}

	if !foundCodeBlock {
		t.Error("Expected to find code block without language")
	}
}

func TestGenerateID(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tests := []struct {
		name     string
		input    string
		expected string
	}{
		{
			name:     "Simple text",
			input:    "Hello World",
			expected: "hello-world",
		},
		{
			name:     "With underscores",
			input:    "hello_world_test",
			expected: "hello-world-test",
		},
		{
			name:     "Special characters",
			input:    "Hello @#$% World!",
			expected: "hello-world",
		},
		{
			name:     "Multiple spaces",
			input:    "hello    world",
			expected: "hello-world",
		},
		{
			name:     "Empty string",
			input:    "",
			expected: "heading",
		},
		{
			name:     "Numbers",
			input:    "Test 123 Example",
			expected: "test-123-example",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			result := exporter.generateID(tt.input)
			if result != tt.expected {
				t.Errorf("Expected '%s', got '%s'", tt.expected, result)
			}
		})
	}
}

func TestJSONExtractor_Paragraphs(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "paragraphs.md")
	jsonFile := filepath.Join(tmpDir, "paragraphs.json")

	markdown := `# Document

First paragraph with **bold** and *italic* text.

Second paragraph with ` + "`code`" + ` and [links](https://example.com).

Third paragraph on its own.
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Count paragraphs
	paragraphCount := 0
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "paragraph" {
			paragraphCount++
			content, ok := elem["content"].(string)
			if !ok {
				t.Error("Expected paragraph to have content")
				continue
			}
			if content == "" {
				t.Error("Expected paragraph content to be non-empty")
			}
		}
	}

	if paragraphCount != 3 {
		t.Errorf("Expected 3 paragraphs, got %d", paragraphCount)
	}
}

func TestJSONExtractor_HeadingsHierarchy(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "headings.md")
	jsonFile := filepath.Join(tmpDir, "headings.json")

	markdown := `# H1 One

## H2 One

### H3 One

#### H4 One

## H2 Two

# H1 Two

### H3 Under H1 Two
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Should have 2 root headings
	if len(doc.Content.Headings) != 2 {
		t.Errorf("Expected 2 root headings, got %d", len(doc.Content.Headings))
	}

	// First H1 should have two H2 children (H2 One and H2 Two)
	firstH1 := doc.Content.Headings[0]
	if firstH1.Text != "H1 One" {
		t.Errorf("Expected 'H1 One', got '%s'", firstH1.Text)
	}
	if len(firstH1.Children) != 2 {
		t.Errorf("Expected first H1 to have 2 children, got %d", len(firstH1.Children))
	}

	// H2 should have one H3 child
	if len(firstH1.Children) > 0 {
		firstH2 := firstH1.Children[0]
		if firstH2.Text != "H2 One" {
			t.Errorf("Expected 'H2 One', got '%s'", firstH2.Text)
		}
		if len(firstH2.Children) != 1 {
			t.Errorf("Expected H2 to have 1 child, got %d", len(firstH2.Children))
		}

		// H3 should have one H4 child
		if len(firstH2.Children) > 0 {
			firstH3 := firstH2.Children[0]
			if firstH3.Text != "H3 One" {
				t.Errorf("Expected 'H3 One', got '%s'", firstH3.Text)
			}
			if len(firstH3.Children) != 1 {
				t.Errorf("Expected H3 to have 1 child, got %d", len(firstH3.Children))
			}

			// H4 should have no children
			if len(firstH3.Children) > 0 {
				firstH4 := firstH3.Children[0]
				if firstH4.Text != "H4 One" {
					t.Errorf("Expected 'H4 One', got '%s'", firstH4.Text)
				}
				if len(firstH4.Children) != 0 {
					t.Errorf("Expected H4 to have 0 children, got %d", len(firstH4.Children))
				}
			}
		}

		// Second H2 should be sibling of first H2 (child of H1)
		if len(firstH1.Children) != 2 {
			t.Errorf("Expected H1 to have 2 H2 children, got %d", len(firstH1.Children))
		} else {
			secondH2 := firstH1.Children[1]
			if secondH2.Text != "H2 Two" {
				t.Errorf("Expected 'H2 Two', got '%s'", secondH2.Text)
			}
		}
	}

	// Second H1 should have one H3 child
	secondH1 := doc.Content.Headings[1]
	if secondH1.Text != "H1 Two" {
		t.Errorf("Expected 'H1 Two', got '%s'", secondH1.Text)
	}
	if len(secondH1.Children) != 1 {
		t.Errorf("Expected second H1 to have 1 child, got %d", len(secondH1.Children))
	}
}

func TestJSONExtractor_UnorderedLists(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "unordered.md")
	jsonFile := filepath.Join(tmpDir, "unordered.json")

	markdown := `# Lists

- Simple item
- Item with **bold**
  - Nested item 1
  - Nested item 2
- Item with ` + "`code`" + `
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Find unordered list
	foundList := false
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "list" {
			if listType, ok := elem["list_type"].(string); ok && listType == "unordered" {
				foundList = true
				items, ok := elem["items"].([]interface{})
				if !ok {
					t.Fatal("Expected list to have items")
				}

				// Should have 3 top-level items
				if len(items) != 3 {
					t.Errorf("Expected 3 list items, got %d", len(items))
				}

				// Check first item
				if len(items) > 0 {
					firstItem, ok := items[0].(map[string]interface{})
					if !ok {
						t.Fatal("Expected item to be a map")
					}

					content, ok := firstItem["content"].(string)
					if !ok {
						t.Error("Expected item to have content")
					}
					if content != "Simple item" {
						t.Errorf("Expected 'Simple item', got '%s'", content)
					}

					// First item should NOT have nested items
					nested, ok := firstItem["items"].([]interface{})
					if ok && len(nested) > 0 {
						t.Error("Expected first item to have no nested items")
					}
				}

				// Check second item has nested items
				if len(items) > 1 {
					secondItem, ok := items[1].(map[string]interface{})
					if !ok {
						t.Fatal("Expected item to be a map")
					}

					// Check nested items
					nested, ok := secondItem["items"].([]interface{})
					if !ok {
						t.Error("Expected second item to have nested items")
					} else if len(nested) != 2 {
						t.Errorf("Expected 2 nested items, got %d", len(nested))
					}
				}
			}
		}
	}

	if !foundList {
		t.Fatal("Expected to find unordered list")
	}
}

func TestJSONExtractor_OrderedLists(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "ordered.md")
	jsonFile := filepath.Join(tmpDir, "ordered.json")

	markdown := `# Ordered Lists

1. First item
2. Second item
   1. Nested ordered 1
   2. Nested ordered 2
3. Third item
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	foundList := false
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "list" {
			if listType, ok := elem["list_type"].(string); ok && listType == "ordered" {
				foundList = true
				items, ok := elem["items"].([]interface{})
				if !ok {
					t.Fatal("Expected list to have items")
				}

				if len(items) != 3 {
					t.Errorf("Expected 3 items, got %d", len(items))
				}

				// Check start number
				start, ok := elem["start"].(float64)
				if !ok {
					t.Error("Expected ordered list to have start number")
				} else if int(start) != 1 {
					t.Errorf("Expected start 1, got %v", start)
				}

				// Check nested ordered list
				if len(items) > 1 {
					secondItem, ok := items[1].(map[string]interface{})
					if ok {
						nested, ok := secondItem["items"].([]interface{})
						if !ok {
							t.Error("Expected second item to have nested items")
						} else if len(nested) != 2 {
							t.Errorf("Expected 2 nested items, got %d", len(nested))
						}
					}
				}
			}
		}
	}

	if !foundList {
		t.Fatal("Expected to find ordered list")
	}
}

func TestJSONExtractor_TaskLists(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "tasks.md")
	jsonFile := filepath.Join(tmpDir, "tasks.json")

	markdown := `# Task List

- [x] Completed task 1
- [ ] Incomplete task
- [x] Completed task 2
  - [ ] Nested incomplete
  - [x] Nested completed
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	foundTaskList := false
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "list" {
			if listType, ok := elem["list_type"].(string); ok && listType == "task" {
				foundTaskList = true
				items, ok := elem["items"].([]interface{})
				if !ok {
					t.Fatal("Expected task list to have items")
				}

				if len(items) != 3 {
					t.Errorf("Expected 3 task items, got %d", len(items))
				}

				// Check first item
				if len(items) > 0 {
					firstItem, ok := items[0].(map[string]interface{})
					if !ok {
						t.Fatal("Expected item to be a map")
					}

					checked, ok := firstItem["checked"].(bool)
					if !ok {
						t.Error("Expected task item to have checked field")
					}
					if !checked {
						t.Error("Expected first task to be checked")
					}

					content, ok := firstItem["content"].(string)
					if !ok {
						t.Error("Expected item to have content")
					}
					if content != "Completed task 1" {
						t.Errorf("Expected 'Completed task 1', got '%s'", content)
					}
				}

				// Check second item (unchecked)
				if len(items) > 1 {
					secondItem, ok := items[1].(map[string]interface{})
					if ok {
						checked, ok := secondItem["checked"].(bool)
						if !ok {
							t.Error("Expected task item to have checked field")
						}
						if checked {
							t.Error("Expected second task to be unchecked")
						}
					}
				}

				// Check nested tasks
				if len(items) > 2 {
					thirdItem, ok := items[2].(map[string]interface{})
					if ok {
						nested, ok := thirdItem["items"].([]interface{})
						if !ok {
							t.Error("Expected third item to have nested tasks")
						} else if len(nested) != 2 {
							t.Errorf("Expected 2 nested tasks, got %d", len(nested))
						}
					}
				}
			}
		}
	}

	if !foundTaskList {
		t.Fatal("Expected to find task list")
	}
}

func TestJSONExtractor_CodeBlocksWithLanguage(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "code.md")
	jsonFile := filepath.Join(tmpDir, "code.json")

	markdown := `# Code Blocks

` + "```go\nfunc hello() {\n    fmt.Println(\"Hello\")\n}\n```" + `

` + "```python\ndef hello():\n    print('Hello')\n```" + `

` + "```javascript\nfunction hello() {\n    console.log('Hello');\n}\n```" + `

` + "```bash\necho 'Hello'\n```" + `

    indented code
    block without
    language
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	codeBlocks := make(map[string]string)
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "code_block" {
			language, ok := elem["language"].(string)
			if !ok {
				t.Error("Expected code_block to have language")
				continue
			}

			code, ok := elem["code"].(string)
			if !ok {
				t.Error("Expected code_block to have code")
				continue
			}

			if code == "" {
				t.Errorf("Expected non-empty code for language %s", language)
			}

			codeBlocks[language] = code
		}
	}

	// Should have 5 code blocks (4 with language, 1 without)
	if len(codeBlocks) != 5 {
		t.Errorf("Expected 5 code blocks, got %d", len(codeBlocks))
	}

	// Check specific languages
	expectedLanguages := []string{"go", "python", "javascript", "bash", ""}
	for _, lang := range expectedLanguages {
		if _, ok := codeBlocks[lang]; !ok {
			t.Errorf("Expected to find code block with language '%s'", lang)
		}
	}

	// Verify Go code content
	goCode := codeBlocks["go"]
	if !strings.Contains(goCode, "func hello") {
		t.Error("Expected Go code to contain 'func hello'")
	}
	if !strings.Contains(goCode, "fmt.Println") {
		t.Error("Expected Go code to contain 'fmt.Println'")
	}
}

func TestJSONExtractor_Tables(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "tables.md")
	jsonFile := filepath.Join(tmpDir, "tables.json")

	markdown := `# Tables

| Left | Center | Right | Default |
|:-----|:------:|------:|---------|
| L1   | C1     | R1    | D1      |
| L2   | C2     | R2    | D2      |
| L3   | C3     | R3    | D3      |
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	foundTable := false
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "table" {
			foundTable = true

			// Check header
			header, ok := elem["header"].([]interface{})
			if !ok {
				t.Fatal("Expected table to have header")
			}

			if len(header) != 4 {
				t.Errorf("Expected 4 header columns, got %d", len(header))
			}

			// Check first column alignment
			if len(header) > 0 {
				firstCol, ok := header[0].(map[string]interface{})
				if !ok {
					t.Fatal("Expected header column to be a map")
				}

				text, ok := firstCol["text"].(string)
				if !ok {
					t.Error("Expected header column to have text")
				}
				if text != "Left" {
					t.Errorf("Expected 'Left', got '%s'", text)
				}

				alignment, ok := firstCol["alignment"].(string)
				if !ok {
					t.Error("Expected header column to have alignment")
				}
				if alignment != "left" {
					t.Errorf("Expected alignment 'left', got '%s'", alignment)
				}
			}

			// Check second column (center)
			if len(header) > 1 {
				secondCol, ok := header[1].(map[string]interface{})
				if ok {
					alignment, ok := secondCol["alignment"].(string)
					if !ok {
						t.Error("Expected header column to have alignment")
					}
					if alignment != "center" {
						t.Errorf("Expected alignment 'center', got '%s'", alignment)
					}
				}
			}

			// Check third column (right)
			if len(header) > 2 {
				thirdCol, ok := header[2].(map[string]interface{})
				if ok {
					alignment, ok := thirdCol["alignment"].(string)
					if !ok {
						t.Error("Expected header column to have alignment")
					}
					if alignment != "right" {
						t.Errorf("Expected alignment 'right', got '%s'", alignment)
					}
				}
			}

			// Check rows
			rows, ok := elem["rows"].([]interface{})
			if !ok {
				t.Fatal("Expected table to have rows")
			}

			if len(rows) != 3 {
				t.Errorf("Expected 3 rows, got %d", len(rows))
			}

			// Check first row
			if len(rows) > 0 {
				firstRow, ok := rows[0].([]interface{})
				if !ok {
					t.Fatal("Expected row to be an array")
				}

				if len(firstRow) != 4 {
					t.Errorf("Expected 4 columns in row, got %d", len(firstRow))
				}

				// Check first cell
				if len(firstRow) > 0 {
					firstCell, ok := firstRow[0].(map[string]interface{})
					if !ok {
						t.Fatal("Expected cell to be a map")
					}

					text, ok := firstCell["text"].(string)
					if !ok {
						t.Error("Expected cell to have text")
					}
					if text != "L1" {
						t.Errorf("Expected 'L1', got '%s'", text)
					}
				}
			}
		}
	}

	if !foundTable {
		t.Fatal("Expected to find table")
	}
}

func TestJSONExtractor_Blockquotes(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "blockquote.md")
	jsonFile := filepath.Join(tmpDir, "blockquote.json")

	markdown := `# Blockquotes

> Simple blockquote
> with multiple lines

> Blockquote with **bold** and *italic*
> and ` + "`code`" + `

> Nested blockquote
> > Inner blockquote
> > Still inner
> Back to outer
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	blockquoteCount := 0
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "blockquote" {
			blockquoteCount++
			content, ok := elem["content"].(string)
			if !ok {
				t.Error("Expected blockquote to have content")
				continue
			}

			if content == "" {
				t.Error("Expected blockquote content to be non-empty")
			}

			// Check that content contains expected words
			if blockquoteCount == 1 {
				if !strings.Contains(content, "Simple") {
					t.Error("Expected first blockquote to contain 'Simple'")
				}
			}
		}
	}

	if blockquoteCount != 3 {
		t.Errorf("Expected 3 blockquotes, got %d", blockquoteCount)
	}
}

func TestJSONExtractor_Links(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "links.md")
	jsonFile := filepath.Join(tmpDir, "links.json")

	markdown := `# Links

Simple [link](https://example.com).

Link with [title](https://example.com "Example Title").

[Reference link][ref]

[ref]: https://example.com

Auto link: <https://auto.com>
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	linkCount := 0
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "link" {
			linkCount++

			url, ok := elem["url"].(string)
			if !ok {
				t.Error("Expected link to have url")
				continue
			}

			if url == "" {
				t.Error("Expected link URL to be non-empty")
			}

			text, ok := elem["text"].(string)
			if !ok {
				t.Error("Expected link to have text")
				continue
			}

			// Check specific link
			if text == "link" {
				if url != "https://example.com" {
					t.Errorf("Expected 'https://example.com', got '%s'", url)
				}
			}
		}
	}

	if linkCount < 2 {
		t.Errorf("Expected at least 2 links, got %d", linkCount)
	}
}

func TestJSONExtractor_Images(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "images.md")
	jsonFile := filepath.Join(tmpDir, "images.json")

	markdown := `# Images

![Alt text](image.png)

![Image with title](photo.jpg "Photo Title")

![Remote](https://example.com/image.png)
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	imageCount := 0
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "image" {
			imageCount++

			src, ok := elem["url"].(string)
			if !ok {
				t.Error("Expected image to have url")
				continue
			}

			if src == "" {
				t.Error("Expected image src to be non-empty")
			}

			alt, ok := elem["alt"].(string)
			if !ok {
				t.Error("Expected image to have alt")
				continue
			}

			// Check specific image
			if alt == "Alt text" {
				if src != "image.png" {
					t.Errorf("Expected 'image.png', got '%s'", src)
				}
			}

			if alt == "Remote" {
				if src != "https://example.com/image.png" {
					t.Errorf("Expected 'https://example.com/image.png', got '%s'", src)
				}
			}
		}
	}

	if imageCount != 3 {
		t.Errorf("Expected 3 images, got %d", imageCount)
	}
}

// Edge case: deeply nested lists (3+ levels)
func TestJSONEdgeCase_DeeplyNestedLists(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "nested.md")
	jsonFile := filepath.Join(tmpDir, "nested.json")

	markdown := `# Deep Nesting

- Level 1
  - Level 2
    - Level 3
      - Level 4
  - Back to Level 2
- Another Level 1
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Find list with deep nesting
	foundDeepNesting := false
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "list" {
			items, ok := elem["items"].([]interface{})
			if !ok {
				continue
			}

			// Check first item
			if len(items) > 0 {
				firstItem, ok := items[0].(map[string]interface{})
				if ok {
					// Check Level 2
					nested2, ok := firstItem["items"].([]interface{})
					if ok && len(nested2) > 0 {
						level2, ok := nested2[0].(map[string]interface{})
						if ok {
							// Check Level 3
							nested3, ok := level2["items"].([]interface{})
							if ok && len(nested3) > 0 {
								level3, ok := nested3[0].(map[string]interface{})
								if ok {
									// Check Level 4
									nested4, ok := level3["items"].([]interface{})
									if ok && len(nested4) > 0 {
										foundDeepNesting = true
									}
								}
							}
						}
					}
				}
			}
		}
	}

	if !foundDeepNesting {
		t.Error("Expected to find deeply nested list structure (4 levels)")
	}
}

// Edge case: special characters in text
func TestJSONEdgeCase_SpecialCharacters(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "special.md")
	jsonFile := filepath.Join(tmpDir, "special.json")

	markdown := `# Special Characters

Paragraph with special chars: < > & " ' ` + "`" + `

Math symbols: â‰¤ â‰¥ â‰  Â± Ã— Ã· Â°

Currency: $ â‚¬ Â£ Â¥ Â¢

Punctuation: Â¡ Â¿ â€  â€¡

Emojis: ğŸ˜€ ğŸ‰ ğŸš€ ğŸ’»

Unicode: ä¸­æ–‡ æ—¥æœ¬èª í•œê¸€ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

Quotes: "smart" 'curly' ` + "``" + `backticks` + "``" + `
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Find paragraph and verify special characters are preserved
	foundSpecialChars := false
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "paragraph" {
			content, ok := elem["content"].(string)
			if !ok {
				continue
			}

			// Check for various special characters
			if strings.Contains(content, "â‚¬") || strings.Contains(content, "â‰¤") ||
				strings.Contains(content, "ğŸ˜€") || strings.Contains(content, "ä¸­æ–‡") {
				foundSpecialChars = true
			}
		}
	}

	if !foundSpecialChars {
		t.Error("Expected to find special characters in exported content")
	}
}

// Edge case: multiple inline formatting combinations
func TestJSONEdgeCase_ComplexInlineFormatting(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "inline.md")
	jsonFile := filepath.Join(tmpDir, "inline.json")

	markdown := `# Inline Formatting

**Bold and *italic* together**

***All three styles*** bold and italic

` + "`" + `code with **bold** inside` + "`" + `

**bold with ` + "`" + `code` + "`" + ` inside**

*italic with ` + "`" + `code` + "`" + ` inside*

[**bold link**](https://example.com)

[*italic link*](https://example.com)

[` + "`" + `code link` + "`" + `](https://example.com)

~~Strikethrough with **bold**~~
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Count paragraphs with inline formatting
	paragraphCount := 0
	linkCount := 0
	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "paragraph" {
			paragraphCount++
			content, ok := elem["content"].(string)
			if !ok {
				continue
			}
			// Check that markdown formatting syntax is preserved
			// (element content is valid if it contains formatting syntax)
			_ = strings.Contains(content, "**") || strings.Contains(content, "*") || strings.Contains(content, "`")
		}
		if elemType, ok := elem["type"].(string); ok && elemType == "link" {
			linkCount++
		}
	}

	if paragraphCount < 5 {
		t.Errorf("Expected at least 5 paragraphs with inline formatting, got %d", paragraphCount)
	}

	if linkCount < 3 {
		t.Errorf("Expected at least 3 links, got %d", linkCount)
	}
}

// Edge case: alternating code blocks and text
func TestJSONEdgeCase_AlternatingCodeBlocks(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "alternating.md")
	jsonFile := filepath.Join(tmpDir, "alternating.json")

	markdown := `# Alternating Content

Text paragraph 1

` + "```go\ncode 1\n```" + `

Text paragraph 2

` + "```python\ncode 2\n```" + `

Text paragraph 3

` + "```javascript\ncode 3\n```" + `

Text paragraph 4
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Count alternating elements
	paragraphCount := 0
	codeBlockCount := 0

	for _, elem := range doc.Content.Elements {
		elemType, ok := elem["type"].(string)
		if !ok {
			continue
		}

		if elemType == "paragraph" {
			paragraphCount++
		}
		if elemType == "code_block" {
			codeBlockCount++
		}
	}

	if paragraphCount != 4 {
		t.Errorf("Expected 4 paragraphs, got %d", paragraphCount)
	}

	if codeBlockCount != 3 {
		t.Errorf("Expected 3 code blocks, got %d", codeBlockCount)
	}
}

// Edge case: document with only H2/H3 (no H1)
func TestJSONEdgeCase_NoH1Title(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "noh1.md")
	jsonFile := filepath.Join(tmpDir, "noh1.json")

	markdown := `## Missing H1

This document has no H1 heading.

### Just H2 and H3

Content here.
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Should have default title when no H1 present
	if doc.Metadata.Title != "Documentation" {
		t.Errorf("Expected default title 'Documentation', got '%s'", doc.Metadata.Title)
	}

	// Should still have headings (H2 and H3)
	if len(doc.Content.Headings) == 0 {
		t.Error("Expected to find H2 and H3 headings even without H1")
	}

	// Verify H2 is root level
	if len(doc.Content.Headings) > 0 {
		firstHeading := doc.Content.Headings[0]
		if firstHeading.Level != 2 {
			t.Errorf("Expected first heading to be level 2, got %d", firstHeading.Level)
		}
	}
}

// Edge case: document with only whitespace
func TestJSONEdgeCase_OnlyWhitespace(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "whitespace.md")
	jsonFile := filepath.Join(tmpDir, "whitespace.json")

	markdown := "   \n\n   \n\t\t\n   "

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Should have default title
	if doc.Metadata.Title != "Documentation" {
		t.Errorf("Expected default title 'Documentation', got '%s'", doc.Metadata.Title)
	}

	// Should have zero word count for whitespace-only content
	if doc.Metadata.WordCount != 0 {
		t.Errorf("Expected 0 word count, got %d", doc.Metadata.WordCount)
	}

	// Should have no elements
	if len(doc.Content.Elements) != 0 {
		t.Errorf("Expected 0 elements, got %d", len(doc.Content.Elements))
	}
}

// Edge case: very long heading text
func TestJSONEdgeCase_LongHeading(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "long.md")
	jsonFile := filepath.Join(tmpDir, "long.json")

	longText := strings.Repeat("This is a very long heading text. ", 20)

	markdown := "# " + longText + `

Content below.
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Title should be preserved
	if !strings.Contains(doc.Metadata.Title, "very long heading") {
		t.Error("Expected long heading to be preserved in title")
	}

	if len(doc.Content.Headings) == 0 {
		t.Fatal("Expected to find heading")
	}

	// ID should be generated (may be truncated/modified)
	if doc.Content.Headings[0].ID == "" {
		t.Error("Expected heading to have ID")
	}
}

// Edge case: mixed list types in sequence
func TestJSONEdgeCase_MixedListTypes(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "mixed.md")
	jsonFile := filepath.Join(tmpDir, "mixed.json")

	markdown := `# Mixed Lists

- Unordered item 1
- Unordered item 2

1. Ordered item 1
2. Ordered item 2

- [x] Task item 1
- [ ] Task item 2

- Back to unordered
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Count different list types
	unorderedCount := 0
	orderedCount := 0
	taskCount := 0

	for _, elem := range doc.Content.Elements {
		if elemType, ok := elem["type"].(string); ok && elemType == "list" {
			listType, ok := elem["list_type"].(string)
			if !ok {
				t.Logf("List element without list_type: %+v", elem)
				continue
			}

			t.Logf("Found list with type: %s", listType)
			switch listType {
			case "unordered":
				unorderedCount++
			case "ordered":
				orderedCount++
			case "task":
				taskCount++
			}
		}
	}

	// Note: Due to goldmark TaskList extension behavior, task lists that come after
	// ordered lists may not have TaskCheckBox nodes added. The items are parsed as
	// a regular unordered list with the checkbox syntax included as text.
	// This is expected behavior given the extension's implementation.
	if unorderedCount != 2 && unorderedCount != 3 {
		// Accept both 2 (if task list not merged) and 3 (if task list detected as unordered)
		t.Errorf("Expected 2 or 3 unordered lists, got %d", unorderedCount)
	}

	if orderedCount != 1 {
		t.Errorf("Expected 1 ordered list, got %d", orderedCount)
	}
}

// Edge case: consecutive headings with no content
func TestJSONEdgeCase_ConsecutiveHeadings(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()
	mdFile := filepath.Join(tmpDir, "consecutive.md")
	jsonFile := filepath.Join(tmpDir, "consecutive.json")

	markdown := `# Heading 1

## Heading 1.1

### Heading 1.1.1

# Heading 2

## Heading 2.1

Some content here.

### Heading 2.1.1
`

	err = os.WriteFile(mdFile, []byte(markdown), 0644)
	if err != nil {
		t.Fatalf("Failed to write markdown: %v", err)
	}

	err = exporter.ExportToJSON(mdFile, jsonFile)
	if err != nil {
		t.Fatalf("Failed to export: %v", err)
	}

	jsonData, err := os.ReadFile(jsonFile)
	if err != nil {
		t.Fatalf("Failed to read JSON: %v", err)
	}

	var doc JSONDocument
	if err := json.Unmarshal(jsonData, &doc); err != nil {
		t.Fatalf("Failed to unmarshal JSON: %v", err)
	}

	// Should have 2 root headings (both H1)
	if len(doc.Content.Headings) != 2 {
		t.Errorf("Expected 2 root headings, got %d", len(doc.Content.Headings))
	}

	// First H1 should have nested structure
	firstH1 := doc.Content.Headings[0]
	if len(firstH1.Children) == 0 {
		t.Error("Expected first H1 to have child headings")
	}

	// Verify hierarchy depth
	if len(firstH1.Children) > 0 {
		firstH2 := firstH1.Children[0]
		if len(firstH2.Children) == 0 {
			t.Error("Expected H2 to have H3 child")
		}
	}
}

// Error handling: malformed markdown should still succeed gracefully
func TestJSONErrorHandling_MalformedMarkdown(t *testing.T) {
	exporter, err := NewJSONExporter()
	if err != nil {
		t.Fatalf("Failed to create exporter: %v", err)
	}

	tmpDir := t.TempDir()

	tests := []struct {
		name     string
		markdown string
		desc     string
	}{
		{
			name: "Unclosed code block",
			markdown: `# Unclosed Code

` + "```go\nfunc incomplete() {\n    // no closing",
			desc: "Code block without closing fence",
		},
		{
			name: "Unclosed link",
			markdown: `# Unclosed Link

This has an [unclosed link(https://example.com`,
			desc: "Link with missing closing bracket",
		},
		{
			name: "Unclosed emphasis",
			markdown: `# Unclosed Emphasis

This has **bold text that never closes

And more text here.`,
			desc: "Bold formatting without closing markers",
		},
		{
			name: "Malformed table",
			markdown: `# Malformed Table

| Col1 | Col2
|------|------
| Val1 | Val2 | Extra
| Val3 |
`,
			desc: "Table with inconsistent column counts",
		},
		{
			name: "Broken list formatting",
			markdown: `# Broken List

- Item 1
- Item 2
  - Nested but wrong indentation
- Item 3

1. First
2. Second
3. Third
   Bad indentation here
4. Fourth
`,
			desc: "Lists with inconsistent indentation",
		},
		{
			name: "Multiple unclosed blocks",
			markdown: `# Multiple Issues

**Bold *italic

` + "```python\ndef bad():\n    pass" + `

[Link](https://example.com

> Blockquote that
> doesn't close properly

More text`,
			desc: "Multiple unclosed formatting elements",
		},
		{
			name: "Empty code fence with language",
			markdown: `# Empty Code Block

This has ` + "```go\n```" + ` an empty code block.

And then more text.`,
			desc: "Code fence with language but no content",
		},
		{
			name: "Mangled heading levels",
			markdown: `# H1

###### H6

####### Invalid H7

##### H5

######## Even more invalid`,
			desc: "Heading levels beyond H6",
		},
		{
			name: "Broken reference link",
			markdown: `# Reference Link

This has [ref link][ref] but ref is not defined.

[Another][link]

[link]: https://example.com`,
			desc: "Reference link with undefined reference",
		},
		{
			name: "Invalid HTML entities",
			markdown: `# Invalid HTML

&invalidentity;

&notanentityatall;

&;

Text after broken entities.`,
			desc: "Invalid HTML entity references",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			mdFile := filepath.Join(tmpDir, tt.name+".md")
			jsonFile := filepath.Join(tmpDir, tt.name+".json")

			err = os.WriteFile(mdFile, []byte(tt.markdown), 0644)
			if err != nil {
				t.Fatalf("Failed to write markdown: %v", err)
			}

			// Should not error even with malformed markdown
			err = exporter.ExportToJSON(mdFile, jsonFile)
			if err != nil {
				t.Errorf("Expected success with %s, got error: %v", tt.desc, err)
				return
			}

			// Verify JSON file was created
			jsonData, err := os.ReadFile(jsonFile)
			if err != nil {
				t.Fatalf("Failed to read JSON output: %v", err)
			}

			// Verify JSON is valid
			var doc JSONDocument
			if err := json.Unmarshal(jsonData, &doc); err != nil {
				t.Errorf("Generated invalid JSON for %s: %v", tt.desc, err)
				return
			}

			// Verify basic structure exists
			if doc.Metadata.Title == "" {
				t.Error("Expected metadata title to be set (should have default)")
			}

			if doc.Metadata.Generator.Name != "Gendocs" {
				t.Error("Expected generator name to be 'Gendocs'")
			}

			// Elements array should exist (may be empty but should be present)
			if doc.Content.Elements == nil {
				t.Error("Expected elements array to be initialized")
			}

			// Headings array should exist
			if doc.Content.Headings == nil {
				t.Error("Expected headings array to be initialized")
			}

			// Verify JSON is well-formed and can be remarshaled
			remarkaled, err := json.Marshal(doc)
			if err != nil {
				t.Errorf("Failed to remarshal JSON document: %v", err)
			}

			if len(remarkaled) == 0 {
				t.Error("Remarshaled JSON is empty")
			}
		})
	}
}
</file>
<file path="internal/gitlab/client.go">
package gitlab

import (
	"context"
	"fmt"
	"net/http"
	"time"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/logging"
)

// Client represents a GitLab API client
type Client struct {
	httpClient   *http.Client
	apiURL       string
	OAuthToken   string
	UserName     string
	UserUsername string
	UserEmail    string
	logger       *logging.Logger
}

// Project represents a GitLab project
type Project struct {
	ID                int       `json:"id"`
	Name              string    `json:"name"`
	PathWithNamespace string    `json:"path_with_namespace"`
	HTTPURL           string    `json:"http_url_to_repo"`
	SSHURL            string    `json:"ssh_url_to_repo"`
	DefaultBranch     string    `json:"default_branch"`
	LastActivityAt    time.Time `json:"last_activity_at"`
	CreatedAt         time.Time `json:"created_at"`
	Archived          bool      `json:"archived"`
}

// MergeRequest represents a GitLab merge request
type MergeRequest struct {
	ID           int    `json:"id"`
	IID          int    `json:"iid"`
	Title        string `json:"title"`
	Description  string `json:"description"`
	SourceBranch string `json:"source_branch"`
	TargetBranch string `json:"target_branch"`
	WebURL       string `json:"web_url"`
}

// NewClient creates a new GitLab client
func NewClient(cfg config.GitLabConfig, logger *logging.Logger) *Client {
	return &Client{
		httpClient: &http.Client{
			Timeout: 30 * time.Second,
		},
		apiURL:       cfg.APIURL,
		OAuthToken:   cfg.OAuthToken,
		UserName:     cfg.UserName,
		UserUsername: cfg.UserUsername,
		UserEmail:    cfg.UserEmail,
		logger:       logger,
	}
}

// FetchProjectsInGroup fetches all projects in a group (including subgroups)
func (c *Client) FetchProjectsInGroup(ctx context.Context, groupID int) ([]Project, error) {
	c.logger.Info(fmt.Sprintf("Fetching projects in group %d", groupID))

	// For now, return empty list - would need full GitLab API implementation
	// This would require using xanzy/go-gitlab or implementing the API calls
	return []Project{}, nil
}

// ProjectFilter determines if a project should be analyzed
type ProjectFilter struct {
	MaxDaysSinceLastCommit int
	IgnoreProjects         map[string]bool
	IgnoreSubgroups        map[string]bool
}

// ShouldAnalyze determines if a project should be analyzed
func (c *Client) ShouldAnalyze(ctx context.Context, project Project, filter ProjectFilter) (bool, error) {
	// Skip archived projects
	if project.Archived {
		return false, nil
	}

	// Skip ignored projects
	if filter.IgnoreProjects[project.PathWithNamespace] {
		return false, nil
	}

	// Check if last commit is too old
	if filter.MaxDaysSinceLastCommit > 0 {
		daysSince := time.Since(project.LastActivityAt).Hours() / 24
		if int(daysSince) > filter.MaxDaysSinceLastCommit {
			return false, nil
		}
	}

	// Check if branch already exists
	branchName := fmt.Sprintf("ai-analyzer-%s", time.Now().Format("2006-01-02"))
	if branchExists, err := c.BranchExists(ctx, project, branchName); err == nil && branchExists {
		return false, nil
	}

	// Check if open MR exists
	if hasMR, err := c.HasOpenMR(ctx, project, branchName); err == nil && hasMR {
		return false, nil
	}

	return true, nil
}

// BranchExists checks if a branch exists in a project
func (c *Client) BranchExists(ctx context.Context, project Project, branchName string) (bool, error) {
	// Placeholder - would implement GitLab API call
	return false, nil
}

// HasOpenMR checks if an open MR exists for a branch
func (c *Client) HasOpenMR(ctx context.Context, project Project, branchName string) (bool, error) {
	// Placeholder - would implement GitLab API call
	return false, nil
}

// CreateBranch creates a new branch in a project
func (c *Client) CreateBranch(ctx context.Context, project Project, branchName, fromBranch string) error {
	// Placeholder - would implement GitLab API call
	c.logger.Info(fmt.Sprintf("Would create branch '%s' in %s", branchName, project.PathWithNamespace))
	return nil
}

// CreateCommit creates a commit with the given files
func (c *Client) CreateCommit(ctx context.Context, project Project, branchName, message string, files map[string]string) error {
	// Placeholder - would implement GitLab API call
	c.logger.Info(fmt.Sprintf("Would create commit in %s on branch %s", project.PathWithNamespace, branchName))
	return nil
}

// CreateMR creates a merge request
func (c *Client) CreateMR(ctx context.Context, project Project, sourceBranch, targetBranch, title, description string) (*MergeRequest, error) {
	// Placeholder - would implement GitLab API call
	c.logger.Info(fmt.Sprintf("Would create MR in %s: %s -> %s", project.PathWithNamespace, sourceBranch, targetBranch))
	return &MergeRequest{
		Title:        title,
		Description:  description,
		SourceBranch: sourceBranch,
		TargetBranch: targetBranch,
	}, nil
}
</file>
<file path="internal/handlers/ai_rules.go">
package handlers

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/agents"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// AIRulesHandler handles the generate ai-rules command
type AIRulesHandler struct {
	*BaseHandler
	config config.AIRulesConfig
}

// NewAIRulesHandler creates a new AI rules handler
func NewAIRulesHandler(cfg config.AIRulesConfig, logger *logging.Logger) *AIRulesHandler {
	return &AIRulesHandler{
		BaseHandler: &BaseHandler{
			Config: cfg.BaseConfig,
			Logger: logger,
		},
		config: cfg,
	}
}

// Handle generates AI rules files
func (h *AIRulesHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting AI rules generation",
		logging.String("repo_path", h.config.RepoPath),
	)

	// Load prompts with override support
	// System prompts: try "./prompts" first, fallback to repo-relative path
	systemPromptsDir := "./prompts"
	if _, err := prompts.NewManager(systemPromptsDir); err != nil {
		// Try relative to repo path
		systemPromptsDir = fmt.Sprintf("%s/../gendocs/prompts", h.config.RepoPath)
	}

	// Project prompts: .ai/prompts/ in the repository
	projectPromptsDir := fmt.Sprintf("%s/.ai/prompts", h.config.RepoPath)

	// Load with override support
	promptManager, err := prompts.NewManagerWithOverrides(systemPromptsDir, projectPromptsDir)
	if err != nil {
		return errors.NewConfigurationError(fmt.Sprintf("failed to load prompts: %v", err))
	}

	// Create AI rules generator agent
	aiRulesAgent := agents.NewAIRulesGeneratorAgent(h.config, promptManager, h.Logger)

	// Run generation
	if err := aiRulesAgent.Run(ctx); err != nil {
		return errors.NewDocumentationError("AI rules", err)
	}

	h.Logger.Info("AI rules files generated successfully")
	return nil
}
</file>
<file path="internal/handlers/analyze.go">
package handlers

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/agents"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

type AnalyzeHandler struct {
	*BaseHandler
	config   config.AnalyzerConfig
	progress agents.ProgressReporter
}

func NewAnalyzeHandler(cfg config.AnalyzerConfig, logger *logging.Logger) *AnalyzeHandler {
	return &AnalyzeHandler{
		BaseHandler: &BaseHandler{
			Config: cfg.BaseConfig,
			Logger: logger,
		},
		config: cfg,
	}
}

func (h *AnalyzeHandler) SetProgressReporter(p agents.ProgressReporter) {
	h.progress = p
}

// Handle executes the analysis
func (h *AnalyzeHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting analyze handler",
		logging.String("repo_path", h.config.RepoPath),
	)

	// Load prompts with override support
	// System prompts: try "./prompts" first, fallback to repo-relative path
	systemPromptsDir := "./prompts"
	if _, err := prompts.NewManager(systemPromptsDir); err != nil {
		// Try relative to repo path
		systemPromptsDir = fmt.Sprintf("%s/../gendocs/prompts", h.config.RepoPath)
	}

	// Project prompts: .ai/prompts/ in the repository
	projectPromptsDir := fmt.Sprintf("%s/.ai/prompts", h.config.RepoPath)

	// Load with override support
	promptManager, err := prompts.NewManagerWithOverrides(systemPromptsDir, projectPromptsDir)
	if err != nil {
		return errors.NewConfigurationError(fmt.Sprintf("failed to load prompts: %v", err))
	}

	analyzerAgent := agents.NewAnalyzerAgent(h.config, promptManager, h.Logger)
	if h.progress != nil {
		analyzerAgent.SetProgressReporter(h.progress)
	}

	result, err := analyzerAgent.Run(ctx)
	if err != nil {
		return errors.NewAnalysisError("analysis execution failed", err)
	}

	// Log results
	h.Logger.Info(fmt.Sprintf("Analysis complete: %d/%d successful",
		len(result.Successful), len(result.Successful)+len(result.Failed)))

	// Determine exit code
	if len(result.Failed) > 0 && len(result.Successful) == 0 {
		return errors.NewAnalysisError("all analyses failed", fmt.Errorf("no successful analyses"))
	}

	if len(result.Failed) > 0 {
		h.Logger.Warn(fmt.Sprintf("Partial success: %d analyses failed", len(result.Failed)))
		for _, failed := range result.Failed {
			h.Logger.Error(fmt.Sprintf("  - %s: %v", failed.Name, failed.Error))
		}
	}

	return nil
}
</file>
<file path="internal/handlers/base.go">
package handlers

import (
	"context"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/logging"
)

// Handler is the interface that all handlers must implement
type Handler interface {
	// Handle executes the handler logic
	Handle(ctx context.Context) error
}

// BaseHandler provides common functionality for all handlers
type BaseHandler struct {
	Config config.BaseConfig
	Logger *logging.Logger
}

// NewBaseHandler creates a new base handler
func NewBaseHandler(cfg config.BaseConfig, logger *logging.Logger) *BaseHandler {
	return &BaseHandler{
		Config: cfg,
		Logger: logger,
	}
}
</file>
<file path="internal/handlers/cronjob.go">
package handlers

import (
	"context"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"time"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/gitlab"
	"github.com/user/gendocs/internal/logging"
)

// CronjobHandler handles the cronjob analyze command
type CronjobHandler struct {
	*BaseHandler
	config      config.CronjobConfig
	gitLabCfg   config.GitLabConfig
	analyzerCfg config.AnalyzerConfig
	gitlab      *gitlab.Client
}

// NewCronjobHandler creates a new cronjob handler
func NewCronjobHandler(
	cronjobCfg config.CronjobConfig,
	gitLabCfg config.GitLabConfig,
	analyzerCfg config.AnalyzerConfig,
	logger *logging.Logger,
) *CronjobHandler {
	return &CronjobHandler{
		BaseHandler: &BaseHandler{
			Config: config.BaseConfig{
				RepoPath: cronjobCfg.WorkingPath,
				Debug:    false,
			},
			Logger: logger,
		},
		config:      cronjobCfg,
		gitLabCfg:   gitLabCfg,
		analyzerCfg: analyzerCfg,
		gitlab:      gitlab.NewClient(gitLabCfg, logger),
	}
}

// ProcessedResult holds the results of processing projects
type ProcessedResult struct {
	ProcessedCount int
	SuccessCount   int
	ErrorCount     int
	SkippedCount   int
	FailedProjects []FailedProject
}

// FailedProject represents a project that failed to process
type FailedProject struct {
	Name  string
	Error error
}

// Handle executes the cronjob analysis
func (h *CronjobHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting cronjob analysis",
		logging.String("working_path", h.config.WorkingPath),
		logging.Int("group_project_id", h.config.GroupProjectID),
		logging.Int("max_days", h.config.MaxDaysSinceLastCommit),
	)

	// Fetch all projects in the group
	projects, err := h.gitlab.FetchProjectsInGroup(ctx, h.config.GroupProjectID)
	if err != nil {
		return errors.NewCronjobError("failed to fetch projects", err)
	}

	h.Logger.Info(fmt.Sprintf("Found %d projects in group", len(projects)))

	// Filter projects
	filter := gitlab.ProjectFilter{
		MaxDaysSinceLastCommit: h.config.MaxDaysSinceLastCommit,
	}

	var applicableProjects []gitlab.Project
	for _, project := range projects {
		shouldAnalyze, err := h.gitlab.ShouldAnalyze(ctx, project, filter)
		if err != nil {
			h.Logger.Warn(fmt.Sprintf("Error checking project %s: %v", project.PathWithNamespace, err))
			continue
		}
		if shouldAnalyze {
			applicableProjects = append(applicableProjects, project)
		}
	}

	h.Logger.Info(fmt.Sprintf("%d projects applicable for analysis", len(applicableProjects)))

	// Process each applicable project
	result := &ProcessedResult{
		FailedProjects: []FailedProject{},
	}

	for _, project := range applicableProjects {
		h.Logger.Info(fmt.Sprintf("Processing %s", project.PathWithNamespace))

		if err := h.processProject(ctx, project); err != nil {
			result.ErrorCount++
			result.FailedProjects = append(result.FailedProjects, FailedProject{
				Name:  project.PathWithNamespace,
				Error: err,
			})
			h.Logger.Error(fmt.Sprintf("Failed to process %s: %v", project.PathWithNamespace, err))
		} else {
			result.SuccessCount++
		}
		result.ProcessedCount++
	}

	// Log summary
	h.Logger.Info(fmt.Sprintf("Cronjob complete: %d processed, %d succeeded, %d failed, %d skipped",
		result.ProcessedCount, result.SuccessCount, result.ErrorCount,
		len(projects)-len(applicableProjects)))

	if result.ErrorCount > 0 && result.SuccessCount == 0 {
		return errors.NewCronjobError("all projects failed", fmt.Errorf("%d failures", result.ErrorCount))
	}

	return nil
}

// processProject processes a single project
func (h *CronjobHandler) processProject(ctx context.Context, project gitlab.Project) error {
	// Create temp directory for cloning
	tempDir := filepath.Join(h.config.WorkingPath, "tmp", fmt.Sprintf("project_%d", project.ID))
	if err := os.MkdirAll(tempDir, 0755); err != nil {
		return fmt.Errorf("failed to create temp dir: %w", err)
	}
	defer func() { _ = os.RemoveAll(tempDir) }()

	// Clone repository
	if err := h.cloneRepository(ctx, project, tempDir); err != nil {
		return errors.NewGitCloneError(project.HTTPURL, "clone failed", err)
	}

	// Create branch
	branchName := fmt.Sprintf("ai-analyzer-%s", time.Now().Format("2006-01-02"))
	if err := h.gitlab.CreateBranch(ctx, project, branchName, project.DefaultBranch); err != nil {
		return fmt.Errorf("failed to create branch: %w", err)
	}

	// Run analysis
	analyzerCfg := h.analyzerCfg
	analyzerCfg.RepoPath = tempDir

	// Run analyze command (via subprocess for now, could be refactored to use handler directly)
	if err := h.runAnalysis(ctx, tempDir); err != nil {
		return fmt.Errorf("analysis failed: %w", err)
	}

	// Commit results
	commitMsg := fmt.Sprintf("[skip ci] AI analysis: %s", time.Now().Format("2006-01-02"))
	if err := h.gitlab.CreateCommit(ctx, project, branchName, commitMsg, nil); err != nil {
		return fmt.Errorf("failed to create commit: %w", err)
	}

	// Create merge request
	mrTitle := fmt.Sprintf("AI Analysis: %s", time.Now().Format("2006-01-02"))
	mrDescription := fmt.Sprintf("Automated AI analysis generated on %s\n\nThis MR contains:\n- Structure analysis\n- Dependency analysis\n- Data flow analysis\n- Request flow analysis\n- API analysis", time.Now().Format("2006-01-02"))
	mr, err := h.gitlab.CreateMR(ctx, project, branchName, project.DefaultBranch, mrTitle, mrDescription)
	if err != nil {
		return fmt.Errorf("failed to create MR: %w", err)
	}

	h.Logger.Info(fmt.Sprintf("Created MR %d for %s", mr.IID, project.PathWithNamespace))
	return nil
}

// cloneRepository clones a GitLab repository
func (h *CronjobHandler) cloneRepository(ctx context.Context, project gitlab.Project, destDir string) error {
	// Clone with authentication
	url := project.HTTPURL
	if h.gitlab.OAuthToken != "" {
		// Inject token into URL
		url = fmt.Sprintf("https://oauth2:%s@%s", h.gitlab.OAuthToken, project.HTTPURL[8:]) // Strip https://
	}

	cmd := exec.CommandContext(ctx, "git", "clone", "--depth", "1", url, destDir)
	output, err := cmd.CombinedOutput()
	if err != nil {
		h.Logger.Info(fmt.Sprintf("Git clone output: %s", string(output)))
		return err
	}

	return nil
}

// runAnalysis runs the analysis on a repository
func (h *CronjobHandler) runAnalysis(ctx context.Context, repoPath string) error {
	// Run gendocs analyze command as subprocess
	cmd := exec.CommandContext(ctx, "./gendocs", "analyze", "--repo-path", repoPath)
	cmd.Dir = filepath.Dir(repoPath) // Run from parent directory to find binary

	output, err := cmd.CombinedOutput()
	if err != nil {
		h.Logger.Info(fmt.Sprintf("Analysis output: %s", string(output)))
		return err
	}

	h.Logger.Info(fmt.Sprintf("Analysis output: %s", string(output)))
	return nil
}
</file>
<file path="internal/handlers/readme.go">
package handlers

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/agents"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// ReadmeHandler handles the generate readme command
type ReadmeHandler struct {
	*BaseHandler
	config config.DocumenterConfig
}

// NewReadmeHandler creates a new readme handler
func NewReadmeHandler(cfg config.DocumenterConfig, logger *logging.Logger) *ReadmeHandler {
	return &ReadmeHandler{
		BaseHandler: &BaseHandler{
			Config: cfg.BaseConfig,
			Logger: logger,
		},
		config: cfg,
	}
}

// Handle generates the README
func (h *ReadmeHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting readme generation",
		logging.String("repo_path", h.config.RepoPath),
	)

	// Load prompts with override support
	// System prompts: try "./prompts" first, fallback to repo-relative path
	systemPromptsDir := "./prompts"
	if _, err := prompts.NewManager(systemPromptsDir); err != nil {
		// Try relative to repo path
		systemPromptsDir = fmt.Sprintf("%s/../gendocs/prompts", h.config.RepoPath)
	}

	// Project prompts: .ai/prompts/ in the repository
	projectPromptsDir := fmt.Sprintf("%s/.ai/prompts", h.config.RepoPath)

	// Load with override support
	promptManager, err := prompts.NewManagerWithOverrides(systemPromptsDir, projectPromptsDir)
	if err != nil {
		return errors.NewConfigurationError(fmt.Sprintf("failed to load prompts: %v", err))
	}

	// Create documenter agent
	documenterAgent := agents.NewDocumenterAgent(h.config, promptManager, h.Logger)

	// Run generation
	if err := documenterAgent.Run(ctx); err != nil {
		return errors.NewDocumentationError("README", err)
	}

	h.Logger.Info("README.md generated successfully")
	return nil
}
</file>
<file path="internal/llm/anthropic.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/user/gendocs/internal/config"
)

// AnthropicClient implements LLMClient for Anthropic Claude
type AnthropicClient struct {
	*BaseLLMClient
	apiKey  string
	model   string
	baseURL string
}

// anthropicRequest represents the request body for Anthropic API
type anthropicRequest struct {
	Model       string             `json:"model"`
	Messages    []anthropicMessage `json:"messages"`
	System      string             `json:"system,omitempty"`
	MaxTokens   int                `json:"max_tokens"`
	Temperature float64            `json:"temperature,omitempty"`
	Tools       []anthropicTool    `json:"tools,omitempty"`
	Stream      bool               `json:"stream,omitempty"`
}

// anthropicMessage represents a message in Anthropic format
type anthropicMessage struct {
	Role    string                  `json:"role"`
	Content []anthropicContentBlock `json:"content"`
}

// anthropicContentBlock represents a content block
type anthropicContentBlock struct {
	Type string `json:"type"`
	Text string `json:"text,omitempty"`
	// Tool use fields (flat when type=="tool_use")
	ID    string                 `json:"id,omitempty"`
	Name  string                 `json:"name,omitempty"`
	Input map[string]interface{} `json:"input,omitempty"`
	// Tool result fields (flat when type=="tool_result")
	ToolUseID string `json:"tool_use_id,omitempty"`
	Content   string `json:"content,omitempty"` // Can be string for tool results
}

// anthropicTool represents a tool definition
type anthropicTool struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	InputSchema map[string]interface{} `json:"input_schema"`
}

// anthropicResponse represents the response from Anthropic API
type anthropicResponse struct {
	ID         string                  `json:"id"`
	Type       string                  `json:"type"`
	Role       string                  `json:"role"`
	Content    []anthropicContentBlock `json:"content"`
	StopReason string                  `json:"stop_reason"`
	Usage      anthropicUsage          `json:"usage"`
	Error      *anthropicError         `json:"error,omitempty"`
}

// anthropicUsage represents token usage
type anthropicUsage struct {
	InputTokens  int `json:"input_tokens"`
	OutputTokens int `json:"output_tokens"`
}

// anthropicError represents an error from Anthropic
type anthropicError struct {
	Type    string `json:"type"`
	Message string `json:"message"`
}

// NewAnthropicClient creates a new Anthropic client
func NewAnthropicClient(cfg config.LLMConfig, retryClient *RetryClient) *AnthropicClient {
	baseURL := cfg.BaseURL
	if baseURL == "" {
		baseURL = "https://api.anthropic.com"
	}
	return &AnthropicClient{
		BaseLLMClient: NewBaseLLMClient(retryClient),
		apiKey:        cfg.APIKey,
		model:         cfg.Model,
		baseURL:       baseURL,
	}
}

// GenerateCompletion generates a completion from Anthropic
func (c *AnthropicClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	anReq := c.convertRequest(req)

	jsonData, err := json.Marshal(anReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := c.baseURL + "/v1/messages"

	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonData))
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("x-api-key", c.apiKey)
	httpReq.Header.Set("anthropic-version", "2023-06-01")

	resp, err := c.retryClient.Do(httpReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("request failed: %w", err)
	}
	defer func() { _ = resp.Body.Close() }()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		var anResp anthropicResponse
		if err := json.Unmarshal(body, &anResp); err == nil && anResp.Error != nil {
			return CompletionResponse{}, fmt.Errorf("API error: %s", anResp.Error.Message)
		}
		return CompletionResponse{}, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))
	}

	return c.parseStreamingResponse(resp.Body)
}

// parseStreamingResponse parses Anthropic's SSE stream and builds the response
func (c *AnthropicClient) parseStreamingResponse(body io.ReadCloser) (CompletionResponse, error) {
	parser := NewSSEParser(body)
	accumulator := newAnthropicAccumulator()

	for {
		event, err := parser.NextEvent()
		if err == io.EOF {
			break
		}
		if err != nil {
			return CompletionResponse{}, fmt.Errorf("stream parsing error: %w", err)
		}

		if err := accumulator.HandleEvent(event); err != nil {
			return CompletionResponse{}, fmt.Errorf("event handling error: %w", err)
		}

		if accumulator.IsComplete() {
			break
		}
	}

	if !accumulator.IsComplete() {
		return CompletionResponse{}, fmt.Errorf("stream ended unexpectedly")
	}

	return accumulator.Build(), nil
}

// anthropicAccumulator builds CompletionResponse from Anthropic streaming events
type anthropicAccumulator struct {
	content         strings.Builder
	toolCalls       []ToolCall
	currentTool     *ToolCall
	toolArgsBuilder strings.Builder
	usage           TokenUsage
	stopReason      string
	complete        bool
}

func newAnthropicAccumulator() *anthropicAccumulator {
	return &anthropicAccumulator{}
}

func (a *anthropicAccumulator) HandleEvent(event SSEEvent) error {
	var data map[string]interface{}
	if err := json.Unmarshal(event.Data, &data); err != nil {
		return fmt.Errorf("failed to parse event data: %w", err)
	}

	eventType, _ := data["type"].(string)
	switch eventType {
	case "message_start":
		if msg, ok := data["message"].(map[string]interface{}); ok {
			if usage, ok := msg["usage"].(map[string]interface{}); ok {
				if input, ok := usage["input_tokens"].(float64); ok {
					a.usage.InputTokens = int(input)
				}
				if output, ok := usage["output_tokens"].(float64); ok {
					a.usage.OutputTokens = int(output)
				}
			}
		}
	case "content_block_start":
		if block, ok := data["content_block"].(map[string]interface{}); ok {
			blockType, _ := block["type"].(string)
			if blockType == "tool_use" {
				name, _ := block["name"].(string)

				a.currentTool = &ToolCall{
					Name: name,
				}
				a.toolArgsBuilder.Reset()
			}
		}
	case "content_block_delta":
		if delta, ok := data["delta"].(map[string]interface{}); ok {
			switch deltaType, _ := delta["type"].(string); deltaType {
			case "text_delta":
				if text, ok := delta["text"].(string); ok {
					a.content.WriteString(text)
				}
			case "input_json_delta":
				if partial, ok := delta["partial_json"].(string); ok && a.currentTool != nil {
					a.toolArgsBuilder.WriteString(partial)
				}
			}
		}
	case "content_block_stop":
		if a.currentTool != nil {
			if a.toolArgsBuilder.Len() > 0 {
				var args map[string]interface{}
				if err := json.Unmarshal([]byte(a.toolArgsBuilder.String()), &args); err != nil {
					return fmt.Errorf("failed to parse tool arguments: %w", err)
				}
				a.currentTool.Arguments = args
			}
			a.toolCalls = append(a.toolCalls, *a.currentTool)
			a.currentTool = nil
			a.toolArgsBuilder.Reset()
		}
	case "message_delta":
		if usage, ok := data["usage"].(map[string]interface{}); ok {
			if output, ok := usage["output_tokens"].(float64); ok {
				a.usage.OutputTokens = int(output)
			}
		}
		if delta, ok := data["delta"].(map[string]interface{}); ok {
			if stopReason, ok := delta["stop_reason"].(string); ok {
				a.stopReason = stopReason
			}
		}
	case "message_stop":
		a.complete = true
	case "error":
		if errMap, ok := data["error"].(map[string]interface{}); ok {
			msg, _ := errMap["message"].(string)
			return fmt.Errorf("API error: %s", msg)
		}
	}
	return nil
}

func (a *anthropicAccumulator) IsComplete() bool {
	return a.complete
}

func (a *anthropicAccumulator) Build() CompletionResponse {
	a.usage.TotalTokens = a.usage.InputTokens + a.usage.OutputTokens
	return CompletionResponse{
		Content:   a.content.String(),
		ToolCalls: a.toolCalls,
		Usage:     a.usage,
	}
}

// SupportsTools returns true
func (c *AnthropicClient) SupportsTools() bool {
	return true
}

// GetProvider returns the provider name
func (c *AnthropicClient) GetProvider() string {
	return "anthropic"
}

// convertRequest converts internal request to Anthropic format
func (c *AnthropicClient) convertRequest(req CompletionRequest) anthropicRequest {
	messages := []anthropicMessage{}

	for _, msg := range req.Messages {
		switch msg.Role {
		case "tool":
			messages = append(messages, anthropicMessage{
				Role: "user",
				Content: []anthropicContentBlock{
					{
						Type:      "tool_result",
						ToolUseID: msg.ToolID,
						Content:   msg.Content,
					},
				},
			})
		case "assistant":
			var contentBlocks []anthropicContentBlock

			if msg.Content != "" {
				contentBlocks = append(contentBlocks, anthropicContentBlock{
					Type: "text",
					Text: msg.Content,
				})
			}

			for _, tc := range msg.ToolCalls {
				contentBlocks = append(contentBlocks, anthropicContentBlock{
					Type:  "tool_use",
					ID:    tc.Name,
					Name:  tc.Name,
					Input: tc.Arguments,
				})
			}

			if len(contentBlocks) > 0 {
				messages = append(messages, anthropicMessage{
					Role:    "assistant",
					Content: contentBlocks,
				})
			}
		case "user":
			if msg.Content != "" {
				messages = append(messages, anthropicMessage{
					Role: "user",
					Content: []anthropicContentBlock{
						{Type: "text", Text: msg.Content},
					},
				})
			}
		}
	}

	if len(messages) == 0 {
		messages = append(messages, anthropicMessage{
			Role: "user",
			Content: []anthropicContentBlock{
				{Type: "text", Text: "Analyze this codebase."},
			},
		})
	}

	var tools []anthropicTool
	if len(req.Tools) > 0 {
		tools = make([]anthropicTool, len(req.Tools))
		for i, tool := range req.Tools {
			tools[i] = anthropicTool{
				Name:        tool.Name,
				Description: tool.Description,
				InputSchema: tool.Parameters,
			}
		}
	}

	return anthropicRequest{
		Model:       c.model,
		Messages:    messages,
		System:      req.SystemPrompt,
		MaxTokens:   req.MaxTokens,
		Temperature: req.Temperature,
		Tools:       tools,
		Stream:      true,
	}
}
</file>
<file path="internal/llm/anthropic_bench_test.go">
package llm

import (
	"context"
	"fmt"
	"net/http"
	"net/http/httptest"
	"testing"
	"time"

	"github.com/user/gendocs/internal/config"
)

// BenchmarkAnthropic_SmallResponse benchmarks a small single-chunk response
func BenchmarkAnthropic_SmallResponse(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Small response - single chunk
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_start","message":{"id":"msg_123","type":"message","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":10,"output_tokens":0}}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_delta")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"Hello!"}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_stop","index":0}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_delta","delta":{"stop_reason":"end_turn","stop_sequence":null},"usage":{"output_tokens":2}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_stop"}`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkAnthropic_MediumResponse benchmarks a medium multi-chunk response
func BenchmarkAnthropic_MediumResponse(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Medium response - multiple chunks
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_start","message":{"id":"msg_123","type":"message","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":15,"output_tokens":0}}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}`)
		_, _ = fmt.Fprintln(w)
		for i := 0; i < 10; i++ {
			_, _ = fmt.Fprintln(w, "event: content_block_delta")
			_, _ = fmt.Fprintln(w, `data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"This is chunk `+fmt.Sprint(i)+` of the response. "}}`)
			_, _ = fmt.Fprintln(w)
		}
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_stop","index":0}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_delta","delta":{"stop_reason":"end_turn","stop_sequence":null},"usage":{"output_tokens":50}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_stop"}`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Tell me a story"},
		},
		MaxTokens:   1000,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkAnthropic_LargeResponse benchmarks a large response with many chunks
func BenchmarkAnthropic_LargeResponse(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Large response - many chunks (simulating ~50KB response)
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_start","message":{"id":"msg_123","type":"message","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":20,"output_tokens":0}}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}`)
		_, _ = fmt.Fprintln(w)
		for i := 0; i < 100; i++ {
			_, _ = fmt.Fprintln(w, "event: content_block_delta")
			_, _ = fmt.Fprintln(w, `data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"This is a longer chunk of text that represents a substantial part of the response. Chunk `+fmt.Sprint(i)+` contains useful information. "}}`)
			_, _ = fmt.Fprintln(w)
		}
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_stop","index":0}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_delta","delta":{"stop_reason":"end_turn","stop_sequence":null},"usage":{"output_tokens":500}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_stop"}`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Analyze this codebase in detail"},
		},
		MaxTokens:   4000,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkAnthropic_ToolCall benchmarks a response with a tool call
func BenchmarkAnthropic_ToolCall(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Tool call response
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_start","message":{"id":"msg_123","type":"message","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":25,"output_tokens":0}}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_start","index":0,"content_block":{"type":"tool_use","id":"toolu_123","name":"read_file","input":null}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_delta")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_delta","index":0,"delta":{"type":"input_json_delta","partial_json":"{\"file_path\":\"src/main.go\",\"start_line\":1,\"end_line\":100}"}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_stop","index":0}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_delta","delta":{"stop_reason":"tool_use","stop_sequence":null},"usage":{"output_tokens":25}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_stop"}`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Read the main.go file"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "read_file",
				Description: "Read a file",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"file_path": map[string]interface{}{
							"type":        "string",
							"description": "Path to the file",
						},
					},
					"required": []string{"file_path"},
				},
			},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkAnthropic_TimeToFirstToken measures time to receive first content
func BenchmarkAnthropic_TimeToFirstToken(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Simulate network delay before first chunk
		time.Sleep(10 * time.Millisecond)
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_start","message":{"id":"msg_123","type":"message","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":10,"output_tokens":0}}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_delta")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"Response"}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"content_block_stop","index":0}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_delta","delta":{"stop_reason":"end_turn","stop_sequence":null},"usage":{"output_tokens":5}}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, `data: {"type":"message_stop"}`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()

	for i := 0; i < b.N; i++ {
		start := time.Now()
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
		elapsed := time.Since(start)
		b.ReportMetric(float64(elapsed.Nanoseconds()), "ns/op")
	}
}
</file>
<file path="internal/llm/anthropic_test.go">
package llm

import (
	"context"
	"fmt"
	"net/http"
	"net/http/httptest"
	"strings"
	"testing"
	"time"

	"github.com/user/gendocs/internal/config"
)

func TestAnthropicClient_GenerateCompletion_Success(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Validate request headers
		if apiKey := r.Header.Get("x-api-key"); apiKey != "test-key" {
			t.Errorf("Expected x-api-key 'test-key', got '%s'", apiKey)
		}

		if version := r.Header.Get("anthropic-version"); version == "" {
			t.Error("Expected anthropic-version header to be set")
		}

		// Send streaming response
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_123\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"model\":\"claude-3-sonnet-20240229\",\"stop_reason\":null,\"usage\":{\"input_tokens\":15,\"output_tokens\":0}}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"test response from claude\"}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_stop\",\"index\":0}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"output_tokens\":8}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_stop\"}")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	// Create client
	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if resp.Content != "test response from claude" {
		t.Errorf("Expected content 'test response from claude', got '%s'", resp.Content)
	}

	if resp.Usage.InputTokens != 15 {
		t.Errorf("Expected 15 input tokens, got %d", resp.Usage.InputTokens)
	}

	if resp.Usage.OutputTokens != 8 {
		t.Errorf("Expected 8 output tokens, got %d", resp.Usage.OutputTokens)
	}
}

func TestAnthropicClient_GenerateCompletion_WithToolCalls(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Send streaming response for tool call
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_123\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"model\":\"claude-3-sonnet-20240229\",\"stop_reason\":null,\"usage\":{\"input_tokens\":20,\"output_tokens\":0}}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"tool_use\",\"id\":\"toolu_123\",\"name\":\"list_files\",\"input\":null}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\"{\\\"path\\\":\\\".\\\"}\"}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_stop\",\"index\":0}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"tool_use\",\"stop_sequence\":null},\"usage\":{\"output_tokens\":12}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_stop\"}")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	// Create client
	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "list files"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "list_files",
				Description: "List files in directory",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"path": map[string]interface{}{
							"type": "string",
						},
					},
				},
			},
		},
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if len(resp.ToolCalls) != 1 {
		t.Fatalf("Expected 1 tool call, got %d", len(resp.ToolCalls))
	}

	if resp.ToolCalls[0].Name != "list_files" {
		t.Errorf("Expected tool call name 'list_files', got '%s'", resp.ToolCalls[0].Name)
	}

	if path, ok := resp.ToolCalls[0].Arguments["path"].(string); !ok || path != "." {
		t.Errorf("Expected path argument '.', got %v", resp.ToolCalls[0].Arguments["path"])
	}
}

func TestAnthropicClient_GenerateCompletion_InvalidAPIKey(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusUnauthorized)
		_, _ = w.Write([]byte(`{"type": "error", "error": {"type": "authentication_error", "message": "Invalid API key"}}`))
	}))
	defer server.Close()

	// Create client
	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "invalid-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	// Execute
	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify
	if err == nil {
		t.Fatal("Expected error for invalid API key, got nil")
	}
}

func TestAnthropicClient_GenerateCompletion_RateLimitRetry(t *testing.T) {
	callCount := 0

	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		callCount++

		// First call returns rate limit error
		if callCount == 1 {
			w.WriteHeader(http.StatusTooManyRequests)
			_, _ = w.Write([]byte(`{"type": "error", "error": {"type": "rate_limit_error", "message": "Rate limit exceeded"}}`))
			return
		}

		// Second call succeeds with streaming response
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_123\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"model\":\"claude-3-sonnet-20240229\",\"stop_reason\":null,\"usage\":{\"input_tokens\":10,\"output_tokens\":0}}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"success after retry\"}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_stop\",\"index\":0}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"output_tokens\":5}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_stop\"}")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	// Create retry client with short delays
	retryClient := NewRetryClient(&RetryConfig{
		MaxAttempts:       2,
		Multiplier:        1,
		MaxWaitPerAttempt: 10 * time.Millisecond,
		MaxTotalWait:      100 * time.Millisecond,
	})

	// Create client
	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, retryClient)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error after retry, got %v", err)
	}

	if resp.Content != "success after retry" {
		t.Errorf("Expected content 'success after retry', got '%s'", resp.Content)
	}

	if callCount != 2 {
		t.Errorf("Expected 2 calls (1 fail + 1 success), got %d", callCount)
	}
}

func TestAnthropicClient_SupportsTools(t *testing.T) {
	client := NewAnthropicClient(config.LLMConfig{
		APIKey: "test-key",
		Model:  "claude-3-sonnet-20240229",
	}, nil)

	if !client.SupportsTools() {
		t.Error("Anthropic client should support tools")
	}
}

func TestAnthropicClient_GetProvider(t *testing.T) {
	client := NewAnthropicClient(config.LLMConfig{
		APIKey: "test-key",
		Model:  "claude-3-sonnet-20240229",
	}, nil)

	if provider := client.GetProvider(); provider != "anthropic" {
		t.Errorf("Expected provider 'anthropic', got '%s'", provider)
	}
}

func TestAnthropicClient_GenerateCompletion_MixedContentTypes(t *testing.T) {
	// Test response with both text and tool_use content blocks
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Send streaming response with mixed content
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_123\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"model\":\"claude-3-sonnet-20240229\",\"stop_reason\":null,\"usage\":{\"input_tokens\":25,\"output_tokens\":0}}}")
		_, _ = fmt.Fprintln(w)
		// First content block: text
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"I'll read the file for you.\"}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_stop\",\"index\":0}")
		_, _ = fmt.Fprintln(w)
		// Second content block: tool_use
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_start\",\"index\":1,\"content_block\":{\"type\":\"tool_use\",\"id\":\"toolu_123\",\"name\":\"read_file\",\"input\":null}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\"{\\\"file_path\\\":\\\"main.go\\\"}\"}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_stop\",\"index\":1}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"tool_use\",\"stop_sequence\":null},\"usage\":{\"output_tokens\":15}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_stop\"}")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	// Create client
	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "read main.go"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "read_file",
				Description: "Read a file",
				Parameters:  map[string]interface{}{},
			},
		},
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Should extract text content
	if resp.Content != "I'll read the file for you." {
		t.Errorf("Expected text content, got '%s'", resp.Content)
	}

	// Should also extract tool calls
	if len(resp.ToolCalls) != 1 {
		t.Fatalf("Expected 1 tool call, got %d", len(resp.ToolCalls))
	}

	if resp.ToolCalls[0].Name != "read_file" {
		t.Errorf("Expected tool call name 'read_file', got '%s'", resp.ToolCalls[0].Name)
	}
}

func TestAnthropicClient_Streaming_MultipleChunks(t *testing.T) {
	// Test large response split across multiple chunks
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_123\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"model\":\"claude-3-sonnet-20240229\",\"stop_reason\":null,\"usage\":{\"input_tokens\":10,\"output_tokens\":0}}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}}")
		_, _ = fmt.Fprintln(w)

		// Send multiple text chunks
		chunks := []string{"This is ", "a large ", "response ", "split across ", "multiple chunks."}
		for _, chunk := range chunks {
			_, _ = fmt.Fprintln(w, "event: content_block_delta")
			_, _ = fmt.Fprintf(w, "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"%s\"}}\n", chunk)
			_, _ = fmt.Fprintln(w)
		}

		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_stop\",\"index\":0}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"output_tokens\":10}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_stop\"}")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "generate a large response"},
		},
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	expected := "This is a large response split across multiple chunks."
	if resp.Content != expected {
		t.Errorf("Expected content '%s', got '%s'", expected, resp.Content)
	}
}

func TestAnthropicClient_Streaming_IncompleteStream(t *testing.T) {
	// Test incomplete stream (missing message_stop event)
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_123\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"model\":\"claude-3-sonnet-20240229\",\"stop_reason\":null,\"usage\":{\"input_tokens\":10,\"output_tokens\":0}}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}}")
		_, _ = fmt.Fprintln(w)
		// Stream ends here without message_stop
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Should return an error due to incomplete stream
	if err == nil {
		t.Fatal("Expected error for incomplete stream, got nil")
	}
}

func TestAnthropicClient_Streaming_APIError(t *testing.T) {
	// Test API error event in stream
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_123\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"model\":\"claude-3-sonnet-20240229\",\"stop_reason\":null,\"usage\":{\"input_tokens\":10,\"output_tokens\":0}}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: error")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"error\",\"error\":{\"type\":\"content_filter\",\"message\":\"Content was filtered\"}}")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	if err == nil {
		t.Fatal("Expected error for API error event, got nil")
	}

	if !strings.Contains(err.Error(), "Content was filtered") {
		t.Errorf("Expected error message to contain 'Content was filtered', got '%s'", err.Error())
	}
}

func TestAnthropicClient_Streaming_MalformedChunk(t *testing.T) {
	// Test malformed JSON in chunk
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_123\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"model\":\"claude-3-sonnet-20240229\",\"stop_reason\":null,\"usage\":{\"input_tokens\":10,\"output_tokens\":0}}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {invalid json here}")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	if err == nil {
		t.Fatal("Expected error for malformed chunk, got nil")
	}
}

func TestAnthropicClient_Streaming_LargeToolArguments(t *testing.T) {
	// Test tool call with large arguments split across chunks
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "event: message_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_123\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"model\":\"claude-3-sonnet-20240229\",\"stop_reason\":null,\"usage\":{\"input_tokens\":10,\"output_tokens\":0}}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: content_block_start")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"tool_use\",\"id\":\"toolu_123\",\"name\":\"search\",\"input\":null}}")
		_, _ = fmt.Fprintln(w)

		// Send partial JSON chunks
		chunks := []string{
			"{\"query\":",
			"\"large search request ",
			"with multiple parameters",
			" split across chunks\"}",
		}
		for _, chunk := range chunks {
			escapedChunk := strings.ReplaceAll(chunk, "\"", "\\\"")
			_, _ = fmt.Fprintln(w, "event: content_block_delta")
			_, _ = fmt.Fprintf(w, "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\"%s\"}}\n", escapedChunk)
			_, _ = fmt.Fprintln(w)
		}

		_, _ = fmt.Fprintln(w, "event: content_block_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"content_block_stop\",\"index\":0}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_delta")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"tool_use\",\"stop_sequence\":null},\"usage\":{\"output_tokens\":15}}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "event: message_stop")
		_, _ = fmt.Fprintln(w, "data: {\"type\":\"message_stop\"}")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "search"},
		},
		Tools: []ToolDefinition{
			{Name: "search", Description: "Search", Parameters: map[string]interface{}{}},
		},
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if len(resp.ToolCalls) != 1 {
		t.Fatalf("Expected 1 tool call, got %d", len(resp.ToolCalls))
	}

	if resp.ToolCalls[0].Name != "search" {
		t.Errorf("Expected tool name 'search', got '%s'", resp.ToolCalls[0].Name)
	}

	expectedQuery := "large search request with multiple parameters split across chunks"
	if query, ok := resp.ToolCalls[0].Arguments["query"].(string); !ok || query != expectedQuery {
		t.Errorf("Expected query '%s', got '%v'", expectedQuery, resp.ToolCalls[0].Arguments["query"])
	}
}
</file>
<file path="internal/llm/cached_client.go">
package llm

import (
	"context"
	"fmt"
	"time"

	"github.com/user/gendocs/internal/llmcache"
)

// CachedLLMClient wraps an LLMClient with caching functionality.
//
// The client implements a two-tier caching strategy:
// 1. Memory cache (LRU): Fast in-memory cache for frequently accessed responses
// 2. Disk cache: Persistent cache across program restarts
//
// Cache hits avoid making API calls entirely, saving both cost and latency.
// When an entry is found in the disk cache, it's promoted to the memory cache.
type CachedLLMClient struct {
	client      LLMClient           // Underlying LLM client
	memoryCache *llmcache.LRUCache  // In-memory LRU cache
	diskCache   *llmcache.DiskCache // Persistent disk cache
	enabled     bool                // Enable/disable caching
	ttl         time.Duration       // Time-to-live for cache entries
}

// NewCachedLLMClient creates a new cached LLM client.
//
// client: The underlying LLM client to wrap
// memoryCache: In-memory LRU cache (can be nil)
// diskCache: Persistent disk cache (can be nil)
// enabled: Whether caching is enabled
// ttl: Time-to-live for cached responses
func NewCachedLLMClient(
	client LLMClient,
	memoryCache *llmcache.LRUCache,
	diskCache *llmcache.DiskCache,
	enabled bool,
	ttl time.Duration,
) *CachedLLMClient {
	return &CachedLLMClient{
		client:      client,
		memoryCache: memoryCache,
		diskCache:   diskCache,
		enabled:     enabled,
		ttl:         ttl,
	}
}

// GenerateCompletion implements LLMClient interface with caching.
//
// The caching strategy is:
// 1. If caching is disabled, delegate directly to the underlying client
// 2. Check memory cache for a hit
// 3. Check disk cache for a hit (promote to memory cache if found)
// 4. Call underlying client and cache the successful response
//
// Cache key generation failures are handled gracefully by bypassing the cache.
// API errors are not cached.
func (c *CachedLLMClient) GenerateCompletion(
	ctx context.Context,
	req CompletionRequest,
) (CompletionResponse, error) {
	if err := ctx.Err(); err != nil {
		return CompletionResponse{}, err
	}

	// If caching disabled, delegate directly
	if !c.enabled {
		return c.client.GenerateCompletion(ctx, req)
	}

	// 1. Generate cache key from request
	cacheKey, err := llmcache.GenerateCacheKey(req)
	if err != nil {
		// Key generation failed, bypass cache gracefully
		return c.client.GenerateCompletion(ctx, req)
	}

	if c.memoryCache != nil {
		if cached, found := c.memoryCache.Get(cacheKey); found {
			return cached.Response, nil
		}
	}

	if c.diskCache != nil {
		if cached, found := c.diskCache.Get(cacheKey); found {
			if c.memoryCache != nil {
				c.memoryCache.Put(cacheKey, cached)
			}
			return cached.Response, nil
		}
	}

	resp, err := c.client.GenerateCompletion(ctx, req)
	if err != nil {
		return CompletionResponse{}, err
	}

	cachedResp := llmcache.NewCachedResponse(cacheKey, llmcache.CacheKeyRequestFrom(req), resp, c.ttl)

	if c.memoryCache != nil {
		c.memoryCache.Put(cacheKey, cachedResp)
	}

	// Store in disk cache (best-effort, non-blocking)
	if c.diskCache != nil {
		// Disk cache write failure is acceptable - just lose persistence benefit
		// TODO: Add logging in subtask 4-2
		_ = c.diskCache.Put(cacheKey, cachedResp)
	}

	return resp, nil
}

// SupportsTools delegates to underlying client.
func (c *CachedLLMClient) SupportsTools() bool {
	return c.client.SupportsTools()
}

// GetProvider returns the underlying provider name with "cached-" prefix.
// This makes it easy to identify when a cached client is being used.
func (c *CachedLLMClient) GetProvider() string {
	return fmt.Sprintf("cached-%s", c.client.GetProvider())
}

// GetStats returns aggregated statistics from both memory and disk cache.
//
// Combines hits, misses, and evictions from both caches.
// Recalculates the hit rate based on the combined data.
func (c *CachedLLMClient) GetStats() llmcache.CacheStats {
	if c.memoryCache == nil {
		return llmcache.CacheStats{}
	}

	// Get memory cache stats
	memStats := c.memoryCache.Stats()

	// Aggregate with disk cache stats if available
	if c.diskCache != nil {
		diskStats := c.diskCache.Stats()

		// Combine statistics
		memStats.Hits += diskStats.Hits
		memStats.Misses += diskStats.Misses
		memStats.Evictions += diskStats.Evictions
		// Recalculate hit rate with combined data
		total := memStats.Hits + memStats.Misses
		if total > 0 {
			memStats.HitRate = float64(memStats.Hits) / float64(total)
		}
	}

	return memStats //nolint:govet // intentional copy of stats
}

// CleanupExpired removes expired entries from both caches.
//
// Returns the number of entries removed from each cache.
// Note: diskExpired is always 0 as the disk cache doesn't report counts.
func (c *CachedLLMClient) CleanupExpired() (memoryExpired, diskExpired int) {
	memoryExpired = 0
	if c.memoryCache != nil {
		memoryExpired = c.memoryCache.CleanupExpired()
	}

	diskExpired = 0
	if c.diskCache != nil {
		// Disk cache cleanup - errors are ignored (non-critical operation)
		// Note: DiskCache.CleanupExpired doesn't return count, so we can't track it
		_ = c.diskCache.CleanupExpired()
	}

	return memoryExpired, diskExpired
}

// Clear clears both memory and disk cache.
// The disk cache is immediately saved to disk after clearing.
func (c *CachedLLMClient) Clear() error {
	if c.memoryCache != nil {
		c.memoryCache.Clear()
	}

	if c.diskCache != nil {
		return c.diskCache.Clear()
	}

	return nil
}

// GetUnderlyingClient returns the underlying unwrapped LLM client.
// This is useful when you need to bypass the caching layer.
func (c *CachedLLMClient) GetUnderlyingClient() LLMClient {
	return c.client
}
</file>
<file path="internal/llm/cached_client_test.go">
package llm

import (
	"context"
	"fmt"
	"net/http"
	"net/http/httptest"
	"os"
	"path/filepath"
	"testing"
	"time"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llmcache"
)

// mockLLMClient is a test double that records calls and returns configurable responses
type mockLLMClient struct {
	callCount   int
	lastRequest CompletionRequest
	response    CompletionResponse
	error       error
	provider    string
}

func (m *mockLLMClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	m.callCount++
	m.lastRequest = req
	if m.error != nil {
		return CompletionResponse{}, m.error
	}
	return m.response, nil
}

func (m *mockLLMClient) SupportsTools() bool {
	return true
}

func (m *mockLLMClient) GetProvider() string {
	return m.provider
}

// TestCachedLLMClient_CacheMiss_CallsUnderlying tests that cache misses call the underlying client
func TestCachedLLMClient_CacheMiss_CallsUnderlying(t *testing.T) {
	// Create mock client
	mockClient := &mockLLMClient{
		response: CompletionResponse{
			Content: "test response",
			Usage: TokenUsage{
				InputTokens:  10,
				OutputTokens: 5,
				TotalTokens:  15,
			},
		},
		provider: "test",
	}

	// Create caches
	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)

	// Create cached client
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	// Execute request
	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test system",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		Temperature: 0.7,
	}

	resp, err := cachedClient.GenerateCompletion(ctx, req)

	// Verify
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if resp.Content != "test response" {
		t.Errorf("Expected content 'test response', got '%s'", resp.Content)
	}

	if mockClient.callCount != 1 {
		t.Errorf("Expected 1 call to underlying client, got %d", mockClient.callCount)
	}

	// Verify cache stats
	stats := cachedClient.GetStats()
	if stats.Misses != 1 {
		t.Errorf("Expected 1 cache miss, got %d", stats.Misses)
	}
}

// TestCachedLLMClient_CacheHit_Memory tests that memory cache hits don't call underlying client
func TestCachedLLMClient_CacheHit_Memory(t *testing.T) {
	// Create mock client
	mockClient := &mockLLMClient{
		response: CompletionResponse{
			Content: "cached response",
			Usage: TokenUsage{
				InputTokens:  10,
				OutputTokens: 5,
				TotalTokens:  15,
			},
		},
		provider: "test",
	}

	// Create caches
	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)

	// Create cached client
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	// Execute same request twice
	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test system",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		Temperature: 0.7,
	}

	// First call - cache miss
	resp1, err1 := cachedClient.GenerateCompletion(ctx, req)
	if err1 != nil {
		t.Fatalf("First call: Expected no error, got %v", err1)
	}

	// Second call - cache hit (memory)
	resp2, err2 := cachedClient.GenerateCompletion(ctx, req)
	if err2 != nil {
		t.Fatalf("Second call: Expected no error, got %v", err2)
	}

	// Verify responses are identical
	if resp1.Content != resp2.Content {
		t.Errorf("Responses should be identical: got '%s' and '%s'", resp1.Content, resp2.Content)
	}

	// Verify underlying client called only once
	if mockClient.callCount != 1 {
		t.Errorf("Expected 1 call to underlying client, got %d", mockClient.callCount)
	}

	// Verify cache stats
	stats := cachedClient.GetStats()
	if stats.Hits != 1 {
		t.Errorf("Expected 1 cache hit, got %d", stats.Hits)
	}
	if stats.Misses != 1 {
		t.Errorf("Expected 1 cache miss, got %d", stats.Misses)
	}
}

// TestCachedLLMClient_CacheHit_DiskPromotedToMemory tests that disk cache hits are promoted to memory
func TestCachedLLMClient_CacheHit_DiskPromotedToMemory(t *testing.T) {
	// Create mock client
	mockClient := &mockLLMClient{
		response: CompletionResponse{
			Content: "disk cached response",
			Usage: TokenUsage{
				InputTokens:  10,
				OutputTokens: 5,
				TotalTokens:  15,
			},
		},
		provider: "test",
	}

	// Create caches with tiny memory cache (size 1)
	tempDir := t.TempDir()
	cachePath := filepath.Join(tempDir, "test-cache.json")
	memoryCache := llmcache.NewLRUCache(1)
	diskCache := llmcache.NewDiskCache(cachePath, llmcache.DefaultTTL, 100*1024*1024)

	// Create first cached client to populate disk cache
	cachedClient1 := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test system",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		Temperature: 0.7,
	}

	// First call - populates both caches
	_, err1 := cachedClient1.GenerateCompletion(ctx, req)
	if err1 != nil {
		t.Fatalf("First call: Expected no error, got %v", err1)
	}

	// Save disk cache
	if err := diskCache.Save(); err != nil {
		t.Fatalf("Failed to save disk cache: %v", err)
	}
	diskCache.Stop()

	// Create new caches (simulating restart)
	memoryCache2 := llmcache.NewLRUCache(10)
	diskCache2 := llmcache.NewDiskCache(cachePath, llmcache.DefaultTTL, 100*1024*1024)
	if err := diskCache2.Load(); err != nil {
		t.Fatalf("Failed to load disk cache: %v", err)
	}

	// Create new cached client with new mock
	mockClient2 := &mockLLMClient{
		response: CompletionResponse{
			Content: "different response",
			Usage: TokenUsage{
				InputTokens:  10,
				OutputTokens: 5,
				TotalTokens:  15,
			},
		},
		provider: "test",
	}

	cachedClient2 := NewCachedLLMClient(mockClient2, memoryCache2, diskCache2, true, time.Hour)

	// Second call - should hit disk cache and promote to memory
	resp2, err2 := cachedClient2.GenerateCompletion(ctx, req)
	if err2 != nil {
		t.Fatalf("Second call: Expected no error, got %v", err2)
	}

	// Verify we got the cached response (from disk), not the new mock response
	if resp2.Content != "disk cached response" {
		t.Errorf("Expected cached response from disk, got '%s'", resp2.Content)
	}

	// Verify new mock client was not called
	if mockClient2.callCount != 0 {
		t.Errorf("Expected 0 calls to new underlying client, got %d", mockClient2.callCount)
	}

	// Verify stats show disk hit
	stats := cachedClient2.GetStats()
	if stats.Hits != 1 {
		t.Errorf("Expected 1 cache hit, got %d", stats.Hits)
	}

	defer diskCache2.Stop()
}

// TestCachedLLMClient_CachingDisabled_BypassesCache tests that caching can be disabled
func TestCachedLLMClient_CachingDisabled_BypassesCache(t *testing.T) {
	// Create mock client
	mockClient := &mockLLMClient{
		response: CompletionResponse{
			Content: "uncached response",
			Usage: TokenUsage{
				InputTokens:  10,
				OutputTokens: 5,
				TotalTokens:  15,
			},
		},
		provider: "test",
	}

	// Create cached client with caching disabled
	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, false, time.Hour)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test system",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		Temperature: 0.7,
	}

	// Execute same request twice
	_, err1 := cachedClient.GenerateCompletion(ctx, req)
	if err1 != nil {
		t.Fatalf("First call: Expected no error, got %v", err1)
	}

	_, err2 := cachedClient.GenerateCompletion(ctx, req)
	if err2 != nil {
		t.Fatalf("Second call: Expected no error, got %v", err2)
	}

	// Verify underlying client was called twice (bypassed cache)
	if mockClient.callCount != 2 {
		t.Errorf("Expected 2 calls to underlying client (cache bypassed), got %d", mockClient.callCount)
	}

	// Verify cache stats show no activity
	stats := cachedClient.GetStats()
	if stats.Hits != 0 || stats.Misses != 0 {
		t.Errorf("Expected no cache activity when disabled, got hits=%d, misses=%d", stats.Hits, stats.Misses)
	}
}

// TestCachedLLMClient_DifferentRequests_DifferentKeys tests that different requests generate different cache entries
func TestCachedLLMClient_DifferentRequests_DifferentKeys(t *testing.T) {
	mockClient := &mockLLMClient{
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	ctx := context.Background()

	// Execute 3 different requests
	requests := []CompletionRequest{
		{
			SystemPrompt: "system 1",
			Messages:     []Message{{Role: "user", Content: "message 1"}},
			Temperature:  0.7,
		},
		{
			SystemPrompt: "system 2", // Different system prompt
			Messages:     []Message{{Role: "user", Content: "message 1"}},
			Temperature:  0.7,
		},
		{
			SystemPrompt: "system 1",
			Messages:     []Message{{Role: "user", Content: "message 2"}}, // Different message
			Temperature:  0.7,
		},
	}

	for _, req := range requests {
		mockClient.response = CompletionResponse{Content: "response"}
		_, err := cachedClient.GenerateCompletion(ctx, req)
		if err != nil {
			t.Fatalf("Expected no error, got %v", err)
		}
	}

	// Verify all 3 requests called the underlying client (no cache hits)
	if mockClient.callCount != 3 {
		t.Errorf("Expected 3 calls to underlying client, got %d", mockClient.callCount)
	}

	stats := cachedClient.GetStats()
	if stats.Misses != 5 {
		t.Errorf("Expected 5 cache misses (1 from mem only + 2*2 from mem+disk), got %d", stats.Misses)
	}
}

// TestCachedLLMClient_APIFailure_NotCached tests that failed API calls are not cached
func TestCachedLLMClient_APIFailure_NotCached(t *testing.T) {
	mockClient := &mockLLMClient{
		error:    &testError{"API error"},
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test",
		Messages:     []Message{{Role: "user", Content: "hello"}},
		Temperature:  0.7,
	}

	// Execute request - should fail
	_, err := cachedClient.GenerateCompletion(ctx, req)
	if err == nil {
		t.Fatal("Expected error, got nil")
	}

	// Verify nothing was cached
	if memoryCache.Size() != 0 {
		t.Errorf("Expected empty memory cache after failed call, got size %d", memoryCache.Size())
	}

	stats := cachedClient.GetStats()
	if stats.Misses != 1 {
		t.Errorf("Expected 1 cache miss for failed call, got %d", stats.Misses)
	}
}

// TestCachedLLMClient_TTLExpiration tests that expired entries are not returned
func TestCachedLLMClient_TTLExpiration(t *testing.T) {
	mockClient := &mockLLMClient{
		response: CompletionResponse{
			Content: "test response",
		},
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)

	// Use very short TTL (1ms)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, 1*time.Millisecond)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test",
		Messages:     []Message{{Role: "user", Content: "hello"}},
		Temperature:  0.7,
	}

	// First call - cache miss
	_, err1 := cachedClient.GenerateCompletion(ctx, req)
	if err1 != nil {
		t.Fatalf("First call: Expected no error, got %v", err1)
	}

	// Wait for TTL to expire
	time.Sleep(10 * time.Millisecond)

	// Second call - should be a cache miss (expired)
	_, err2 := cachedClient.GenerateCompletion(ctx, req)
	if err2 != nil {
		t.Fatalf("Second call: Expected no error, got %v", err2)
	}

	// Verify underlying client was called twice
	if mockClient.callCount != 2 {
		t.Errorf("Expected 2 calls to underlying client (entry expired), got %d", mockClient.callCount)
	}
}

// TestCachedLLMClient_SupportsTools_Delegates tests that SupportsTools delegates to underlying client
func TestCachedLLMClient_SupportsTools_Delegates(t *testing.T) {
	mockClient := &mockLLMClient{
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	if !cachedClient.SupportsTools() {
		t.Error("Expected SupportsTools to return true")
	}
}

// TestCachedLLMClient_GetProvider_ReturnsPrefixedName tests that GetProvider returns prefixed name
func TestCachedLLMClient_GetProvider_ReturnsPrefixedName(t *testing.T) {
	mockClient := &mockLLMClient{
		provider: "openai",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	expectedProvider := "cached-openai"
	if provider := cachedClient.GetProvider(); provider != expectedProvider {
		t.Errorf("Expected provider '%s', got '%s'", expectedProvider, provider)
	}
}

// TestCachedLLMClient_GetStats_AggregatesStats tests that stats aggregate from both caches
func TestCachedLLMClient_GetStats_AggregatesStats(t *testing.T) {
	mockClient := &mockLLMClient{
		response: CompletionResponse{Content: "test"},
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test",
		Messages:     []Message{{Role: "user", Content: "hello"}},
		Temperature:  0.7,
	}

	// Generate some activity
	for i := 0; i < 5; i++ {
		_, _ = cachedClient.GenerateCompletion(ctx, req)
	}

	stats := cachedClient.GetStats()

	// Verify stats are aggregated
	if stats.Hits+stats.Misses != 5 {
		t.Errorf("Expected total lookups to be 5, got %d", stats.Hits+stats.Misses)
	}

	// Verify hit rate is calculated
	expectedHitRate := float64(stats.Hits) / float64(stats.Hits+stats.Misses)
	if stats.HitRate != expectedHitRate {
		t.Errorf("Expected hit rate %f, got %f", expectedHitRate, stats.HitRate)
	}
}

// TestCachedLLMClient_CleanupExpired_CleansBothCaches tests cleanup of expired entries
func TestCachedLLMClient_CleanupExpired_CleansBothCaches(t *testing.T) {
	mockClient := &mockLLMClient{
		response: CompletionResponse{Content: "test"},
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)

	// Use short TTL
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, 10*time.Millisecond)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test",
		Messages:     []Message{{Role: "user", Content: "hello"}},
		Temperature:  0.7,
	}

	// Populate cache
	_, _ = cachedClient.GenerateCompletion(ctx, req)

	// Wait for expiration
	time.Sleep(20 * time.Millisecond)

	// Cleanup expired
	memoryExpired, _ := cachedClient.CleanupExpired()

	if memoryExpired < 1 {
		t.Errorf("Expected at least 1 expired entry in memory cache, got %d", memoryExpired)
	}
}

// TestCachedLLMClient_Clear_EmptiesBothCaches tests clearing both caches
func TestCachedLLMClient_Clear_EmptiesBothCaches(t *testing.T) {
	mockClient := &mockLLMClient{
		response: CompletionResponse{Content: "test"},
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test",
		Messages:     []Message{{Role: "user", Content: "hello"}},
		Temperature:  0.7,
	}

	// Populate cache
	_, _ = cachedClient.GenerateCompletion(ctx, req)

	// Verify cache has entries
	statsBefore := cachedClient.GetStats()
	if statsBefore.Size == 0 {
		t.Error("Expected cache to have entries before clear")
	}

	// Clear cache
	err := cachedClient.Clear()
	if err != nil {
		t.Fatalf("Expected no error from Clear, got %v", err)
	}

	// Verify cache is empty
	statsAfter := cachedClient.GetStats()
	if statsAfter.Size != 0 {
		t.Errorf("Expected empty cache after clear, got size %d", statsAfter.Size)
	}
}

// TestCachedLLMClient_GetUnderlyingClient_ReturnsClient tests getting underlying client
func TestCachedLLMClient_GetUnderlyingClient_ReturnsClient(t *testing.T) {
	mockClient := &mockLLMClient{
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	underlying := cachedClient.GetUnderlyingClient()
	if underlying != mockClient {
		t.Error("Expected underlying client to be the mock client")
	}
}

// TestCachedLLMClient_IntegrationWithOpenAI tests integration with real OpenAI client
func TestCachedLLMClient_IntegrationWithOpenAI(t *testing.T) {
	// Setup mock server
	callCount := 0
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		callCount++

		w.Header().Set("Content-Type", "text/event-stream")

		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)

		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"openai test response\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)

		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}")
		_, _ = fmt.Fprintln(w)

		_, _ = fmt.Fprintln(w, "data: [DONE]")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	// Create real OpenAI client
	openaiClient := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	// Wrap with caching
	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(openaiClient, memoryCache, diskCache, true, time.Hour)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test system",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		Temperature: 0.7,
	}

	// First call - should hit API
	resp1, err1 := cachedClient.GenerateCompletion(ctx, req)
	if err1 != nil {
		t.Fatalf("First call: Expected no error, got %v", err1)
	}

	if resp1.Content != "openai test response" {
		t.Errorf("First call: Expected content 'openai test response', got '%s'", resp1.Content)
	}

	// Second call - should hit cache
	resp2, err2 := cachedClient.GenerateCompletion(ctx, req)
	if err2 != nil {
		t.Fatalf("Second call: Expected no error, got %v", err2)
	}

	if resp2.Content != "openai test response" {
		t.Errorf("Second call: Expected content 'openai test response', got '%s'", resp2.Content)
	}

	// Verify API was called only once
	if callCount != 1 {
		t.Errorf("Expected 1 API call, got %d", callCount)
	}

	// Verify cache stats
	stats := cachedClient.GetStats()
	if stats.Hits != 1 {
		t.Errorf("Expected 1 cache hit, got %d", stats.Hits)
	}
	if stats.Misses != 1 {
		t.Errorf("Expected 1 cache miss, got %d", stats.Misses)
	}
}

// TestCachedLLMClient_DiskCacheFailure_GracefulDegradation tests graceful degradation on disk cache failure
func TestCachedLLMClient_DiskCacheFailure_GracefulDegradation(t *testing.T) {
	mockClient := &mockLLMClient{
		response: CompletionResponse{Content: "test"},
		provider: "test",
	}

	// Create disk cache in read-only directory (will fail writes)
	tempDir := t.TempDir()
	readonlyDir := filepath.Join(tempDir, "readonly")
	if err := os.Mkdir(readonlyDir, 0444); err != nil {
		t.Fatalf("Failed to create readonly directory: %v", err)
	}
	// Make directory read-only
	if err := os.Chmod(readonlyDir, 0444); err != nil {
		t.Fatalf("Failed to chmod directory: %v", err)
	}

	cachePath := filepath.Join(readonlyDir, "test-cache.json")

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(cachePath, llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test",
		Messages:     []Message{{Role: "user", Content: "hello"}},
		Temperature:  0.7,
	}

	// Should succeed despite disk cache write failure
	resp, err := cachedClient.GenerateCompletion(ctx, req)
	if err != nil {
		t.Fatalf("Expected no error despite disk cache failure, got %v", err)
	}

	if resp.Content != "test" {
		t.Errorf("Expected content 'test', got '%s'", resp.Content)
	}

	// Verify memory cache still works (second call should hit memory)
	resp2, err2 := cachedClient.GenerateCompletion(ctx, req)
	if err2 != nil {
		t.Fatalf("Second call: Expected no error, got %v", err2)
	}

	if resp2.Content != "test" {
		t.Errorf("Second call: Expected content 'test', got '%s'", resp2.Content)
	}

	if mockClient.callCount != 1 {
		t.Errorf("Expected 1 call to underlying client (memory cache worked), got %d", mockClient.callCount)
	}
}

// TestCachedLLMClient_NilMemoryCache_WorksCorrectly tests behavior with nil memory cache
func TestCachedLLMClient_NilMemoryCache_WorksCorrectly(t *testing.T) {
	mockClient := &mockLLMClient{
		response: CompletionResponse{Content: "test"},
		provider: "test",
	}

	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, nil, diskCache, true, time.Hour)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test",
		Messages:     []Message{{Role: "user", Content: "hello"}},
		Temperature:  0.7,
	}

	// Should work with disk cache only
	resp, err := cachedClient.GenerateCompletion(ctx, req)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if resp.Content != "test" {
		t.Errorf("Expected content 'test', got '%s'", resp.Content)
	}
}

// TestCachedLLMClient_NilDiskCache_WorksCorrectly tests behavior with nil disk cache
func TestCachedLLMClient_NilDiskCache_WorksCorrectly(t *testing.T) {
	mockClient := &mockLLMClient{
		response: CompletionResponse{Content: "test"},
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, nil, true, time.Hour)

	ctx := context.Background()
	req := CompletionRequest{
		SystemPrompt: "test",
		Messages:     []Message{{Role: "user", Content: "hello"}},
		Temperature:  0.7,
	}

	// Should work with memory cache only
	resp, err := cachedClient.GenerateCompletion(ctx, req)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if resp.Content != "test" {
		t.Errorf("Expected content 'test', got '%s'", resp.Content)
	}

	// Second call should hit memory cache
	_, err2 := cachedClient.GenerateCompletion(ctx, req)
	if err2 != nil {
		t.Fatalf("Second call: Expected no error, got %v", err2)
	}

	if mockClient.callCount != 1 {
		t.Errorf("Expected 1 call to underlying client, got %d", mockClient.callCount)
	}
}

// TestCachedLLMClient_ContextCancellation_PropagatesError tests that context cancellation is handled
func TestCachedLLMClient_ContextCancellation_PropagatesError(t *testing.T) {
	mockClient := &mockLLMClient{
		response: CompletionResponse{Content: "test"},
		provider: "test",
	}

	memoryCache := llmcache.NewLRUCache(10)
	diskCache := llmcache.NewDiskCache(t.TempDir()+"/test-cache.json", llmcache.DefaultTTL, 100*1024*1024)
	cachedClient := NewCachedLLMClient(mockClient, memoryCache, diskCache, true, time.Hour)

	// Create canceled context
	ctx, cancel := context.WithCancel(context.Background())
	cancel()

	req := CompletionRequest{
		SystemPrompt: "test",
		Messages:     []Message{{Role: "user", Content: "hello"}},
		Temperature:  0.7,
	}

	// Should return error (context is checked before cache lookup in real scenario)
	_, err := cachedClient.GenerateCompletion(ctx, req)
	if err == nil {
		t.Fatal("Expected error for canceled context, got nil")
	}
}

// testError is a simple error type for testing
type testError struct {
	msg string
}

func (e *testError) Error() string {
	return e.msg
}
</file>
<file path="internal/llm/client.go">
package llm

import (
	"context"

	"github.com/user/gendocs/internal/llmtypes"
)

// Type aliases for backward compatibility
type Message = llmtypes.Message
type ToolCall = llmtypes.ToolCall
type CompletionRequest = llmtypes.CompletionRequest
type CompletionResponse = llmtypes.CompletionResponse
type TokenUsage = llmtypes.TokenUsage
type ToolDefinition = llmtypes.ToolDefinition

// LLMClient is the interface for LLM providers
type LLMClient interface {
	// GenerateCompletion generates a completion from the LLM
	GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error)

	// SupportsTools returns true if the client supports tool calling
	SupportsTools() bool

	// GetProvider returns the provider name
	GetProvider() string
}

// BaseLLMClient provides common functionality for all LLM clients
type BaseLLMClient struct {
	retryClient *RetryClient
}

// NewBaseLLMClient creates a new base LLM client
func NewBaseLLMClient(retryClient *RetryClient) *BaseLLMClient {
	// If no retry client provided, create a default one
	if retryClient == nil {
		retryClient = NewRetryClient(nil) // Uses default config
	}
	return &BaseLLMClient{
		retryClient: retryClient,
	}
}

// doHTTPRequest executes an HTTP request with retry and standard error handling.
// It handles JSON marshaling, request creation, header setting, execution with retry,
// response reading, and status code validation.
//
// Parameters:
//   - ctx: Context for request cancellation and timeout control
</file>
<file path="internal/llm/example_test.go">
package llm

import (
	"fmt"
	"net/http"
	"time"
)

// Example_defaultConfiguration demonstrates using the RetryClient with
// default optimized connection pooling settings.
//
// This is the recommended approach for most use cases. The defaults are
// optimized for LLM API usage and provide good performance out of the box.
func Example_defaultConfiguration() {
	// Create a client with default configuration
	client := NewRetryClient(nil)

	// The client is now ready to use with optimized connection pooling:
	// - MaxIdleConns: 100 (global connection pool)
	// - MaxIdleConnsPerHost: 10 (per-host pool, e.g., api.openai.com)
	// - IdleConnTimeout: 90s (connections idle for 90s are kept alive)
	// - TLSHandshakeTimeout: 10s
	// - ExpectContinueTimeout: 1s
	// - HTTP/2 enabled
	// - TLS 1.2 minimum

	// You can verify the configuration
	stats := client.GetConnectionStats()
	fmt.Printf("Transport type: %s\n", stats.TransportType)
	fmt.Printf("HTTP/2 enabled: %v\n", stats.HTTP2Enabled)
	fmt.Printf("Max idle connections: %d\n", stats.MaxIdleConns)
	fmt.Printf("Max idle connections per host: %d\n", stats.MaxIdleConnsPerHost)
	fmt.Printf("Idle connection timeout: %v\n", stats.IdleConnTimeout)

	// Use the client for API calls
	// req, _ := http.NewRequest("GET", "https://api.openai.com/v1/models", nil)
	// resp, err := client.Do(req)
	// _ = resp // Handle response

	_ = client // In real usage, you would make API calls here
}

// Example_customHighThroughput demonstrates custom connection pooling
// configuration for high-throughput scenarios.
//
// Use this configuration when you need to handle many concurrent requests
// to LLM APIs, such as in a server environment handling multiple users.
func Example_customHighThroughput() {
	config := &RetryConfig{
		// Retry settings
		MaxAttempts:       5,
		Multiplier:        1,
		MaxWaitPerAttempt: 60 * time.Second,
		MaxTotalWait:      300 * time.Second,

		// Connection pooling for high throughput
		// Increase pool sizes to handle more concurrent connections
		MaxIdleConns:        500,               // Larger global pool (default: 100)
		MaxIdleConnsPerHost: 50,                // More connections per host (default: 10)
		IdleConnTimeout:     120 * time.Second, // Keep connections alive longer (default: 90s)

		// Timeout settings
		TLSHandshakeTimeout:   10 * time.Second,
		ExpectContinueTimeout: 1 * time.Second,
	}

	client := NewRetryClient(config)

	stats := client.GetConnectionStats()
	fmt.Printf("High-throughput configuration:\n")
	fmt.Printf("  Max idle connections: %d\n", stats.MaxIdleConns)
	fmt.Printf("  Max idle connections per host: %d\n", stats.MaxIdleConnsPerHost)
	fmt.Printf("  Idle connection timeout: %v\n", stats.IdleConnTimeout)

	_ = client // In real usage, you would make API calls here
}

// Example_customMemoryConstrained demonstrates custom connection pooling
// configuration for memory-constrained environments.
//
// Use this configuration when running in environments with limited memory,
// such as AWS Lambda, Cloud Functions, or embedded devices.
func Example_customMemoryConstrained() {
	config := &RetryConfig{
		// Retry settings
		MaxAttempts:       3, // Fewer retries to save resources
		Multiplier:        1,
		MaxWaitPerAttempt: 30 * time.Second,
		MaxTotalWait:      60 * time.Second,

		// Connection pooling for memory efficiency
		// Reduce pool sizes to minimize memory footprint
		MaxIdleConns:        20,               // Smaller global pool (default: 100)
		MaxIdleConnsPerHost: 5,                // Fewer connections per host (default: 10)
		IdleConnTimeout:     30 * time.Second, // Shorter timeout to free resources faster (default: 90s)

		// Timeout settings
		TLSHandshakeTimeout:   5 * time.Second, // Faster timeout
		ExpectContinueTimeout: 1 * time.Second,
	}

	client := NewRetryClient(config)

	stats := client.GetConnectionStats()
	fmt.Printf("Memory-constrained configuration:\n")
	fmt.Printf("  Max idle connections: %d\n", stats.MaxIdleConns)
	fmt.Printf("  Max idle connections per host: %d\n", stats.MaxIdleConnsPerHost)
	fmt.Printf("  Idle connection timeout: %v\n", stats.IdleConnTimeout)

	_ = client // In real usage, you would make API calls here
}

// Example_customTimeout demonstrates creating a RetryClient with a custom
// timeout while maintaining optimized connection pooling.
func Example_customTimeout() {
	// Create a client with a 2-minute timeout
	// Connection pooling settings will use optimized defaults
	client := NewRetryClientWithTimeout(2*time.Minute, nil)

	fmt.Printf("Client timeout: %v\n", client.GetTimeout())

	_ = client // In real usage, you would make API calls here
}

// Example_customTransport demonstrates providing a completely custom HTTP transport.
//
// Use this when you need advanced features like:
// - Custom proxy configuration
// - Custom TLS configuration
// - Custom dialer (e.g., for SOCKS proxy)
// - Advanced connection management
func Example_customTransport() {
	// Create a custom transport with specific requirements
	customTransport := &http.Transport{
		MaxIdleConns:        200,
		MaxIdleConnsPerHost: 20,
		IdleConnTimeout:     90 * time.Second,
		// Add your custom configuration here
		// For example, proxy, custom dialer, etc.
	}

	config := &RetryConfig{
		// Retry settings
		MaxAttempts:       5,
		Multiplier:        1,
		MaxWaitPerAttempt: 60 * time.Second,
		MaxTotalWait:      300 * time.Second,

		// Use custom transport - connection pooling fields above will be ignored
		Transport: customTransport,
	}

	client := NewRetryClient(config)

	stats := client.GetConnectionStats()
	fmt.Printf("Custom transport type: %s\n", stats.TransportType)
	fmt.Printf("Max idle connections from custom transport: %d\n", stats.MaxIdleConns)

	_ = client // In real usage, you would make API calls here
}

// Example_connectionStats demonstrates how to retrieve connection pool
// statistics for monitoring and debugging.
func Example_connectionStats() {
	client := NewRetryClient(nil)

	stats := client.GetConnectionStats()

	fmt.Printf("=== Connection Pool Statistics ===\n")
	fmt.Printf("Transport type: %s\n", stats.TransportType)
	fmt.Printf("HTTP/2 enabled: %v\n", stats.HTTP2Enabled)
	fmt.Printf("TLS minimum version: %d\n", stats.TLSMinVersion)
	fmt.Printf("\nConnection Pool Configuration:\n")
	fmt.Printf("  Max idle connections (global): %d\n", stats.MaxIdleConns)
	fmt.Printf("  Max idle connections per host: %d\n", stats.MaxIdleConnsPerHost)
	fmt.Printf("  Idle connection timeout: %v\n", stats.IdleConnTimeout)
	fmt.Printf("\nTimeout Configuration:\n")
	fmt.Printf("  TLS handshake timeout: %v\n", stats.TLSHandshakeTimeout)
	fmt.Printf("  Expect continue timeout: %v\n", stats.ExpectContinueTimeout)
	fmt.Printf("  Client timeout: %v\n", stats.ClientTimeout)

	_ = client // In real usage, you would make API calls here
}

// Example_closeIdleConnections demonstrates how to manually close idle
// connections to free up resources.
//
// This can be useful in long-running applications when you know the client
// will not be used for a while, or when shutting down gracefully.
func Example_closeIdleConnections() {
	client := NewRetryClient(nil)

	// ... use the client for API calls ...

	// When done, explicitly close idle connections to free resources
	client.CloseIdleConnections()

	// The client can still be used after closing idle connections
	// New connections will be established as needed

	_ = client
}

// Example_multipleProviders demonstrates using connection pooling with
// multiple LLM API providers.
//
// The global connection pool (MaxIdleConns) manages connections across all
// providers, while MaxIdleConnsPerHost limits connections per provider.
func Example_multipleProviders() {
	config := &RetryConfig{
		// Retry settings
		MaxAttempts:       5,
		Multiplier:        1,
		MaxWaitPerAttempt: 60 * time.Second,
		MaxTotalWait:      300 * time.Second,

		// Connection pooling for multiple providers
		MaxIdleConns:        200, // Total pool for all providers
		MaxIdleConnsPerHost: 10,  // Per-provider limit (e.g., api.openai.com, api.anthropic.com)
		IdleConnTimeout:     90 * time.Second,

		// Timeout settings
		TLSHandshakeTimeout:   10 * time.Second,
		ExpectContinueTimeout: 1 * time.Second,
	}

	client := NewRetryClient(config)

	// This client can efficiently handle requests to multiple LLM providers:
	// - api.openai.com: up to 10 idle connections
	// - api.anthropic.com: up to 10 idle connections
	// - generativelanguage.googleapis.com: up to 10 idle connections
	// All while keeping total idle connections under 200

	stats := client.GetConnectionStats()
	fmt.Printf("Multiple provider configuration:\n")
	fmt.Printf("  Total max idle connections: %d\n", stats.MaxIdleConns)
	fmt.Printf("  Max idle connections per provider: %d\n", stats.MaxIdleConnsPerHost)

	_ = client
}
</file>
<file path="internal/llm/factory.go">
package llm

import (
	"fmt"
	"time"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llmcache"
)

// Factory creates LLM clients
type Factory struct {
	retryClient  *RetryClient
	memoryCache  *llmcache.LRUCache
	diskCache    *llmcache.DiskCache
	cacheEnabled bool
	cacheTTL     time.Duration
}

// NewFactory creates a new LLM factory
// Optional cache parameters can be provided to enable caching
func NewFactory(retryClient *RetryClient, memoryCache *llmcache.LRUCache, diskCache *llmcache.DiskCache, cacheEnabled bool, cacheTTL time.Duration) *Factory {
	return &Factory{
		retryClient:  retryClient,
		memoryCache:  memoryCache,
		diskCache:    diskCache,
		cacheEnabled: cacheEnabled,
		cacheTTL:     cacheTTL,
	}
}

// CreateClient creates an LLM client based on the provider configuration
// If caching is enabled and cache instances are available, wraps the client with caching
func (f *Factory) CreateClient(cfg config.LLMConfig) (LLMClient, error) {
	// Create base client (without caching)
	var baseClient LLMClient
	switch cfg.Provider {
	case "openai":
		baseClient = NewOpenAIClient(cfg, f.retryClient)
	case "anthropic":
		baseClient = NewAnthropicClient(cfg, f.retryClient)
	case "gemini":
		baseClient = NewGeminiClient(cfg, f.retryClient)
	default:
		return nil, fmt.Errorf("unsupported LLM provider: %s (supported: openai, anthropic, gemini)", cfg.Provider)
	}

	// Wrap with caching if enabled and cache instances are available
	if f.cacheEnabled && f.memoryCache != nil {
		ttl := f.cacheTTL
		if ttl == 0 {
			ttl = llmcache.DefaultTTL
		}
		return NewCachedLLMClient(baseClient, f.memoryCache, f.diskCache, true, ttl), nil
	}

	return baseClient, nil
}
</file>
<file path="internal/llm/gemini.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/user/gendocs/internal/config"
)

// GeminiClient implements LLMClient for Google Gemini
type GeminiClient struct {
	*BaseLLMClient
	apiKey  string
	model   string
	baseURL string
}

// geminiRequest represents the request body for Gemini API
type geminiRequest struct {
	Contents          []geminiContent        `json:"contents"`
	Tools             []geminiTool           `json:"tools,omitempty"`
	GenerationConfig  geminiGenerationConfig `json:"generationConfig,omitempty"`
	SystemInstruction *geminiContent         `json:"systemInstruction,omitempty"`
}

// geminiContent represents content in Gemini format
type geminiContent struct {
	Role  string       `json:"role,omitempty"`
	Parts []geminiPart `json:"parts"`
}

// geminiPart represents a part of content
type geminiPart struct {
	Text             string                  `json:"text,omitempty"`
	FunctionCall     map[string]interface{}  `json:"functionCall,omitempty"`
	FunctionResponse *geminiFunctionResponse `json:"functionResponse,omitempty"`
	ThoughtSignature string                  `json:"thoughtSignature,omitempty"` // Required for Gemini 3 function calling
}

// geminiFunctionResponse represents a function response
// Gemini format: {"name": "function_name", "response": {...}}
type geminiFunctionResponse struct {
	Name     string                 `json:"name"`
	Response map[string]interface{} `json:"response,omitempty"`
}

// geminiTool represents a tool declaration
type geminiTool struct {
	FunctionDeclarations []geminiFunctionDeclaration `json:"functionDeclarations,omitempty"`
}

// geminiFunctionDeclaration represents a function declaration
type geminiFunctionDeclaration struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Parameters  map[string]interface{} `json:"parameters"`
}

// geminiGenerationConfig represents generation configuration
type geminiGenerationConfig struct {
	Temperature     float64 `json:"temperature,omitempty"`
	MaxOutputTokens int     `json:"maxOutputTokens,omitempty"`
}

// geminiUsageMetadata represents token usage
type geminiUsageMetadata struct {
	PromptTokenCount     int `json:"promptTokenCount"`
	CandidatesTokenCount int `json:"candidatesTokenCount"`
	TotalTokenCount      int `json:"totalTokenCount"`
}

// geminiError represents an error
type geminiError struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Status  string `json:"status"`
}

// geminiStreamChunk represents a single streaming response chunk (NDJSON line)
type geminiStreamChunk struct {
	Candidates    []geminiStreamCandidate `json:"candidates"`
	UsageMetadata *geminiUsageMetadata    `json:"usageMetadata,omitempty"`
	Error         *geminiError            `json:"error,omitempty"`
}

// geminiStreamCandidate represents a candidate in a streaming chunk
type geminiStreamCandidate struct {
	Content      geminiStreamContent `json:"content"`
	FinishReason *string             `json:"finishReason,omitempty"`
	Index        int                 `json:"index"`
}

// geminiStreamContent represents content in a streaming chunk
type geminiStreamContent struct {
	Parts []geminiStreamPart `json:"parts"`
	Role  string             `json:"role,omitempty"`
}

// geminiStreamPart represents a part in streaming content
type geminiStreamPart struct {
	Text             string                    `json:"text,omitempty"`
	FunctionCall     *geminiStreamFunctionCall `json:"functionCall,omitempty"`
	ThoughtSignature string                    `json:"thoughtSignature,omitempty"` // Required for Gemini 2.0+/3.0 function calling
}

// geminiStreamFunctionCall represents a function call in streaming
type geminiStreamFunctionCall struct {
	Name string                 `json:"name"`
	Args map[string]interface{} `json:"args"`
}

// geminiAccumulator builds CompletionResponse from streaming chunks
type geminiAccumulator struct {
	textBuilder   strings.Builder
	toolCalls     []ToolCall
	usage         geminiUsageMetadata
	finishReason  string
	complete      bool
	hasCandidates bool
}

// newGeminiAccumulator creates a new accumulator
func newGeminiAccumulator() *geminiAccumulator {
	return &geminiAccumulator{}
}

// HandleChunk processes a single streaming chunk
func (a *geminiAccumulator) HandleChunk(chunk geminiStreamChunk) error {
	// Check for API error
	if chunk.Error != nil {
		return fmt.Errorf("API error: %s", chunk.Error.Message)
	}

	// Skip if no candidates
	if len(chunk.Candidates) == 0 {
		return nil
	}

	a.hasCandidates = true
	candidate := chunk.Candidates[0]

	// Check for safety block
	if candidate.FinishReason != nil && *candidate.FinishReason == "SAFETY" {
		return fmt.Errorf("response blocked for safety reasons")
	}

	// Accumulate usage metadata if present
	if chunk.UsageMetadata != nil {
		a.usage = *chunk.UsageMetadata
	}

	// Process parts
	for _, part := range candidate.Content.Parts {
		if part.Text != "" {
			a.textBuilder.WriteString(part.Text)
		}
		if part.FunctionCall != nil {
			// Function calls arrive complete in Gemini (no partial JSON)
			// Preserve thoughtSignature for Gemini 2.0+/3.0 multi-turn function calling
			a.toolCalls = append(a.toolCalls, ToolCall{
				Name:             part.FunctionCall.Name,
				Arguments:        part.FunctionCall.Args,
				ThoughtSignature: part.ThoughtSignature, // Required for subsequent API calls
				RawFunctionCall: map[string]interface{}{
					"name": part.FunctionCall.Name,
					"args": part.FunctionCall.Args,
				},
			})
		}
	}

	// Check if complete (finishReason is set)
	if candidate.FinishReason != nil {
		a.finishReason = *candidate.FinishReason
		a.complete = true
	}

	return nil
}

// Build constructs the final CompletionResponse
func (a *geminiAccumulator) Build() CompletionResponse {
	return CompletionResponse{
		Content:   a.textBuilder.String(),
		ToolCalls: a.toolCalls,
		Usage: TokenUsage{
			InputTokens:  a.usage.PromptTokenCount,
			OutputTokens: a.usage.CandidatesTokenCount,
			TotalTokens:  a.usage.TotalTokenCount,
		},
	}
}

// IsComplete returns true if finishReason was received
func (a *geminiAccumulator) IsComplete() bool {
	return a.complete
}

// NewGeminiClient creates a new Gemini client
func NewGeminiClient(cfg config.LLMConfig, retryClient *RetryClient) *GeminiClient {
	baseURL := cfg.BaseURL
	if baseURL == "" {
		baseURL = "https://generativelanguage.googleapis.com"
	}
	return &GeminiClient{
		BaseLLMClient: NewBaseLLMClient(retryClient),
		apiKey:        cfg.APIKey,
		model:         cfg.Model,
		baseURL:       baseURL,
	}
}

// GenerateCompletion generates a completion from Gemini
func (c *GeminiClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	// Convert to Gemini format
	gemReq := c.convertRequest(req)

	jsonData, err := json.Marshal(gemReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	// Model format: models/gemini-1.5-pro or models/gemini-pro
	// Use streaming endpoint: streamGenerateContent
	modelName := c.model
	if !strings.HasPrefix(modelName, "models/") {
		modelName = "models/" + modelName
	}
	url := fmt.Sprintf("%s/v1beta/%s:streamGenerateContent?key=%s", c.baseURL, modelName, c.apiKey)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonData))
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")

	// Execute with retry
	resp, err := c.retryClient.Do(httpReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("request failed: %w", err)
	}
	defer func() { _ = resp.Body.Close() }()

	// Check for error status
	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return CompletionResponse{}, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))
	}

	// Parse streaming response
	return c.parseStreamingResponse(resp.Body)
}

// parseStreamingResponse parses Gemini's streaming response (JSON array format)
func (c *GeminiClient) parseStreamingResponse(body io.ReadCloser) (CompletionResponse, error) {
	// Gemini streaming API returns a JSON array of chunks: [{...}, {...}, ...]
	// Read entire response and parse as array
	data, err := io.ReadAll(body)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to read response body: %w", err)
	}

	var chunks []geminiStreamChunk
	if err := json.Unmarshal(data, &chunks); err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to parse stream response: %w", err)
	}

	accumulator := newGeminiAccumulator()

	for _, chunk := range chunks {
		if err := accumulator.HandleChunk(chunk); err != nil {
			return CompletionResponse{}, fmt.Errorf("chunk handling error: %w", err)
		}
	}

	if !accumulator.hasCandidates {
		return CompletionResponse{}, fmt.Errorf("no candidates returned")
	}

	return accumulator.Build(), nil
}

// SupportsTools returns true
func (c *GeminiClient) SupportsTools() bool {
	return true
}

// GetProvider returns the provider name
func (c *GeminiClient) GetProvider() string {
	return "gemini"
}

// convertRequest converts internal request to Gemini format
func (c *GeminiClient) convertRequest(req CompletionRequest) geminiRequest {
	// Build contents
	contents := []geminiContent{}

	// Add system instruction as first content with role "user"
	// Gemini doesn't have a separate system field, it's part of content
	if req.SystemPrompt != "" {
		contents = append(contents, geminiContent{
			Role: "user",
			Parts: []geminiPart{
				{Text: req.SystemPrompt},
			},
		})
		// Add empty model response
		contents = append(contents, geminiContent{
			Role: "model",
			Parts: []geminiPart{
				{Text: "Understood. I will analyze the codebase according to your instructions."},
			},
		})
	}

	// Add messages
	for _, msg := range req.Messages {
		if msg.Role == "tool" {
			// Tool response - extract function name from tool ID or content
			// Format: {"name": "function_name", "response": {"result": "content"}}
			funcName := msg.ToolID
			if funcName == "" {
				// Try to extract from Content if it's JSON
				var toolData map[string]interface{}
				if err := json.Unmarshal([]byte(msg.Content), &toolData); err == nil {
					if name, ok := toolData["name"].(string); ok {
						funcName = name
					}
				}
			}
			// Fallback to a default name if still empty
			if funcName == "" {
				funcName = "unknown_function"
			}

			contents = append(contents, geminiContent{
				Role: "user",
				Parts: []geminiPart{
					{
						FunctionResponse: &geminiFunctionResponse{
							Name: funcName,
							Response: map[string]interface{}{
								"result": msg.Content,
							},
						},
					},
				},
			})
		} else if msg.Role == "assistant" {
			// Model/assistant message - include function calls if present
			var parts []geminiPart

			// Add text content if present
			if msg.Content != "" {
				parts = append(parts, geminiPart{Text: msg.Content})
			}

			// Add function calls if present - include ThoughtSignature for Gemini 3
			for _, tc := range msg.ToolCalls {
				part := geminiPart{
					ThoughtSignature: tc.ThoughtSignature, // Include thought signature at part level
				}
				if tc.RawFunctionCall != nil {
					part.FunctionCall = tc.RawFunctionCall
				} else {
					part.FunctionCall = map[string]interface{}{
						"name": tc.Name,
						"args": tc.Arguments,
					}
				}
				parts = append(parts, part)
			}

			// Only add the message if there are parts (text or function calls)
			if len(parts) > 0 {
				contents = append(contents, geminiContent{
					Role:  "model",
					Parts: parts,
				})
			}
		} else if msg.Role == "user" {
			// User message - skip empty content
			if msg.Content == "" {
				continue
			}
			contents = append(contents, geminiContent{
				Role: "user",
				Parts: []geminiPart{
					{Text: msg.Content},
				},
			})
		}
	}

	// Build tools
	var tools []geminiTool
	if len(req.Tools) > 0 {
		tools = make([]geminiTool, 1)
		functions := make([]geminiFunctionDeclaration, len(req.Tools))
		for i, tool := range req.Tools {
			functions[i] = geminiFunctionDeclaration{
				Name:        tool.Name,
				Description: tool.Description,
				Parameters:  tool.Parameters,
			}
		}
		tools[0] = geminiTool{
			FunctionDeclarations: functions,
		}
	}

	return geminiRequest{
		Contents: contents,
		Tools:    tools,
		GenerationConfig: geminiGenerationConfig{
			Temperature:     req.Temperature,
			MaxOutputTokens: req.MaxTokens,
		},
	}
}
</file>
<file path="internal/llm/gemini_bench_test.go">
package llm

import (
	"context"
	"fmt"
	"net/http"
	"net/http/httptest"
	"testing"
	"time"

	"github.com/user/gendocs/internal/config"
)

// BenchmarkGemini_SmallResponse benchmarks a small single-chunk response
func BenchmarkGemini_SmallResponse(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Small response - single text chunk
		_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"text":"Hello!"}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":2,"totalTokenCount":12}}`)
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkGemini_MediumResponse benchmarks a medium multi-chunk response
func BenchmarkGemini_MediumResponse(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Medium response - multiple chunks
		for i := 0; i < 10; i++ {
			_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"text":"This is chunk `+fmt.Sprint(i)+` of the response. "}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":15,"candidatesTokenCount":`+fmt.Sprint(2+i*5)+`,"totalTokenCount":`+fmt.Sprint(17+i*5)+`}}`)
		}
		_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":15,"candidatesTokenCount":50,"totalTokenCount":65}}`)
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Tell me a story"},
		},
		MaxTokens:   1000,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkGemini_LargeResponse benchmarks a large response with many chunks
func BenchmarkGemini_LargeResponse(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Large response - many chunks (simulating ~50KB response)
		for i := 0; i < 100; i++ {
			_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"text":"This is a longer chunk of text that represents a substantial part of the response. Chunk `+fmt.Sprint(i)+` contains useful information. "}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":20,"candidatesTokenCount":`+fmt.Sprint(2+i*5)+`,"totalTokenCount":`+fmt.Sprint(22+i*5)+`}}`)
		}
		_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":20,"candidatesTokenCount":500,"totalTokenCount":520}}`)
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Analyze this codebase in detail"},
		},
		MaxTokens:   4000,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkGemini_FunctionCall benchmarks a response with a function call
func BenchmarkGemini_FunctionCall(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Function call response (complete, not partial like Anthropic/OpenAI)
		_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"text":"I'll read the file for you."}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":20,"candidatesTokenCount":5,"totalTokenCount":25}}`)
		_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"functionCall":{"name":"read_file","args":{"file_path":"src/main.go","start_line":1,"end_line":100}}}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":20,"candidatesTokenCount":15,"totalTokenCount":35}}`)
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Read the main.go file"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "read_file",
				Description: "Read a file",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"file_path": map[string]interface{}{
							"type":        "string",
							"description": "Path to the file",
						},
						"start_line": map[string]interface{}{
							"type": "integer",
						},
						"end_line": map[string]interface{}{
							"type": "integer",
						},
					},
					"required": []string{"file_path"},
				},
			},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkGemini_MultipleFunctionCalls benchmarks multiple function calls in one response
func BenchmarkGemini_MultipleFunctionCalls(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Multiple function calls in a single chunk
		_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"text":"I'll read the file and list the directory."}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":25,"candidatesTokenCount":8,"totalTokenCount":33}}`)
		_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"functionCall":{"name":"read_file","args":{"path":"main.go"}}},{"functionCall":{"name":"list_files","args":{"path":"src"}}}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":25,"candidatesTokenCount":20,"totalTokenCount":45}}`)
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Read main.go and list files in src"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "read_file",
				Description: "Read a file",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"path": map[string]interface{}{
							"type": "string",
						},
					},
					"required": []string{"path"},
				},
			},
			{
				Name:        "list_files",
				Description: "List files",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"path": map[string]interface{}{
							"type": "string",
						},
					},
					"required": []string{"path"},
				},
			},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkGemini_TimeToFirstToken measures time to receive first content
func BenchmarkGemini_TimeToFirstToken(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Simulate network delay before first chunk
		time.Sleep(10 * time.Millisecond)
		_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"text":"Response"}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":5,"totalTokenCount":15}}`)
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()

	for i := 0; i < b.N; i++ {
		start := time.Now()
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
		elapsed := time.Since(start)
		b.ReportMetric(float64(elapsed.Nanoseconds()), "ns/op")
	}
}

// BenchmarkGemini_MixedContent benchmarks a response with text followed by function call
func BenchmarkGemini_MixedContent(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Text response across multiple chunks
		for i := 0; i < 5; i++ {
			_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"text":"Chunk `+fmt.Sprint(i)+` "}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":15,"candidatesTokenCount":`+fmt.Sprint(2+i*2)+`,"totalTokenCount":`+fmt.Sprint(17+i*2)+`}}`)
		}
		// Final chunk with function call
		_, _ = fmt.Fprintln(w, `{"candidates":[{"content":{"parts":[{"functionCall":{"name":"list_files","args":{"path":"src"}}}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":15,"candidatesTokenCount":15,"totalTokenCount":30}}`)
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "List files in src"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "list_files",
				Description: "List files",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"path": map[string]interface{}{
							"type": "string",
						},
					},
					"required": []string{"path"},
				},
			},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}
</file>
<file path="internal/llm/gemini_test.go">
package llm

import (
	"context"
	"net/http"
	"net/http/httptest"
	"testing"
	"time"

	"github.com/user/gendocs/internal/config"
)

func TestGeminiClient_GenerateCompletion_Success(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Validate API key in query param
		apiKey := r.URL.Query().Get("key")
		if apiKey != "test-key" {
			t.Errorf("Expected API key 'test-key' in query, got '%s'", apiKey)
		}

		// Send JSON array streaming response (Gemini format)
		w.Header().Set("Content-Type", "application/json")
		_, _ = w.Write([]byte(`[
			{"candidates":[{"content":{"parts":[{"text":"test response"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":12,"candidatesTokenCount":2,"totalTokenCount":14}},
			{"candidates":[{"content":{"parts":[{"text":" from gemini"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":12,"candidatesTokenCount":4,"totalTokenCount":16}},
			{"candidates":[{"content":{"parts":[],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":12,"candidatesTokenCount":6,"totalTokenCount":18}}
		]`))
	}))
	defer server.Close()

	// Create client
	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if resp.Content != "test response from gemini" {
		t.Errorf("Expected content 'test response from gemini', got '%s'", resp.Content)
	}

	if resp.Usage.InputTokens != 12 {
		t.Errorf("Expected 12 input tokens, got %d", resp.Usage.InputTokens)
	}

	if resp.Usage.OutputTokens != 6 {
		t.Errorf("Expected 6 output tokens, got %d", resp.Usage.OutputTokens)
	}
}

func TestGeminiClient_GenerateCompletion_WithToolCalls(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Send JSON array streaming response with function call
		w.Header().Set("Content-Type", "application/json")
		_, _ = w.Write([]byte(`[
			{"candidates":[{"content":{"parts":[{"text":"I'll list the files."}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":18,"candidatesTokenCount":4,"totalTokenCount":22}},
			{"candidates":[{"content":{"parts":[{"functionCall":{"name":"list_files","args":{"path":"src"}}}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":18,"candidatesTokenCount":10,"totalTokenCount":28}}
		]`))
	}))
	defer server.Close()

	// Create client
	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "list files in src"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "list_files",
				Description: "List files in directory",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"path": map[string]interface{}{
							"type": "string",
						},
					},
				},
			},
		},
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if len(resp.ToolCalls) != 1 {
		t.Fatalf("Expected 1 tool call, got %d", len(resp.ToolCalls))
	}

	if resp.ToolCalls[0].Name != "list_files" {
		t.Errorf("Expected tool call name 'list_files', got '%s'", resp.ToolCalls[0].Name)
	}

	if path, ok := resp.ToolCalls[0].Arguments["path"].(string); !ok || path != "src" {
		t.Errorf("Expected path argument 'src', got %v", resp.ToolCalls[0].Arguments["path"])
	}
}

func TestGeminiClient_GenerateCompletion_InvalidAPIKey(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusBadRequest)
		_, _ = w.Write([]byte(`{"error": {"code": 400, "message": "API key not valid", "status": "INVALID_ARGUMENT"}}`))
	}))
	defer server.Close()

	// Create client
	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "invalid-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	// Execute
	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify
	if err == nil {
		t.Fatal("Expected error for invalid API key, got nil")
	}
}

func TestGeminiClient_GenerateCompletion_SafetyBlocked(t *testing.T) {
	// Setup mock server - Gemini may block responses for safety reasons
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Send JSON array streaming response with safety finish reason
		w.Header().Set("Content-Type", "application/json")
		_, _ = w.Write([]byte(`[
			{"candidates":[{"content":{"parts":[{"text":"I cannot"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":2,"totalTokenCount":12}},
			{"candidates":[{"content":{"parts":[],"role":"model"},"finishReason":"SAFETY","index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":3,"totalTokenCount":13}}
		]`))
	}))
	defer server.Close()

	// Create client
	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	// Execute
	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "potentially unsafe content"},
		},
	})

	// Verify - should return error for safety-blocked content
	if err == nil {
		t.Fatal("Expected error for safety-blocked content, got nil")
	}
}

func TestGeminiClient_GenerateCompletion_RateLimitRetry(t *testing.T) {
	callCount := 0

	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		callCount++

		// First call returns rate limit error
		if callCount == 1 {
			w.WriteHeader(http.StatusTooManyRequests)
			_, _ = w.Write([]byte(`{"error": {"code": 429, "message": "Resource exhausted", "status": "RESOURCE_EXHAUSTED"}}`))
			return
		}

		// Second call succeeds with JSON array streaming response
		w.Header().Set("Content-Type", "application/json")
		_, _ = w.Write([]byte(`[
			{"candidates":[{"content":{"parts":[{"text":"success after"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":2,"totalTokenCount":12}},
			{"candidates":[{"content":{"parts":[{"text":" retry"}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":5,"totalTokenCount":15}}
		]`))
	}))
	defer server.Close()

	// Create retry client with short delays
	retryClient := NewRetryClient(&RetryConfig{
		MaxAttempts:       2,
		Multiplier:        1,
		MaxWaitPerAttempt: 10 * time.Millisecond,
		MaxTotalWait:      100 * time.Millisecond,
	})

	// Create client
	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, retryClient)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error after retry, got %v", err)
	}

	if resp.Content != "success after retry" {
		t.Errorf("Expected content 'success after retry', got '%s'", resp.Content)
	}

	if callCount != 2 {
		t.Errorf("Expected 2 calls (1 fail + 1 success), got %d", callCount)
	}
}

func TestGeminiClient_SupportsTools(t *testing.T) {
	client := NewGeminiClient(config.LLMConfig{
		APIKey: "test-key",
		Model:  "gemini-pro",
	}, nil)

	if !client.SupportsTools() {
		t.Error("Gemini client should support tools")
	}
}

func TestGeminiClient_GetProvider(t *testing.T) {
	client := NewGeminiClient(config.LLMConfig{
		APIKey: "test-key",
		Model:  "gemini-pro",
	}, nil)

	if provider := client.GetProvider(); provider != "gemini" {
		t.Errorf("Expected provider 'gemini', got '%s'", provider)
	}
}

func TestGeminiClient_GenerateCompletion_NoCandidates(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Send JSON array with empty candidates
		w.Header().Set("Content-Type", "application/json")
		_, _ = w.Write([]byte(`[{"candidates":[]}]`))
	}))
	defer server.Close()

	// Create client
	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	// Execute
	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify
	if err == nil {
		t.Fatal("Expected error for no candidates, got nil")
	}
}

func TestGeminiClient_GenerateCompletion_MultipleParts(t *testing.T) {
	// Test response with multiple text parts in a single chunk
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Send JSON array streaming response with multiple parts in one chunk
		w.Header().Set("Content-Type", "application/json")
		_, _ = w.Write([]byte(`[{"candidates":[{"content":{"parts":[{"text":"First part. "},{"text":"Second part."}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":8,"totalTokenCount":18}}]`))
	}))
	defer server.Close()

	// Create client
	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Should concatenate all text parts
	expected := "First part. Second part."
	if resp.Content != expected {
		t.Errorf("Expected content '%s', got '%s'", expected, resp.Content)
	}
}

func TestGeminiClient_Streaming_MultipleChunks(t *testing.T) {
	// Test large response split across multiple JSON array chunks
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Send multiple text chunks as JSON array
		_, _ = w.Write([]byte(`[
			{"candidates":[{"content":{"parts":[{"text":"This is"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":2,"totalTokenCount":12}},
			{"candidates":[{"content":{"parts":[{"text":" a large"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":3,"totalTokenCount":13}},
			{"candidates":[{"content":{"parts":[{"text":" response"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":4,"totalTokenCount":14}},
			{"candidates":[{"content":{"parts":[{"text":" split"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":5,"totalTokenCount":15}},
			{"candidates":[{"content":{"parts":[{"text":" across"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":6,"totalTokenCount":16}},
			{"candidates":[{"content":{"parts":[{"text":" multiple"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":7,"totalTokenCount":17}},
			{"candidates":[{"content":{"parts":[{"text":" chunks."}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":8,"totalTokenCount":18}}
		]`))
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "generate a large response"},
		},
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	expected := "This is a large response split across multiple chunks."
	if resp.Content != expected {
		t.Errorf("Expected content '%s', got '%s'", expected, resp.Content)
	}

	if resp.Usage.OutputTokens != 8 {
		t.Errorf("Expected 8 output tokens, got %d", resp.Usage.OutputTokens)
	}
}

func TestGeminiClient_Streaming_IncompleteStream(t *testing.T) {
	// Test incomplete stream (connection closes without finishReason)
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Send some text but never send finishReason (valid JSON array though)
		_, _ = w.Write([]byte(`[{"candidates":[{"content":{"parts":[{"text":"Partial response"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":2,"totalTokenCount":12}}]`))
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	// Should succeed with partial content (no error - scanner just reaches EOF)
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	if err != nil {
		t.Fatalf("Expected no error for incomplete stream, got %v", err)
	}

	// Should have accumulated partial content
	if resp.Content != "Partial response" {
		t.Errorf("Expected partial content, got '%s'", resp.Content)
	}
}

func TestGeminiClient_Streaming_MalformedChunk(t *testing.T) {
	// Test malformed JSON in stream
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Send malformed JSON array (broken)
		_, _ = w.Write([]byte(`[{"candidates":[{"content":{"parts":[{"text":"broken`))
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Should return error for malformed JSON
	if err == nil {
		t.Fatal("Expected error for malformed chunk, got nil")
	}
}

func TestGeminiClient_Streaming_APIErrorInStream(t *testing.T) {
	// Test API error in stream chunk
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// Send JSON array with error in chunk
		_, _ = w.Write([]byte(`[
			{"candidates":[{"content":{"parts":[{"text":"Starting"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":1,"totalTokenCount":11}},
			{"error":{"code":400,"message":"Invalid request","status":"INVALID_ARGUMENT"}}
		]`))
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Should return error from stream
	if err == nil {
		t.Fatal("Expected error for API error in stream, got nil")
	}
}

func TestGeminiClient_Streaming_FunctionCallComplete(t *testing.T) {
	// Test that function calls arrive complete (not partial like Anthropic/OpenAI)
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// JSON array with text chunks and function call
		_, _ = w.Write([]byte(`[
			{"candidates":[{"content":{"parts":[{"text":"I'll"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":15,"candidatesTokenCount":1,"totalTokenCount":16}},
			{"candidates":[{"content":{"parts":[{"text":" search"}],"role":"model"},"finishReason":null,"index":0}],"usageMetadata":{"promptTokenCount":15,"candidatesTokenCount":2,"totalTokenCount":17}},
			{"candidates":[{"content":{"parts":[{"functionCall":{"name":"search","args":{"query":"example","limit":10}}}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":15,"candidatesTokenCount":8,"totalTokenCount":23}}
		]`))
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "search for example"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "search",
				Description: "Search",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"query": map[string]interface{}{"type": "string"},
						"limit": map[string]interface{}{"type": "integer"},
					},
				},
			},
		},
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if len(resp.ToolCalls) != 1 {
		t.Fatalf("Expected 1 tool call, got %d", len(resp.ToolCalls))
	}

	if resp.ToolCalls[0].Name != "search" {
		t.Errorf("Expected tool call name 'search', got '%s'", resp.ToolCalls[0].Name)
	}

	if query, ok := resp.ToolCalls[0].Arguments["query"].(string); !ok || query != "example" {
		t.Errorf("Expected query argument 'example', got %v", resp.ToolCalls[0].Arguments["query"])
	}

	if limit, ok := resp.ToolCalls[0].Arguments["limit"].(float64); !ok || int(limit) != 10 {
		t.Errorf("Expected limit argument 10, got %v", resp.ToolCalls[0].Arguments["limit"])
	}

	// Text should also be accumulated
	if resp.Content != "I'll search" {
		t.Errorf("Expected text content 'I'll search', got '%s'", resp.Content)
	}
}

func TestGeminiClient_Streaming_MultipleFunctionCalls(t *testing.T) {
	// Test multiple function calls in a single chunk
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "application/json")
		// JSON array with multiple function calls in one chunk
		_, _ = w.Write([]byte(`[{"candidates":[{"content":{"parts":[{"functionCall":{"name":"search","args":{"query":"cats"}}},{"functionCall":{"name":"search","args":{"query":"dogs"}}}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{"promptTokenCount":20,"candidatesTokenCount":10,"totalTokenCount":30}}]`))
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-pro",
	}, nil)

	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "search for cats and dogs"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "search",
				Description: "Search",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"query": map[string]interface{}{"type": "string"},
					},
				},
			},
		},
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if len(resp.ToolCalls) != 2 {
		t.Fatalf("Expected 2 tool calls, got %d", len(resp.ToolCalls))
	}

	if resp.ToolCalls[0].Name != "search" || resp.ToolCalls[1].Name != "search" {
		t.Errorf("Expected both tool calls to be 'search', got '%s' and '%s'", resp.ToolCalls[0].Name, resp.ToolCalls[1].Name)
	}

	if query0, ok := resp.ToolCalls[0].Arguments["query"].(string); !ok || query0 != "cats" {
		t.Errorf("Expected first query 'cats', got %v", resp.ToolCalls[0].Arguments["query"])
	}

	if query1, ok := resp.ToolCalls[1].Arguments["query"].(string); !ok || query1 != "dogs" {
		t.Errorf("Expected second query 'dogs', got %v", resp.ToolCalls[1].Arguments["query"])
	}
}
</file>
<file path="internal/llm/openai.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/user/gendocs/internal/config"
)

// OpenAIClient implements LLMClient for OpenAI-compatible APIs
type OpenAIClient struct {
	*BaseLLMClient
	apiKey  string
	baseURL string
	model   string
}

// openaiRequest represents the request body for OpenAI API
type openaiRequest struct {
	Model       string          `json:"model"`
	Messages    []openaiMessage `json:"messages"`
	MaxTokens   int             `json:"max_tokens"`
	Temperature float64         `json:"temperature"`
	Tools       []openaiTool    `json:"tools,omitempty"`
	Stream      bool            `json:"stream,omitempty"`
}

// openaiMessage represents a message in OpenAI format
type openaiMessage struct {
	Role       string           `json:"role"`
	Content    string           `json:"content"`
	ToolCalls  []openaiToolCall `json:"tool_calls,omitempty"`
	ToolCallID string           `json:"tool_call_id,omitempty"`
}

// openaiTool represents a tool definition in OpenAI format
type openaiTool struct {
	Type     string             `json:"type"`
	Function openaiToolFunction `json:"function"`
}

// openaiToolFunction represents tool function parameters
type openaiToolFunction struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Parameters  map[string]interface{} `json:"parameters"`
}

// openaiToolCall represents a tool call in OpenAI format
type openaiToolCall struct {
	ID       string             `json:"id"`
	Type     string             `json:"type"`
	Function openaiToolCallFunc `json:"function"`
}

// openaiToolCallFunc represents function call details
type openaiToolCallFunc struct {
	Name      string `json:"name"`
	Arguments string `json:"arguments"`
}

// openaiUsage represents token usage
type openaiUsage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

// openaiStreamChunk represents a single chunk in OpenAI's streaming response
type openaiStreamChunk struct {
	ID      string               `json:"id"`
	Object  string               `json:"object"`
	Created int64                `json:"created"`
	Model   string               `json:"model"`
	Choices []openaiStreamChoice `json:"choices"`
}

// openaiStreamChoice represents a choice in streaming chunks
type openaiStreamChoice struct {
	Index        int               `json:"index"`
	Delta        openaiStreamDelta `json:"delta"`
	FinishReason string            `json:"finish_reason,omitempty"`
}

// openaiStreamDelta represents the delta field in streaming chunks
type openaiStreamDelta struct {
	Content   string                `json:"content,omitempty"`
	Role      string                `json:"role,omitempty"`
	ToolCalls []openaiToolCallDelta `json:"tool_calls,omitempty"`
}

// openaiToolCallDelta represents a tool call in the delta
type openaiToolCallDelta struct {
	Index    int                     `json:"index"`
	Function openaiToolCallFuncDelta `json:"function,omitempty"`
}

// openaiToolCallFuncDelta represents function call details in streaming
type openaiToolCallFuncDelta struct {
	Name      string `json:"name,omitempty"`
	Arguments string `json:"arguments,omitempty"`
}

// openaiAccumulator builds CompletionResponse from OpenAI streaming chunks
type openaiAccumulator struct {
	content      strings.Builder
	toolCalls    []openaiToolCall
	partialArgs  map[int]*strings.Builder // Accumulates arguments by index
	usage        openaiUsage
	finishReason string
	complete     bool
}

// newOpenAIAccumulator creates a new accumulator
func newOpenAIAccumulator() *openaiAccumulator {
	return &openaiAccumulator{
		partialArgs: make(map[int]*strings.Builder),
	}
}

// HandleChunk processes a single streaming chunk
func (a *openaiAccumulator) HandleChunk(data []byte) error {
	// Check for [DONE] marker
	if IsSSEDone(data) {
		a.complete = true
		return nil
	}

	var chunk openaiStreamChunk
	if err := ParseSSEData(data, &chunk); err != nil {
		return fmt.Errorf("failed to parse chunk: %w", err)
	}

	if len(chunk.Choices) == 0 {
		return nil
	}

	choice := chunk.Choices[0]
	delta := choice.Delta

	// Accumulate content
	if delta.Content != "" {
		a.content.WriteString(delta.Content)
	}

	// Accumulate tool calls
	for _, tc := range delta.ToolCalls {
		idx := tc.Index

		// Ensure toolCalls slice is large enough
		for len(a.toolCalls) <= idx {
			a.toolCalls = append(a.toolCalls, openaiToolCall{
				Type: "function",
			})
		}

		// Set ID if not set
		if a.toolCalls[idx].ID == "" {
			a.toolCalls[idx].ID = chunk.ID + "-" + fmt.Sprintf("%d", idx)
		}

		// Accumulate function name
		if tc.Function.Name != "" {
			a.toolCalls[idx].Function.Name = tc.Function.Name
		}

		// Accumulate arguments incrementally
		if tc.Function.Arguments != "" {
			if _, exists := a.partialArgs[idx]; !exists {
				a.partialArgs[idx] = &strings.Builder{}
			}
			a.partialArgs[idx].WriteString(tc.Function.Arguments)
		}
	}

	// Check for completion
	if choice.FinishReason != "" {
		a.finishReason = choice.FinishReason

		// Parse accumulated tool arguments
		for idx, argBuilder := range a.partialArgs {
			if idx < len(a.toolCalls) && argBuilder.Len() > 0 {
				var args map[string]interface{}
				if err := json.Unmarshal([]byte(argBuilder.String()), &args); err != nil {
					return fmt.Errorf("failed to parse tool %d arguments: %w", idx, err)
				}
				a.toolCalls[idx].Function.Arguments = argBuilder.String()
			}
		}
	}

	return nil
}

// Build constructs the final CompletionResponse
func (a *openaiAccumulator) Build() CompletionResponse {
	result := CompletionResponse{
		Content: a.content.String(),
		Usage: TokenUsage{
			InputTokens:  a.usage.PromptTokens,
			OutputTokens: a.usage.CompletionTokens,
			TotalTokens:  a.usage.TotalTokens,
		},
	}

	// Convert tool calls
	if len(a.toolCalls) > 0 {
		result.ToolCalls = make([]ToolCall, len(a.toolCalls))
		for i, tc := range a.toolCalls {
			var args map[string]interface{}
			if tc.Function.Arguments != "" {
				_ = json.Unmarshal([]byte(tc.Function.Arguments), &args)
			}

			result.ToolCalls[i] = ToolCall{
				Name:      tc.Function.Name,
				Arguments: args,
			}
		}
	}

	return result
}

// IsComplete returns true when stream is complete
func (a *openaiAccumulator) IsComplete() bool {
	return a.complete || a.finishReason != ""
}

// NewOpenAIClient creates a new OpenAI client
func NewOpenAIClient(cfg config.LLMConfig, retryClient *RetryClient) *OpenAIClient {
	baseURL := cfg.BaseURL
	if baseURL == "" {
		baseURL = "https://api.openai.com/v1"
	}

	return &OpenAIClient{
		BaseLLMClient: NewBaseLLMClient(retryClient),
		apiKey:        cfg.APIKey,
		baseURL:       baseURL,
		model:         cfg.Model,
	}
}

// GenerateCompletion generates a completion from OpenAI
func (c *OpenAIClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	// Convert to OpenAI format
	oaReq := c.convertRequest(req)

	jsonData, err := json.Marshal(oaReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	url := fmt.Sprintf("%s/chat/completions", c.baseURL)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonData))
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.apiKey))

	// Execute with retry
	resp, err := c.retryClient.Do(httpReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("request failed: %w", err)
	}
	defer func() { _ = resp.Body.Close() }()

	// Check for error status
	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return CompletionResponse{}, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))
	}

	// Parse streaming response
	return c.parseStreamingResponse(resp.Body)
}

// parseStreamingResponse parses OpenAI's SSE stream and builds the response
func (c *OpenAIClient) parseStreamingResponse(body io.ReadCloser) (CompletionResponse, error) {
	parser := NewSSEParser(body)
	accumulator := newOpenAIAccumulator()

	for {
		event, err := parser.NextEvent()
		if err == io.EOF {
			break
		}
		if err != nil {
			return CompletionResponse{}, fmt.Errorf("stream parsing error: %w", err)
		}

		// Handle event data (OpenAI doesn't use event types, all chunks are in data field)
		if err := accumulator.HandleChunk(event.Data); err != nil {
			return CompletionResponse{}, fmt.Errorf("chunk handling error: %w", err)
		}

		// Check if stream is complete
		if accumulator.IsComplete() {
			break
		}
	}

	if !accumulator.IsComplete() {
		return CompletionResponse{}, fmt.Errorf("stream ended unexpectedly")
	}

	return accumulator.Build(), nil
}

// SupportsTools returns true
func (c *OpenAIClient) SupportsTools() bool {
	return true
}

// GetProvider returns the provider name
func (c *OpenAIClient) GetProvider() string {
	return "openai"
}

// convertRequest converts internal request to OpenAI format
func (c *OpenAIClient) convertRequest(req CompletionRequest) openaiRequest {
	messages := []openaiMessage{}

	// Add system prompt if provided
	if req.SystemPrompt != "" {
		messages = append(messages, openaiMessage{
			Role:    "system",
			Content: req.SystemPrompt,
		})
	}

	// Add messages
	for _, msg := range req.Messages {
		messages = append(messages, openaiMessage{
			Role:    msg.Role,
			Content: msg.Content,
		})
	}

	oaReq := openaiRequest{
		Model:       c.model,
		Messages:    messages,
		MaxTokens:   req.MaxTokens,
		Temperature: req.Temperature,
	}

	// Add tools if provided
	if len(req.Tools) > 0 {
		oaReq.Tools = make([]openaiTool, len(req.Tools))
		for i, tool := range req.Tools {
			oaReq.Tools[i] = openaiTool{
				Type: "function",
				Function: openaiToolFunction{
					Name:        tool.Name,
					Description: tool.Description,
					Parameters:  tool.Parameters,
				},
			}
		}
	}

	oaReq.Stream = true // Enable streaming response

	return oaReq
}
</file>
<file path="internal/llm/openai_bench_test.go">
package llm

import (
	"context"
	"fmt"
	"net/http"
	"net/http/httptest"
	"testing"
	"time"

	"github.com/user/gendocs/internal/config"
)

// BenchmarkOpenAI_SmallResponse benchmarks a small single-chunk response
func BenchmarkOpenAI_SmallResponse(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Small response - single content chunk
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"content":"Hello!"},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: [DONE]`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkOpenAI_MediumResponse benchmarks a medium multi-chunk response
func BenchmarkOpenAI_MediumResponse(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Medium response - multiple chunks
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		for i := 0; i < 10; i++ {
			_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"content":"This is chunk `+fmt.Sprint(i)+` of the response. "},"finish_reason":null}]}`)
			_, _ = fmt.Fprintln(w)
		}
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: [DONE]`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Tell me a story"},
		},
		MaxTokens:   1000,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkOpenAI_LargeResponse benchmarks a large response with many chunks
func BenchmarkOpenAI_LargeResponse(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Large response - many chunks (simulating ~50KB response)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		for i := 0; i < 100; i++ {
			_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"content":"This is a longer chunk of text that represents a substantial part of the response. Chunk `+fmt.Sprint(i)+` contains useful information. "},"finish_reason":null}]}`)
			_, _ = fmt.Fprintln(w)
		}
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: [DONE]`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Analyze this codebase in detail"},
		},
		MaxTokens:   4000,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkOpenAI_ToolCall benchmarks a response with a tool call
func BenchmarkOpenAI_ToolCall(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Tool call response with delta accumulation
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"tool_calls":[{"index":0,"id":"call_123","type":"function","function":{"name":"read_file","arguments":""}}]},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"tool_calls":[{"index":0,"function":{"arguments":"{\"file_path\":\"src/main.go\",\"start_line\":1,\"end_line\":100}"}}]},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{},"finish_reason":"tool_calls"}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: [DONE]`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Read the main.go file"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "read_file",
				Description: "Read a file",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"file_path": map[string]interface{}{
							"type":        "string",
							"description": "Path to the file",
						},
						"start_line": map[string]interface{}{
							"type": "integer",
						},
						"end_line": map[string]interface{}{
							"type": "integer",
						},
					},
					"required": []string{"file_path"},
				},
			},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkOpenAI_MultipleToolCalls benchmarks multiple tool calls in one response
func BenchmarkOpenAI_MultipleToolCalls(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Multiple tool calls
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"tool_calls":[{"index":0,"id":"call_123","type":"function","function":{"name":"read_file","arguments":""}},{"index":1,"id":"call_124","type":"function","function":{"name":"list_files","arguments":""}}]},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"tool_calls":[{"index":0,"function":{"arguments":"{\"path\":\"file1.go\"}"}}]},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"tool_calls":[{"index":1,"function":{"arguments":"{\"path\":\"src\"}"}}]},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{},"finish_reason":"tool_calls"}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: [DONE]`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Read file1.go and list files in src"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "read_file",
				Description: "Read a file",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"path": map[string]interface{}{
							"type": "string",
						},
					},
					"required": []string{"path"},
				},
			},
			{
				Name:        "list_files",
				Description: "List files",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"path": map[string]interface{}{
							"type": "string",
						},
					},
					"required": []string{"path"},
				},
			},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
	}
}

// BenchmarkOpenAI_TimeToFirstToken measures time to receive first content
func BenchmarkOpenAI_TimeToFirstToken(b *testing.B) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		// Simulate network delay before first chunk
		time.Sleep(10 * time.Millisecond)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{"content":"Response"},"finish_reason":null}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}`)
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, `data: [DONE]`)
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	req := CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	}

	b.ResetTimer()

	for i := 0; i < b.N; i++ {
		start := time.Now()
		_, err := client.GenerateCompletion(context.Background(), req)
		if err != nil {
			b.Fatalf("GenerateCompletion failed: %v", err)
		}
		elapsed := time.Since(start)
		b.ReportMetric(float64(elapsed.Nanoseconds()), "ns/op")
	}
}
</file>
<file path="internal/llm/openai_test.go">
package llm

import (
	"context"
	"fmt"
	"net/http"
	"net/http/httptest"
	"strings"
	"testing"
	"time"

	"github.com/user/gendocs/internal/config"
)

func TestOpenAIClient_GenerateCompletion_Success(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Validate request
		if r.Method != "POST" {
			t.Errorf("Expected POST request, got %s", r.Method)
		}

		if auth := r.Header.Get("Authorization"); auth != "Bearer test-key" {
			t.Errorf("Expected Authorization header 'Bearer test-key', got '%s'", auth)
		}

		// Send streaming response
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"test response\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: [DONE]")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	// Create client
	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "You are a test assistant",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if resp.Content != "test response" {
		t.Errorf("Expected content 'test response', got '%s'", resp.Content)
	}

	if resp.Usage.InputTokens != 0 {
		t.Errorf("Expected 0 input tokens (not tracked in streaming), got %d", resp.Usage.InputTokens)
	}

	if resp.Usage.OutputTokens != 0 {
		t.Errorf("Expected 0 output tokens (not tracked in streaming), got %d", resp.Usage.OutputTokens)
	}
}

func TestOpenAIClient_GenerateCompletion_WithToolCalls(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Send streaming response with tool call
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"id\":\"call_123\",\"type\":\"function\",\"function\":{\"name\":\"read_file\",\"arguments\":\"\"}}]},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"arguments\":\"{\\\"file_path\\\":\\\"test.go\\\"}\"}}]},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"tool_calls\"}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: [DONE]")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	// Create client
	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "You are a test assistant",
		Messages: []Message{
			{Role: "user", Content: "read test.go"},
		},
		Tools: []ToolDefinition{
			{
				Name:        "read_file",
				Description: "Read a file",
				Parameters: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"file_path": map[string]interface{}{
							"type": "string",
						},
					},
				},
			},
		},
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if len(resp.ToolCalls) != 1 {
		t.Fatalf("Expected 1 tool call, got %d", len(resp.ToolCalls))
	}

	if resp.ToolCalls[0].Name != "read_file" {
		t.Errorf("Expected tool call name 'read_file', got '%s'", resp.ToolCalls[0].Name)
	}

	if filePath, ok := resp.ToolCalls[0].Arguments["file_path"].(string); !ok || filePath != "test.go" {
		t.Errorf("Expected file_path argument 'test.go', got %v", resp.ToolCalls[0].Arguments["file_path"])
	}
}

func TestOpenAIClient_GenerateCompletion_InvalidAPIKey(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusUnauthorized)
		_, _ = w.Write([]byte(`{"error": {"message": "Invalid API key", "type": "invalid_request_error"}}`))
	}))
	defer server.Close()

	// Create client
	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "invalid-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	// Execute
	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify
	if err == nil {
		t.Fatal("Expected error for invalid API key, got nil")
	}
}

func TestOpenAIClient_GenerateCompletion_RateLimitRetry(t *testing.T) {
	callCount := 0

	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		callCount++

		// First call returns rate limit error
		if callCount == 1 {
			w.WriteHeader(http.StatusTooManyRequests)
			_, _ = w.Write([]byte(`{"error": {"message": "Rate limit exceeded", "type": "rate_limit_error"}}`))
			return
		}

		// Second call succeeds with streaming
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"success after retry\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: [DONE]")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	// Create retry client with short delays
	retryClient := NewRetryClient(&RetryConfig{
		MaxAttempts:       2,
		Multiplier:        1,
		MaxWaitPerAttempt: 10 * time.Millisecond,
		MaxTotalWait:      100 * time.Millisecond,
	})

	// Create client
	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, retryClient)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify
	if err != nil {
		t.Fatalf("Expected no error after retry, got %v", err)
	}

	if resp.Content != "success after retry" {
		t.Errorf("Expected content 'success after retry', got '%s'", resp.Content)
	}

	if callCount != 2 {
		t.Errorf("Expected 2 calls (1 fail + 1 success), got %d", callCount)
	}
}

func TestOpenAIClient_SupportsTools(t *testing.T) {
	client := NewOpenAIClient(config.LLMConfig{
		APIKey: "test-key",
		Model:  "gpt-4",
	}, nil)

	if !client.SupportsTools() {
		t.Error("OpenAI client should support tools")
	}
}

func TestOpenAIClient_GetProvider(t *testing.T) {
	client := NewOpenAIClient(config.LLMConfig{
		APIKey: "test-key",
		Model:  "gpt-4",
	}, nil)

	if provider := client.GetProvider(); provider != "openai" {
		t.Errorf("Expected provider 'openai', got '%s'", provider)
	}
}

func TestOpenAIClient_GenerateCompletion_EmptyResponse(t *testing.T) {
	// Setup mock server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Send streaming response with empty content
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: [DONE]")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	// Create client
	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	// Execute
	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify - empty response should not error, just return empty content
	if err != nil {
		t.Fatalf("Expected no error for empty response, got %v", err)
	}

	if resp.Content != "" {
		t.Errorf("Expected empty content, got '%s'", resp.Content)
	}
}

func TestOpenAIClient_GenerateCompletion_ContextCanceled(t *testing.T) {
	// Setup mock server with delay
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// This should never complete due to context cancellation
		<-r.Context().Done()
	}))
	defer server.Close()

	// Create client
	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	// Create canceled context
	ctx, cancel := context.WithCancel(context.Background())
	cancel() // Cancel immediately

	// Execute
	_, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Verify
	if err == nil {
		t.Fatal("Expected error for canceled context, got nil")
	}

	// Error should be wrapped, so use errors.Is
	if !strings.Contains(err.Error(), "context canceled") {
		t.Errorf("Expected context.Canceled error, got %v", err)
	}
}

func TestOpenAIClient_Streaming_MultipleChunks(t *testing.T) {
	// Test large response split across multiple chunks
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)

		// Send multiple content chunks
		chunks := []string{"This is ", "a large ", "response ", "split across ", "multiple chunks."}
		for _, chunk := range chunks {
			_, _ = fmt.Fprintf(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"%s\"},\"finish_reason\":null}]}\n", chunk)
			_, _ = fmt.Fprintln(w)
		}

		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: [DONE]")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "generate a large response"},
		},
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	expected := "This is a large response split across multiple chunks."
	if resp.Content != expected {
		t.Errorf("Expected content '%s', got '%s'", expected, resp.Content)
	}
}

func TestOpenAIClient_Streaming_ToolCallsWithChunks(t *testing.T) {
	// Test tool calls with arguments split across multiple chunks
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)

		// Start tool call
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"id\":\"call_123\",\"type\":\"function\",\"function\":{\"name\":\"search\",\"arguments\":\"\"}}]},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)

		// Send arguments in multiple chunks
		argChunks := []string{
			"{\"query\":",
			"\"large search request ",
			"with multiple parameters",
			" split across chunks\"}",
		}
		for _, chunk := range argChunks {
			escapedChunk := strings.ReplaceAll(chunk, "\"", "\\\"")
			_, _ = fmt.Fprintf(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"arguments\":\"%s\"}}]},\"finish_reason\":null}]}\n", escapedChunk)
			_, _ = fmt.Fprintln(w)
		}

		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"tool_calls\"}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: [DONE]")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "search"},
		},
		Tools: []ToolDefinition{
			{Name: "search", Description: "Search", Parameters: map[string]interface{}{}},
		},
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if len(resp.ToolCalls) != 1 {
		t.Fatalf("Expected 1 tool call, got %d", len(resp.ToolCalls))
	}

	if resp.ToolCalls[0].Name != "search" {
		t.Errorf("Expected tool name 'search', got '%s'", resp.ToolCalls[0].Name)
	}

	expectedQuery := "large search request with multiple parameters split across chunks"
	if query, ok := resp.ToolCalls[0].Arguments["query"].(string); !ok || query != expectedQuery {
		t.Errorf("Expected query '%s', got '%v'", expectedQuery, resp.ToolCalls[0].Arguments["query"])
	}
}

func TestOpenAIClient_Streaming_IncompleteStream(t *testing.T) {
	// Test incomplete stream (missing [DONE] marker and finish_reason)
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"partial response\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		// Stream ends here without [DONE] or finish_reason
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	// Should return an error due to incomplete stream
	if err == nil {
		t.Fatal("Expected error for incomplete stream, got nil")
	}
}

func TestOpenAIClient_Streaming_MalformedChunk(t *testing.T) {
	// Test malformed JSON in chunk
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {invalid json here}")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	_, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "hello"},
		},
	})

	if err == nil {
		t.Fatal("Expected error for malformed chunk, got nil")
	}
}

func TestOpenAIClient_Streaming_MultipleToolCalls(t *testing.T) {
	// Test multiple tool calls in streaming
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)

		// First tool call
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"id\":\"call_1\",\"type\":\"function\",\"function\":{\"name\":\"read_file\",\"arguments\":\"\"}}]},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"arguments\":\"{\\\"path\\\":\\\"file1.txt\\\"}\"}}]},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)

		// Second tool call
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":1,\"id\":\"call_2\",\"type\":\"function\",\"function\":{\"name\":\"read_file\",\"arguments\":\"\"}}]},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":1,\"function\":{\"arguments\":\"{\\\"path\\\":\\\"file2.txt\\\"}\"}}]},\"finish_reason\":null}]}")
		_, _ = fmt.Fprintln(w)

		_, _ = fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"tool_calls\"}]}")
		_, _ = fmt.Fprintln(w)
		_, _ = fmt.Fprintln(w, "data: [DONE]")
		_, _ = fmt.Fprintln(w)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	resp, err := client.GenerateCompletion(context.Background(), CompletionRequest{
		SystemPrompt: "test",
		Messages: []Message{
			{Role: "user", Content: "read two files"},
		},
		Tools: []ToolDefinition{
			{Name: "read_file", Description: "Read file", Parameters: map[string]interface{}{}},
		},
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if len(resp.ToolCalls) != 2 {
		t.Fatalf("Expected 2 tool calls, got %d", len(resp.ToolCalls))
	}

	// Verify first tool call
	if resp.ToolCalls[0].Name != "read_file" {
		t.Errorf("Expected first tool call name 'read_file', got '%s'", resp.ToolCalls[0].Name)
	}

	// Verify second tool call
	if resp.ToolCalls[1].Name != "read_file" {
		t.Errorf("Expected second tool call name 'read_file', got '%s'", resp.ToolCalls[1].Name)
	}
}
</file>
<file path="internal/llm/retry_client.go">
package llm

import (
	"bytes"
	"context"
	"crypto/tls"
	"fmt"
	"io"
	"math"
	"net/http"
	"time"
)

// RetryConfig holds retry and connection pooling configuration.
//
// The connection pooling settings are optimized for LLM API usage, where
// requests to the same API endpoint are common. Keeping connections open
// and reusing them significantly reduces latency by eliminating the need
// for repeated TCP and TLS handshakes.
type RetryConfig struct {
	// Retry settings
	MaxAttempts       int           // Maximum number of retry attempts (default: 5)
	Multiplier        int           // Exponential backoff multiplier in seconds (default: 1)
	MaxWaitPerAttempt time.Duration // Maximum wait time per retry attempt (default: 60s)
	MaxTotalWait      time.Duration // Maximum total wait time across all retries (default: 300s)

	// Connection Pooling Settings
	//
	// These settings control how HTTP connections are managed and reused.
	// Proper connection pooling is critical for LLM API performance because:
	// - It reduces latency by reusing existing connections
	// - It minimizes TCP/TLS handshake overhead
	// - It improves throughput by maintaining connection pools to frequently-used hosts
	//
	// MaxIdleConns controls the maximum number of idle connections across ALL hosts.
	// This is the global pool size. Increasing this allows more connections to be kept
	// open simultaneously, useful when making requests to multiple different LLM providers.
	// Higher values = more memory usage but better performance for concurrent requests.
	// Default: 100, Range: 10-1000 recommended
	MaxIdleConns int

	// MaxIdleConnsPerHost controls the maximum number of idle connections PER HOST.
	// This is the pool size for each unique hostname (e.g., "api.openai.com").
	// LLM APIs typically benefit from higher values here because:
	// - Multiple concurrent requests to the same API endpoint are common
	// - Keeping multiple connections open allows for better request pipelining
	// - It prevents connection churn under high load
	// Default: 10, Range: 5-100 recommended. Set to 2-3x your expected concurrent request rate.
	MaxIdleConnsPerHost int

	// IdleConnTimeout controls how long an idle connection remains in the pool before
	// being closed. Idle connections that exceed this duration are pruned to free resources.
	// For LLM APIs with intermittent but bursty traffic, a longer timeout is beneficial
	// because it keeps connections available between bursts of activity.
	// Default: 90s, Range: 30s-300s recommended
	IdleConnTimeout time.Duration

	// TLSHandshakeTimeout specifies the maximum time to wait for a TLS handshake to complete.
	// TLS handshakes establish the secure connection and happen on new connections or when
	// a connection is being reused after a long idle period.
	// Default: 10s, Range: 5s-30s recommended
	TLSHandshakeTimeout time.Duration

	// ExpectContinueTimeout specifies the maximum time to wait for a server's "100 Continue"
	// response when sending a request with an Expect: 100-continue header.
	// This is an optimization for requests with large bodies (common in LLM APIs).
	// The timeout allows the server to indicate whether it will accept the request body
	// before the client sends it, saving bandwidth if the request will be rejected.
	// Default: 1s, Range: 1s-5s recommended
	ExpectContinueTimeout time.Duration

	// Transport allows providing a custom HTTP transport.
	//
	// If nil, a transport with optimized connection pooling settings will be created
	// using the fields above. This is recommended for most use cases.
	//
	// If set, the custom transport will be used directly and the connection pooling
	// fields above (MaxIdleConns, MaxIdleConnsPerHost, etc.) will be ignored.
	// Use this when you need complete control over HTTP transport behavior,
	// such as custom proxies, authentication, or advanced connection management.
	Transport http.RoundTripper
}

// DefaultRetryConfig returns default retry configuration with optimized connection pooling
func DefaultRetryConfig() *RetryConfig {
	return &RetryConfig{
		MaxAttempts:       5,
		Multiplier:        1,
		MaxWaitPerAttempt: 60 * time.Second,
		MaxTotalWait:      300 * time.Second,
		// Connection pooling defaults optimized for LLM APIs
		MaxIdleConns:          100,
		MaxIdleConnsPerHost:   10,
		IdleConnTimeout:       90 * time.Second,
		TLSHandshakeTimeout:   10 * time.Second,
		ExpectContinueTimeout: 1 * time.Second,
	}
}

// getTransport returns the appropriate HTTP transport for the given config
// If a custom transport is provided in the config, it will be used
// Otherwise, an optimized transport will be created using the config's connection pooling settings
func getTransport(config *RetryConfig) http.RoundTripper {
	if config.Transport != nil {
		// Use custom transport provided by user
		return config.Transport
	}
	// Create optimized transport with config settings
	return createOptimizedTransport(config)
}

// createOptimizedTransport creates an http.Transport with optimal settings for LLM API calls
// It configures connection pooling, timeouts, and HTTP/2 support for improved performance
func createOptimizedTransport(config *RetryConfig) *http.Transport {
	transport := &http.Transport{
		// Connection pooling settings
		MaxIdleConns:        config.MaxIdleConns,
		MaxIdleConnsPerHost: config.MaxIdleConnsPerHost,
		IdleConnTimeout:     config.IdleConnTimeout,

		// Timeout settings
		TLSHandshakeTimeout:   config.TLSHandshakeTimeout,
		ExpectContinueTimeout: config.ExpectContinueTimeout,

		// Force attempt HTTP/2 for HTTPS connections
		// Note: Go's http2.ConfigureTransport will enable HTTP/2 if supported
		ForceAttemptHTTP2: true,
	}

	// Configure TLS settings for optimal performance
	transport.TLSClientConfig = &tls.Config{
		// Use reasonable defaults for TLS
		MinVersion: tls.VersionTLS12,
		// Enable HTTP/2 properly (will be configured by http2.ConfigureTransport if available)
	}

	return transport
}

// ConnectionStats represents connection pool statistics and configuration
type ConnectionStats struct {
	// Transport type
	TransportType string // "http.Transport", "custom", or "unknown"

	// Connection pool configuration
	MaxIdleConns        int           // Maximum idle connections across all hosts
	MaxIdleConnsPerHost int           // Maximum idle connections per host
	IdleConnTimeout     time.Duration // Maximum idle time for a connection

	// Timeout configuration
	TLSHandshakeTimeout   time.Duration // TLS handshake timeout
	ExpectContinueTimeout time.Duration // Expect continue timeout

	// HTTP/2 support
	HTTP2Enabled bool // Whether HTTP/2 is enabled

	// TLS configuration
	TLSMinVersion uint16 // Minimum TLS version

	// Client configuration
	ClientTimeout time.Duration // Client timeout
}

// RetryClient wraps http.Client with retry logic
type RetryClient struct {
	client *http.Client
	config *RetryConfig
}

// NewRetryClient creates a new retry client
func NewRetryClient(config *RetryConfig) *RetryClient {
	if config == nil {
		config = DefaultRetryConfig()
	}

	return &RetryClient{
		client: &http.Client{
			Timeout:   180 * time.Second, // Default timeout
			Transport: getTransport(config),
		},
		config: config,
	}
}

// NewRetryClientWithTimeout creates a retry client with custom timeout
func NewRetryClientWithTimeout(timeout time.Duration, config *RetryConfig) *RetryClient {
	if config == nil {
		config = DefaultRetryConfig()
	}

	return &RetryClient{
		client: &http.Client{
			Timeout:   timeout,
			Transport: getTransport(config),
		},
		config: config,
	}
}

// Do executes an HTTP request with retry logic
func (rc *RetryClient) Do(req *http.Request) (*http.Response, error) {
	return rc.DoWithContext(req.Context(), req)
}

// DoWithContext executes an HTTP request with retry logic and context
func (rc *RetryClient) DoWithContext(ctx context.Context, req *http.Request) (*http.Response, error) {
	var resp *http.Response
	var err error

	// Read request body once and store for potential retries
	var bodyBytes []byte
	if req.Body != nil {
		bodyBytes, err = io.ReadAll(req.Body)
		_ = req.Body.Close()
		if err != nil {
			return nil, fmt.Errorf("failed to read request body: %w", err)
		}
	}

	totalStartTime := time.Now()

	for attempt := 0; attempt < rc.config.MaxAttempts; attempt++ {
		// Clone the request for each attempt
		reqClone := req.Clone(ctx)

		// Restore body for this attempt
		if len(bodyBytes) > 0 {
			reqClone.Body = io.NopCloser(bytes.NewReader(bodyBytes))
			reqClone.ContentLength = int64(len(bodyBytes))
		}

		resp, err = rc.client.Do(reqClone)

		// Check if we should NOT retry
		if err == nil && resp != nil {
			// Success on 2xx and 3xx
			// Also retry on 429 (Too Many Requests) and 5xx
			if resp.StatusCode < 500 && resp.StatusCode != 429 {
				return resp, nil
			}

			// For 4xx errors (except 429), don't retry
			if resp.StatusCode >= 400 && resp.StatusCode < 500 && resp.StatusCode != 429 {
				return resp, nil // Return the error response to caller
			}
		}

		// Calculate wait time with exponential backoff
		waitTime := rc.calculateWaitTime(attempt)

		// Check if we've exceeded max total wait time
		if time.Since(totalStartTime)+waitTime > rc.config.MaxTotalWait {
			break
		}

		// Wait before retry (but not after the last attempt)
		if attempt < rc.config.MaxAttempts-1 {
			select {
			case <-time.After(waitTime):
				// Continue to next attempt
			case <-ctx.Done():
				return nil, ctx.Err()
			}
		}
	}

	// All retries exhausted
	if err != nil {
		return nil, fmt.Errorf("request failed after %d attempts: %w", rc.config.MaxAttempts, err)
	}

	if resp != nil {
		return nil, fmt.Errorf("request failed with status %d after %d attempts", resp.StatusCode, rc.config.MaxAttempts)
	}

	return nil, fmt.Errorf("request failed after %d attempts", rc.config.MaxAttempts)
}

// calculateWaitTime calculates wait time using exponential backoff
func (rc *RetryClient) calculateWaitTime(attempt int) time.Duration {
	// Exponential backoff: 2^attempt * multiplier seconds
	baseWait := time.Duration(math.Pow(2, float64(attempt))) * time.Duration(rc.config.Multiplier) * time.Second

	// Cap at max wait per attempt
	if baseWait > rc.config.MaxWaitPerAttempt {
		baseWait = rc.config.MaxWaitPerAttempt
	}

	return baseWait
}

// SetTimeout updates the client timeout
func (rc *RetryClient) SetTimeout(timeout time.Duration) {
	rc.client.Timeout = timeout
}

// GetTimeout returns the current client timeout
func (rc *RetryClient) GetTimeout() time.Duration {
	return rc.client.Timeout
}

// GetConnectionStats returns connection pool statistics and configuration
// This is useful for debugging, monitoring, and verifying connection pooling settings
func (rc *RetryClient) GetConnectionStats() ConnectionStats {
	stats := ConnectionStats{
		ClientTimeout: rc.client.Timeout,
	}

	// Use type assertion to check if we have an http.Transport
	if transport, ok := rc.client.Transport.(*http.Transport); ok {
		stats.TransportType = "http.Transport"
		stats.MaxIdleConns = transport.MaxIdleConns
		stats.MaxIdleConnsPerHost = transport.MaxIdleConnsPerHost
		stats.IdleConnTimeout = transport.IdleConnTimeout
		stats.TLSHandshakeTimeout = transport.TLSHandshakeTimeout
		stats.ExpectContinueTimeout = transport.ExpectContinueTimeout
		stats.HTTP2Enabled = transport.ForceAttemptHTTP2

		// Get TLS configuration if available
		if transport.TLSClientConfig != nil {
			stats.TLSMinVersion = transport.TLSClientConfig.MinVersion
		}
	} else if rc.client.Transport != nil {
		stats.TransportType = "custom"
	} else {
		stats.TransportType = "unknown"
	}

	return stats
}

// CloseIdleConnections closes any idle connections in the transport's connection pool
// This can be useful to free up resources when the client will no longer be used
func (rc *RetryClient) CloseIdleConnections() {
	if transport, ok := rc.client.Transport.(*http.Transport); ok {
		transport.CloseIdleConnections()
	}
}
</file>
<file path="internal/llm/retry_client_bench_test.go">
package llm

import (
	"crypto/tls"
	"net/http"
	"net/http/httptest"
	"sync"
	"testing"
	"time"
)

// createUnoptimizedTransport creates a basic http.Transport without connection pooling optimization
// This simulates the old behavior before connection pooling was implemented
func createUnoptimizedTransport() *http.Transport {
	return &http.Transport{
		// No connection pooling configuration - uses Go's defaults
		// Default MaxIdleConns: 100 (but may not be optimal)
		// Default MaxIdleConnsPerHost: 2 (too low for LLM APIs)
		TLSClientConfig: &tls.Config{
			InsecureSkipVerify: true, // For test servers
		},
	}
}

// BenchmarkSequentialRequests_Optimized benchmarks sequential requests with optimized connection pooling
func BenchmarkSequentialRequests_Optimized(b *testing.B) {
	// Create test server
	server := httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte(`{"result": "success"}`))
	}))
	defer server.Close()

	// Create client with optimized transport
	client := NewRetryClient(nil)

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		req, err := http.NewRequest("GET", server.URL, nil)
		if err != nil {
			b.Error("Failed to create request:", err) //nolint:testinggoroutine
		}

		resp, err := client.Do(req)
		if err != nil {
			b.Error("Request failed:", err) //nolint:testinggoroutine
		}
		_ = resp.Body.Close()
	}
}

// BenchmarkSequentialRequests_Unoptimized benchmarks sequential requests without optimized connection pooling
func BenchmarkSequentialRequests_Unoptimized(b *testing.B) {
	// Create test server
	server := httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte(`{"result": "success"}`))
	}))
	defer server.Close()

	// Create client with unoptimized transport (simulating old behavior)
	httpClient := &http.Client{
		Timeout:   180 * time.Second,
		Transport: createUnoptimizedTransport(),
	}
	client := &RetryClient{
		client: httpClient,
		config: DefaultRetryConfig(),
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		req, err := http.NewRequest("GET", server.URL, nil)
		if err != nil {
			b.Error("Failed to create request:", err) //nolint:testinggoroutine
		}

		resp, err := client.Do(req)
		if err != nil {
			b.Error("Request failed:", err) //nolint:testinggoroutine
		}
		_ = resp.Body.Close()
	}
}

// BenchmarkConcurrentRequests_Optimized benchmarks concurrent requests with optimized connection pooling
func BenchmarkConcurrentRequests_Optimized(b *testing.B) {
	// Create test server
	server := httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Simulate API latency
		time.Sleep(10 * time.Millisecond)
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte(`{"result": "success"}`))
	}))
	defer server.Close()

	// Create client with optimized transport
	client := NewRetryClient(nil)

	b.ResetTimer()
	b.ReportAllocs()

	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			req, err := http.NewRequest("GET", server.URL, nil)
			if err != nil {
				b.Error("Failed to create request:", err) //nolint:testinggoroutine
			}

			resp, err := client.Do(req)
			if err != nil {
				b.Error("Request failed:", err) //nolint:testinggoroutine
			}
			_ = resp.Body.Close()
		}
	})
}

// BenchmarkConcurrentRequests_Unoptimized benchmarks concurrent requests without optimized connection pooling
func BenchmarkConcurrentRequests_Unoptimized(b *testing.B) {
	// Create test server
	server := httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Simulate API latency
		time.Sleep(10 * time.Millisecond)
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte(`{"result": "success"}`))
	}))
	defer server.Close()

	// Create client with unoptimized transport
	httpClient := &http.Client{
		Timeout:   180 * time.Second,
		Transport: createUnoptimizedTransport(),
	}
	client := &RetryClient{
		client: httpClient,
		config: DefaultRetryConfig(),
	}

	b.ResetTimer()
	b.ReportAllocs()

	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			req, err := http.NewRequest("GET", server.URL, nil)
			if err != nil {
				b.Error("Failed to create request:", err) //nolint:testinggoroutine
			}

			resp, err := client.Do(req)
			if err != nil {
				b.Error("Request failed:", err) //nolint:testinggoroutine
			}
			_ = resp.Body.Close()
		}
	})
}

// BenchmarkConcurrentRequests_10 benchmarks concurrent requests with 10 goroutines
func BenchmarkConcurrentRequests_10_Optimized(b *testing.B) {
	benchmarkConcurrentRequests(b, 10, true)
}

// BenchmarkConcurrentRequests_50 benchmarks concurrent requests with 50 goroutines
func BenchmarkConcurrentRequests_50_Optimized(b *testing.B) {
	benchmarkConcurrentRequests(b, 50, true)
}

// BenchmarkConcurrentRequests_100 benchmarks concurrent requests with 100 goroutines
func BenchmarkConcurrentRequests_100_Optimized(b *testing.B) {
	benchmarkConcurrentRequests(b, 100, true)
}

// BenchmarkConcurrentRequests_10_Unoptimized benchmarks concurrent requests with 10 goroutines (unoptimized)
func BenchmarkConcurrentRequests_10_Unoptimized(b *testing.B) {
	benchmarkConcurrentRequests(b, 10, false)
}

// BenchmarkConcurrentRequests_50_Unoptimized benchmarks concurrent requests with 50 goroutines (unoptimized)
func BenchmarkConcurrentRequests_50_Unoptimized(b *testing.B) {
	benchmarkConcurrentRequests(b, 50, false)
}

// BenchmarkConcurrentRequests_100_Unoptimized benchmarks concurrent requests with 100 goroutines (unoptimized)
func BenchmarkConcurrentRequests_100_Unoptimized(b *testing.B) {
	benchmarkConcurrentRequests(b, 100, false)
}

// benchmarkConcurrentRequests is a helper function for concurrent request benchmarks
func benchmarkConcurrentRequests(b *testing.B, numGoroutines int, optimized bool) {
	// Create test server
	server := httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Simulate API latency
		time.Sleep(10 * time.Millisecond)
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte(`{"result": "success"}`))
	}))
	defer server.Close()

	var client *RetryClient
	if optimized {
		client = NewRetryClient(nil)
	} else {
		httpClient := &http.Client{
			Timeout:   180 * time.Second,
			Transport: createUnoptimizedTransport(),
		}
		client = &RetryClient{
			client: httpClient,
			config: DefaultRetryConfig(),
		}
	}

	b.ResetTimer()
	b.ReportAllocs()

	// Run b.N iterations, each with numGoroutines concurrent requests
	for i := 0; i < b.N; i++ {
		var wg sync.WaitGroup
		wg.Add(numGoroutines)

		for j := 0; j < numGoroutines; j++ {
			go func() {
				defer wg.Done()
				req, err := http.NewRequest("GET", server.URL, nil)
				if err != nil {
					b.Error("Failed to create request:", err) //nolint:testinggoroutine
				}

				resp, err := client.Do(req)
				if err != nil {
					b.Error("Request failed:", err) //nolint:testinggoroutine
				}
				_ = resp.Body.Close()
			}()
		}

		wg.Wait()
	}
}

// BenchmarkConnectionPoolSettings benchmarks different connection pool configurations
func BenchmarkConnectionPoolSettings_HighThroughput(b *testing.B) {
	config := &RetryConfig{
		MaxIdleConns:          200,
		MaxIdleConnsPerHost:   20,
		IdleConnTimeout:       120 * time.Second,
		TLSHandshakeTimeout:   15 * time.Second,
		ExpectContinueTimeout: 2 * time.Second,
	}

	benchmarkWithConfig(b, config)
}

// BenchmarkConnectionPoolSettings_MemoryConstrained benchmarks memory-constrained configuration
func BenchmarkConnectionPoolSettings_MemoryConstrained(b *testing.B) {
	config := &RetryConfig{
		MaxIdleConns:          20,
		MaxIdleConnsPerHost:   2,
		IdleConnTimeout:       30 * time.Second,
		TLSHandshakeTimeout:   5 * time.Second,
		ExpectContinueTimeout: 500 * time.Millisecond,
	}

	benchmarkWithConfig(b, config)
}

// BenchmarkConnectionPoolSettings_Default benchmarks default configuration
func BenchmarkConnectionPoolSettings_Default(b *testing.B) {
	config := DefaultRetryConfig()
	benchmarkWithConfig(b, config)
}

// benchmarkWithConfig is a helper function for benchmarking with specific configuration
func benchmarkWithConfig(b *testing.B, config *RetryConfig) {
	// Create test server
	server := httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte(`{"result": "success"}`))
	}))
	defer server.Close()

	client := NewRetryClient(config)

	b.ResetTimer()
	b.ReportAllocs()

	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			req, err := http.NewRequest("GET", server.URL, nil)
			if err != nil {
				b.Error("Failed to create request:", err) //nolint:testinggoroutine
			}

			resp, err := client.Do(req)
			if err != nil {
				b.Error("Request failed:", err) //nolint:testinggoroutine
			}
			_ = resp.Body.Close()
		}
	})
}

// BenchmarkNewClientCreation benchmarks the overhead of creating new clients
func BenchmarkNewClientCreation(b *testing.B) {
	config := DefaultRetryConfig()

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_ = NewRetryClient(config)
	}
}

// BenchmarkNewClientCreationWithCustomTransport benchmarks client creation with custom transport
func BenchmarkNewClientCreationWithCustomTransport(b *testing.B) {
	customTransport := &http.Transport{
		MaxIdleConns:        50,
		MaxIdleConnsPerHost: 5,
		IdleConnTimeout:     30 * time.Second,
	}

	config := &RetryConfig{
		Transport: customTransport,
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_ = NewRetryClient(config)
	}
}

// BenchmarkGetConnectionStats benchmarks the overhead of retrieving connection statistics
func BenchmarkGetConnectionStats(b *testing.B) {
	client := NewRetryClient(nil)

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_ = client.GetConnectionStats()
	}
}

// BenchmarkCloseIdleConnections benchmarks the overhead of closing idle connections
func BenchmarkCloseIdleConnections(b *testing.B) {
	server := httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte(`{"result": "success"}`))
	}))
	defer server.Close()

	client := NewRetryClient(nil)

	// Make a few requests to establish connections
	for i := 0; i < 5; i++ {
		req, _ := http.NewRequest("GET", server.URL, nil)
		resp, err := client.Do(req)
		if err != nil {
			b.Error("Request failed:", err) //nolint:testinggoroutine
		}
		_ = resp.Body.Close()
	}

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		client.CloseIdleConnections()
	}
}

// BenchmarkTransportCreation benchmarks the overhead of creating optimized transport
func BenchmarkTransportCreation(b *testing.B) {
	config := DefaultRetryConfig()

	b.ResetTimer()
	b.ReportAllocs()

	for i := 0; i < b.N; i++ {
		_ = createOptimizedTransport(config)
	}
}
</file>
<file path="internal/llm/retry_client_test.go">
package llm

import (
	"crypto/tls"
	"net"
	"net/http"
	"net/http/httptest"
	"sync"
	"testing"
	"time"
)

// TestDefaultRetryConfig verifies that DefaultRetryConfig returns expected values
func TestDefaultRetryConfig(t *testing.T) {
	config := DefaultRetryConfig()

	if config.MaxAttempts != 5 {
		t.Errorf("Expected MaxAttempts 5, got %d", config.MaxAttempts)
	}

	if config.Multiplier != 1 {
		t.Errorf("Expected Multiplier 1, got %d", config.Multiplier)
	}

	if config.MaxWaitPerAttempt != 60*time.Second {
		t.Errorf("Expected MaxWaitPerAttempt 60s, got %v", config.MaxWaitPerAttempt)
	}

	if config.MaxTotalWait != 300*time.Second {
		t.Errorf("Expected MaxTotalWait 300s, got %v", config.MaxTotalWait)
	}

	// Connection pooling defaults
	if config.MaxIdleConns != 100 {
		t.Errorf("Expected MaxIdleConns 100, got %d", config.MaxIdleConns)
	}

	if config.MaxIdleConnsPerHost != 10 {
		t.Errorf("Expected MaxIdleConnsPerHost 10, got %d", config.MaxIdleConnsPerHost)
	}

	if config.IdleConnTimeout != 90*time.Second {
		t.Errorf("Expected IdleConnTimeout 90s, got %v", config.IdleConnTimeout)
	}

	if config.TLSHandshakeTimeout != 10*time.Second {
		t.Errorf("Expected TLSHandshakeTimeout 10s, got %v", config.TLSHandshakeTimeout)
	}

	if config.ExpectContinueTimeout != 1*time.Second {
		t.Errorf("Expected ExpectContinueTimeout 1s, got %v", config.ExpectContinueTimeout)
	}
}

// TestNewRetryClient_DefaultTransport verifies that NewRetryClient creates a client with optimized transport
func TestNewRetryClient_DefaultTransport(t *testing.T) {
	client := NewRetryClient(nil)

	// Verify transport is set
	if client.client.Transport == nil {
		t.Fatal("Expected transport to be set, got nil")
	}

	// Verify it's an http.Transport
	transport, ok := client.client.Transport.(*http.Transport)
	if !ok {
		t.Fatalf("Expected *http.Transport, got %T", client.client.Transport)
	}

	// Verify connection pooling settings
	if transport.MaxIdleConns != 100 {
		t.Errorf("Expected MaxIdleConns 100, got %d", transport.MaxIdleConns)
	}

	if transport.MaxIdleConnsPerHost != 10 {
		t.Errorf("Expected MaxIdleConnsPerHost 10, got %d", transport.MaxIdleConnsPerHost)
	}

	if transport.IdleConnTimeout != 90*time.Second {
		t.Errorf("Expected IdleConnTimeout 90s, got %v", transport.IdleConnTimeout)
	}

	if transport.TLSHandshakeTimeout != 10*time.Second {
		t.Errorf("Expected TLSHandshakeTimeout 10s, got %v", transport.TLSHandshakeTimeout)
	}

	if transport.ExpectContinueTimeout != 1*time.Second {
		t.Errorf("Expected ExpectContinueTimeout 1s, got %v", transport.ExpectContinueTimeout)
	}

	// Verify HTTP/2 is enabled
	if !transport.ForceAttemptHTTP2 {
		t.Error("Expected ForceAttemptHTTP2 to be true")
	}

	// Verify TLS config
	if transport.TLSClientConfig == nil {
		t.Fatal("Expected TLSClientConfig to be set, got nil")
	}

	if transport.TLSClientConfig.MinVersion != tls.VersionTLS12 {
		t.Errorf("Expected MinVersion TLS 1.2, got %v", transport.TLSClientConfig.MinVersion)
	}

	// Verify default timeout
	if client.client.Timeout != 180*time.Second {
		t.Errorf("Expected client timeout 180s, got %v", client.client.Timeout)
	}
}

// TestNewRetryClient_CustomTransport verifies that custom transport is used when provided
func TestNewRetryClient_CustomTransport(t *testing.T) {
	customTransport := &http.Transport{
		MaxIdleConns:        50,
		MaxIdleConnsPerHost: 5,
		IdleConnTimeout:     30 * time.Second,
	}

	config := &RetryConfig{
		Transport: customTransport,
	}

	client := NewRetryClient(config)

	// Verify custom transport is used
	if client.client.Transport != customTransport {
		t.Error("Expected custom transport to be used")
	}
}

// TestNewRetryClient_CustomConnectionPoolSettings verifies custom connection pooling settings
func TestNewRetryClient_CustomConnectionPoolSettings(t *testing.T) {
	tests := []struct {
		name                 string
		config               *RetryConfig
		expectedMaxIdle      int
		expectedMaxIdleHost  int
		expectedIdleTimeout  time.Duration
		expectedTLSTimeout   time.Duration
		expectedContinueTime time.Duration
	}{
		{
			name: "high throughput settings",
			config: &RetryConfig{
				MaxIdleConns:          200,
				MaxIdleConnsPerHost:   20,
				IdleConnTimeout:       120 * time.Second,
				TLSHandshakeTimeout:   15 * time.Second,
				ExpectContinueTimeout: 2 * time.Second,
			},
			expectedMaxIdle:      200,
			expectedMaxIdleHost:  20,
			expectedIdleTimeout:  120 * time.Second,
			expectedTLSTimeout:   15 * time.Second,
			expectedContinueTime: 2 * time.Second,
		},
		{
			name: "memory constrained settings",
			config: &RetryConfig{
				MaxIdleConns:          20,
				MaxIdleConnsPerHost:   2,
				IdleConnTimeout:       30 * time.Second,
				TLSHandshakeTimeout:   5 * time.Second,
				ExpectContinueTimeout: 500 * time.Millisecond,
			},
			expectedMaxIdle:      20,
			expectedMaxIdleHost:  2,
			expectedIdleTimeout:  30 * time.Second,
			expectedTLSTimeout:   5 * time.Second,
			expectedContinueTime: 500 * time.Millisecond,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			client := NewRetryClient(tt.config)

			transport, ok := client.client.Transport.(*http.Transport)
			if !ok {
				t.Fatalf("Expected *http.Transport, got %T", client.client.Transport)
			}

			if transport.MaxIdleConns != tt.expectedMaxIdle {
				t.Errorf("Expected MaxIdleConns %d, got %d", tt.expectedMaxIdle, transport.MaxIdleConns)
			}

			if transport.MaxIdleConnsPerHost != tt.expectedMaxIdleHost {
				t.Errorf("Expected MaxIdleConnsPerHost %d, got %d", tt.expectedMaxIdleHost, transport.MaxIdleConnsPerHost)
			}

			if transport.IdleConnTimeout != tt.expectedIdleTimeout {
				t.Errorf("Expected IdleConnTimeout %v, got %v", tt.expectedIdleTimeout, transport.IdleConnTimeout)
			}

			if transport.TLSHandshakeTimeout != tt.expectedTLSTimeout {
				t.Errorf("Expected TLSHandshakeTimeout %v, got %v", tt.expectedTLSTimeout, transport.TLSHandshakeTimeout)
			}

			if transport.ExpectContinueTimeout != tt.expectedContinueTime {
				t.Errorf("Expected ExpectContinueTimeout %v, got %v", tt.expectedContinueTime, transport.ExpectContinueTimeout)
			}
		})
	}
}

// TestNewRetryClientWithTimeout_CustomTimeout verifies custom timeout is respected
func TestNewRetryClientWithTimeout_CustomTimeout(t *testing.T) {
	tests := []struct {
		name           string
		timeout        time.Duration
		expectedConfig *RetryConfig
	}{
		{
			name:    "short timeout",
			timeout: 30 * time.Second,
		},
		{
			name:    "long timeout",
			timeout: 300 * time.Second,
		},
		{
			name:    "very short timeout",
			timeout: 5 * time.Second,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			client := NewRetryClientWithTimeout(tt.timeout, nil)

			if client.client.Timeout != tt.timeout {
				t.Errorf("Expected timeout %v, got %v", tt.timeout, client.client.Timeout)
			}

			// Verify transport is still optimized
			transport, ok := client.client.Transport.(*http.Transport)
			if !ok {
				t.Fatalf("Expected *http.Transport, got %T", client.client.Transport)
			}

			if transport.MaxIdleConns != 100 {
				t.Errorf("Expected MaxIdleConns 100, got %d", transport.MaxIdleConns)
			}
		})
	}
}

// TestGetConnectionStats_DefaultTransport verifies connection stats are reported correctly
func TestGetConnectionStats_DefaultTransport(t *testing.T) {
	client := NewRetryClient(nil)
	stats := client.GetConnectionStats()

	// Verify transport type
	if stats.TransportType != "http.Transport" {
		t.Errorf("Expected TransportType 'http.Transport', got '%s'", stats.TransportType)
	}

	// Verify connection pool settings
	if stats.MaxIdleConns != 100 {
		t.Errorf("Expected MaxIdleConns 100, got %d", stats.MaxIdleConns)
	}

	if stats.MaxIdleConnsPerHost != 10 {
		t.Errorf("Expected MaxIdleConnsPerHost 10, got %d", stats.MaxIdleConnsPerHost)
	}

	if stats.IdleConnTimeout != 90*time.Second {
		t.Errorf("Expected IdleConnTimeout 90s, got %v", stats.IdleConnTimeout)
	}

	// Verify timeout settings
	if stats.TLSHandshakeTimeout != 10*time.Second {
		t.Errorf("Expected TLSHandshakeTimeout 10s, got %v", stats.TLSHandshakeTimeout)
	}

	if stats.ExpectContinueTimeout != 1*time.Second {
		t.Errorf("Expected ExpectContinueTimeout 1s, got %v", stats.ExpectContinueTimeout)
	}

	// Verify HTTP/2 support
	if !stats.HTTP2Enabled {
		t.Error("Expected HTTP2Enabled to be true")
	}

	// Verify TLS configuration
	if stats.TLSMinVersion != tls.VersionTLS12 {
		t.Errorf("Expected TLSMinVersion %d, got %d", tls.VersionTLS12, stats.TLSMinVersion)
	}

	// Verify client timeout
	if stats.ClientTimeout != 180*time.Second {
		t.Errorf("Expected ClientTimeout 180s, got %v", stats.ClientTimeout)
	}
}

// TestGetConnectionStats_CustomTransport verifies stats handle custom transport
func TestGetConnectionStats_CustomTransport(t *testing.T) {
	customTransport := &http.Transport{
		MaxIdleConns:        50,
		MaxIdleConnsPerHost: 5,
		IdleConnTimeout:     30 * time.Second,
		ForceAttemptHTTP2:   false,
	}

	config := &RetryConfig{
		Transport: customTransport,
	}

	client := NewRetryClient(config)
	stats := client.GetConnectionStats()

	// Verify transport type is still http.Transport (custom transport is http.Transport)
	if stats.TransportType != "http.Transport" {
		t.Errorf("Expected TransportType 'http.Transport', got '%s'", stats.TransportType)
	}

	// Verify custom settings are reported
	if stats.MaxIdleConns != 50 {
		t.Errorf("Expected MaxIdleConns 50, got %d", stats.MaxIdleConns)
	}

	if stats.MaxIdleConnsPerHost != 5 {
		t.Errorf("Expected MaxIdleConnsPerHost 5, got %d", stats.MaxIdleConnsPerHost)
	}

	if stats.HTTP2Enabled {
		t.Error("Expected HTTP2Enabled to be false for custom transport")
	}
}

// TestGetConnectionStats_NonHTTPTransport verifies stats handle non-http.Transport
func TestGetConnectionStats_NonHTTPTransport(t *testing.T) {
	// Create a mock RoundTripper that's not http.Transport
	mockTransport := &mockRoundTripper{}

	config := &RetryConfig{
		Transport: mockTransport,
	}

	client := NewRetryClient(config)
	stats := client.GetConnectionStats()

	// Verify transport type is "custom"
	if stats.TransportType != "custom" {
		t.Errorf("Expected TransportType 'custom', got '%s'", stats.TransportType)
	}

	// Verify default values for non-http.Transport
	if stats.MaxIdleConns != 0 {
		t.Errorf("Expected MaxIdleConns 0 for custom transport, got %d", stats.MaxIdleConns)
	}
}

// TestCloseIdleConnections verifies the method closes idle connections
func TestCloseIdleConnections(t *testing.T) {
	client := NewRetryClient(nil)

	// This should not panic
	client.CloseIdleConnections()
}

// TestCloseIdleConnections_CustomTransport verifies custom transport is handled
func TestCloseIdleConnections_CustomTransport(t *testing.T) {
	mockTransport := &mockRoundTripper{}

	config := &RetryConfig{
		Transport: mockTransport,
	}

	client := NewRetryClient(config)

	// This should not panic even with non-http.Transport
	client.CloseIdleConnections()
}

// TestGetTimeout verifies timeout getter
func TestGetTimeout(t *testing.T) {
	client := NewRetryClient(nil)

	if timeout := client.GetTimeout(); timeout != 180*time.Second {
		t.Errorf("Expected timeout 180s, got %v", timeout)
	}

	customTimeout := 45 * time.Second
	client.SetTimeout(customTimeout)

	if timeout := client.GetTimeout(); timeout != customTimeout {
		t.Errorf("Expected timeout %v, got %v", customTimeout, timeout)
	}
}

// TestSetTimeout verifies timeout setter
func TestSetTimeout(t *testing.T) {
	client := NewRetryClient(nil)

	newTimeout := 120 * time.Second
	client.SetTimeout(newTimeout)

	if client.client.Timeout != newTimeout {
		t.Errorf("Expected timeout %v, got %v", newTimeout, client.client.Timeout)
	}
}

// mockRoundTripper is a mock implementation of http.RoundTripper for testing
type mockRoundTripper struct{}

func (m *mockRoundTripper) RoundTrip(*http.Request) (*http.Response, error) {
	return nil, nil
}

// TestConnectionReuse_Integration verifies that HTTP connections are reused across multiple requests
func TestConnectionReuse_Integration(t *testing.T) {
	// Track the number of connections established
	var connectionCount int
	var connectionCountMu sync.Mutex

	// Create a custom listener that counts connections
	listener, err := net.Listen("tcp", "127.0.0.1:0")
	if err != nil {
		t.Fatalf("Failed to create listener: %v", err)
	}
	defer func() { _ = listener.Close() }()

	// Wrap the listener to count Accept() calls
	countingListener := &countingListener{
		Listener: listener,
		onAccept: func() {
			connectionCountMu.Lock()
			connectionCount++
			connectionCountMu.Unlock()
		},
	}

	// Create test server with the custom listener
	server := httptest.NewUnstartedServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte(`{"status": "ok"}`))
	}))
	server.Listener = countingListener
	server.StartTLS()
	defer server.Close()

	// Create RetryClient with optimized connection pooling
	retryConfig := DefaultRetryConfig()
	retryConfig.Transport = &http.Transport{
		TLSClientConfig:     &tls.Config{InsecureSkipVerify: true},
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 10,
		IdleConnTimeout:     90 * time.Second,
		ForceAttemptHTTP2:   true,
	}
	client := NewRetryClient(retryConfig)

	// Make concurrent requests
	numRequests := 20
	var wg sync.WaitGroup
	wg.Add(numRequests)

	for i := 0; i < numRequests; i++ {
		go func() {
			defer wg.Done()
			req, err := http.NewRequest("GET", server.URL, nil)
			if err != nil {
				t.Errorf("Failed to create request: %v", err)
				return
			}

			resp, err := client.Do(req)
			if err != nil {
				t.Errorf("Request failed: %v", err)
				return
			}
			_ = resp.Body.Close()
		}()
	}

	wg.Wait()

	// Verify connections were reused even with concurrent requests
	// With HTTP/2 and connection pooling, concurrent requests should share connections
	connectionCountMu.Lock()
	finalConnectionCount := connectionCount
	connectionCountMu.Unlock()

	// We expect significantly fewer connections than requests
	// With HTTP/2 multiplexing, all requests could theoretically use 1 connection
	// But we allow for multiple connections due to concurrent nature
	if finalConnectionCount > numRequests/2 {
		t.Logf("Warning: Made %d concurrent requests using %d connections. Consider optimizing connection pooling.",
			numRequests, finalConnectionCount)
	}

	// Verify at least one connection was established
	if finalConnectionCount < 1 {
		t.Errorf("Expected at least 1 connection, got %d", finalConnectionCount)
	}

	t.Logf("Made %d concurrent requests using %d connections",
		numRequests, finalConnectionCount)
}

// countingListener wraps a net.Listener and counts Accept() calls
type countingListener struct {
	net.Listener
	onAccept func()
}

func (l *countingListener) Accept() (net.Conn, error) {
	conn, err := l.Listener.Accept()
	if l.onAccept != nil {
		l.onAccept()
	}
	return conn, err
}
</file>
<file path="internal/llm/sse_parser.go">
package llm

import (
	"bufio"
	"bytes"
	"encoding/json"
	"fmt"
	"io"
)

// SSEEvent represents a single Server-Sent Event
type SSEEvent struct {
	Event string // Event type (optional, empty if not specified)
	Data  []byte // Concatenated data lines
	ID    string // Event ID (optional)
}

// SSEParser parses Server-Sent Events (SSE) streams
type SSEParser struct {
	reader    *bufio.Reader
	buffer    *bytes.Buffer // Accumulates data for the current event
	eventType string        // Current event type
	eventID   string        // Current event ID
}

// NewSSEParser creates a new SSE parser
func NewSSEParser(reader io.Reader) *SSEParser {
	return &SSEParser{
		reader: bufio.NewReader(reader),
		buffer: &bytes.Buffer{},
	}
}

// NextEvent reads the next SSE event from the stream
// Returns io.EOF when the stream is complete
// Returns io.ErrUnexpectedEOF if the stream ends mid-event
func (p *SSEParser) NextEvent() (SSEEvent, error) {
	debug := false // Set to true to enable debug logging
	for {
		line, err := p.reader.ReadBytes('\n')
		if err != nil {
			// If we have buffered data, the stream ended unexpectedly
			if p.buffer.Len() > 0 || p.eventType != "" {
				return SSEEvent{}, fmt.Errorf("stream ended mid-event: %w", io.ErrUnexpectedEOF)
			}
			return SSEEvent{}, err
		}

		if debug {
			fmt.Printf("DEBUG: ReadBytes returned: %v\n", line)
		}

		// Remove trailing \n first (from ReadBytes delimiter)
		line = bytes.TrimSuffix(line, []byte{'\n'})
		// Then remove trailing \r if present (Windows line endings CRLF)
		line = bytes.TrimSuffix(line, []byte{'\r'})

		if debug {
			fmt.Printf("DEBUG: After trimming: %v (len=%d)\n", line, len(line))
		}

		// Empty line marks the end of an event
		if len(line) == 0 {
			// If we have accumulated data, an event type, or an ID, return the event
			if p.buffer.Len() > 0 || p.eventType != "" || p.eventID != "" {
				event := SSEEvent{
					Event: p.eventType,
					Data:  p.buffer.Bytes(),
					ID:    p.eventID,
				}
				// Reset for next event
				p.reset()
				return event, nil
			}
			// Otherwise, continue to next line
			continue
		}

		// Skip comments (lines starting with ':')
		if len(line) > 0 && line[0] == ':' {
			continue
		}

		// Parse field
		if idx := bytes.IndexByte(line, ':'); idx != -1 {
			field := string(line[:idx])
			value := string(line[idx+1:])

			// Remove leading space from value if present (SSE spec)
			if len(value) > 0 && value[0] == ' ' {
				value = value[1:]
			}

			switch field {
			case "event":
				p.eventType = value
			case "data":
				if p.buffer.Len() > 0 {
					p.buffer.WriteByte('\n')
				}
				p.buffer.WriteString(value)
			case "id":
				p.eventID = value
			case "retry":
				// Ignore retry field for now
				// Could be used to configure reconnection time
			}
		}
		// If no colon found, ignore the line (per SSE spec)
	}
}

// reset clears the parser state for the next event
func (p *SSEParser) reset() {
	p.buffer.Reset()
	p.eventType = ""
	p.eventID = ""
}

// ParseSSEData parses JSON data from an SSE event
// This is a helper to avoid importing json in the parser package
func ParseSSEData(data []byte, v interface{}) error {
	return json.Unmarshal(data, v)
}

// IsSSEDone checks if the SSE data is the OpenAI [DONE] marker
func IsSSEDone(data []byte) bool {
	return bytes.Equal(data, []byte("[DONE]"))
}
</file>
<file path="internal/llm/sse_parser_test.go">
package llm

import (
	"bytes"
	"errors"
	"io"
	"strings"
	"testing"
)

func TestSSEParser_SimpleEvent(t *testing.T) {
	input := "data: hello world\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if event.Event != "" {
		t.Errorf("Expected empty event type, got '%s'", event.Event)
	}

	if string(event.Data) != "hello world" {
		t.Errorf("Expected data 'hello world', got '%s'", string(event.Data))
	}

	// Should return EOF on next call
	_, err = parser.NextEvent()
	if err != io.EOF {
		t.Errorf("Expected EOF, got %v", err)
	}
}

func TestSSEParser_WithEventType(t *testing.T) {
	input := "event: message_start\ndata: {\"type\":\"message_start\"}\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if event.Event != "message_start" {
		t.Errorf("Expected event type 'message_start', got '%s'", event.Event)
	}

	if string(event.Data) != `{"type":"message_start"}` {
		t.Errorf("Expected data '{\"type\":\"message_start\"}', got '%s'", string(event.Data))
	}
}

func TestSSEParser_MultipleEvents(t *testing.T) {
	input := "event: message_start\ndata: start\n\nevent: content_block\ndata: chunk1\n\ndata: chunk2\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	// First event
	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}
	if event.Event != "message_start" {
		t.Errorf("Expected event type 'message_start', got '%s'", event.Event)
	}
	if string(event.Data) != "start" {
		t.Errorf("Expected data 'start', got '%s'", string(event.Data))
	}

	// Second event
	event, err = parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}
	if event.Event != "content_block" {
		t.Errorf("Expected event type 'content_block', got '%s'", event.Event)
	}
	if string(event.Data) != "chunk1" {
		t.Errorf("Expected data 'chunk1', got '%s'", string(event.Data))
	}

	// Third event
	event, err = parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}
	if event.Event != "" {
		t.Errorf("Expected empty event type, got '%s'", event.Event)
	}
	if string(event.Data) != "chunk2" {
		t.Errorf("Expected data 'chunk2', got '%s'", string(event.Data))
	}

	// Should return EOF on next call
	_, err = parser.NextEvent()
	if err != io.EOF {
		t.Errorf("Expected EOF, got %v", err)
	}
}

func TestSSEParser_MultipleDataLines(t *testing.T) {
	input := "data: line1\ndata: line2\ndata: line3\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	expected := "line1\nline2\nline3"
	if string(event.Data) != expected {
		t.Errorf("Expected data '%s', got '%s'", expected, string(event.Data))
	}
}

func TestSSEParser_WithEventID(t *testing.T) {
	input := "id: 123\nevent: message\ndata: test\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if event.ID != "123" {
		t.Errorf("Expected ID '123', got '%s'", event.ID)
	}

	if event.Event != "message" {
		t.Errorf("Expected event type 'message', got '%s'", event.Event)
	}

	if string(event.Data) != "test" {
		t.Errorf("Expected data 'test', got '%s'", string(event.Data))
	}
}

func TestSSEParser_CommentLines(t *testing.T) {
	input := ": this is a comment\ndata: actual data\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if string(event.Data) != "actual data" {
		t.Errorf("Expected data 'actual data', got '%s'", string(event.Data))
	}
}

func TestSSEParser_EmptyLinesBetweenEvents(t *testing.T) {
	input := "data: event1\n\n\ndata: event2\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	// First event
	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}
	if string(event.Data) != "event1" {
		t.Errorf("Expected data 'event1', got '%s'", string(event.Data))
	}

	// Second event
	event, err = parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}
	if string(event.Data) != "event2" {
		t.Errorf("Expected data 'event2', got '%s'", string(event.Data))
	}
}

func TestSSEParser_LeadingSpaceInValue(t *testing.T) {
	input := "data:  hello\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Per SSE spec: "If the line starts with a U+003E COLON character (':'), ignore the line."
	// "If the line contains a U+003A COLON character character (':'), collect the field name and field value..."
	// "If value starts with a U+0020 SPACE character, remove it from the value."
	// So for "data:  hello" (two spaces after colon), we remove ONE space, leaving " hello"
	if string(event.Data) != " hello" {
		t.Errorf("Expected data ' hello', got '%s'", string(event.Data))
	}
}

func TestSSEParser_RetryField(t *testing.T) {
	input := "retry: 10000\ndata: test\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Retry field should be ignored (just verify parsing doesn't break)
	if string(event.Data) != "test" {
		t.Errorf("Expected data 'test', got '%s'", string(event.Data))
	}
}

func TestSSEParser_CRLFLineEndings(t *testing.T) {
	input := "data: line1\r\ndata: line2\r\n\r\n"
	t.Logf("Input bytes: %v", []byte(input))
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	expected := "line1\nline2"
	if string(event.Data) != expected {
		t.Errorf("Expected data '%s', got '%s'", expected, string(event.Data))
	}
}

func TestSSEParser_UnexpectedEOF(t *testing.T) {
	input := "data: incomplete event"
	parser := NewSSEParser(strings.NewReader(input))

	_, err := parser.NextEvent()
	if err == nil {
		t.Fatal("Expected error, got nil")
	}
	// Since we haven't successfully parsed any data yet, we get EOF, not wrapped UnexpectedEOF
	// The data "incomplete event" was never added to the buffer because there was no newline
	if err != io.EOF {
		t.Errorf("Expected EOF, got %v", err)
	}
}

func TestSSEParser_UnexpectedEOFWithData(t *testing.T) {
	input := "event: test\ndata: partial data"
	parser := NewSSEParser(strings.NewReader(input))

	_, err := parser.NextEvent()
	if err == nil {
		t.Fatal("Expected error, got nil")
	}
	// The error is wrapped, so we need to use errors.Is or check the message
	if !errors.Is(err, io.ErrUnexpectedEOF) && err.Error() != "stream ended mid-event: unexpected EOF" {
		t.Errorf("Expected ErrUnexpectedEOF (or wrapped), got %v", err)
	}
}

func TestSSEParser_EmptyStream(t *testing.T) {
	input := ""
	parser := NewSSEParser(strings.NewReader(input))

	_, err := parser.NextEvent()
	if err != io.EOF {
		t.Errorf("Expected EOF, got %v", err)
	}
}

func TestSSEParser_OnlyEmptyLines(t *testing.T) {
	input := "\n\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	_, err := parser.NextEvent()
	if err != io.EOF {
		t.Errorf("Expected EOF, got %v", err)
	}
}

func TestSSEParser_OnlyComments(t *testing.T) {
	input := ": comment1\n: comment2\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	_, err := parser.NextEvent()
	if err != io.EOF {
		t.Errorf("Expected EOF, got %v", err)
	}
}

func TestSSEParser_LineWithoutColon(t *testing.T) {
	input := "invalid line without colon\ndata: valid\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Invalid line should be ignored, only valid data should be returned
	if string(event.Data) != "valid" {
		t.Errorf("Expected data 'valid', got '%s'", string(event.Data))
	}
}

func TestSSEParser_ComplexEvent(t *testing.T) {
	input := `id: msg-123
event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"Hello"}}
data:  world
retry: 1000

`
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if event.ID != "msg-123" {
		t.Errorf("Expected ID 'msg-123', got '%s'", event.ID)
	}

	if event.Event != "content_block_delta" {
		t.Errorf("Expected event type 'content_block_delta', got '%s'", event.Event)
	}

	expectedData := `{"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"Hello"}}
 world`
	if string(event.Data) != expectedData {
		t.Errorf("Expected data '%s', got '%s'", expectedData, string(event.Data))
	}
}

func TestIsSSEDone(t *testing.T) {
	tests := []struct {
		name     string
		data     []byte
		expected bool
	}{
		{
			name:     "OpenAI DONE marker",
			data:     []byte("[DONE]"),
			expected: true,
		},
		{
			name:     "Regular data",
			data:     []byte(`{"type":"message"}`),
			expected: false,
		},
		{
			name:     "Empty data",
			data:     []byte(""),
			expected: false,
		},
		{
			name:     "Almost DONE",
			data:     []byte("[DONE]extra"),
			expected: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			result := IsSSEDone(tt.data)
			if result != tt.expected {
				t.Errorf("IsSSEDone(%q) = %v, want %v", string(tt.data), result, tt.expected)
			}
		})
	}
}

func TestSSEParser_LargeEvent(t *testing.T) {
	// Build a large event with multiple data lines
	var buf bytes.Buffer
	for i := 0; i < 100; i++ {
		buf.WriteString("data: ")
		buf.WriteString(strings.Repeat("text", 100))
		buf.WriteString("\n")
	}
	buf.WriteString("\n")

	parser := NewSSEParser(&buf)

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Verify we got all the data
	expectedLines := 100
	actualLines := strings.Count(string(event.Data), "\n") + 1
	if actualLines != expectedLines {
		t.Errorf("Expected %d lines, got %d", expectedLines, actualLines)
	}

	// Should return EOF on next call
	_, err = parser.NextEvent()
	if err != io.EOF {
		t.Errorf("Expected EOF, got %v", err)
	}
}

func TestSSEParser_EventWithOnlyID(t *testing.T) {
	input := "id: 456\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if event.ID != "456" {
		t.Errorf("Expected ID '456', got '%s'", event.ID)
	}

	if len(event.Data) != 0 {
		t.Errorf("Expected empty data, got '%s'", string(event.Data))
	}
}

func TestSSEParser_EventWithOnlyEvent(t *testing.T) {
	input := "event: custom_event\n\n"
	parser := NewSSEParser(strings.NewReader(input))

	event, err := parser.NextEvent()
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if event.Event != "custom_event" {
		t.Errorf("Expected event type 'custom_event', got '%s'", event.Event)
	}

	if len(event.Data) != 0 {
		t.Errorf("Expected empty data, got '%s'", string(event.Data))
	}
}
</file>
<file path="internal/llm/streaming_integration_test.go">
//go:build integration
// +build integration

package llm

import (
	"context"
	"fmt"
	"net/http"
	"net/http/httptest"
	"strings"
	"testing"
	"time"

	"github.com/user/gendocs/internal/config"
)

// TestAnthropicStreaming_AgentWorkflow tests Anthropic streaming with a simulated agent workflow
// This test validates that the streaming implementation works correctly with real agent workloads
// including tool calling and multiple LLM requests.
func TestAnthropicStreaming_AgentWorkflow(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	callCount := 0

	// Create mock server that simulates an agent workflow with tool calling
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		callCount++

		// Validate streaming request
		if r.Method != "POST" {
			t.Errorf("Expected POST request, got %s", r.Method)
		}

		// Check for stream parameter
		body := make([]byte, 1024)
		n, _ := r.Body.Read(body)
		bodyStr := string(body[:n])
		if !strings.Contains(bodyStr, `"stream":true`) {
			t.Errorf("Expected stream=true in request body")
		}

		w.Header().Set("Content-Type", "text/event-stream")

		// Simulate different responses based on call count (agent workflow)
		switch callCount {
		case 1:
			// First call: LLM requests to list files
			sendSSEEvent(w, "message_start", `{"type":"message_start","message":{"id":"msg_1","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":100,"output_tokens":0}}}`)
			sendSSEEvent(w, "content_block_start", `{"type":"content_block_start","index":0,"content_block":{"type":"tool_use","id":"toolu_1","name":"list_files","input":null}}`)
			sendSSEEvent(w, "content_block_delta", `{"type":"content_block_delta","index":0,"delta":{"type":"input_json_delta","partial_json":"{\"directory\":\".\\"}}`)
			sendSSEEvent(w, "content_block_stop", `{"type":"content_block_stop","index":0}`)
			sendSSEEvent(w, "message_delta", `{"type":"message_delta","delta":{"stop_reason":"tool_use","stop_sequence":null},"usage":{"output_tokens":25}}`)
			sendSSEEvent(w, "message_stop", `{"type":"message_stop"}`)

		case 2:
			// Second call: LLM requests to read a file
			sendSSEEvent(w, "message_start", `{"type":"message_start","message":{"id":"msg_2","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":200,"output_tokens":0}}}`)
			sendSSEEvent(w, "content_block_start", `{"type":"content_block_start","index":0,"content_block":{"type":"tool_use","id":"toolu_2","name":"read_file","input":null}}`)
			sendSSEEvent(w, "content_block_delta", `{"type":"content_block_delta","index":0,"delta":{"type":"input_json_delta","partial_json":"{\"file_path\":\"main.go\\"}}`)
			sendSSEEvent(w, "content_block_stop", `{"type":"content_block_stop","index":0}`)
			sendSSEEvent(w, "message_delta", `{"type":"message_delta","delta":{"stop_reason":"tool_use","stop_sequence":null},"usage":{"output_tokens":30}}`)
			sendSSEEvent(w, "message_stop", `{"type":"message_stop"}`)

		case 3:
			// Third call: LLM provides final response (large response to test streaming)
			sendSSEEvent(w, "message_start", `{"type":"message_start","message":{"id":"msg_3","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":500,"output_tokens":0}}}`)
			sendSSEEvent(w, "content_block_start", `{"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}`)

			// Send content in multiple chunks to simulate streaming
			chunks := []string{
				"Based on my analysis of the codebase, here is a comprehensive summary:\n\n",
				"## Project Structure\n",
				"The project follows a clean architecture with clear separation of concerns.\n\n",
				"## Main Components\n",
				"1. **Internal Package**: Contains core business logic\n",
				"2. **Agents Package**: Implements various analysis agents\n",
				"3. **LLM Package**: Provides LLM client interfaces\n\n",
				"## Key Findings\n",
				"- The codebase is well-organized and maintainable\n",
				"- Proper error handling is implemented throughout\n",
				"- The streaming implementation reduces memory usage by 90-95%\n",
				"- All agents use the LLMClient interface for consistency\n\n",
				"## Recommendations\n",
				"Continue using the streaming implementation for all LLM calls.",
			}

			for _, chunk := range chunks {
				sendSSEEvent(w, "content_block_delta", fmt.Sprintf(`{"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"%s"}}`, chunk))
			}

			sendSSEEvent(w, "content_block_stop", `{"type":"content_block_stop","index":0}`)
			sendSSEEvent(w, "message_delta", `{"type":"message_delta","delta":{"stop_reason":"end_turn","stop_sequence":null},"usage":{"output_tokens":150}}`)
			sendSSEEvent(w, "message_stop", `{"type":"message_stop"}`)

		default:
			t.Errorf("Unexpected call count: %d", callCount)
		}

		// Add small delay to simulate network latency
		time.Sleep(10 * time.Millisecond)
	}))
	defer server.Close()

	// Create Anthropic client
	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	ctx := context.Background()

	// Call 1: Simulate agent requesting to list files
	resp1, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "You are a code analyzer",
		Messages: []Message{
			{Role: "user", Content: "Analyze this repository"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	if err != nil {
		t.Fatalf("Call 1 failed: %v", err)
	}

	if len(resp1.ToolCalls) != 1 {
		t.Errorf("Expected 1 tool call, got %d", len(resp1.ToolCalls))
	}

	if resp1.ToolCalls[0].Name != "list_files" {
		t.Errorf("Expected tool name 'list_files', got '%s'", resp1.ToolCalls[0].Name)
	}

	// Call 2: Simulate agent requesting to read a file (with tool result)
	resp2, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "You are a code analyzer",
		Messages: []Message{
			{Role: "user", Content: "Analyze this repository"},
			{Role: "assistant", Content: "", ToolCalls: resp1.ToolCalls},
			{Role: "tool", Content: `{"files": ["main.go", "go.mod"]}`, ToolID: "toolu_1"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	if err != nil {
		t.Fatalf("Call 2 failed: %v", err)
	}

	if len(resp2.ToolCalls) != 1 {
		t.Errorf("Expected 1 tool call, got %d", len(resp2.ToolCalls))
	}

	if resp2.ToolCalls[0].Name != "read_file" {
		t.Errorf("Expected tool name 'read_file', got '%s'", resp2.ToolCalls[0].Name)
	}

	// Call 3: Simulate agent providing final analysis (large response)
	resp3, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "You are a code analyzer",
		Messages: []Message{
			{Role: "user", Content: "Analyze this repository"},
			{Role: "assistant", Content: "", ToolCalls: resp1.ToolCalls},
			{Role: "tool", Content: `{"files": ["main.go", "go.mod"]}`, ToolID: "toolu_1"},
			{Role: "assistant", Content: "", ToolCalls: resp2.ToolCalls},
			{Role: "tool", Content: "package main\n\nfunc main() {\n\tprintln(\"hello\")\n}", ToolID: "toolu_2"},
		},
		MaxTokens:   200,
		Temperature: 0.0,
	})

	if err != nil {
		t.Fatalf("Call 3 failed: %v", err)
	}

	// Verify large response was received correctly
	if len(resp3.Content) < 500 {
		t.Errorf("Expected large response (>500 chars), got %d chars", len(resp3.Content))
	}

	if !strings.Contains(resp3.Content, "Project Structure") {
		t.Errorf("Expected response to contain 'Project Structure'")
	}

	if !strings.Contains(resp3.Content, "streaming implementation") {
		t.Errorf("Expected response to contain 'streaming implementation'")
	}

	// Verify token counts
	if resp3.Usage.InputTokens != 500 {
		t.Errorf("Expected 500 input tokens, got %d", resp3.Usage.InputTokens)
	}

	if resp3.Usage.OutputTokens != 150 {
		t.Errorf("Expected 150 output tokens, got %d", resp3.Usage.OutputTokens)
	}

	// Verify we made exactly 3 calls
	if callCount != 3 {
		t.Errorf("Expected 3 HTTP calls, got %d", callCount)
	}
}

// TestOpenAIStreaming_AgentWorkflow tests OpenAI streaming with a simulated agent workflow
func TestOpenAIStreaming_AgentWorkflow(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	callCount := 0

	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		callCount++

		// Check for stream parameter
		body := make([]byte, 1024)
		n, _ := r.Body.Read(body)
		bodyStr := string(body[:n])
		if !strings.Contains(bodyStr, `"stream":true`) {
			t.Errorf("Expected stream=true in request body")
		}

		w.Header().Set("Content-Type", "text/event-stream")

		// Simulate agent workflow
		switch callCount {
		case 1:
			// First call: LLM requests tool use
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-1\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\"},\"finish_reason\":null}]}")
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-1\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"id\":\"call_1\",\"type\":\"function\",\"function\":{\"name\":\"search\",\"arguments\":\"\"}}]},\"finish_reason\":null}]}")
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-1\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"arguments\":\"{\\\"query\\\":\\\"golang\\\"}\"}}]},\"finish_reason\":null}]}")
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-1\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"tool_calls\"}]}")
			fmt.Fprintln(w, "data: [DONE]")

		case 2:
			// Second call: LLM provides final response
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-2\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"Here\"},\"finish_reason\":null}]}")
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-2\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" is\"},\"finish_reason\":null}]}")
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-2\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" the\"},\"finish_reason\":null}]}")
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-2\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" analysis\"},\"finish_reason\":null}]}")
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-2\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" result\"},\"finish_reason\":null}]}")
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-2\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\"},\"finish_reason\":null}]}")
			fmt.Fprintln(w, "data: {\"id\":\"chatcmpl-2\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}")
			fmt.Fprintln(w, "data: [DONE]")

		default:
			t.Errorf("Unexpected call count: %d", callCount)
		}

		time.Sleep(10 * time.Millisecond)
	}))
	defer server.Close()

	client := NewOpenAIClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gpt-4",
	}, nil)

	ctx := context.Background()

	// Call 1: Request tool use
	resp1, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Search for golang information"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	if err != nil {
		t.Fatalf("Call 1 failed: %v", err)
	}

	if len(resp1.ToolCalls) != 1 {
		t.Errorf("Expected 1 tool call, got %d", len(resp1.ToolCalls))
	}

	// Call 2: Get final response after tool result
	resp2, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Search for golang information"},
			{Role: "assistant", Content: "", ToolCalls: resp1.ToolCalls},
			{Role: "tool", Content: `{"results": ["Go is a programming language"]}`, ToolID: "call_1"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	if err != nil {
		t.Fatalf("Call 2 failed: %v", err)
	}

	if resp2.Content != "Here is the analysis result." {
		t.Errorf("Expected 'Here is the analysis result.', got '%s'", resp2.Content)
	}

	if callCount != 2 {
		t.Errorf("Expected 2 HTTP calls, got %d", callCount)
	}
}

// TestGeminiStreaming_AgentWorkflow tests Gemini streaming with a simulated agent workflow
func TestGeminiStreaming_AgentWorkflow(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	callCount := 0

	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		callCount++

		// Verify streaming endpoint
		if !strings.Contains(r.URL.Path, "streamGenerateContent") {
			t.Errorf("Expected streaming endpoint, got %s", r.URL.Path)
		}

		w.Header().Set("Content-Type", "application/json")

		switch callCount {
		case 1:
			// First call: Function call
			fmt.Fprintln(w, `{"candidates":[{"finishReason":"STOP","content":{"parts":[{"functionCall":{"name":"list_files","args":{"path":"."}}}]}}],"usageMetadata":{"promptTokenCount":100,"candidatesTokenCount":20,"totalTokenCount":120}}`)

		case 2:
			// Second call: Final response (streamed across multiple chunks)
			fmt.Fprintln(w, `{"candidates":[{"finishReason":null,"content":{"parts":[{"text":"Analysis"}]}}],"usageMetadata":{"promptTokenCount":200,"candidatesTokenCount":5,"totalTokenCount":205}}`)
			fmt.Fprintln(w, `{"candidates":[{"finishReason":null,"content":{"parts":[{"text":" complete"}]}}],"usageMetadata":{"promptTokenCount":200,"candidatesTokenCount":10,"totalTokenCount":210}}`)
			fmt.Fprintln(w, `{"candidates":[{"finishReason":"STOP","content":{"parts":[{"text":"."}]}}],"usageMetadata":{"promptTokenCount":200,"candidatesTokenCount":15,"totalTokenCount":215}}`)

		default:
			t.Errorf("Unexpected call count: %d", callCount)
		}

		time.Sleep(10 * time.Millisecond)
	}))
	defer server.Close()

	client := NewGeminiClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "gemini-2.0-flash-exp",
	}, nil)

	ctx := context.Background()

	// Call 1: Request function call
	resp1, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "List files in current directory"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	if err != nil {
		t.Fatalf("Call 1 failed: %v", err)
	}

	if len(resp1.ToolCalls) != 1 {
		t.Errorf("Expected 1 tool call, got %d", len(resp1.ToolCalls))
	}

	if resp1.ToolCalls[0].Name != "list_files" {
		t.Errorf("Expected tool name 'list_files', got '%s'", resp1.ToolCalls[0].Name)
	}

	// Call 2: Get final response after function result
	resp2, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "List files in current directory"},
			{Role: "assistant", Content: "", ToolCalls: resp1.ToolCalls},
			{Role: "tool", Content: `{"files": ["main.go"]}`, ToolID: "list_files"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	if err != nil {
		t.Fatalf("Call 2 failed: %v", err)
	}

	if resp2.Content != "Analysis complete." {
		t.Errorf("Expected 'Analysis complete.', got '%s'", resp2.Content)
	}

	if callCount != 2 {
		t.Errorf("Expected 2 HTTP calls, got %d", callCount)
	}
}

// TestStreamingLargeResponse tests that large responses are handled correctly with streaming
// This is particularly important for validating the memory efficiency improvements
func TestAnthropicStreaming_LargeResponse(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")

		// Send message_start
		sendSSEEvent(w, "message_start", `{"type":"message_start","message":{"id":"msg_1","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":100,"output_tokens":0}}}`)

		// Send content_block_start
		sendSSEEvent(w, "content_block_start", `{"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}`)

		// Send a large response in 50 chunks (simulating ~50KB response)
		for i := 0; i < 50; i++ {
			chunk := strings.Repeat(fmt.Sprintf("Chunk %d: ", i), 20) // ~200 bytes per chunk
			sendSSEEvent(w, "content_block_delta", fmt.Sprintf(`{"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"%s\n"}}`, chunk))
		}

		// Send completion events
		sendSSEEvent(w, "content_block_stop", `{"type":"content_block_stop","index":0}`)
		sendSSEEvent(w, "message_delta", `{"type":"message_delta","delta":{"stop_reason":"end_turn","stop_sequence":null},"usage":{"output_tokens":1000}}`)
		sendSSEEvent(w, "message_stop", `{"type":"message_stop"}`)

		// Add small delay to simulate network latency
		time.Sleep(10 * time.Millisecond)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	ctx := context.Background()

	resp, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []Message{
			{Role: "user", Content: "Generate a large response"},
		},
		MaxTokens:   2000,
		Temperature: 0.0,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Verify we received a large response
	if len(resp.Content) < 10000 {
		t.Errorf("Expected large response (>10KB), got %d bytes", len(resp.Content))
	}

	// Verify content integrity
	if !strings.Contains(resp.Content, "Chunk 0:") {
		t.Errorf("Expected response to contain 'Chunk 0:'")
	}

	if !strings.Contains(resp.Content, "Chunk 49:") {
		t.Errorf("Expected response to contain 'Chunk 49:'")
	}

	// Verify token counts
	if resp.Usage.OutputTokens != 1000 {
		t.Errorf("Expected 1000 output tokens, got %d", resp.Usage.OutputTokens)
	}
}

// TestStreamingConcurrentRequests tests that multiple concurrent streaming requests work correctly
// This simulates the real-world scenario where multiple agents run concurrently
func TestAnthropicStreaming_ConcurrentRequests(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	requestCount := 0
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		requestCount++
		w.Header().Set("Content-Type", "text/event-stream")

		// Send a simple streaming response
		sendSSEEvent(w, "message_start", fmt.Sprintf(`{"type":"message_start","message":{"id":"msg_%d","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":10,"output_tokens":0}}}`, requestCount))
		sendSSEEvent(w, "content_block_start", `{"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}`)
		sendSSEEvent(w, "content_block_delta", fmt.Sprintf(`{"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"Response %d"}}`, requestCount))
		sendSSEEvent(w, "content_block_stop", `{"type":"content_block_stop","index":0}`)
		sendSSEEvent(w, "message_delta", `{"type":"message_delta","delta":{"stop_reason":"end_turn","stop_sequence":null},"usage":{"output_tokens":5}}`)
		sendSSEEvent(w, "message_stop", `{"type":"message_stop"}`)

		// Add small delay to simulate network latency
		time.Sleep(10 * time.Millisecond)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	ctx := context.Background()

	// Launch 5 concurrent requests
	numConcurrent := 5
	results := make(chan string, numConcurrent)
	errors := make(chan error, numConcurrent)

	for i := 0; i < numConcurrent; i++ {
		go func(id int) {
			resp, err := client.GenerateCompletion(ctx, CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages: []Message{
					{Role: "user", Content: fmt.Sprintf("Request %d", id)},
				},
				MaxTokens:   100,
				Temperature: 0.0,
			})
			if err != nil {
				errors <- err
				return
			}
			results <- resp.Content
		}(i)
	}

	// Collect results
	successCount := 0
	for i := 0; i < numConcurrent; i++ {
		select {
		case <-results:
			successCount++
		case err := <-errors:
			t.Errorf("Concurrent request failed: %v", err)
		case <-time.After(5 * time.Second):
			t.Errorf("Timeout waiting for concurrent requests")
		}
	}

	// Verify all requests succeeded
	if successCount != numConcurrent {
		t.Errorf("Expected %d successful requests, got %d", numConcurrent, successCount)
	}

	// Verify server received all requests
	if requestCount != numConcurrent {
		t.Errorf("Expected %d server requests, got %d", numConcurrent, requestCount)
	}
}

// Helper function to send SSE events in proper SSE format
// SSE format requires:
// event: <event_type>
// data: <json_data>
// <empty line>
func sendSSEEvent(w http.ResponseWriter, event, data string) {
	fmt.Fprintf(w, "event: %s\n", event)
	fmt.Fprintf(w, "data: %s\n", data)
	fmt.Fprintln(w)
}

// TestStreamingWithToolLargeArguments tests streaming with large tool arguments
// This validates that partial JSON accumulation works correctly for large tool inputs
func TestAnthropicStreaming_ToolLargeArguments(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}

	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")

		// Send message_start
		sendSSEEvent(w, "message_start", `{"type":"message_start","message":{"id":"msg_1","role":"assistant","content":[],"model":"claude-3-sonnet-20240229","stop_reason":null,"usage":{"input_tokens":100,"output_tokens":0}}}`)

		// Send content_block_start for tool_use
		sendSSEEvent(w, "content_block_start", `{"type":"content_block_start","index":0,"content_block":{"type":"tool_use","id":"toolu_1","name":"analyze_files","input":null}}`)

		// Send large tool arguments in multiple chunks (simulating analysis of many files)
		argChunks := []string{
			`{"files":[`,
			`"file1.go","file2.go","file3.go","file4.go","file5.go",`,
			`"file6.go","file7.go","file8.go","file9.go","file10.go",`,
			`"file11.go","file12.go","file13.go","file14.go","file15.go",`,
			`"file16.go","file17.go","file18.go","file19.go","file20.go"`,
			`],"depth":5}`,
		}

		for _, chunk := range argChunks {
			// Escape quotes for JSON
			escapedChunk := strings.ReplaceAll(chunk, `"`, `\"`)
			sendSSEEvent(w, "content_block_delta", fmt.Sprintf(`{"type":"content_block_delta","index":0,"delta":{"type":"input_json_delta","partial_json":"%s"}}`, escapedChunk))
		}

		// Send completion events
		sendSSEEvent(w, "content_block_stop", `{"type":"content_block_stop","index":0}`)
		sendSSEEvent(w, "message_delta", `{"type":"message_delta","delta":{"stop_reason":"tool_use","stop_sequence":null},"usage":{"output_tokens":50}}`)
		sendSSEEvent(w, "message_stop", `{"type":"message_stop"}`)
	}))
	defer server.Close()

	client := NewAnthropicClient(config.LLMConfig{
		APIKey:  "test-key",
		BaseURL: server.URL,
		Model:   "claude-3-sonnet-20240229",
	}, nil)

	ctx := context.Background()

	resp, err := client.GenerateCompletion(ctx, CompletionRequest{
		SystemPrompt: "You are a code analyzer",
		Messages: []Message{
			{Role: "user", Content: "Analyze all Go files"},
		},
		MaxTokens:   100,
		Temperature: 0.0,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Verify tool call
	if len(resp.ToolCalls) != 1 {
		t.Fatalf("Expected 1 tool call, got %d", len(resp.ToolCalls))
	}

	toolCall := resp.ToolCalls[0]
	if toolCall.Name != "analyze_files" {
		t.Errorf("Expected tool name 'analyze_files', got '%s'", toolCall.Name)
	}

	// Verify large arguments were accumulated correctly
	files, ok := toolCall.Arguments["files"].([]interface{})
	if !ok {
		t.Fatalf("Expected 'files' argument to be a list")
	}

	if len(files) != 20 {
		t.Errorf("Expected 20 files, got %d", len(files))
	}

	// Verify depth argument
	depth, ok := toolCall.Arguments["depth"].(float64)
	if !ok || int(depth) != 5 {
		t.Errorf("Expected depth=5, got %v", toolCall.Arguments["depth"])
	}
}
</file>
<file path="internal/llmcache/cache.go">
package llmcache

import (
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/user/gendocs/internal/logging"
)

const (
	// CacheVersion is the current cache format version.
	// This is used to detect incompatible cache formats when loading from disk.
	CacheVersion = 1
	// DefaultCacheFileName is the default cache file name.
	DefaultCacheFileName = ".ai/llm_cache.json"
	// DefaultTTL is the default time-to-live for cache entries.
	// Cached responses older than this are considered stale.
	DefaultTTL = 7 * 24 * time.Hour // 7 days
)

// CacheStats tracks cache performance metrics.
// These statistics help monitor cache effectiveness and efficiency.
type CacheStats struct {
	Hits           int64   `json:"hits"`             // Number of cache hits
	Misses         int64   `json:"misses"`           // Number of cache misses
	Evictions      int64   `json:"evictions"`        // Number of entries evicted
	Size           int     `json:"size"`             // Current number of entries
	MaxSize        int     `json:"max_size"`         // Maximum number of entries allowed
	TotalSizeBytes int64   `json:"total_size_bytes"` // Total size of all entries in bytes
	HitRate        float64 `json:"hit_rate"`         // Cache hit rate (0.0 to 1.0)
	mu             sync.RWMutex
}

// updateHitRate updates the hit rate calculation.
// Hit rate is the ratio of hits to total requests (hits + misses).
func (s *CacheStats) updateHitRate() {
	total := s.Hits + s.Misses
	if total > 0 {
		s.HitRate = float64(s.Hits) / float64(total)
	}
}

// RecordHit records a cache hit.
// This should be called when a cache lookup succeeds.
func (s *CacheStats) RecordHit() {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.Hits++
	s.updateHitRate()
}

// RecordMiss records a cache miss.
// This should be called when a cache lookup fails.
func (s *CacheStats) RecordMiss() {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.Misses++
	s.updateHitRate()
}

// RecordEviction records a cache eviction.
// This should be called when an entry is evicted from the cache.
func (s *CacheStats) RecordEviction() {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.Evictions++
}

// lruEntry represents a single cache entry in the LRU list.
// It's an internal type used by LRUCache to track cached items.
type lruEntry struct {
	key        string          // Cache key for this entry
	value      *CachedResponse // The cached response
	createdAt  time.Time       // When the entry was created
	accessedAt time.Time       // When the entry was last accessed
	sizeBytes  int64           // Approximate size in bytes
	prev, next *lruEntry       // Pointers for LRU doubly-linked list
}

// LRUCache implements a thread-safe LRU (Least Recently Used) cache for LLM responses.
//
// The cache automatically evicts the least recently used entries when it reaches capacity.
// It tracks hit/miss statistics and provides thread-safe access to cached data.
type LRUCache struct {
	maxSize    int                  // Maximum number of entries allowed
	size       int                  // Current number of entries
	cache      map[string]*lruEntry // Map from key to entry
	head, tail *lruEntry            // Head (most recent) and tail (least recent) of LRU list
	mu         sync.RWMutex         // Protects all cache access
	stats      CacheStats           // Cache performance statistics
	logger     *logging.Logger      // Logger for cache operations
}

// NewLRUCache creates a new LRU cache with the given maximum size.
//
// The cache will evict entries when it exceeds this size, using LRU policy.
func NewLRUCache(maxSize int) *LRUCache {
	return &LRUCache{
		maxSize: maxSize,
		cache:   make(map[string]*lruEntry),
		stats:   CacheStats{MaxSize: maxSize},
		logger:  logging.NewNopLogger(),
	}
}

// SetLogger sets the logger for the cache.
func (c *LRUCache) SetLogger(logger *logging.Logger) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.logger = logger
}

// Get retrieves a value from the cache.
//
// Returns the cached response and true if found and not expired.
// Returns nil and false if not found or expired.
// Updates the LRU order on cache hits.
func (c *LRUCache) Get(key string) (*CachedResponse, bool) {
	c.mu.Lock()
	defer c.mu.Unlock()

	entry, exists := c.cache[key]
	if !exists {
		c.stats.Misses++
		c.stats.updateHitRate()
		c.logger.Debug("cache_miss",
			logging.String("key", key),
			logging.Int64("total_misses", c.stats.Misses),
			logging.Float64("hit_rate", c.stats.HitRate))
		return nil, false
	}

	// Check TTL
	if time.Now().After(entry.value.ExpiresAt) {
		// Expired, remove from cache
		c.removeEntry(entry)
		c.stats.Misses++
		c.stats.updateHitRate()
		c.logger.Debug("cache_miss_expired",
			logging.String("key", key),
			logging.Time("expired_at", entry.value.ExpiresAt))
		return nil, false
	}

	// Move to front (most recently used)
	c.moveToFront(entry)

	// Update access time and count
	entry.accessedAt = time.Now()
	entry.value.RecordAccess()

	c.stats.Hits++
	c.stats.updateHitRate()
	c.logger.Debug("cache_hit",
		logging.String("key", key),
		logging.Int64("total_hits", c.stats.Hits),
		logging.Int("access_count", entry.value.AccessCount),
		logging.Float64("hit_rate", c.stats.HitRate))
	return entry.value, true
}

// Put stores a value in the cache.
//
// If the key already exists, the value is updated.
// If the cache is at capacity, the least recently used entry is evicted.
func (c *LRUCache) Put(key string, value *CachedResponse) {
	c.mu.Lock()
	defer c.mu.Unlock()

	if value.SizeBytes == 0 {
		value.SizeBytes = value.EstimateSize()
	}

	if value.ExpiresAt.IsZero() {
		value.ExpiresAt = time.Now().Add(DefaultTTL)
	}

	// Check if key already exists
	if entry, exists := c.cache[key]; exists {
		// Update existing entry
		oldSize := entry.sizeBytes
		entry.value = value
		entry.accessedAt = time.Now()
		entry.sizeBytes = value.SizeBytes
		c.moveToFront(entry)
		c.stats.TotalSizeBytes += (entry.sizeBytes - oldSize)
		c.logger.Debug("cache_update",
			logging.String("key", key),
			logging.Int64("old_size_bytes", oldSize),
			logging.Int64("new_size_bytes", entry.sizeBytes))
		return
	}

	// Create new entry
	entry := &lruEntry{
		key:        key,
		value:      value,
		createdAt:  time.Now(),
		accessedAt: time.Now(),
		sizeBytes:  value.SizeBytes,
	}

	// Add to cache
	c.cache[key] = entry
	c.addToFront(entry)
	c.size++
	c.stats.Size = c.size
	c.stats.TotalSizeBytes += entry.sizeBytes

	c.logger.Debug("cache_store",
		logging.String("key", key),
		logging.Int64("size_bytes", entry.sizeBytes),
		logging.Int("current_size", c.size),
		logging.Int("max_size", c.maxSize))

	// Evict if over capacity
	for c.size > c.maxSize {
		c.evictLRU()
	}
}

// Delete removes a value from the cache.
// Does nothing if the key doesn't exist.
func (c *LRUCache) Delete(key string) {
	c.mu.Lock()
	defer c.mu.Unlock()

	if entry, exists := c.cache[key]; exists {
		c.removeEntry(entry)
	}
}

// Clear removes all entries from the cache.
// Resets all statistics except MaxSize.
func (c *LRUCache) Clear() {
	c.mu.Lock()
	defer c.mu.Unlock()

	c.cache = make(map[string]*lruEntry)
	c.head = nil
	c.tail = nil
	c.size = 0
	c.stats.Size = 0
	c.stats.TotalSizeBytes = 0
}

// Size returns the current number of entries in the cache.
func (c *LRUCache) Size() int {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return c.size
}

// Stats returns a copy of the cache statistics.
// The copy is thread-safe and won't be affected by subsequent cache operations.
func (c *LRUCache) Stats() CacheStats {
	c.mu.RLock()
	defer c.mu.RUnlock()

	// Create a copy to avoid race conditions
	stats := c.stats //nolint:govet // intentional copy of stats
	return stats     //nolint:govet
}

// moveToFront moves an entry to the front of the LRU list.
// This is called when an entry is accessed to mark it as recently used.
func (c *LRUCache) moveToFront(entry *lruEntry) {
	if entry == c.head {
		return
	}

	// Remove from current position
	c.removeEntryList(entry)

	// Add to front
	c.addToFront(entry)
}

// addToFront adds an entry to the front of the LRU list.
func (c *LRUCache) addToFront(entry *lruEntry) {
	entry.prev = nil
	entry.next = c.head

	if c.head != nil {
		c.head.prev = entry
	}

	c.head = entry

	if c.tail == nil {
		c.tail = entry
	}
}

// removeEntry removes an entry from the cache and LRU list.
// Updates size statistics and decrements the cache size.
func (c *LRUCache) removeEntry(entry *lruEntry) {
	// Remove from map
	delete(c.cache, entry.key)

	// Update size tracking
	c.stats.TotalSizeBytes -= entry.sizeBytes
	c.size--
	c.stats.Size = c.size

	// Remove from list
	c.removeEntryList(entry)
}

// removeEntryList removes an entry from the LRU list only.
// Does not remove from the map or update statistics.
func (c *LRUCache) removeEntryList(entry *lruEntry) {
	if entry.prev != nil {
		entry.prev.next = entry.next
	} else {
		c.head = entry.next
	}

	if entry.next != nil {
		entry.next.prev = entry.prev
	} else {
		c.tail = entry.prev
	}
}

// evictLRU evicts the least recently used entry from the cache.
// This is called automatically when the cache exceeds its maximum size.
func (c *LRUCache) evictLRU() {
	if c.tail == nil {
		return
	}

	evictedKey := c.tail.key
	evictedSize := c.tail.sizeBytes

	// Remove from map
	delete(c.cache, c.tail.key)

	// Update size tracking
	c.stats.TotalSizeBytes -= c.tail.sizeBytes
	c.size--
	c.stats.Size = c.size

	// Remove from list
	if c.tail.prev != nil {
		c.tail.prev.next = nil
		c.tail = c.tail.prev
	} else {
		// Cache is now empty
		c.head = nil
		c.tail = nil
	}

	c.stats.Evictions++
	c.logger.Debug("cache_evict",
		logging.String("key", evictedKey),
		logging.Int64("size_bytes", evictedSize),
		logging.Int64("total_evictions", c.stats.Evictions),
		logging.Int("current_size", c.size))
}

// CleanupExpired removes all expired entries from the cache.
// Returns the number of entries that were removed.
func (c *LRUCache) CleanupExpired() int {
	c.mu.Lock()
	defer c.mu.Unlock()

	now := time.Now()
	expired := []*lruEntry{}

	// Find expired entries
	for _, entry := range c.cache {
		if now.After(entry.value.ExpiresAt) {
			expired = append(expired, entry)
		}
	}

	// Remove expired entries
	for _, entry := range expired {
		c.removeEntry(entry)
		c.stats.Evictions++
	}

	if len(expired) > 0 {
		c.logger.Info("cache_cleanup_expired",
			logging.Int("expired_count", len(expired)),
			logging.Int("remaining_size", c.size))
	}

	return len(expired)
}

// DiskCacheData represents the on-disk cache format.
// This structure is serialized to JSON for persistent storage.
type DiskCacheData struct {
	Version   int                       `json:"version"`    // Cache format version
	CreatedAt time.Time                 `json:"created_at"` // When the cache was created
	UpdatedAt time.Time                 `json:"updated_at"` // When the cache was last updated
	Entries   map[string]CachedResponse `json:"entries"`    // Cached entries
	Stats     DiskCacheStats            `json:"stats"`      // Cache statistics
	mu        sync.RWMutex              // Protects stats fields
}

// DiskCacheStats tracks disk cache statistics.
type DiskCacheStats struct {
	TotalEntries   int     `json:"total_entries"`    // Total number of entries in the cache
	ExpiredEntries int     `json:"expired_entries"`  // Number of entries that have expired
	TotalSizeBytes int64   `json:"total_size_bytes"` // Total size of all entries in bytes
	Hits           int64   `json:"hits"`             // Number of cache hits
	Misses         int64   `json:"misses"`           // Number of cache misses
	Evictions      int64   `json:"evictions"`        // Number of entries evicted
	HitRate        float64 `json:"hit_rate"`         // Cache hit rate (0.0 to 1.0)
}

// updateHitRate updates the hit rate calculation for disk cache stats.
func (s *DiskCacheStats) updateHitRate() {
	total := s.Hits + s.Misses
	if total > 0 {
		s.HitRate = float64(s.Hits) / float64(total)
	}
}

// DiskCache manages persistent storage of cached responses.
//
// The disk cache provides persistence across program restarts, allowing
// cached LLM responses to be reused between runs. It uses a JSON file
// for storage and supports atomic writes and checksum validation.
type DiskCache struct {
	filePath    string          // Path to the cache file
	ttl         time.Duration   // Default TTL for entries
	maxDiskSize int64           // Maximum disk size (not currently enforced)
	mu          sync.Mutex      // Protects all access
	data        *DiskCacheData  // In-memory cache data
	dirty       bool            // Whether data has changed since last save
	autoSave    bool            // Whether auto-save is running
	stopSave    chan struct{}   // Channel to stop auto-save goroutine
	logger      *logging.Logger // Logger for disk cache operations
}

// NewDiskCache creates a new disk cache.
//
// filePath: Path to the cache file (will be created if it doesn't exist)
// ttl: Default time-to-live for cached entries
// maxDiskSize: Maximum size of the disk cache in bytes (reserved for future use)
func NewDiskCache(filePath string, ttl time.Duration, maxDiskSize int64) *DiskCache {
	return &DiskCache{
		filePath:    filePath,
		ttl:         ttl,
		maxDiskSize: maxDiskSize,
		logger:      logging.NewNopLogger(),
	}
}

// SetLogger sets the logger for the disk cache.
func (dc *DiskCache) SetLogger(logger *logging.Logger) {
	dc.mu.Lock()
	defer dc.mu.Unlock()
	dc.logger = logger
}

// Load loads the cache from disk.
//
// If the cache file doesn't exist, creates an empty cache.
// If the file is corrupted or has an incompatible version,
// backs up the file and starts fresh.
// Validates checksums of all entries and removes corrupted ones.
func (dc *DiskCache) Load() error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	// Read file
	data, err := os.ReadFile(dc.filePath)
	if err != nil {
		if os.IsNotExist(err) {
			// New cache, create empty structure
			dc.data = dc.newCacheData()
			dc.logger.Info("disk_cache_load", logging.String("status", "new_cache"))
			return nil
		}
		dc.logger.Error("disk_cache_load_failed", logging.Error(err))
		return fmt.Errorf("failed to read cache file: %w", err)
	}

	// Unmarshal JSON
	var cacheData DiskCacheData
	if err := json.Unmarshal(data, &cacheData); err != nil {
		// Corrupted cache, backup and start fresh
		dc.backupCorruptedCache()
		dc.data = dc.newCacheData()
		dc.logger.Warn("disk_cache_corrupted", logging.String("action", "backup_and_reset"))
		return nil
	}

	// Check version
	if cacheData.Version != CacheVersion {
		// Version mismatch, start fresh
		dc.data = dc.newCacheData()
		dc.logger.Warn("disk_cache_version_mismatch",
			logging.Int("loaded_version", cacheData.Version),
			logging.Int("expected_version", CacheVersion),
			logging.String("action", "reset"))
		return nil
	}

	// Validate entry checksums and remove corrupted entries
	corruptedCount := 0
	for key, entry := range cacheData.Entries {
		if !entry.ValidateChecksum() {
			// Checksum validation failed, remove corrupted entry
			delete(cacheData.Entries, key)
			corruptedCount++
			dc.logger.Warn("disk_cache_corrupted_entry",
				logging.String("key", key),
				logging.String("action", "removed"))
		}
	}

	if corruptedCount > 0 {
		dc.logger.Info("disk_cache_validation",
			logging.Int("corrupted_entries", corruptedCount),
			logging.Int("valid_entries", len(cacheData.Entries)),
			logging.String("file_path", dc.filePath))
	}

	dc.data = &cacheData
	dc.logger.Info("disk_cache_load",
		logging.String("status", "success"),
		logging.Int("entries", len(dc.data.Entries)),
		logging.String("file_path", dc.filePath))
	return nil
}

// Save saves the cache to disk.
//
// Uses atomic writes (write to temp file, then rename) to prevent corruption.
func (dc *DiskCache) Save() error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	return dc.saveLocked()
}

// saveLocked saves the cache to disk (must be called with lock held).
func (dc *DiskCache) saveLocked() error {
	// Ensure directory exists
	dir := filepath.Dir(dc.filePath)
	if err := os.MkdirAll(dir, 0755); err != nil {
		dc.logger.Error("disk_cache_save_failed", logging.Error(err))
		return fmt.Errorf("failed to create cache directory: %w", err)
	}

	// Update metadata
	dc.data.UpdatedAt = time.Now()
	dc.updateStats()

	// Marshal to JSON
	data, err := json.MarshalIndent(dc.data, "", "  ")
	if err != nil {
		dc.logger.Error("disk_cache_save_failed", logging.Error(err))
		return fmt.Errorf("failed to marshal cache: %w", err)
	}

	// Write to temporary file first (atomic write)
	tmpFile := dc.filePath + ".tmp"
	if err := os.WriteFile(tmpFile, data, 0644); err != nil {
		dc.logger.Error("disk_cache_save_failed", logging.Error(err))
		return fmt.Errorf("failed to write cache: %w", err)
	}

	// Rename to actual file (atomic on Unix)
	if err := os.Rename(tmpFile, dc.filePath); err != nil {
		_ = os.Remove(tmpFile) // Clean up temp file
		dc.logger.Error("disk_cache_save_failed", logging.Error(err))
		return fmt.Errorf("failed to save cache: %w", err)
	}

	dc.dirty = false
	dc.logger.Debug("disk_cache_save",
		logging.Int("entries", len(dc.data.Entries)),
		logging.Int("total_size_bytes", int(dc.data.Stats.TotalSizeBytes)),
		logging.String("file_path", dc.filePath))
	return nil
}

// Get retrieves a value from the disk cache.
//
// Returns the cached response and true if found and not expired.
// Returns nil and false if not found or expired.
// Expired entries are removed from the cache.
func (dc *DiskCache) Get(key string) (*CachedResponse, bool) {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	if dc.data == nil {
		dc.recordMiss()
		dc.logger.Debug("disk_cache_miss", logging.String("reason", "cache_not_loaded"))
		return nil, false
	}

	entry, exists := dc.data.Entries[key]
	if !exists {
		dc.recordMiss()
		dc.logger.Debug("disk_cache_miss",
			logging.String("key", key),
			logging.Int64("total_misses", dc.data.Stats.Misses),
			logging.Float64("hit_rate", dc.data.Stats.HitRate))
		return nil, false
	}

	// Check TTL
	if entry.IsExpired() {
		delete(dc.data.Entries, key)
		dc.dirty = true
		dc.recordMiss()
		dc.logger.Debug("disk_cache_miss_expired",
			logging.String("key", key),
			logging.Time("expired_at", entry.ExpiresAt))
		return nil, false
	}

	// Record hit
	dc.recordHit()

	dc.logger.Debug("disk_cache_hit",
		logging.String("key", key),
		logging.Int64("total_hits", dc.data.Stats.Hits),
		logging.Float64("hit_rate", dc.data.Stats.HitRate))

	// Return a copy to avoid race conditions
	result := entry
	return &result, true
}

// Put stores a value in the disk cache.
//
// The value's checksum is automatically calculated and updated before storage.
// Marks the cache as dirty (needs to be saved to disk).
func (dc *DiskCache) Put(key string, value *CachedResponse) error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	if dc.data == nil {
		dc.data = dc.newCacheData()
	}

	// Ensure checksum is calculated and up-to-date before storing
	value.UpdateChecksum()

	existing, exists := dc.data.Entries[key]
	isNew := !exists || (&existing).IsExpired()
	dc.data.Entries[key] = *value
	dc.dirty = true

	if isNew {
		dc.logger.Debug("disk_cache_store",
			logging.String("key", key),
			logging.Int64("size_bytes", value.SizeBytes),
			logging.Int("total_entries", len(dc.data.Entries)))
	} else {
		dc.logger.Debug("disk_cache_update",
			logging.String("key", key),
			logging.Int64("size_bytes", value.SizeBytes))
	}

	return nil
}

// recordHit records a disk cache hit.
func (dc *DiskCache) recordHit() {
	if dc.data == nil {
		return
	}
	dc.data.mu.Lock()
	defer dc.data.mu.Unlock()
	dc.data.Stats.Hits++
	dc.data.Stats.updateHitRate()
}

// recordMiss records a disk cache miss.
func (dc *DiskCache) recordMiss() {
	if dc.data == nil {
		return
	}
	dc.data.mu.Lock()
	defer dc.data.mu.Unlock()
	dc.data.Stats.Misses++
	dc.data.Stats.updateHitRate()
}

// recordEviction records a disk cache eviction.
func (dc *DiskCache) recordEviction(count int) {
	if dc.data == nil {
		return
	}
	dc.data.mu.Lock()
	defer dc.data.mu.Unlock()
	dc.data.Stats.Evictions += int64(count)
}

// Stats returns a copy of the disk cache statistics.
// The copy is thread-safe and won't be affected by subsequent cache operations.
func (dc *DiskCache) Stats() DiskCacheStats {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	if dc.data == nil {
		return DiskCacheStats{}
	}

	// Update stats before returning to ensure TotalEntries is current
	dc.updateStats()

	dc.data.mu.RLock()
	defer dc.data.mu.RUnlock()

	// Create a copy to avoid race conditions
	stats := dc.data.Stats
	return stats
}

// Delete removes a value from the disk cache.
// Marks the cache as dirty (needs to be saved to disk).
func (dc *DiskCache) Delete(key string) error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	if dc.data == nil {
		return nil
	}

	if _, exists := dc.data.Entries[key]; exists {
		delete(dc.data.Entries, key)
		dc.dirty = true
	}

	return nil
}

// Clear removes all entries from the disk cache.
// Saves the empty cache to disk immediately.
func (dc *DiskCache) Clear() error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	dc.data = dc.newCacheData()
	dc.dirty = true

	return dc.saveLocked()
}

// CleanupExpired removes expired entries from the disk cache.
// Saves the cache to disk if any entries were removed.
func (dc *DiskCache) CleanupExpired() error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	if dc.data == nil {
		return nil
	}

	now := time.Now()
	expiredCount := 0

	// Remove expired entries
	for key, entry := range dc.data.Entries {
		if now.After(entry.ExpiresAt) {
			delete(dc.data.Entries, key)
			expiredCount++
		}
	}

	if expiredCount > 0 {
		dc.dirty = true
		dc.recordEviction(expiredCount)
		dc.logger.Info("disk_cache_cleanup_expired",
			logging.Int("expired_count", expiredCount),
			logging.Int("remaining_entries", len(dc.data.Entries)),
			logging.String("file_path", dc.filePath))
		return dc.saveLocked()
	}

	return nil
}

// newCacheData creates a new empty cache data structure.
func (dc *DiskCache) newCacheData() *DiskCacheData {
	return &DiskCacheData{
		Version:   CacheVersion,
		CreatedAt: time.Now(),
		UpdatedAt: time.Now(),
		Entries:   make(map[string]CachedResponse),
	}
}

// updateStats updates the disk cache statistics.
// Calculates total entries, expired entries, and total size.
func (dc *DiskCache) updateStats() {
	if dc.data == nil {
		return
	}

	now := time.Now()
	expiredCount := 0
	totalSize := int64(0)

	for _, entry := range dc.data.Entries {
		if now.After(entry.ExpiresAt) {
			expiredCount++
		}
		totalSize += entry.SizeBytes
	}

	// Preserve Hits, Misses, and Evictions counts while updating other fields
	dc.data.Stats.TotalEntries = len(dc.data.Entries)
	dc.data.Stats.ExpiredEntries = expiredCount
	dc.data.Stats.TotalSizeBytes = totalSize
}

// backupCorruptedCache backs up a corrupted cache file.
// Adds a timestamp to the backup file name.
func (dc *DiskCache) backupCorruptedCache() {
	timestamp := time.Now().Format("20060102-150405")
	backupPath := dc.filePath + ".corrupted." + timestamp
	_ = os.Rename(dc.filePath, backupPath)
}

// StartAutoSave starts background auto-save with the given interval.
//
// The cache is saved automatically at the specified interval if it has been modified.
// The save operation is non-blocking and runs in a separate goroutine.
func (dc *DiskCache) StartAutoSave(interval time.Duration) {
	dc.mu.Lock()
	if dc.autoSave {
		// Already started
		dc.mu.Unlock()
		return
	}
	dc.autoSave = true
	dc.mu.Unlock()

	dc.stopSave = make(chan struct{})

	go func() {
		ticker := time.NewTicker(interval)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				dc.mu.Lock()
				if dc.dirty {
					// Save without holding lock (copy data first)
					dataToSave := *dc.data //nolint:govet // intentional copy for async save
					dirtyFlag := dc.dirty
					dc.mu.Unlock()

					// Save asynchronously
					if dirtyFlag {
						// Log error but continue - non-blocking
						// TODO: Add logging in subtask 4-2
						_ = dc.saveData(&dataToSave)
					}
				} else {
					dc.mu.Unlock()
				}
			case <-dc.stopSave:
				// Stop signal received, do one final save if dirty
				dc.mu.Lock()
				if dc.dirty {
					dataToSave := *dc.data //nolint:govet // intentional copy for final save
					dc.mu.Unlock()
					_ = dc.saveData(&dataToSave)
				} else {
					dc.mu.Unlock()
				}
				return
			}
		}
	}()
}

// Stop stops the disk cache and performs final save if needed.
//
// Waits for the auto-save goroutine to finish and saves any pending changes.
func (dc *DiskCache) Stop() {
	dc.mu.Lock()
	if !dc.autoSave {
		dc.mu.Unlock()
		return
	}
	dc.autoSave = false
	dc.mu.Unlock()

	// Signal the background goroutine to stop
	if dc.stopSave != nil {
		close(dc.stopSave)
		dc.stopSave = nil
	}
}

// saveData saves the cache data to disk without holding the lock.
// This is used by the auto-save goroutine to avoid blocking cache operations.
func (dc *DiskCache) saveData(data *DiskCacheData) error {
	// Ensure directory exists
	dir := filepath.Dir(dc.filePath)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create cache directory: %w", err)
	}

	// Update metadata
	data.UpdatedAt = time.Now()

	// Marshal to JSON
	jsonData, err := json.MarshalIndent(data, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal cache: %w", err)
	}

	// Write to temporary file first (atomic write)
	tmpFile := dc.filePath + ".tmp"
	if err := os.WriteFile(tmpFile, jsonData, 0644); err != nil {
		return fmt.Errorf("failed to write cache: %w", err)
	}

	// Rename to actual file (atomic on Unix)
	if err := os.Rename(tmpFile, dc.filePath); err != nil {
		_ = os.Remove(tmpFile) // Clean up temp file
		return fmt.Errorf("failed to save cache: %w", err)
	}

	// Clear dirty flag after successful save
	dc.mu.Lock()
	dc.dirty = false
	dc.mu.Unlock()

	return nil
}
</file>
<file path="internal/llmcache/cache_test.go">
package llmcache

import (
	"sync"
	"testing"
	"time"

	"github.com/user/gendocs/internal/llmtypes"
)

// TestLRUCache_BasicOperations tests basic Get/Put/Delete operations
func TestLRUCache_BasicOperations(t *testing.T) {
	cache := NewLRUCache(10)

	t.Run("Put and Get", func(t *testing.T) {
		key := "test-key-1"
		value := &CachedResponse{
			Key: key,
			Response: llmtypes.CompletionResponse{
				Content: "test response",
			},
		}

		// Put value
		cache.Put(key, value)

		// Get value
		retrieved, found := cache.Get(key)
		if !found {
			t.Fatal("Expected to find value in cache")
		}

		if retrieved.Response.Content != "test response" {
			t.Errorf("Expected content 'test response', got '%s'", retrieved.Response.Content)
		}
	})

	t.Run("Get non-existent key", func(t *testing.T) {
		_, found := cache.Get("non-existent")
		if found {
			t.Error("Expected not to find non-existent key")
		}
	})

	t.Run("Delete key", func(t *testing.T) {
		key := "test-key-2"
		value := &CachedResponse{
			Key: key,
			Response: llmtypes.CompletionResponse{
				Content: "to be deleted",
			},
		}

		cache.Put(key, value)
		cache.Delete(key)

		_, found := cache.Get(key)
		if found {
			t.Error("Expected deleted key to not be found")
		}
	})

	t.Run("Update existing key", func(t *testing.T) {
		key := "test-key-3"
		value1 := &CachedResponse{
			Key: key,
			Response: llmtypes.CompletionResponse{
				Content: "first value",
			},
		}
		value2 := &CachedResponse{
			Key: key,
			Response: llmtypes.CompletionResponse{
				Content: "second value",
			},
		}

		cache.Put(key, value1)
		cache.Put(key, value2)

		retrieved, _ := cache.Get(key)
		if retrieved.Response.Content != "second value" {
			t.Errorf("Expected 'second value', got '%s'", retrieved.Response.Content)
		}

		// Size should still be 2 (test-key-1 from first test + test-key-3 update)
		if cache.Size() != 2 {
			t.Errorf("Expected cache size 2, got %d", cache.Size())
		}
	})
}

// TestLRUCache_LRUEviction tests that LRU eviction works correctly
func TestLRUCache_LRUEviction(t *testing.T) {
	t.Run("evict when exceeding maxSize", func(t *testing.T) {
		cache := NewLRUCache(3)

		// Fill cache to max
		cache.Put("key1", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value1"}})
		cache.Put("key2", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value2"}})
		cache.Put("key3", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value3"}})

		if cache.Size() != 3 {
			t.Errorf("Expected cache size 3, got %d", cache.Size())
		}

		// Add one more - should evict key1 (least recently used)
		cache.Put("key4", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value4"}})

		if cache.Size() != 3 {
			t.Errorf("Expected cache size 3 after eviction, got %d", cache.Size())
		}

		// key1 should be evicted
		_, found := cache.Get("key1")
		if found {
			t.Error("Expected key1 to be evicted")
		}

		// key2, key3, key4 should still exist
		for _, key := range []string{"key2", "key3", "key4"} {
			_, found := cache.Get(key)
			if !found {
				t.Errorf("Expected %s to still exist in cache", key)
			}
		}
	})

	t.Run("eviction respects access order", func(t *testing.T) {
		cache := NewLRUCache(3)

		// Fill cache
		cache.Put("key1", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value1"}})
		cache.Put("key2", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value2"}})
		cache.Put("key3", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value3"}})

		// Access key1 to make it more recently used
		cache.Get("key1")

		// Add key4 - should evict key2 (now least recently used)
		cache.Put("key4", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value4"}})

		// key1 should still exist (was accessed)
		_, found := cache.Get("key1")
		if !found {
			t.Error("Expected key1 to still exist after being accessed")
		}

		// key2 should be evicted
		_, found = cache.Get("key2")
		if found {
			t.Error("Expected key2 to be evicted")
		}

		// key3 and key4 should exist
		for _, key := range []string{"key3", "key4"} {
			_, found := cache.Get(key)
			if !found {
				t.Errorf("Expected %s to still exist", key)
			}
		}
	})

	t.Run("update makes entry recently used", func(t *testing.T) {
		cache := NewLRUCache(3)

		// Fill cache
		cache.Put("key1", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value1"}})
		cache.Put("key2", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value2"}})
		cache.Put("key3", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value3"}})

		// Update key1 to make it recently used
		cache.Put("key1", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value1-updated"}})

		// Add key4 - should evict key2 (least recently used)
		cache.Put("key4", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value4"}})

		// key1 should still exist (was updated)
		retrieved, found := cache.Get("key1")
		if !found {
			t.Error("Expected key1 to still exist after being updated")
		}
		if retrieved.Response.Content != "value1-updated" {
			t.Errorf("Expected updated value for key1, got '%s'", retrieved.Response.Content)
		}

		// key2 should be evicted
		_, found = cache.Get("key2")
		if found {
			t.Error("Expected key2 to be evicted")
		}
	})

	t.Run("multiple evictions", func(t *testing.T) {
		cache := NewLRUCache(3)

		// Fill cache
		cache.Put("key1", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value1"}})
		cache.Put("key2", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value2"}})
		cache.Put("key3", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value3"}})

		// Add 3 more entries - should evict key1, key2, key3 in that order
		for i := 4; i <= 6; i++ {
			key := string(rune('0' + i))
			cache.Put("key"+key, &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value" + key}})
		}

		if cache.Size() != 3 {
			t.Errorf("Expected cache size 3, got %d", cache.Size())
		}

		// Only key4, key5, key6 should exist
		expectedKeys := []string{"key4", "key5", "key6"}
		for _, key := range expectedKeys {
			_, found := cache.Get(key)
			if !found {
				t.Errorf("Expected %s to exist", key)
			}
		}

		// key1, key2, key3 should be evicted
		evictedKeys := []string{"key1", "key2", "key3"}
		for _, key := range evictedKeys {
			_, found := cache.Get(key)
			if found {
				t.Errorf("Expected %s to be evicted", key)
			}
		}
	})
}

// TestLRUCache_ConcurrentAccess tests thread-safety under concurrent access
func TestLRUCache_ConcurrentAccess(t *testing.T) {
	t.Run("concurrent reads", func(t *testing.T) {
		cache := NewLRUCache(100)

		// Pre-fill cache
		for i := 0; i < 50; i++ {
			key := string(rune('0' + i))
			cache.Put("key"+key, &CachedResponse{
				Response: llmtypes.CompletionResponse{Content: "value" + key},
			})
		}

		// Concurrent reads
		var wg sync.WaitGroup
		numGoroutines := 10
		readsPerGoroutine := 100

		for g := 0; g < numGoroutines; g++ {
			wg.Add(1)
			go func(goroutineID int) {
				defer wg.Done()
				for i := 0; i < readsPerGoroutine; i++ {
					key := "key" + string(rune('0'+(i%50)))
					cache.Get(key)
				}
			}(g)
		}

		wg.Wait()

		// Verify cache integrity
		stats := cache.Stats()
		if stats.Hits+stats.Misses != int64(numGoroutines*readsPerGoroutine) {
			t.Errorf("Expected %d total lookups, got %d", numGoroutines*readsPerGoroutine, stats.Hits+stats.Misses)
		}
	})

	t.Run("concurrent writes", func(t *testing.T) {
		cache := NewLRUCache(1000)

		var wg sync.WaitGroup
		numGoroutines := 10
		writesPerGoroutine := 100

		for g := 0; g < numGoroutines; g++ {
			wg.Add(1)
			go func(goroutineID int) {
				defer wg.Done()
				for i := 0; i < writesPerGoroutine; i++ {
					key := string(rune('a' + i))
					cache.Put("key"+string(rune('0'+goroutineID))+key, &CachedResponse{
						Response: llmtypes.CompletionResponse{Content: "value"},
					})
				}
			}(g)
		}

		wg.Wait()

		// All writes should succeed
		expectedSize := numGoroutines * writesPerGoroutine
		if cache.Size() != expectedSize {
			t.Errorf("Expected cache size %d, got %d", expectedSize, cache.Size())
		}
	})

	t.Run("concurrent reads and writes", func(t *testing.T) {
		cache := NewLRUCache(500)

		var wg sync.WaitGroup
		numGoroutines := 10
		operationsPerGoroutine := 100

		for g := 0; g < numGoroutines; g++ {
			wg.Add(2) // One reader, one writer per goroutine

			// Reader
			go func(goroutineID int) {
				defer wg.Done()
				for i := 0; i < operationsPerGoroutine; i++ {
					key := "key" + string(rune('0'+(i%100)))
					cache.Get(key)
				}
			}(g)

			// Writer
			go func(goroutineID int) {
				defer wg.Done()
				for i := 0; i < operationsPerGoroutine; i++ {
					key := string(rune('a' + i))
					cache.Put("key"+string(rune('0'+goroutineID))+key, &CachedResponse{
						Response: llmtypes.CompletionResponse{Content: "value"},
					})
				}
			}(g)
		}

		wg.Wait()

		// Verify cache integrity - should not panic or deadlock
		stats := cache.Stats()
		if stats.Hits+stats.Misses == 0 {
			t.Error("Expected some cache operations to occur")
		}
	})

	t.Run("concurrent updates to same key", func(t *testing.T) {
		cache := NewLRUCache(100)

		key := "shared-key"
		cache.Put(key, &CachedResponse{Response: llmtypes.CompletionResponse{Content: "initial"}})

		var wg sync.WaitGroup
		numGoroutines := 10
		updatesPerGoroutine := 50

		for g := 0; g < numGoroutines; g++ {
			wg.Add(1)
			go func(goroutineID int) {
				defer wg.Done()
				for i := 0; i < updatesPerGoroutine; i++ {
					cache.Put(key, &CachedResponse{
						Response: llmtypes.CompletionResponse{Content: "value"},
					})
				}
			}(g)
		}

		wg.Wait()

		// Key should still exist
		_, found := cache.Get(key)
		if !found {
			t.Error("Expected shared key to still exist after concurrent updates")
		}

		// Size should be 1 (all updates to same key)
		if cache.Size() != 1 {
			t.Errorf("Expected cache size 1, got %d", cache.Size())
		}
	})

	t.Run("concurrent deletions", func(t *testing.T) {
		cache := NewLRUCache(1000)

		// Pre-fill cache
		for i := 0; i < 100; i++ {
			key := "key" + string(rune('0'+i))
			cache.Put(key, &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value"}})
		}

		var wg sync.WaitGroup
		numGoroutines := 10

		// Start concurrent deletions
		for g := 0; g < numGoroutines; g++ {
			wg.Add(1)
			go func(goroutineID int) {
				defer wg.Done()
				for i := 0; i < 10; i++ {
					key := "key" + string(rune('0'+(i+goroutineID*10)))
					cache.Delete(key)
				}
			}(g)
		}

		wg.Wait()

		// All entries should be deleted
		if cache.Size() != 0 {
			t.Errorf("Expected cache size 0, got %d", cache.Size())
		}
	})
}

// TestLRUCache_Stats tests statistics tracking
func TestLRUCache_Stats(t *testing.T) {
	cache := NewLRUCache(10)

	t.Run("initial stats", func(t *testing.T) {
		stats := cache.Stats()

		if stats.Size != 0 {
			t.Errorf("Expected initial size 0, got %d", stats.Size)
		}

		if stats.Hits != 0 {
			t.Errorf("Expected initial hits 0, got %d", stats.Hits)
		}

		if stats.Misses != 0 {
			t.Errorf("Expected initial misses 0, got %d", stats.Misses)
		}

		if stats.Evictions != 0 {
			t.Errorf("Expected initial evictions 0, got %d", stats.Evictions)
		}
	})

	t.Run("hit and miss tracking", func(t *testing.T) {
		cache := NewLRUCache(10)
		cache.Put("key1", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value1"}})

		// Hit
		cache.Get("key1")

		// Miss
		cache.Get("non-existent")

		stats := cache.Stats()

		if stats.Hits != 1 {
			t.Errorf("Expected 1 hit, got %d", stats.Hits)
		}

		if stats.Misses != 1 {
			t.Errorf("Expected 1 miss, got %d", stats.Misses)
		}

		expectedHitRate := 1.0 / 2.0
		if stats.HitRate != expectedHitRate {
			t.Errorf("Expected hit rate %f, got %f", expectedHitRate, stats.HitRate)
		}
	})

	t.Run("eviction tracking", func(t *testing.T) {
		cache := NewLRUCache(2)

		cache.Put("key1", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value1"}})
		cache.Put("key2", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value2"}})
		cache.Put("key3", &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value3"}})

		stats := cache.Stats()

		if stats.Evictions != 1 {
			t.Errorf("Expected 1 eviction, got %d", stats.Evictions)
		}
	})
}

// TestLRUCache_SizeLimit tests that size limits are enforced
func TestLRUCache_SizeLimit(t *testing.T) {
	tests := []struct {
		name      string
		maxSize   int
		numItems  int
		finalSize int
	}{
		{"size limit 1", 1, 10, 1},
		{"size limit 5", 5, 20, 5},
		{"size limit 100", 100, 200, 100},
		{"size limit 1000", 1000, 1500, 1000},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			cache := NewLRUCache(tt.maxSize)

			// Add more items than maxSize
			for i := 0; i < tt.numItems; i++ {
				key := "key" + string(rune('0'+(i%10))) + string(rune('a'+(i/10)))
				cache.Put(key, &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value"}})
			}

			// Size should not exceed maxSize
			if cache.Size() != tt.finalSize {
				t.Errorf("Expected cache size %d, got %d", tt.finalSize, cache.Size())
			}

			stats := cache.Stats()
			if stats.Size != tt.finalSize {
				t.Errorf("Expected stats size %d, got %d", tt.finalSize, stats.Size)
			}
		})
	}
}

// TestLRUCache_Clear tests cache clearing
func TestLRUCache_Clear(t *testing.T) {
	cache := NewLRUCache(10)

	// Add some entries
	for i := 0; i < 5; i++ {
		key := "key" + string(rune('0'+i))
		cache.Put(key, &CachedResponse{Response: llmtypes.CompletionResponse{Content: "value"}})
	}

	if cache.Size() != 5 {
		t.Errorf("Expected cache size 5, got %d", cache.Size())
	}

	// Clear cache
	cache.Clear()

	// Verify all entries are gone
	if cache.Size() != 0 {
		t.Errorf("Expected cache size 0 after clear, got %d", cache.Size())
	}

	_, found := cache.Get("key0")
	if found {
		t.Error("Expected no entries after clear")
	}
}

// TestLRUCache_TTL tests TTL expiration
func TestLRUCache_TTL(t *testing.T) {
	t.Run("expired entry returns not found", func(t *testing.T) {
		cache := NewLRUCache(10)

		key := "expired-key"
		value := &CachedResponse{
			Key:       key,
			ExpiresAt: time.Now().Add(-1 * time.Hour), // Expired 1 hour ago
			Response:  llmtypes.CompletionResponse{Content: "expired value"},
		}

		cache.Put(key, value)

		// Get should return not found for expired entry
		_, found := cache.Get(key)
		if found {
			t.Error("Expected expired entry to return not found")
		}

		// Stats should record this as a miss
		stats := cache.Stats()
		if stats.Misses == 0 {
			t.Error("Expected miss for expired entry")
		}
	})

	t.Run("non-expired entry returns found", func(t *testing.T) {
		cache := NewLRUCache(10)

		key := "valid-key"
		value := &CachedResponse{
			Key:       key,
			ExpiresAt: time.Now().Add(1 * time.Hour), // Expires in 1 hour
			Response:  llmtypes.CompletionResponse{Content: "valid value"},
		}

		cache.Put(key, value)

		// Get should return the entry
		retrieved, found := cache.Get(key)
		if !found {
			t.Error("Expected valid entry to be found")
		}

		if retrieved.Response.Content != "valid value" {
			t.Errorf("Expected 'valid value', got '%s'", retrieved.Response.Content)
		}

		// Stats should record this as a hit
		stats := cache.Stats()
		if stats.Hits == 0 {
			t.Error("Expected hit for valid entry")
		}
	})

	t.Run("cleanupExpired removes expired entries", func(t *testing.T) {
		cache := NewLRUCache(10)

		// Add expired entries
		for i := 0; i < 3; i++ {
			key := "expired" + string(rune('0'+i))
			cache.Put(key, &CachedResponse{
				Key:       key,
				ExpiresAt: time.Now().Add(-1 * time.Hour),
				Response:  llmtypes.CompletionResponse{Content: "expired"},
			})
		}

		// Add valid entries
		for i := 0; i < 5; i++ {
			key := "valid" + string(rune('0'+i))
			cache.Put(key, &CachedResponse{
				Key:       key,
				ExpiresAt: time.Now().Add(1 * time.Hour),
				Response:  llmtypes.CompletionResponse{Content: "valid"},
			})
		}

		if cache.Size() != 8 {
			t.Errorf("Expected cache size 8, got %d", cache.Size())
		}

		// Cleanup expired entries
		expiredCount := cache.CleanupExpired()

		if expiredCount != 3 {
			t.Errorf("Expected 3 expired entries, got %d", expiredCount)
		}

		if cache.Size() != 5 {
			t.Errorf("Expected cache size 5 after cleanup, got %d", cache.Size())
		}

		// Verify expired entries are gone
		for i := 0; i < 3; i++ {
			key := "expired" + string(rune('0'+i))
			_, found := cache.Get(key)
			if found {
				t.Errorf("Expected expired entry %s to be removed", key)
			}
		}

		// Verify valid entries still exist
		for i := 0; i < 5; i++ {
			key := "valid" + string(rune('0'+i))
			_, found := cache.Get(key)
			if !found {
				t.Errorf("Expected valid entry %s to still exist", key)
			}
		}
	})
}

// TestLRUCache_AccessCount tests access count tracking
func TestLRUCache_AccessCount(t *testing.T) {
	cache := NewLRUCache(10)

	key := "test-key"
	value := &CachedResponse{
		Key: key,
		Response: llmtypes.CompletionResponse{
			Content: "test value",
		},
		AccessCount: 0,
	}

	cache.Put(key, value)

	// First access
	cache.Get(key)
	retrieved, _ := cache.Get(key)

	if retrieved.AccessCount != 2 {
		t.Errorf("Expected access count 2, got %d", retrieved.AccessCount)
	}
}
</file>
<file path="internal/llmcache/entry.go">
package llmcache

import (
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"time"

	"github.com/user/gendocs/internal/llmtypes"
)

// CachedResponse represents a cached LLM response with metadata.
// It stores the original request, the LLM's response, and various metadata
// for cache management and validation.
type CachedResponse struct {
	Key         string                      `json:"key"`          // Cache key (SHA256 hash)
	Request     CacheKeyRequest             `json:"request"`      // Original request (for validation)
	Response    llmtypes.CompletionResponse `json:"response"`     // LLM response content
	CreatedAt   time.Time                   `json:"created_at"`   // Timestamp when the entry was cached
	ExpiresAt   time.Time                   `json:"expires_at"`   // Timestamp when the entry expires
	SizeBytes   int64                       `json:"size_bytes"`   // Approximate size in memory (in bytes)
	AccessCount int                         `json:"access_count"` // Number of times this entry has been accessed
	Checksum    string                      `json:"checksum"`     // SHA256 checksum for data integrity validation
}

// NewCachedResponse creates a new CachedResponse with the given TTL.
//
// The TTL (time-to-live) determines how long the cached response remains valid.
// After expiration, the cached response will not be used even if found in cache.
func NewCachedResponse(key string, request CacheKeyRequest, response llmtypes.CompletionResponse, ttl time.Duration) *CachedResponse {
	now := time.Now()
	return &CachedResponse{
		Key:         key,
		Request:     request,
		Response:    response,
		CreatedAt:   now,
		ExpiresAt:   now.Add(ttl),
		SizeBytes:   0, // Will be estimated on first use
		AccessCount: 0,
	}
}

// EstimateSize calculates the approximate size of this entry in bytes.
//
// This is useful for cache management to track memory usage.
// If JSON marshaling fails, it returns a conservative 1KB estimate.
func (cr *CachedResponse) EstimateSize() int64 {
	// Serialize to JSON to estimate size
	data, err := json.Marshal(cr)
	if err != nil {
		// Fallback estimate if marshaling fails
		return 1024 // Assume 1KB
	}
	return int64(len(data))
}

// IsExpired checks if this cache entry has expired.
//
// Returns true if the current time is past the ExpiresAt timestamp,
// meaning the cached response should not be used.
func (cr *CachedResponse) IsExpired() bool {
	return time.Now().After(cr.ExpiresAt)
}

// RecordAccess updates access metadata when this entry is accessed.
//
// This increments the AccessCount counter, which can be useful
// for analytics and cache management decisions.
func (cr *CachedResponse) RecordAccess() {
	cr.AccessCount++
}

// CalculateChecksum computes the SHA256 checksum of the cached response data.
//
// The checksum is computed over the key, request, and response (not metadata)
// to detect data corruption. This is important for disk cache integrity validation.
func (cr *CachedResponse) CalculateChecksum() string {
	// Create a representation of the data to checksum
	// We include all fields that represent the actual cached data
	dataToHash := struct {
		Key      string                      `json:"key"`
		Request  CacheKeyRequest             `json:"request"`
		Response llmtypes.CompletionResponse `json:"response"`
	}{
		Key:      cr.Key,
		Request:  cr.Request,
		Response: cr.Response,
	}

	// Serialize to JSON for consistent hashing
	jsonData, err := json.Marshal(dataToHash)
	if err != nil {
		// Fallback: hash the key if serialization fails
		hash := sha256.Sum256([]byte(cr.Key))
		return hex.EncodeToString(hash[:])
	}

	// Compute SHA256 hash
	hash := sha256.Sum256(jsonData)
	return hex.EncodeToString(hash[:])
}

// ValidateChecksum checks if the stored checksum matches the calculated checksum.
//
// Returns true if the checksum is valid or if no checksum is stored (for backward compatibility).
// This is used to detect data corruption in cached entries, particularly for disk cache.
func (cr *CachedResponse) ValidateChecksum() bool {
	// If no checksum is stored, consider it valid (backward compatibility)
	if cr.Checksum == "" {
		return true
	}

	calculatedChecksum := cr.CalculateChecksum()
	return cr.Checksum == calculatedChecksum
}

// UpdateChecksum recalculates and updates the checksum for this entry.
//
// This should be called after modifying the entry to ensure data integrity.
func (cr *CachedResponse) UpdateChecksum() {
	cr.Checksum = cr.CalculateChecksum()
}
</file>
<file path="internal/llmcache/key.go">
package llmcache

import (
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"sort"
	"strings"

	"github.com/user/gendocs/internal/llmtypes"
)

// CacheKeyRequest represents the fields used for cache key generation.
// It contains the essential elements of an LLM request that affect the response.
type CacheKeyRequest struct {
	SystemPrompt string            `json:"system_prompt"` // System prompt that guides the LLM behavior
	Messages     []CacheKeyMessage `json:"messages"`      // Conversation messages (order matters)
	Tools        []CacheKeyTool    `json:"tools"`         // Available tools (sorted for order independence)
	Temperature  float64           `json:"temperature"`   // Sampling temperature (affects response randomness)
}

// CacheKeyMessage represents a message in cache key generation.
// It captures the essential components of a chat message.
type CacheKeyMessage struct {
	Role    string `json:"role"`              // Message role: "system", "user", "assistant", or "tool"
	Content string `json:"content"`           // Message content text
	ToolID  string `json:"tool_id,omitempty"` // ID of the tool being responded to (for tool messages)
}

// CacheKeyTool represents a tool in cache key generation.
// It defines a tool/function that the LLM can call.
type CacheKeyTool struct {
	Name        string                 `json:"name"`        // Tool identifier name
	Description string                 `json:"description"` // Tool description for the LLM
	Parameters  map[string]interface{} `json:"parameters"`  // Tool parameter schema
}

// GenerateCacheKey generates a unique cache key from a CompletionRequest.
//
// The cache key is a SHA256 hash derived from the canonical JSON representation
// of the request. This ensures that identical requests generate the same key,
// enabling efficient cache lookups.
//
// Key generation details:
// - System prompt is trimmed and included
// - Messages are preserved in order (order affects LLM responses)
// - Tools are sorted by name for order-independent hashing
// - Temperature is included (affects response randomness)
//
// Returns an error if JSON marshaling fails.
func GenerateCacheKey(req llmtypes.CompletionRequest) (string, error) {
	// Create cache key request
	keyReq := CacheKeyRequest{
		SystemPrompt: strings.TrimSpace(req.SystemPrompt),
		Temperature:  req.Temperature,
	}

	// Convert messages (preserving order - message order is significant)
	for _, msg := range req.Messages {
		keyReq.Messages = append(keyReq.Messages, CacheKeyMessage{
			Role:    msg.Role,
			Content: strings.TrimSpace(msg.Content),
			ToolID:  msg.ToolID,
		})
	}

	// Convert tools and sort by name (tool order doesn't affect LLM behavior)
	tools := make([]CacheKeyTool, len(req.Tools))
	for i, tool := range req.Tools {
		tools[i] = CacheKeyTool{
			Name:        strings.TrimSpace(tool.Name),
			Description: strings.TrimSpace(tool.Description),
			Parameters:  tool.Parameters,
		}
	}
	// Sort tools by name for order-independent hashing
	sort.Slice(tools, func(i, j int) bool {
		return tools[i].Name < tools[j].Name
	})
	keyReq.Tools = tools

	// Marshal to canonical JSON
	data, err := json.Marshal(keyReq)
	if err != nil {
		return "", fmt.Errorf("failed to marshal cache key request: %w", err)
	}

	// Compute SHA256 hash
	hash := sha256.Sum256(data)

	// Return hex-encoded hash
	return hex.EncodeToString(hash[:]), nil
}

// CacheKeyRequestFrom converts a CompletionRequest to a CacheKeyRequest.
//
// This function extracts and normalizes the fields needed for cache key generation.
// It's useful when you need to store the request alongside the cached response
// for validation or debugging purposes.
func CacheKeyRequestFrom(req llmtypes.CompletionRequest) CacheKeyRequest {
	keyReq := CacheKeyRequest{
		SystemPrompt: strings.TrimSpace(req.SystemPrompt),
		Temperature:  req.Temperature,
	}

	// Convert messages (preserving order)
	for _, msg := range req.Messages {
		keyReq.Messages = append(keyReq.Messages, CacheKeyMessage{
			Role:    msg.Role,
			Content: strings.TrimSpace(msg.Content),
			ToolID:  msg.ToolID,
		})
	}

	// Convert tools and sort by name
	tools := make([]CacheKeyTool, len(req.Tools))
	for i, tool := range req.Tools {
		tools[i] = CacheKeyTool{
			Name:        strings.TrimSpace(tool.Name),
			Description: strings.TrimSpace(tool.Description),
			Parameters:  tool.Parameters,
		}
	}
	sort.Slice(tools, func(i, j int) bool {
		return tools[i].Name < tools[j].Name
	})
	keyReq.Tools = tools

	return keyReq
}
</file>
<file path="internal/llmcache/key_test.go">
package llmcache

import (
	"testing"

	"github.com/user/gendocs/internal/llmtypes"
)

func TestGenerateCacheKey_IdenticalRequests_SameKey(t *testing.T) {
	tests := []struct {
		name string
		req  llmtypes.CompletionRequest
	}{
		{
			name: "simple request",
			req: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages: []llmtypes.Message{
					{Role: "user", Content: "Hello"},
				},
				Temperature: 0.7,
			},
		},
		{
			name: "request with tools",
			req: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages: []llmtypes.Message{
					{Role: "user", Content: "Read a file"},
				},
				Tools: []llmtypes.ToolDefinition{
					{Name: "read_file", Description: "Read a file", Parameters: map[string]interface{}{"type": "object"}},
				},
				Temperature: 0.5,
			},
		},
		{
			name: "request with multiple messages",
			req: llmtypes.CompletionRequest{
				SystemPrompt: "You are a code analyzer",
				Messages: []llmtypes.Message{
					{Role: "user", Content: "Analyze this code"},
					{Role: "assistant", Content: "I'll analyze it"},
					{Role: "user", Content: "Thanks"},
				},
				Temperature: 0.0,
			},
		},
		{
			name: "request with multiple tools",
			req: llmtypes.CompletionRequest{
				SystemPrompt: "You are a file system assistant",
				Messages: []llmtypes.Message{
					{Role: "user", Content: "List files"},
				},
				Tools: []llmtypes.ToolDefinition{
					{Name: "read_file", Description: "Read file", Parameters: map[string]interface{}{"type": "object"}},
					{Name: "list_files", Description: "List files", Parameters: map[string]interface{}{"type": "object"}},
					{Name: "write_file", Description: "Write file", Parameters: map[string]interface{}{"type": "object"}},
				},
				Temperature: 0.3,
			},
		},
		{
			name: "request with tool calls in messages",
			req: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages: []llmtypes.Message{
					{Role: "user", Content: "Read file test.txt"},
					{Role: "assistant", Content: "", ToolID: "call_123"},
					{Role: "tool", Content: "File content here", ToolID: "call_123"},
				},
				Temperature: 0.7,
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Generate key twice
			key1, err1 := GenerateCacheKey(tt.req)
			key2, err2 := GenerateCacheKey(tt.req)

			// Verify no errors
			if err1 != nil {
				t.Fatalf("First GenerateCacheKey failed: %v", err1)
			}
			if err2 != nil {
				t.Fatalf("Second GenerateCacheKey failed: %v", err2)
			}

			// Verify keys are identical
			if key1 != key2 {
				t.Errorf("Expected identical keys, got:\n  key1: %s\n  key2: %s", key1, key2)
			}

			// Verify key is not empty
			if key1 == "" {
				t.Error("Generated key is empty")
			}

			// Verify key is SHA256 length (64 hex characters)
			if len(key1) != 64 {
				t.Errorf("Expected key length 64, got %d", len(key1))
			}
		})
	}
}

func TestGenerateCacheKey_DifferentRequests_DifferentKeys(t *testing.T) {
	tests := []struct {
		name     string
		req1     llmtypes.CompletionRequest
		req2     llmtypes.CompletionRequest
		expected bool // true if keys should be different
	}{
		{
			name: "different system prompt",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a coding assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.7,
			},
			expected: true,
		},
		{
			name: "different message content",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Goodbye"}},
				Temperature:  0.7,
			},
			expected: true,
		},
		{
			name: "different message order",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages: []llmtypes.Message{
					{Role: "user", Content: "First"},
					{Role: "user", Content: "Second"},
				},
				Temperature: 0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages: []llmtypes.Message{
					{Role: "user", Content: "Second"},
					{Role: "user", Content: "First"},
				},
				Temperature: 0.7,
			},
			expected: true, // Message order matters
		},
		{
			name: "different temperature",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.0,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  1.0,
			},
			expected: true,
		},
		{
			name: "different tools",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools: []llmtypes.ToolDefinition{
					{Name: "read_file", Description: "Read file", Parameters: map[string]interface{}{"type": "object"}},
				},
				Temperature: 0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools: []llmtypes.ToolDefinition{
					{Name: "write_file", Description: "Write file", Parameters: map[string]interface{}{"type": "object"}},
				},
				Temperature: 0.7,
			},
			expected: true,
		},
		{
			name: "different tool parameters",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools: []llmtypes.ToolDefinition{
					{Name: "read_file", Description: "Read file", Parameters: map[string]interface{}{"type": "object"}},
				},
				Temperature: 0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools: []llmtypes.ToolDefinition{
					{Name: "read_file", Description: "Read file", Parameters: map[string]interface{}{"type": "string"}},
				},
				Temperature: 0.7,
			},
			expected: true,
		},
		{
			name: "same tool different order",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools: []llmtypes.ToolDefinition{
					{Name: "read_file", Description: "Read file", Parameters: map[string]interface{}{"type": "object"}},
					{Name: "write_file", Description: "Write file", Parameters: map[string]interface{}{"type": "object"}},
				},
				Temperature: 0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools: []llmtypes.ToolDefinition{
					{Name: "write_file", Description: "Write file", Parameters: map[string]interface{}{"type": "object"}},
					{Name: "read_file", Description: "Read file", Parameters: map[string]interface{}{"type": "object"}},
				},
				Temperature: 0.7,
			},
			expected: false, // Tool order doesn't matter - tools are sorted
		},
		{
			name: "different message roles",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "assistant", Content: "Hello"}},
				Temperature:  0.7,
			},
			expected: true,
		},
		{
			name: "different tool IDs",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "tool", Content: "Result", ToolID: "call_123"}},
				Temperature:  0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "tool", Content: "Result", ToolID: "call_456"}},
				Temperature:  0.7,
			},
			expected: true,
		},
		{
			name: "with vs without tools",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools:        []llmtypes.ToolDefinition{{Name: "read_file", Description: "Read"}},
				Temperature:  0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.7,
			},
			expected: true,
		},
		{
			name: "max tokens ignored (same key)",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				MaxTokens:    100,
				Temperature:  0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				MaxTokens:    500,
				Temperature:  0.7,
			},
			expected: false, // MaxTokens should not affect key
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			key1, err1 := GenerateCacheKey(tt.req1)
			key2, err2 := GenerateCacheKey(tt.req2)

			// Verify no errors
			if err1 != nil {
				t.Fatalf("First GenerateCacheKey failed: %v", err1)
			}
			if err2 != nil {
				t.Fatalf("Second GenerateCacheKey failed: %v", err2)
			}

			// Verify keys match expectation
			if tt.expected && key1 == key2 {
				t.Errorf("Expected different keys, but got same key: %s", key1)
			}
			if !tt.expected && key1 != key2 {
				t.Errorf("Expected same keys, got:\n  key1: %s\n  key2: %s", key1, key2)
			}
		})
	}
}

func TestGenerateCacheKey_WhitespaceTrimming(t *testing.T) {
	tests := []struct {
		name string
		req1 llmtypes.CompletionRequest
		req2 llmtypes.CompletionRequest
	}{
		{
			name: "system prompt whitespace",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "  You are a helpful assistant  ",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.7,
			},
		},
		{
			name: "message content whitespace",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "  Hello  "}},
				Temperature:  0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.7,
			},
		},
		{
			name: "tool description whitespace",
			req1: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools: []llmtypes.ToolDefinition{
					{Name: "read_file", Description: "  Read a file  ", Parameters: map[string]interface{}{"type": "object"}},
				},
				Temperature: 0.7,
			},
			req2: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools: []llmtypes.ToolDefinition{
					{Name: "read_file", Description: "Read a file", Parameters: map[string]interface{}{"type": "object"}},
				},
				Temperature: 0.7,
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			key1, err1 := GenerateCacheKey(tt.req1)
			key2, err2 := GenerateCacheKey(tt.req2)

			// Verify no errors
			if err1 != nil {
				t.Fatalf("First GenerateCacheKey failed: %v", err1)
			}
			if err2 != nil {
				t.Fatalf("Second GenerateCacheKey failed: %v", err2)
			}

			// Verify keys are identical after trimming
			if key1 != key2 {
				t.Errorf("Expected identical keys after whitespace trimming, got:\n  key1: %s\n  key2: %s", key1, key2)
			}
		})
	}
}

func TestGenerateCacheKey_EmptyFields(t *testing.T) {
	tests := []struct {
		name string
		req  llmtypes.CompletionRequest
	}{
		{
			name: "empty system prompt",
			req: llmtypes.CompletionRequest{
				SystemPrompt: "",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Temperature:  0.7,
			},
		},
		{
			name: "empty messages",
			req: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{},
				Temperature:  0.7,
			},
		},
		{
			name: "empty tools",
			req: llmtypes.CompletionRequest{
				SystemPrompt: "You are a helpful assistant",
				Messages:     []llmtypes.Message{{Role: "user", Content: "Hello"}},
				Tools:        []llmtypes.ToolDefinition{},
				Temperature:  0.7,
			},
		},
		{
			name: "all fields empty or zero",
			req: llmtypes.CompletionRequest{
				SystemPrompt: "",
				Messages:     []llmtypes.Message{},
				Tools:        []llmtypes.ToolDefinition{},
				Temperature:  0.0,
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			key, err := GenerateCacheKey(tt.req)

			// Verify no errors
			if err != nil {
				t.Fatalf("GenerateCacheKey failed: %v", err)
			}

			// Verify key is not empty
			if key == "" {
				t.Error("Generated key is empty for request with empty fields")
			}

			// Verify key is SHA256 length
			if len(key) != 64 {
				t.Errorf("Expected key length 64, got %d", len(key))
			}

			// Verify consistency - same request should produce same key
			key2, err2 := GenerateCacheKey(tt.req)
			if err2 != nil {
				t.Fatalf("Second GenerateCacheKey failed: %v", err2)
			}
			if key != key2 {
				t.Errorf("Expected consistent keys for empty fields request, got:\n  key1: %s\n  key2: %s", key, key2)
			}
		})
	}
}

func TestCacheKeyRequestFrom_ConsistencyWithGenerateCacheKey(t *testing.T) {
	req := llmtypes.CompletionRequest{
		SystemPrompt: "You are a helpful assistant",
		Messages: []llmtypes.Message{
			{Role: "user", Content: "Hello"},
			{Role: "assistant", Content: "Hi there"},
		},
		Tools: []llmtypes.ToolDefinition{
			{Name: "read_file", Description: "Read file", Parameters: map[string]interface{}{"type": "object"}},
		},
		Temperature: 0.7,
	}

	// Generate key using GenerateCacheKey
	key1, err1 := GenerateCacheKey(req)
	if err1 != nil {
		t.Fatalf("GenerateCacheKey failed: %v", err1)
	}

	// Convert to CacheKeyRequest and verify it has the expected fields
	keyReq := CacheKeyRequestFrom(req)

	// Verify fields match
	if keyReq.SystemPrompt != req.SystemPrompt {
		t.Errorf("Expected SystemPrompt '%s', got '%s'", req.SystemPrompt, keyReq.SystemPrompt)
	}

	if len(keyReq.Messages) != len(req.Messages) {
		t.Errorf("Expected %d messages, got %d", len(req.Messages), len(keyReq.Messages))
	}

	if len(keyReq.Tools) != len(req.Tools) {
		t.Errorf("Expected %d tools, got %d", len(req.Tools), len(keyReq.Tools))
	}

	if keyReq.Temperature != req.Temperature {
		t.Errorf("Expected Temperature %f, got %f", req.Temperature, keyReq.Temperature)
	}

	// Verify that GenerateCacheKey is essentially doing what CacheKeyRequestFrom does
	// by calling GenerateCacheKey again and getting the same result
	key2, err2 := GenerateCacheKey(req)
	if err2 != nil {
		t.Fatalf("Second GenerateCacheKey failed: %v", err2)
	}

	if key1 != key2 {
		t.Errorf("Expected consistent keys, got:\n  key1: %s\n  key2: %s", key1, key2)
	}
}
</file>
<file path="internal/llmcache/persistence_test.go">
package llmcache

import (
	"encoding/json"
	"os"
	"path/filepath"
	"testing"
	"time"

	"github.com/user/gendocs/internal/llmtypes"
)

// TestDiskCache_BasicOperations tests basic Get/Put/Delete/Clear operations
func TestDiskCache_BasicOperations(t *testing.T) {
	t.Run("Put and Get", func(t *testing.T) {
		// Create temporary cache file
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024) // 100MB

		// Load the cache
		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		key := "test-key-1"
		value := &CachedResponse{
			Key: key,
			Response: llmtypes.CompletionResponse{
				Content: "test response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}

		// Put value
		if err := cache.Put(key, value); err != nil {
			t.Fatalf("Failed to put value: %v", err)
		}

		// Get value
		retrieved, found := cache.Get(key)
		if !found {
			t.Fatal("Expected to find value in cache")
		}

		if retrieved.Response.Content != "test response" {
			t.Errorf("Expected content 'test response', got '%s'", retrieved.Response.Content)
		}

		// Verify checksum was calculated
		if retrieved.Checksum == "" {
			t.Error("Expected checksum to be calculated")
		}

		// Cleanup
		cache.Stop()
	})

	t.Run("Get non-existent key", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		_, found := cache.Get("non-existent")
		if found {
			t.Error("Expected not to find non-existent key")
		}

		cache.Stop()
	})

	t.Run("Delete key", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		key := "test-key-2"
		value := &CachedResponse{
			Key: key,
			Response: llmtypes.CompletionResponse{
				Content: "to be deleted",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}

		// Put value
		if err := cache.Put(key, value); err != nil {
			t.Fatalf("Failed to put value: %v", err)
		}

		// Delete value
		if err := cache.Delete(key); err != nil {
			t.Fatalf("Failed to delete value: %v", err)
		}

		// Verify deleted
		_, found := cache.Get(key)
		if found {
			t.Error("Expected key to be deleted")
		}

		cache.Stop()
	})

	t.Run("Clear all entries", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Add multiple entries
		for i := 1; i <= 5; i++ {
			key := "test-key-" + string(rune('0'+i))
			value := &CachedResponse{
				Key: key,
				Response: llmtypes.CompletionResponse{
					Content: "test response",
				},
				CreatedAt:   time.Now(),
				ExpiresAt:   time.Now().Add(DefaultTTL),
				AccessCount: 0,
			}
			if err := cache.Put(key, value); err != nil {
				t.Fatalf("Failed to put value %d: %v", i, err)
			}
		}

		// Clear cache
		if err := cache.Clear(); err != nil {
			t.Fatalf("Failed to clear cache: %v", err)
		}

		// Verify all entries are gone
		stats := cache.Stats()
		if stats.TotalEntries != 0 {
			t.Errorf("Expected 0 entries after clear, got %d", stats.TotalEntries)
		}

		cache.Stop()
	})
}

// TestDiskCache_Persistence tests saving and loading cache data
func TestDiskCache_Persistence(t *testing.T) {
	t.Run("Save and load cache", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")

		// Create cache and add entries
		cache1 := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)
		if err := cache1.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Add test entries
		entry1 := &CachedResponse{
			Key: "key1",
			Response: llmtypes.CompletionResponse{
				Content: "response 1",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}
		entry2 := &CachedResponse{
			Key: "key2",
			Response: llmtypes.CompletionResponse{
				Content: "response 2",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}

		if err := cache1.Put("key1", entry1); err != nil {
			t.Fatalf("Failed to put entry1: %v", err)
		}
		if err := cache1.Put("key2", entry2); err != nil {
			t.Fatalf("Failed to put entry2: %v", err)
		}

		// Save cache
		if err := cache1.Save(); err != nil {
			t.Fatalf("Failed to save cache: %v", err)
		}
		cache1.Stop()

		// Load cache into new instance
		cache2 := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)
		if err := cache2.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Verify entries were loaded
		stats := cache2.Stats()
		if stats.TotalEntries != 2 {
			t.Errorf("Expected 2 entries, got %d", stats.TotalEntries)
		}

		// Verify entry content
		retrieved1, found := cache2.Get("key1")
		if !found {
			t.Fatal("Expected to find key1")
		}
		if retrieved1.Response.Content != "response 1" {
			t.Errorf("Expected 'response 1', got '%s'", retrieved1.Response.Content)
		}

		retrieved2, found := cache2.Get("key2")
		if !found {
			t.Fatal("Expected to find key2")
		}
		if retrieved2.Response.Content != "response 2" {
			t.Errorf("Expected 'response 2', got '%s'", retrieved2.Response.Content)
		}

		cache2.Stop()
	})

	t.Run("Load creates new cache if file doesn't exist", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "nonexistent-cache.json")

		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)
		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load nonexistent cache: %v", err)
		}

		// Verify cache is initialized
		stats := cache.Stats()
		if stats.TotalEntries != 0 {
			t.Errorf("Expected 0 entries in new cache, got %d", stats.TotalEntries)
		}

		cache.Stop()
	})
}

// TestDiskCache_CorruptionHandling tests handling of corrupted cache files
func TestDiskCache_CorruptionHandling(t *testing.T) {
	t.Run("Corrupted JSON file", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "corrupted-cache.json")

		// Create a corrupted JSON file
		corruptedContent := []byte("{invalid json content")
		if err := os.WriteFile(cachePath, corruptedContent, 0644); err != nil {
			t.Fatalf("Failed to create corrupted file: %v", err)
		}

		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)
		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load corrupted cache: %v", err)
		}

		// Verify cache was reset to empty state
		stats := cache.Stats()
		if stats.TotalEntries != 0 {
			t.Errorf("Expected 0 entries after corrupted load, got %d", stats.TotalEntries)
		}

		// Verify backup was created
		backupFiles, err := filepath.Glob(cachePath + ".corrupted.*")
		if err != nil {
			t.Fatalf("Failed to glob backup files: %v", err)
		}
		if len(backupFiles) != 1 {
			t.Errorf("Expected 1 backup file, got %d", len(backupFiles))
		}

		cache.Stop()
	})

	t.Run("Corrupted entry checksum", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")

		// Create cache with valid entries
		cache1 := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)
		if err := cache1.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Add entries
		entry1 := &CachedResponse{
			Key: "valid-key",
			Response: llmtypes.CompletionResponse{
				Content: "valid response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}
		if err := cache1.Put("valid-key", entry1); err != nil {
			t.Fatalf("Failed to put valid entry: %v", err)
		}

		if err := cache1.Save(); err != nil {
			t.Fatalf("Failed to save cache: %v", err)
		}
		cache1.Stop()

		// Manually corrupt the cache file by modifying an entry's checksum
		data, err := os.ReadFile(cachePath)
		if err != nil {
			t.Fatalf("Failed to read cache file: %v", err)
		}

		var cacheData DiskCacheData
		if err := json.Unmarshal(data, &cacheData); err != nil {
			t.Fatalf("Failed to unmarshal cache: %v", err)
		}

		// Add a corrupted entry with invalid checksum
		corruptedEntry := CachedResponse{
			Key: "corrupted-key",
			Response: llmtypes.CompletionResponse{
				Content: "corrupted response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
			Checksum:    "invalid-checksum-0123456789abcdef",
		}
		cacheData.Entries["corrupted-key"] = corruptedEntry

		// Save corrupted data
		corruptedJSON, _ := json.MarshalIndent(cacheData, "", "  ") //nolint:govet
		if err := os.WriteFile(cachePath, corruptedJSON, 0644); err != nil {
			t.Fatalf("Failed to write corrupted cache: %v", err)
		}

		// Load corrupted cache
		cache2 := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)
		if err := cache2.Load(); err != nil {
			t.Fatalf("Failed to load corrupted cache: %v", err)
		}

		// Verify corrupted entry was removed
		_, found := cache2.Get("corrupted-key")
		if found {
			t.Error("Expected corrupted entry to be removed")
		}

		// Verify valid entry is still present
		_, found = cache2.Get("valid-key")
		if !found {
			t.Error("Expected valid entry to still be present")
		}

		// Verify stats
		stats := cache2.Stats()
		if stats.TotalEntries != 1 {
			t.Errorf("Expected 1 valid entry, got %d", stats.TotalEntries)
		}

		cache2.Stop()
	})

	t.Run("Entry without checksum (backward compatibility)", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")

		// Create cache file with entry without checksum (old format)
		cacheData := DiskCacheData{
			Version:   CacheVersion,
			CreatedAt: time.Now(),
			UpdatedAt: time.Now(),
			Entries: map[string]CachedResponse{
				"old-key": {
					Key: "old-key",
					Response: llmtypes.CompletionResponse{
						Content: "old response",
					},
					CreatedAt:   time.Now(),
					ExpiresAt:   time.Now().Add(DefaultTTL),
					AccessCount: 0,
					Checksum:    "", // No checksum (old format)
				},
			},
			Stats: DiskCacheStats{},
		}

		data, _ := json.MarshalIndent(cacheData, "", "  ") //nolint:govet
		if err := os.WriteFile(cachePath, data, 0644); err != nil {
			t.Fatalf("Failed to write old format cache: %v", err)
		}

		// Load old format cache
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)
		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load old format cache: %v", err)
		}

		// Verify entry is still accessible (backward compatible)
		retrieved, found := cache.Get("old-key")
		if !found {
			t.Fatal("Expected old format entry to be accessible")
		}
		if retrieved.Response.Content != "old response" {
			t.Errorf("Expected 'old response', got '%s'", retrieved.Response.Content)
		}

		cache.Stop()
	})
}

// TestDiskCache_VersionMismatch tests handling of version mismatches
func TestDiskCache_VersionMismatch(t *testing.T) {
	tmpDir := t.TempDir()
	cachePath := filepath.Join(tmpDir, "test-cache.json")

	// Create cache file with wrong version
	cacheData := DiskCacheData{
		Version:   999, // Wrong version
		CreatedAt: time.Now(),
		UpdatedAt: time.Now(),
		Entries: map[string]CachedResponse{
			"old-key": {
				Key: "old-key",
				Response: llmtypes.CompletionResponse{
					Content: "old response",
				},
				CreatedAt:   time.Now(),
				ExpiresAt:   time.Now().Add(DefaultTTL),
				AccessCount: 0,
			},
		},
		Stats: DiskCacheStats{},
	}

	data, _ := json.MarshalIndent(cacheData, "", "  ") //nolint:govet
	if err := os.WriteFile(cachePath, data, 0644); err != nil {
		t.Fatalf("Failed to write cache: %v", err)
	}

	// Load cache with version mismatch
	cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)
	if err := cache.Load(); err != nil {
		t.Fatalf("Failed to load cache: %v", err)
	}

	// Verify cache was reset to empty state
	stats := cache.Stats()
	if stats.TotalEntries != 0 {
		t.Errorf("Expected 0 entries after version mismatch, got %d", stats.TotalEntries)
	}

	cache.Stop()
}

// TestDiskCache_TTLExpiration tests TTL-based expiration
func TestDiskCache_TTLExpiration(t *testing.T) {
	t.Run("Expired entry returns not found", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, time.Hour, 100*1024*1024) // 1 hour TTL

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Create already-expired entry
		key := "expired-key"
		value := &CachedResponse{
			Key: key,
			Response: llmtypes.CompletionResponse{
				Content: "expired response",
			},
			CreatedAt:   time.Now().Add(-2 * time.Hour),
			ExpiresAt:   time.Now().Add(-1 * time.Hour),
			AccessCount: 0,
		}

		if err := cache.Put(key, value); err != nil {
			t.Fatalf("Failed to put expired entry: %v", err)
		}

		// Try to get expired entry
		_, found := cache.Get(key)
		if found {
			t.Error("Expected expired entry to not be found")
		}

		cache.Stop()
	})

	t.Run("Non-expired entry returns found", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		key := "valid-key"
		value := &CachedResponse{
			Key: key,
			Response: llmtypes.CompletionResponse{
				Content: "valid response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}

		if err := cache.Put(key, value); err != nil {
			t.Fatalf("Failed to put valid entry: %v", err)
		}

		// Get valid entry
		retrieved, found := cache.Get(key)
		if !found {
			t.Fatal("Expected valid entry to be found")
		}
		if retrieved.Response.Content != "valid response" {
			t.Errorf("Expected 'valid response', got '%s'", retrieved.Response.Content)
		}

		cache.Stop()
	})

	t.Run("CleanupExpired removes expired entries", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Add expired entry
		expiredEntry := &CachedResponse{
			Key: "expired-key",
			Response: llmtypes.CompletionResponse{
				Content: "expired response",
			},
			CreatedAt:   time.Now().Add(-2 * time.Hour),
			ExpiresAt:   time.Now().Add(-1 * time.Hour),
			AccessCount: 0,
		}
		if err := cache.Put("expired-key", expiredEntry); err != nil {
			t.Fatalf("Failed to put expired entry: %v", err)
		}

		// Add valid entry
		validEntry := &CachedResponse{
			Key: "valid-key",
			Response: llmtypes.CompletionResponse{
				Content: "valid response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}
		if err := cache.Put("valid-key", validEntry); err != nil {
			t.Fatalf("Failed to put valid entry: %v", err)
		}

		// Cleanup expired
		if err := cache.CleanupExpired(); err != nil {
			t.Fatalf("Failed to cleanup expired: %v", err)
		}

		// Verify expired entry was removed
		_, found := cache.Get("expired-key")
		if found {
			t.Error("Expected expired entry to be removed")
		}

		// Verify valid entry still exists
		_, found = cache.Get("valid-key")
		if !found {
			t.Error("Expected valid entry to still exist")
		}

		cache.Stop()
	})
}

// TestDiskCache_Statistics tests statistics tracking
func TestDiskCache_Statistics(t *testing.T) {
	t.Run("Initial stats", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		stats := cache.Stats()
		if stats.TotalEntries != 0 {
			t.Errorf("Expected 0 total entries, got %d", stats.TotalEntries)
		}
		if stats.ExpiredEntries != 0 {
			t.Errorf("Expected 0 expired entries, got %d", stats.ExpiredEntries)
		}
		if stats.Hits != 0 {
			t.Errorf("Expected 0 hits, got %d", stats.Hits)
		}
		if stats.Misses != 0 {
			t.Errorf("Expected 0 misses, got %d", stats.Misses)
		}

		cache.Stop()
	})

	t.Run("Hit and miss tracking", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Add entry
		entry := &CachedResponse{
			Key: "test-key",
			Response: llmtypes.CompletionResponse{
				Content: "test response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}
		if err := cache.Put("test-key", entry); err != nil {
			t.Fatalf("Failed to put entry: %v", err)
		}

		// Hit
		cache.Get("test-key")
		stats := cache.Stats()
		if stats.Hits != 1 {
			t.Errorf("Expected 1 hit, got %d", stats.Hits)
		}

		// Miss
		cache.Get("non-existent")
		stats = cache.Stats()
		if stats.Misses != 1 {
			t.Errorf("Expected 1 miss, got %d", stats.Misses)
		}

		// Check hit rate
		expectedHitRate := 0.5 // 1 hit / 2 total lookups
		if stats.HitRate != expectedHitRate {
			t.Errorf("Expected hit rate %f, got %f", expectedHitRate, stats.HitRate)
		}

		cache.Stop()
	})

	t.Run("Eviction tracking", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Add expired entries
		for i := 1; i <= 3; i++ {
			entry := &CachedResponse{
				Key: "expired-" + string(rune('0'+i)),
				Response: llmtypes.CompletionResponse{
					Content: "expired",
				},
				CreatedAt:   time.Now().Add(-2 * time.Hour),
				ExpiresAt:   time.Now().Add(-1 * time.Hour),
				AccessCount: 0,
			}
			if err := cache.Put(entry.Key, entry); err != nil {
				t.Fatalf("Failed to put entry %d: %v", i, err)
			}
		}

		// Cleanup expired entries
		if err := cache.CleanupExpired(); err != nil {
			t.Fatalf("Failed to cleanup expired: %v", err)
		}

		stats := cache.Stats()
		if stats.Evictions != 3 {
			t.Errorf("Expected 3 evictions, got %d", stats.Evictions)
		}

		cache.Stop()
	})
}

// TestDiskCache_ConcurrentAccess tests thread-safety
func TestDiskCache_ConcurrentAccess(t *testing.T) {
	t.Run("Concurrent reads", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Add entry
		entry := &CachedResponse{
			Key: "test-key",
			Response: llmtypes.CompletionResponse{
				Content: "test response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}
		if err := cache.Put("test-key", entry); err != nil {
			t.Fatalf("Failed to put entry: %v", err)
		}

		// Concurrent reads
		done := make(chan bool)
		for i := 0; i < 10; i++ {
			go func() {
				cache.Get("test-key")
				done <- true
			}()
		}

		// Wait for all goroutines
		for i := 0; i < 10; i++ {
			<-done
		}

		// Verify cache is still functional
		_, found := cache.Get("test-key")
		if !found {
			t.Error("Expected entry to still be found after concurrent reads")
		}

		cache.Stop()
	})

	t.Run("Concurrent writes", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Concurrent writes
		done := make(chan bool)
		for i := 0; i < 10; i++ {
			go func(i int) {
				key := "key-" + string(rune('0'+i))
				entry := &CachedResponse{
					Key: key,
					Response: llmtypes.CompletionResponse{
						Content: "response",
					},
					CreatedAt:   time.Now(),
					ExpiresAt:   time.Now().Add(DefaultTTL),
					AccessCount: 0,
				}
				_ = cache.Put(key, entry)
				done <- true
			}(i)
		}

		// Wait for all goroutines
		for i := 0; i < 10; i++ {
			<-done
		}

		// Verify all entries were added
		stats := cache.Stats()
		if stats.TotalEntries != 10 {
			t.Errorf("Expected 10 entries, got %d", stats.TotalEntries)
		}

		cache.Stop()
	})

	t.Run("Concurrent reads and writes", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		done := make(chan bool)
		// Start readers
		for i := 0; i < 5; i++ {
			go func() {
				for j := 0; j < 10; j++ {
					cache.Get("key-1")
				}
				done <- true
			}()
		}
		// Start writers
		for i := 0; i < 5; i++ {
			go func(i int) {
				for j := 0; j < 10; j++ {
					key := "key-" + string(rune('0'+i))
					entry := &CachedResponse{
						Key: key,
						Response: llmtypes.CompletionResponse{
							Content: "response",
						},
						CreatedAt:   time.Now(),
						ExpiresAt:   time.Now().Add(DefaultTTL),
						AccessCount: 0,
					}
					_ = cache.Put(key, entry)
				}
				done <- true
			}(i)
		}

		// Wait for all goroutines
		for i := 0; i < 10; i++ {
			<-done
		}

		// Verify cache is still functional
		stats := cache.Stats()
		if stats.TotalEntries == 0 {
			t.Error("Expected entries to exist after concurrent operations")
		}

		cache.Stop()
	})
}

// TestDiskCache_AutoSave tests background auto-save functionality
func TestDiskCache_AutoSave(t *testing.T) {
	t.Run("Auto-save starts and stops", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Start auto-save with 100ms interval
		cache.StartAutoSave(100 * time.Millisecond)

		// Add entry
		entry := &CachedResponse{
			Key: "test-key",
			Response: llmtypes.CompletionResponse{
				Content: "test response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}
		if err := cache.Put("test-key", entry); err != nil {
			t.Fatalf("Failed to put entry: %v", err)
		}

		// Wait for auto-save to trigger
		time.Sleep(200 * time.Millisecond)

		// Stop auto-save (should do final save)
		cache.Stop()

		// Verify data was persisted by loading into new cache
		cache2 := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)
		if err := cache2.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		_, found := cache2.Get("test-key")
		if !found {
			t.Error("Expected entry to be persisted by auto-save")
		}

		cache2.Stop()
	})

	t.Run("Multiple StartAutoSave calls are idempotent", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Start auto-save multiple times
		cache.StartAutoSave(100 * time.Millisecond)
		cache.StartAutoSave(100 * time.Millisecond)
		cache.StartAutoSave(100 * time.Millisecond)

		// Add entry
		entry := &CachedResponse{
			Key: "test-key",
			Response: llmtypes.CompletionResponse{
				Content: "test response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}
		if err := cache.Put("test-key", entry); err != nil {
			t.Fatalf("Failed to put entry: %v", err)
		}

		// Wait and stop
		time.Sleep(200 * time.Millisecond)
		cache.Stop()

		// Verify cache is still functional
		_, found := cache.Get("test-key")
		if !found {
			t.Error("Expected entry to exist")
		}
	})

	t.Run("Stop without auto-save started is safe", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Stop without starting auto-save
		cache.Stop()

		// Verify cache is still functional
		entry := &CachedResponse{
			Key: "test-key",
			Response: llmtypes.CompletionResponse{
				Content: "test response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}
		if err := cache.Put("test-key", entry); err != nil {
			t.Fatalf("Failed to put entry: %v", err)
		}

		_, found := cache.Get("test-key")
		if !found {
			t.Error("Expected entry to exist")
		}
	})
}

// TestDiskCache_AtomicWrite tests atomic write pattern
func TestDiskCache_AtomicWrite(t *testing.T) {
	t.Run("Save uses atomic write pattern", func(t *testing.T) {
		tmpDir := t.TempDir()
		cachePath := filepath.Join(tmpDir, "test-cache.json")
		cache := NewDiskCache(cachePath, DefaultTTL, 100*1024*1024)

		if err := cache.Load(); err != nil {
			t.Fatalf("Failed to load cache: %v", err)
		}

		// Add entry
		entry := &CachedResponse{
			Key: "test-key",
			Response: llmtypes.CompletionResponse{
				Content: "test response",
			},
			CreatedAt:   time.Now(),
			ExpiresAt:   time.Now().Add(DefaultTTL),
			AccessCount: 0,
		}
		if err := cache.Put("test-key", entry); err != nil {
			t.Fatalf("Failed to put entry: %v", err)
		}

		// Save cache
		if err := cache.Save(); err != nil {
			t.Fatalf("Failed to save cache: %v", err)
		}

		// Verify temp file was cleaned up
		tmpFile := cachePath + ".tmp"
		if _, err := os.Stat(tmpFile); !os.IsNotExist(err) {
			t.Error("Expected temp file to be cleaned up after successful save")
		}

		// Verify main file exists
		if _, err := os.Stat(cachePath); os.IsNotExist(err) {
			t.Error("Expected cache file to exist after save")
		}

		cache.Stop()
	})
}
</file>
<file path="internal/llmtypes/types.go">
package llmtypes

// Message represents a chat message
type Message struct {
	Role      string // "system", "user", "assistant", "tool"
	Content   string
	ToolID    string     // ID of the tool that was called (for role="tool")
	ToolCalls []ToolCall // Tool calls made by assistant (for role="assistant")
}

// ToolCall represents a tool/function call from the LLM
type ToolCall struct {
	Name             string                 // Name of the tool to call
	Arguments        map[string]interface{} // Arguments for the tool
	RawFunctionCall  map[string]interface{} // Preserves complete function call data
	ThoughtSignature string                 // Required for Gemini 3 function calling
}

// CompletionRequest is a request for LLM completion
type CompletionRequest struct {
	SystemPrompt string
	Messages     []Message
	Tools        []ToolDefinition
	MaxTokens    int
	Temperature  float64
}

// CompletionResponse is the response from LLM
type CompletionResponse struct {
	Content   string
	ToolCalls []ToolCall
	Usage     TokenUsage
}

// TokenUsage tracks token usage
type TokenUsage struct {
	InputTokens  int
	OutputTokens int
	TotalTokens  int
}

// ToolDefinition defines a tool for the LLM
type ToolDefinition struct {
	Name        string
	Description string
	Parameters  map[string]interface{}
}
</file>
<file path="internal/logging/logger.go">
package logging

import (
	"os"
	"path/filepath"

	"go.uber.org/zap"
	"go.uber.org/zap/zapcore"
)

// Field is a type alias for zap.Field
type Field = zap.Field

// Common field constructors
var (
	String   = zap.String
	Int      = zap.Int
	Int64    = zap.Int64
	Float64  = zap.Float64
	Bool     = zap.Bool
	Any      = zap.Any
	Error    = zap.Error
	Err      = zap.NamedError
	Duration = zap.Duration
	Time     = zap.Time
)

// LevelFromString converts a string level to zapcore.Level
func LevelFromString(level string) zapcore.Level {
	switch level {
	case "debug":
		return zapcore.DebugLevel
	case "info":
		return zapcore.InfoLevel
	case "warn":
		return zapcore.WarnLevel
	case "error":
		return zapcore.ErrorLevel
	default:
		return zapcore.InfoLevel
	}
}

// Logger wraps zap.Logger with application-specific methods
type Logger struct {
	zap *zap.Logger
}

// Config holds logger configuration
type Config struct {
	LogDir         string
	FileLevel      zapcore.Level
	ConsoleLevel   zapcore.Level
	EnableCaller   bool
	ConsoleEnabled bool
}

// DefaultConfig returns default logger configuration
func DefaultConfig() *Config {
	return &Config{
		LogDir:         ".ai/logs",
		FileLevel:      zapcore.InfoLevel,
		ConsoleLevel:   zapcore.DebugLevel,
		EnableCaller:   true,
		ConsoleEnabled: true,
	}
}

// NewLogger creates a new logger with file and optional console output
func NewLogger(cfg *Config) (*Logger, error) {
	if cfg == nil {
		cfg = DefaultConfig()
	}

	// Ensure log directory exists
	if err := os.MkdirAll(cfg.LogDir, 0755); err != nil {
		return nil, err
	}

	// File encoder (JSON)
	fileEncoderConfig := zap.NewProductionEncoderConfig()
	fileEncoderConfig.TimeKey = "timestamp"
	fileEncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
	fileEncoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder
	fileEncoder := zapcore.NewJSONEncoder(fileEncoderConfig)

	// File writer
	logFile := filepath.Join(cfg.LogDir, "gendocs.log")
	file, err := os.OpenFile(logFile, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		return nil, err
	}
	fileWriter := zapcore.AddSync(file)

	var core zapcore.Core

	if cfg.ConsoleEnabled {
		// Console encoder (human-readable with colors)
		consoleEncoderConfig := zap.NewDevelopmentEncoderConfig()
		consoleEncoderConfig.EncodeLevel = zapcore.CapitalColorLevelEncoder
		consoleEncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
		consoleEncoder := zapcore.NewConsoleEncoder(consoleEncoderConfig)

		// Console writer
		consoleWriter := zapcore.AddSync(os.Stderr)

		// Core with both outputs
		core = zapcore.NewTee(
			zapcore.NewCore(fileEncoder, fileWriter, cfg.FileLevel),
			zapcore.NewCore(consoleEncoder, consoleWriter, cfg.ConsoleLevel),
		)
	} else {
		// File-only logging when console is disabled
		core = zapcore.NewCore(fileEncoder, fileWriter, cfg.FileLevel)
	}

	// Create logger
	opts := []zap.Option{zap.AddStacktrace(zapcore.ErrorLevel)}
	if cfg.EnableCaller {
		opts = append(opts, zap.AddCaller())
	}
	zapLogger := zap.New(core, opts...)

	return &Logger{zap: zapLogger}, nil
}

// NewNopLogger creates a no-op logger for testing
func NewNopLogger() *Logger {
	return &Logger{zap: zap.NewNop()}
}

// Sync flushes any buffered log entries
func (l *Logger) Sync() error {
	return l.zap.Sync()
}

// Debug logs a debug message
func (l *Logger) Debug(msg string, fields ...zap.Field) {
	l.zap.Debug(msg, fields...)
}

// Info logs an info message
func (l *Logger) Info(msg string, fields ...zap.Field) {
	l.zap.Info(msg, fields...)
}

// Warn logs a warning message
func (l *Logger) Warn(msg string, fields ...zap.Field) {
	l.zap.Warn(msg, fields...)
}

// Error logs an error message
func (l *Logger) Error(msg string, fields ...zap.Field) {
	l.zap.Error(msg, fields...)
}

// Fatal logs a fatal message and exits
func (l *Logger) Fatal(msg string, fields ...zap.Field) {
	l.zap.Fatal(msg, fields...)
}

// With creates a child logger with additional fields
func (l *Logger) With(fields ...zap.Field) *Logger {
	return &Logger{zap: l.zap.With(fields...)}
}

// Named creates a named child logger
func (l *Logger) Named(name string) *Logger {
	return &Logger{zap: l.zap.Named(name)}
}
</file>
<file path="internal/prompts/integration_test.go">
package prompts

import (
	"os"
	"path/filepath"
	"strings"
	"testing"
)

// TestIntegration_RealProjectPrompts tests loading prompts from actual project structure
func TestIntegration_RealProjectPrompts(t *testing.T) {
	// Get project root (assume we're in internal/prompts/)
	projectRoot, err := findProjectRoot()
	if err != nil {
		t.Skipf("Skipping integration test: %v", err)
	}

	systemPromptsDir := filepath.Join(projectRoot, "prompts")
	projectPromptsDir := filepath.Join(projectRoot, ".ai/prompts")

	// Verify system prompts directory exists
	if _, err := os.Stat(systemPromptsDir); os.IsNotExist(err) {
		t.Fatalf("System prompts directory not found: %s", systemPromptsDir)
	}

	// Load with overrides
	mgr, err := NewManagerWithOverrides(systemPromptsDir, projectPromptsDir)
	if err != nil {
		t.Fatalf("Failed to load prompts: %v", err)
	}

	// Verify all required prompts exist
	requiredPrompts := []string{
		"structure_analyzer_system",
		"structure_analyzer_user",
		"dependency_analyzer_system",
		"dependency_analyzer_user",
		"data_flow_analyzer_system",
		"data_flow_analyzer_user",
		"request_flow_analyzer_system",
		"request_flow_analyzer_user",
		"api_analyzer_system",
		"api_analyzer_user",
		"documenter_system_prompt",
		"documenter_user_prompt",
		"ai_rules_system_prompt",
		"ai_rules_user_prompt",
	}

	for _, promptName := range requiredPrompts {
		if !mgr.HasPrompt(promptName) {
			t.Errorf("Required prompt missing: %s", promptName)
		}
	}

	// If project prompts directory exists, verify overrides work
	if _, err := os.Stat(projectPromptsDir); err == nil {
		overrides := mgr.ListOverrides()
		t.Logf("Found %d custom prompt overrides:", len(overrides))
		for _, override := range overrides {
			t.Logf("  - %s (source: %s)", override, mgr.GetSource(override))

			// Verify override is actually loaded
			prompt, err := mgr.Get(override)
			if err != nil {
				t.Errorf("Failed to get overridden prompt %s: %v", override, err)
			}

			// Verify it's from project, not system
			source := mgr.GetSource(override)
			if !strings.HasPrefix(source, "project:") {
				t.Errorf("Override %s should have 'project:' source, got: %s", override, source)
			}

			t.Logf("    Content length: %d chars", len(prompt))
		}
	} else {
		t.Log("No project prompts found (this is OK for testing)")
	}

	// Test prompt count
	totalPrompts := mgr.CountPrompts()
	if totalPrompts < len(requiredPrompts) {
		t.Errorf("Expected at least %d prompts, got %d", len(requiredPrompts), totalPrompts)
	}
	t.Logf("Total prompts loaded: %d", totalPrompts)
}

// TestIntegration_PromptRendering tests template rendering with real prompts
func TestIntegration_PromptRendering(t *testing.T) {
	projectRoot, err := findProjectRoot()
	if err != nil {
		t.Skipf("Skipping integration test: %v", err)
	}

	systemPromptsDir := filepath.Join(projectRoot, "prompts")
	projectPromptsDir := filepath.Join(projectRoot, ".ai/prompts")

	mgr, err := NewManagerWithOverrides(systemPromptsDir, projectPromptsDir)
	if err != nil {
		t.Fatalf("Failed to load prompts: %v", err)
	}

	// Test rendering analyzer user prompts (they use templates)
	testCases := []struct {
		promptName string
		vars       map[string]interface{}
	}{
		{
			promptName: "structure_analyzer_user",
			vars: map[string]interface{}{
				"RepoPath": "/test/repo",
				"Language": "Go",
			},
		},
		{
			promptName: "dependency_analyzer_user",
			vars: map[string]interface{}{
				"RepoPath": "/test/repo",
				"Language": "Go",
			},
		},
	}

	for _, tc := range testCases {
		t.Run(tc.promptName, func(t *testing.T) {
			// Try to render - if it has no template vars, this will still work
			result, err := mgr.Render(tc.promptName, tc.vars)
			if err != nil {
				// If it fails, try Get (maybe it's not a template)
				result, err = mgr.Get(tc.promptName)
				if err != nil {
					t.Fatalf("Failed to get prompt: %v", err)
				}
			}

			if result == "" {
				t.Error("Rendered prompt is empty")
			}

			t.Logf("Rendered %s (%d chars)", tc.promptName, len(result))
		})
	}
}

// findProjectRoot walks up from current directory to find project root
func findProjectRoot() (string, error) {
	dir, err := os.Getwd()
	if err != nil {
		return "", err
	}

	// Walk up until we find go.mod
	for {
		if _, err := os.Stat(filepath.Join(dir, "go.mod")); err == nil {
			return dir, nil
		}

		parent := filepath.Dir(dir)
		if parent == dir {
			return "", os.ErrNotExist
		}
		dir = parent
	}
}
</file>
<file path="internal/prompts/manager.go">
package prompts

import (
	"bytes"
	"fmt"
	"os"
	"path/filepath"
	textTemplate "text/template"

	"gopkg.in/yaml.v3"
)

// Manager handles loading and rendering prompt templates
type Manager struct {
	prompts map[string]string
	sources map[string]string // Track which file provided each prompt (for debugging)
}

// PromptTemplate represents a prompt with system and user components
type PromptTemplate struct {
	SystemPrompt string `yaml:"system_prompt"`
	UserPrompt   string `yaml:"user_prompt"`
}

// NewManager creates a new prompt manager by loading prompts from a directory
func NewManager(promptsDir string) (*Manager, error) {
	pm := &Manager{
		prompts: make(map[string]string),
		sources: make(map[string]string),
	}

	// Load from single directory (backward compatibility)
	if err := pm.loadDirectory(promptsDir, "system"); err != nil {
		return nil, err
	}

	return pm, nil
}

// NewManagerWithOverrides creates manager with system + project overrides
func NewManagerWithOverrides(systemDir, projectDir string) (*Manager, error) {
	pm := &Manager{
		prompts: make(map[string]string),
		sources: make(map[string]string),
	}

	// 1. Load system prompts first (baseline)
	if err := pm.loadDirectory(systemDir, "system"); err != nil {
		return nil, fmt.Errorf("failed to load system prompts: %w", err)
	}

	// 2. Load project prompts (overrides) if directory exists
	if projectDir != "" {
		if _, err := os.Stat(projectDir); err == nil {
			if err := pm.loadDirectory(projectDir, "project"); err != nil {
				return nil, fmt.Errorf("failed to load project prompts: %w", err)
			}
		}
		// If project dir doesn't exist, that's OK - no overrides
	}

	// 3. Validate required prompts exist
	if err := pm.validateRequiredPrompts(); err != nil {
		return nil, err
	}

	return pm, nil
}

// loadDirectory loads all YAML files from a directory
func (pm *Manager) loadDirectory(dir, source string) error {
	entries, err := os.ReadDir(dir)
	if err != nil {
		return fmt.Errorf("failed to read directory %s: %w", dir, err)
	}

	for _, entry := range entries {
		if entry.IsDir() {
			continue
		}

		ext := filepath.Ext(entry.Name())
		if ext != ".yaml" && ext != ".yml" {
			continue
		}

		filePath := filepath.Join(dir, entry.Name())
		data, err := os.ReadFile(filePath)
		if err != nil {
			return fmt.Errorf("failed to read %s: %w", filePath, err)
		}

		var prompts map[string]string
		if err := yaml.Unmarshal(data, &prompts); err != nil {
			return fmt.Errorf("failed to parse %s: %w", filePath, err)
		}

		// Merge into main map (later loads override earlier)
		for key, value := range prompts {
			pm.prompts[key] = value
			pm.sources[key] = fmt.Sprintf("%s:%s", source, entry.Name())
		}
	}

	return nil
}

// validateRequiredPrompts ensures critical prompts exist
func (pm *Manager) validateRequiredPrompts() error {
	required := []string{
		"structure_analyzer_system",
		"structure_analyzer_user",
		"dependency_analyzer_system",
		"dependency_analyzer_user",
		"data_flow_analyzer_system",
		"data_flow_analyzer_user",
		"request_flow_analyzer_system",
		"request_flow_analyzer_user",
		"api_analyzer_system",
		"api_analyzer_user",
		"documenter_system_prompt",
		"documenter_user_prompt",
		"ai_rules_system_prompt",
		"ai_rules_user_prompt",
	}

	var missing []string
	for _, key := range required {
		if _, ok := pm.prompts[key]; !ok {
			missing = append(missing, key)
		}
	}

	if len(missing) > 0 {
		return fmt.Errorf("missing required prompts: %v", missing)
	}

	return nil
}

// NewManagerFromMap creates a prompt manager from a map (useful for testing)
func NewManagerFromMap(prompts map[string]string) *Manager {
	sources := make(map[string]string)
	for key := range prompts {
		sources[key] = "test:map"
	}
	return &Manager{
		prompts: prompts,
		sources: sources,
	}
}

// Get returns a raw prompt by name
func (pm *Manager) Get(name string) (string, error) {
	prompt, ok := pm.prompts[name]
	if !ok {
		return "", fmt.Errorf("prompt '%s' not found (available: %v)", name, pm.getAvailableNames())
	}
	return prompt, nil
}

// Render renders a prompt template with the given variables
func (pm *Manager) Render(name string, vars map[string]interface{}) (string, error) {
	promptTemplate, err := pm.Get(name)
	if err != nil {
		return "", err
	}

	// Parse and execute template
	tmpl, err := textTemplate.New(name).Option("missingkey=error").Parse(promptTemplate)
	if err != nil {
		return "", fmt.Errorf("failed to parse template '%s': %w", name, err)
	}

	var buf bytes.Buffer
	if err := tmpl.Execute(&buf, vars); err != nil {
		return "", fmt.Errorf("failed to execute template '%s': %w", name, err)
	}

	return buf.String(), nil
}

// GetPromptTemplate returns a PromptTemplate with system and user prompts
func (pm *Manager) GetPromptTemplate(name string) (*PromptTemplate, error) {
	systemPrompt, err := pm.Render(name+"_system_prompt", nil)
	if err != nil {
		// Try without suffix
		systemPrompt, err = pm.Get(name + "_system")
		if err != nil {
			return nil, fmt.Errorf("system prompt '%s' not found", name)
		}
	}

	userPrompt, err := pm.Get(name + "_user_prompt")
	if err != nil {
		return nil, fmt.Errorf("user prompt '%s' not found", name)
	}

	return &PromptTemplate{
		SystemPrompt: systemPrompt,
		UserPrompt:   userPrompt,
	}, nil
}

// RenderTemplate renders both system and user prompts with variables
func (pm *Manager) RenderTemplate(name string, vars map[string]interface{}) (*PromptTemplate, error) {
	template, err := pm.GetPromptTemplate(name)
	if err != nil {
		return nil, err
	}

	// Render system prompt with variables
	systemTmpl, err := textTemplate.New("system").Parse(template.SystemPrompt)
	if err != nil {
		return nil, fmt.Errorf("failed to parse system prompt: %w", err)
	}

	var systemBuf bytes.Buffer
	if err := systemTmpl.Execute(&systemBuf, vars); err != nil {
		return nil, fmt.Errorf("failed to render system prompt: %w", err)
	}

	// Render user prompt with variables
	userTmpl, err := textTemplate.New("user").Parse(template.UserPrompt)
	if err != nil {
		return nil, fmt.Errorf("failed to parse user prompt: %w", err)
	}

	var userBuf bytes.Buffer
	if err := userTmpl.Execute(&userBuf, vars); err != nil {
		return nil, fmt.Errorf("failed to render user prompt: %w", err)
	}

	return &PromptTemplate{
		SystemPrompt: systemBuf.String(),
		UserPrompt:   userBuf.String(),
	}, nil
}

// getAvailableNames returns a list of available prompt names
func (pm *Manager) getAvailableNames() []string {
	names := make([]string, 0, len(pm.prompts))
	for name := range pm.prompts {
		names = append(names, name)
	}
	return names
}

// HasPrompt checks if a prompt exists
func (pm *Manager) HasPrompt(name string) bool {
	_, ok := pm.prompts[name]
	return ok
}

// GetSource returns which file provided a prompt (for debugging)
func (pm *Manager) GetSource(name string) string {
	if source, ok := pm.sources[name]; ok {
		return source
	}
	return "unknown"
}

// ListOverrides returns all prompts that were overridden from project
func (pm *Manager) ListOverrides() []string {
	var overrides []string
	for key, source := range pm.sources {
		if len(source) > 0 && source[0:7] == "project" {
			overrides = append(overrides, key)
		}
	}
	return overrides
}

// CountPrompts returns the total number of loaded prompts
func (pm *Manager) CountPrompts() int {
	return len(pm.prompts)
}
</file>
<file path="internal/prompts/manager_test.go">
package prompts

import (
	"os"
	"path/filepath"
	"testing"
)

func TestNewManagerFromMap_Get(t *testing.T) {
	prompts := map[string]string{
		"test_system": "You are a test assistant",
		"test_user":   "Analyze the code",
	}

	mgr := NewManagerFromMap(prompts)

	// Test Get
	prompt, err := mgr.Get("test_system")
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if prompt != "You are a test assistant" {
		t.Errorf("Expected 'You are a test assistant', got '%s'", prompt)
	}
}

func TestManager_Get_NotFound(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"exists": "value",
	})

	_, err := mgr.Get("nonexistent")
	if err == nil {
		t.Fatal("Expected error for non-existent prompt, got nil")
	}
}

func TestManager_Render_WithVariables(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"template": "Path: {{.RepoPath}}, Workers: {{.Workers}}",
	})

	result, err := mgr.Render("template", map[string]interface{}{
		"RepoPath": "/test/path",
		"Workers":  4,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	expected := "Path: /test/path, Workers: 4"
	if result != expected {
		t.Errorf("Expected '%s', got '%s'", expected, result)
	}
}

func TestManager_Render_MissingVariable(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"template": "Path: {{.RepoPath}}",
	})

	// Render with empty vars - should error on missing variable
	_, err := mgr.Render("template", map[string]interface{}{})

	if err == nil {
		t.Fatal("Expected error for missing variable, got nil")
	}
}

func TestManager_Render_ComplexTemplate(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"complex": `Repository: {{.Repo}}
{{if .Debug}}Debug mode enabled{{end}}
Files: {{range .Files}}
  - {{.}}{{end}}`,
	})

	result, err := mgr.Render("complex", map[string]interface{}{
		"Repo":  "test-repo",
		"Debug": true,
		"Files": []string{"file1.go", "file2.go"},
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if !containsString(result, "Repository: test-repo") {
		t.Error("Expected 'Repository: test-repo' in result")
	}

	if !containsString(result, "Debug mode enabled") {
		t.Error("Expected 'Debug mode enabled' in result")
	}

	if !containsString(result, "file1.go") {
		t.Error("Expected 'file1.go' in result")
	}
}

func TestManager_HasPrompt(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"exists": "value",
	})

	if !mgr.HasPrompt("exists") {
		t.Error("Expected HasPrompt to return true for existing prompt")
	}

	if mgr.HasPrompt("nonexistent") {
		t.Error("Expected HasPrompt to return false for non-existent prompt")
	}
}

func TestNewManager_LoadFromDirectory(t *testing.T) {
	// Create temp directory with YAML files
	tmpDir := t.TempDir()

	// Create analyzer.yaml
	analyzerYAML := `
test_system: "System prompt"
test_user: "User prompt"
`
	_ = os.WriteFile(filepath.Join(tmpDir, "analyzer.yaml"), []byte(analyzerYAML), 0644)

	// Create documenter.yml
	documenterYML := `
doc_system: "Doc system prompt"
doc_user: "Doc user prompt"
`
	_ = os.WriteFile(filepath.Join(tmpDir, "documenter.yml"), []byte(documenterYML), 0644)

	// Load manager
	mgr, err := NewManager(tmpDir)
	if err != nil {
		t.Fatalf("Expected no error loading manager, got %v", err)
	}

	// Verify all prompts loaded
	prompts := []string{"test_system", "test_user", "doc_system", "doc_user"}
	for _, p := range prompts {
		if !mgr.HasPrompt(p) {
			t.Errorf("Expected prompt '%s' to be loaded", p)
		}
	}
}

func TestNewManager_InvalidYAML(t *testing.T) {
	tmpDir := t.TempDir()

	// Create invalid YAML
	invalidYAML := `
test: this is not
  valid: yaml: structure
`
	_ = os.WriteFile(filepath.Join(tmpDir, "invalid.yaml"), []byte(invalidYAML), 0644)

	_, err := NewManager(tmpDir)
	if err == nil {
		t.Fatal("Expected error for invalid YAML, got nil")
	}
}

func TestNewManager_DirectoryNotExists(t *testing.T) {
	_, err := NewManager("/nonexistent/directory")
	if err == nil {
		t.Fatal("Expected error for non-existent directory, got nil")
	}
}

func TestNewManager_IgnoresNonYAMLFiles(t *testing.T) {
	tmpDir := t.TempDir()

	// Create YAML file
	yamlContent := `
prompt1: "value1"
`
	_ = os.WriteFile(filepath.Join(tmpDir, "prompts.yaml"), []byte(yamlContent), 0644)

	// Create non-YAML files (should be ignored)
	_ = os.WriteFile(filepath.Join(tmpDir, "readme.md"), []byte("# README"), 0644)
	_ = os.WriteFile(filepath.Join(tmpDir, "script.sh"), []byte("#!/bin/bash"), 0644)
	_ = os.WriteFile(filepath.Join(tmpDir, "data.json"), []byte("{}"), 0644)

	mgr, err := NewManager(tmpDir)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Should only have prompts from YAML file
	if !mgr.HasPrompt("prompt1") {
		t.Error("Expected 'prompt1' to be loaded from YAML")
	}

	// Other files should not create prompts
	if mgr.HasPrompt("readme.md") {
		t.Error("Non-YAML file should not be loaded as prompt")
	}
}

func TestNewManager_IgnoresSubdirectories(t *testing.T) {
	tmpDir := t.TempDir()

	// Create YAML in root
	_ = os.WriteFile(filepath.Join(tmpDir, "root.yaml"), []byte("root_prompt: value\n"), 0644)

	// Create subdirectory with YAML (should be ignored by Walk behavior)
	subdir := filepath.Join(tmpDir, "subdir")
	_ = os.MkdirAll(subdir, 0755)
	_ = os.WriteFile(filepath.Join(subdir, "sub.yaml"), []byte("sub_prompt: value\n"), 0644)

	mgr, err := NewManager(tmpDir)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Should have root prompt
	if !mgr.HasPrompt("root_prompt") {
		t.Error("Expected 'root_prompt' from root directory")
	}

	// Current implementation uses ReadDir which doesn't recurse
	// So subdirectory prompts should NOT be loaded
	if mgr.HasPrompt("sub_prompt") {
		t.Error("Prompts from subdirectories should not be loaded")
	}
}

func TestManager_GetPromptTemplate(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"test_system_prompt": "System prompt content",
		"test_user_prompt":   "User prompt content",
	})

	template, err := mgr.GetPromptTemplate("test")
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if template.SystemPrompt != "System prompt content" {
		t.Errorf("Expected system prompt, got '%s'", template.SystemPrompt)
	}

	if template.UserPrompt != "User prompt content" {
		t.Errorf("Expected user prompt, got '%s'", template.UserPrompt)
	}
}

func TestManager_GetPromptTemplate_NotFound(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"other_prompt": "value",
	})

	_, err := mgr.GetPromptTemplate("test")
	if err == nil {
		t.Fatal("Expected error for non-existent template, got nil")
	}
}

func TestManager_RenderTemplate(t *testing.T) {
	// RenderTemplate looks for name_system_prompt and name_user_prompt
	// But GetPromptTemplate tries to call Render on name_system_prompt first,
	// falling back to Get name_system. Let's provide both formats.
	mgr := NewManagerFromMap(map[string]string{
		"test_system":      "Analyze {{.Language}} code",
		"test_user_prompt": "Path: {{.RepoPath}}",
	})

	template, err := mgr.RenderTemplate("test", map[string]interface{}{
		"Language": "Go",
		"RepoPath": "/test",
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if template.SystemPrompt != "Analyze Go code" {
		t.Errorf("Expected 'Analyze Go code', got '%s'", template.SystemPrompt)
	}

	if template.UserPrompt != "Path: /test" {
		t.Errorf("Expected 'Path: /test', got '%s'", template.UserPrompt)
	}
}

func TestManager_EmptyPrompt(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"empty": "",
	})

	prompt, err := mgr.Get("empty")
	if err != nil {
		t.Fatalf("Expected no error for empty prompt, got %v", err)
	}

	if prompt != "" {
		t.Errorf("Expected empty string, got '%s'", prompt)
	}
}

func TestManager_WhitespacePrompt(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"whitespace": "   \n\t\n   ",
	})

	prompt, err := mgr.Get("whitespace")
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Should preserve whitespace exactly
	if prompt != "   \n\t\n   " {
		t.Errorf("Expected whitespace to be preserved, got '%s'", prompt)
	}
}

func TestManager_MultilinePrompt(t *testing.T) {
	multiline := `Line 1
Line 2
Line 3
`

	mgr := NewManagerFromMap(map[string]string{
		"multiline": multiline,
	})

	prompt, err := mgr.Get("multiline")
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if prompt != multiline {
		t.Errorf("Expected multiline prompt to be preserved")
	}
}

func TestManager_SpecialCharacters(t *testing.T) {
	special := "Test with {{special}} {{.characters}} and $symbols @#%"

	mgr := NewManagerFromMap(map[string]string{
		"special": special,
	})

	prompt, err := mgr.Get("special")
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	if prompt != special {
		t.Errorf("Expected special characters to be preserved")
	}
}

// Helper function for string contains check
func containsString(haystack, needle string) bool {
	return len(haystack) >= len(needle) &&
		(haystack == needle || len(needle) == 0 || findSubstring(haystack, needle))
}

func findSubstring(s, substr string) bool {
	for i := 0; i <= len(s)-len(substr); i++ {
		if s[i:i+len(substr)] == substr {
			return true
		}
	}
	return false
}
</file>
<file path="internal/prompts/manager_test_override.go">
package prompts

import (
	"os"
	"path/filepath"
	"strings"
	"testing"
)

func TestNewManagerWithOverrides_SystemOnly(t *testing.T) {
	// Create system prompts directory
	systemDir := t.TempDir()

	systemYAML := `
structure_analyzer_system: "System structure prompt"
structure_analyzer_user: "User structure prompt"
dependency_analyzer_system: "System dependency prompt"
dependency_analyzer_user: "User dependency prompt"
data_flow_analyzer_system: "System data flow prompt"
data_flow_analyzer_user: "User data flow prompt"
request_flow_analyzer_system: "System request flow prompt"
request_flow_analyzer_user: "User request flow prompt"
api_analyzer_system: "System API prompt"
api_analyzer_user: "User API prompt"
documenter_system_prompt: "System doc prompt"
documenter_user_prompt: "User doc prompt"
ai_rules_system_prompt: "System rules prompt"
ai_rules_user_prompt: "User rules prompt"
`
	_ = os.WriteFile(filepath.Join(systemDir, "analyzer.yaml"), []byte(systemYAML), 0644)

	// Create manager with no project overrides
	mgr, err := NewManagerWithOverrides(systemDir, "")
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Verify system prompts loaded
	prompt, err := mgr.Get("structure_analyzer_system")
	if err != nil {
		t.Fatalf("Expected prompt to exist, got error: %v", err)
	}

	if prompt != "System structure prompt" {
		t.Errorf("Expected 'System structure prompt', got '%s'", prompt)
	}

	// Verify source tracking
	source := mgr.GetSource("structure_analyzer_system")
	if !strings.Contains(source, "system") {
		t.Errorf("Expected source to contain 'system', got '%s'", source)
	}
}

func TestNewManagerWithOverrides_WithProjectOverrides(t *testing.T) {
	// Create system prompts directory
	systemDir := t.TempDir()

	systemYAML := `
structure_analyzer_system: "System structure prompt"
structure_analyzer_user: "User structure prompt"
dependency_analyzer_system: "System dependency prompt"
dependency_analyzer_user: "User dependency prompt"
data_flow_analyzer_system: "System data flow prompt"
data_flow_analyzer_user: "User data flow prompt"
request_flow_analyzer_system: "System request flow prompt"
request_flow_analyzer_user: "User request flow prompt"
api_analyzer_system: "System API prompt"
api_analyzer_user: "User API prompt"
documenter_system_prompt: "System doc prompt"
documenter_user_prompt: "User doc prompt"
ai_rules_system_prompt: "System rules prompt"
ai_rules_user_prompt: "User rules prompt"
test_only_system: "Only in system"
`
	_ = os.WriteFile(filepath.Join(systemDir, "analyzer.yaml"), []byte(systemYAML), 0644)

	// Create project prompts directory with overrides
	projectDir := t.TempDir()

	projectYAML := `
structure_analyzer_system: "CUSTOM structure prompt"
documenter_system_prompt: "CUSTOM doc prompt"
test_only_project: "Only in project"
`
	_ = os.WriteFile(filepath.Join(projectDir, "custom.yaml"), []byte(projectYAML), 0644)

	// Create manager with overrides
	mgr, err := NewManagerWithOverrides(systemDir, projectDir)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Verify overridden prompts
	prompt, err := mgr.Get("structure_analyzer_system")
	if err != nil {
		t.Fatalf("Expected prompt to exist, got error: %v", err)
	}

	if prompt != "CUSTOM structure prompt" {
		t.Errorf("Expected 'CUSTOM structure prompt', got '%s'", prompt)
	}

	// Verify override source tracking
	source := mgr.GetSource("structure_analyzer_system")
	if !strings.Contains(source, "project") {
		t.Errorf("Expected source to contain 'project' for overridden prompt, got '%s'", source)
	}

	// Verify non-overridden prompts still work
	prompt2, err := mgr.Get("dependency_analyzer_system")
	if err != nil {
		t.Fatalf("Expected prompt to exist, got error: %v", err)
	}

	if prompt2 != "System dependency prompt" {
		t.Errorf("Expected 'System dependency prompt', got '%s'", prompt2)
	}

	source2 := mgr.GetSource("dependency_analyzer_system")
	if !strings.Contains(source2, "system") {
		t.Errorf("Expected source to contain 'system' for non-overridden prompt, got '%s'", source2)
	}

	// Verify project-only prompts exist
	prompt3, err := mgr.Get("test_only_project")
	if err != nil {
		t.Fatalf("Expected project-only prompt to exist, got error: %v", err)
	}

	if prompt3 != "Only in project" {
		t.Errorf("Expected 'Only in project', got '%s'", prompt3)
	}

	// Verify system-only prompts exist
	prompt4, err := mgr.Get("test_only_system")
	if err != nil {
		t.Fatalf("Expected system-only prompt to exist, got error: %v", err)
	}

	if prompt4 != "Only in system" {
		t.Errorf("Expected 'Only in system', got '%s'", prompt4)
	}
}

func TestNewManagerWithOverrides_ListOverrides(t *testing.T) {
	systemDir := t.TempDir()

	systemYAML := `
structure_analyzer_system: "System"
dependency_analyzer_system: "System"
data_flow_analyzer_system: "System"
request_flow_analyzer_system: "System"
api_analyzer_system: "System"
structure_analyzer_user: "System"
dependency_analyzer_user: "System"
data_flow_analyzer_user: "System"
request_flow_analyzer_user: "System"
api_analyzer_user: "System"
documenter_system_prompt: "System"
documenter_user_prompt: "System"
ai_rules_system_prompt: "System"
ai_rules_user_prompt: "System"
`
	_ = os.WriteFile(filepath.Join(systemDir, "analyzer.yaml"), []byte(systemYAML), 0644)

	projectDir := t.TempDir()
	projectYAML := `
structure_analyzer_system: "Custom"
documenter_system_prompt: "Custom"
`
	_ = os.WriteFile(filepath.Join(projectDir, "custom.yaml"), []byte(projectYAML), 0644)

	mgr, err := NewManagerWithOverrides(systemDir, projectDir)
	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	overrides := mgr.ListOverrides()
	if len(overrides) != 2 {
		t.Errorf("Expected 2 overrides, got %d: %v", len(overrides), overrides)
	}

	// Check that overrides contain the expected prompts
	hasStructure := false
	hasDocumenter := false
	for _, override := range overrides {
		if override == "structure_analyzer_system" {
			hasStructure = true
		}
		if override == "documenter_system_prompt" {
			hasDocumenter = true
		}
	}

	if !hasStructure {
		t.Error("Expected overrides to contain 'structure_analyzer_system'")
	}

	if !hasDocumenter {
		t.Error("Expected overrides to contain 'documenter_system_prompt'")
	}
}

func TestNewManagerWithOverrides_NonexistentProjectDir(t *testing.T) {
	systemDir := t.TempDir()

	systemYAML := `
structure_analyzer_system: "System"
dependency_analyzer_system: "System"
data_flow_analyzer_system: "System"
request_flow_analyzer_system: "System"
api_analyzer_system: "System"
structure_analyzer_user: "System"
dependency_analyzer_user: "System"
data_flow_analyzer_user: "System"
request_flow_analyzer_user: "System"
api_analyzer_user: "System"
documenter_system_prompt: "System"
documenter_user_prompt: "System"
ai_rules_system_prompt: "System"
ai_rules_user_prompt: "System"
`
	_ = os.WriteFile(filepath.Join(systemDir, "analyzer.yaml"), []byte(systemYAML), 0644)

	// Use nonexistent project directory
	projectDir := filepath.Join(t.TempDir(), "nonexistent")

	mgr, err := NewManagerWithOverrides(systemDir, projectDir)
	if err != nil {
		t.Fatalf("Expected no error with nonexistent project dir, got %v", err)
	}

	// Should work fine, just no overrides
	overrides := mgr.ListOverrides()
	if len(overrides) != 0 {
		t.Errorf("Expected 0 overrides with nonexistent project dir, got %d", len(overrides))
	}

	// System prompts should still be loaded
	if !mgr.HasPrompt("structure_analyzer_system") {
		t.Error("Expected system prompt to be loaded even without project dir")
	}
}

func TestNewManagerWithOverrides_MissingRequiredPrompt(t *testing.T) {
	systemDir := t.TempDir()

	// Create incomplete system prompts (missing required prompts)
	incompleteYAML := `
structure_analyzer_system: "System"
# Missing other required prompts
`
	_ = os.WriteFile(filepath.Join(systemDir, "incomplete.yaml"), []byte(incompleteYAML), 0644)

	_, err := NewManagerWithOverrides(systemDir, "")
	if err == nil {
		t.Fatal("Expected error for missing required prompts, got nil")
	}

	if !strings.Contains(err.Error(), "missing required prompts") {
		t.Errorf("Expected error message to mention missing prompts, got: %v", err)
	}
}

func TestManager_CountPrompts(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"prompt1": "value1",
		"prompt2": "value2",
		"prompt3": "value3",
	})

	count := mgr.CountPrompts()
	if count != 3 {
		t.Errorf("Expected count of 3, got %d", count)
	}
}

func TestManager_GetSource(t *testing.T) {
	mgr := NewManagerFromMap(map[string]string{
		"test": "value",
	})

	source := mgr.GetSource("test")
	if source != "test:map" {
		t.Errorf("Expected 'test:map', got '%s'", source)
	}

	// Test unknown prompt
	unknownSource := mgr.GetSource("nonexistent")
	if unknownSource != "unknown" {
		t.Errorf("Expected 'unknown' for nonexistent prompt, got '%s'", unknownSource)
	}
}
</file>
<file path="internal/testing/fixtures.go">
package testing

// SampleGoProject returns a sample Go project structure for testing
func SampleGoProject() map[string]string {
	return map[string]string{
		"go.mod": `module github.com/example/testproject

go 1.22

require (
	github.com/gorilla/mux v1.8.0
	github.com/lib/pq v1.10.0
)
`,
		"main.go": `package main

import (
	"fmt"
	"net/http"

	"github.com/gorilla/mux"
)

func main() {
	r := mux.NewRouter()
	r.HandleFunc("/", HomeHandler)
	r.HandleFunc("/api/users", UsersHandler)

	fmt.Println("Server starting on :8080")
	http.ListenAndServe(":8080", r)
}

func HomeHandler(w http.ResponseWriter, r *http.Request) {
	w.Write([]byte("Welcome"))
}

func UsersHandler(w http.ResponseWriter, r *http.Request) {
	w.Write([]byte("Users API"))
}
`,
		"internal/database/db.go": `package database

import (
	"database/sql"
	_ "github.com/lib/pq"
)

type Database struct {
	conn *sql.DB
}

func NewDatabase(connStr string) (*Database, error) {
	db, err := sql.Open("postgres", connStr)
	if err != nil {
		return nil, err
	}
	return &Database{conn: db}, nil
}

func (d *Database) Close() error {
	return d.conn.Close()
}
`,
		"internal/models/user.go": `package models

type User struct {
	ID       int64
	Username string
	Email    string
	Active   bool
}

func NewUser(username, email string) *User {
	return &User{
		Username: username,
		Email:    email,
		Active:   true,
	}
}
`,
		"README.md": `# Test Project

A sample Go web application for testing.

## Features

- REST API with Gorilla Mux
- PostgreSQL database integration
- User management

## Installation

` + "```bash" + `
go build -o app .
./app
` + "```" + `
`,
	}
}

// SamplePythonProject returns a sample Python project structure for testing
func SamplePythonProject() map[string]string {
	return map[string]string{
		"requirements.txt": `flask==2.3.0
sqlalchemy==2.0.0
pydantic==2.0.0
`,
		"app.py": `from flask import Flask, jsonify
from database import db
from models import User

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///app.db'
db.init_app(app)

@app.route('/')
def home():
    return jsonify({"message": "Welcome"})

@app.route('/api/users')
def users():
    users = User.query.all()
    return jsonify([u.to_dict() for u in users])

if __name__ == '__main__':
    app.run(debug=True)
`,
		"database.py": `from flask_sqlalchemy import SQLAlchemy

db = SQLAlchemy()
`,
		"models.py": `from database import db

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(80), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    active = db.Column(db.Boolean, default=True)

    def to_dict(self):
        return {
            'id': self.id,
            'username': self.username,
            'email': self.email,
            'active': self.active
        }
`,
	}
}

// SampleAnalysisOutput returns mock analysis output for testing
func SampleAnalysisOutput() string {
	return `# Code Structure Analysis

## Architectural Overview

This is a Go web application using the Gorilla Mux router framework and PostgreSQL database.

## Core Components

### HTTP Server (main.go)
- Entry point of the application
- Configures routes using Gorilla Mux
- Exposes REST API endpoints

### Database Layer (internal/database/)
- PostgreSQL connection management
- Database abstraction

### Models (internal/models/)
- User entity definition
- Data structure representations

## Service Definitions

The application follows a simple layered architecture:
- **Presentation Layer**: HTTP handlers in main.go
- **Data Layer**: Database package for persistence
- **Domain Layer**: Models package for business entities

## Interface Contracts

### Database Interface
The Database struct provides methods for:
- Connection initialization
- Connection cleanup

### Model Interfaces
User model provides:
- User creation
- User data representation

## Design Patterns Identified

- **Repository Pattern**: Database package abstracts data access
- **Constructor Pattern**: NewDatabase and NewUser factory functions
- **Singleton Pattern**: Database connection is managed centrally

## Component Relationships

main.go â†’ database â†’ PostgreSQL
main.go â†’ models â†’ User entities
HTTP Router â†’ Handlers â†’ Models

## Key Methods & Functions

- main(): Application entry point, server initialization
- HomeHandler(): Handles root endpoint
- UsersHandler(): Handles user API endpoint
- NewDatabase(): Database connection factory
- NewUser(): User entity factory
`
}

// SampleREADME returns a mock README for testing
func SampleREADME() string {
	return `# Test Project

Generated documentation for test project.

## Overview

This is a test project for validating gendocs functionality.

## Architecture

The project follows a standard layered architecture with:
- API layer
- Business logic layer
- Data access layer

## Getting Started

` + "```bash" + `
# Install dependencies
go mod download

# Run the application
go run main.go
` + "```" + `

## API Endpoints

### GET /
Returns welcome message

### GET /api/users
Returns list of users

## Database

PostgreSQL database with the following tables:
- users

## Contributing

Pull requests are welcome.

## License

MIT
`
}

// SampleLLMToolCallResponse returns a mock LLM response with tool calls
func SampleLLMToolCallResponse(toolName string, args map[string]interface{}) string {
	return `I need to examine the files in the repository.`
}
</file>
<file path="internal/testing/helpers.go">
package testing

import (
	"context"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"testing"

	"github.com/user/gendocs/internal/llm"
)

// MockLLMClient implements llm.LLMClient for testing
type MockLLMClient struct {
	Responses      []llm.CompletionResponse
	CallCount      int
	LastRequest    llm.CompletionRequest
	ShouldError    bool
	ErrorToReturn  error
	RequestHistory []llm.CompletionRequest
}

// NewMockLLMClient creates a new mock LLM client with predefined responses
func NewMockLLMClient(responses ...llm.CompletionResponse) *MockLLMClient {
	return &MockLLMClient{
		Responses:      responses,
		RequestHistory: make([]llm.CompletionRequest, 0),
	}
}

// GenerateCompletion implements llm.LLMClient
func (m *MockLLMClient) GenerateCompletion(ctx context.Context, req llm.CompletionRequest) (llm.CompletionResponse, error) {
	m.LastRequest = req
	m.RequestHistory = append(m.RequestHistory, req)

	if m.ShouldError {
		m.CallCount++
		return llm.CompletionResponse{}, m.ErrorToReturn
	}

	if m.CallCount >= len(m.Responses) {
		// Return last response if we've exhausted the list
		if len(m.Responses) > 0 {
			resp := m.Responses[len(m.Responses)-1]
			m.CallCount++
			return resp, nil
		}
		return llm.CompletionResponse{}, fmt.Errorf("no responses configured")
	}

	resp := m.Responses[m.CallCount]
	m.CallCount++
	return resp, nil
}

// SupportsTools implements llm.LLMClient
func (m *MockLLMClient) SupportsTools() bool {
	return true
}

// GetProvider implements llm.LLMClient
func (m *MockLLMClient) GetProvider() string {
	return "mock"
}

// Reset resets the mock state
func (m *MockLLMClient) Reset() {
	m.CallCount = 0
	m.LastRequest = llm.CompletionRequest{}
	m.RequestHistory = make([]llm.CompletionRequest, 0)
	m.ShouldError = false
	m.ErrorToReturn = nil
}

// SetError configures the mock to return an error
func (m *MockLLMClient) SetError(err error) {
	m.ShouldError = true
	m.ErrorToReturn = err
}

// CreateTempRepo creates a temporary git repository for testing
func CreateTempRepo(t *testing.T, files map[string]string) string {
	t.Helper()

	// Create temp directory
	tmpDir := t.TempDir()

	// Initialize git repo
	cmd := exec.Command("git", "init")
	cmd.Dir = tmpDir
	if err := cmd.Run(); err != nil {
		t.Fatalf("Failed to init git repo: %v", err)
	}

	// Configure git user (required for commits)
	configUser := exec.Command("git", "config", "user.email", "test@example.com")
	configUser.Dir = tmpDir
	_ = configUser.Run()

	configName := exec.Command("git", "config", "user.name", "Test User")
	configName.Dir = tmpDir
	_ = configName.Run()

	// Create files
	for relPath, content := range files {
		fullPath := filepath.Join(tmpDir, relPath)

		// Create parent directories
		parentDir := filepath.Dir(fullPath)
		if err := os.MkdirAll(parentDir, 0755); err != nil {
			t.Fatalf("Failed to create directory %s: %v", parentDir, err)
		}

		// Write file
		if err := os.WriteFile(fullPath, []byte(content), 0644); err != nil {
			t.Fatalf("Failed to write file %s: %v", fullPath, err)
		}
	}

	// Make initial commit
	addCmd := exec.Command("git", "add", ".")
	addCmd.Dir = tmpDir
	_ = addCmd.Run()

	commitCmd := exec.Command("git", "commit", "-m", "Initial commit")
	commitCmd.Dir = tmpDir
	_ = commitCmd.Run()

	return tmpDir
}

// AssertFileExists checks if a file exists at the given path
func AssertFileExists(t *testing.T, path string) {
	t.Helper()
	if _, err := os.Stat(path); os.IsNotExist(err) {
		t.Errorf("Expected file to exist: %s", path)
	}
}

// AssertFileNotExists checks if a file does not exist at the given path
func AssertFileNotExists(t *testing.T, path string) {
	t.Helper()
	if _, err := os.Stat(path); err == nil {
		t.Errorf("Expected file to not exist: %s", path)
	}
}

// AssertFileContains checks if a file contains the expected content
func AssertFileContains(t *testing.T, path, expected string) {
	t.Helper()
	content, err := os.ReadFile(path)
	if err != nil {
		t.Fatalf("Failed to read file %s: %v", path, err)
	}

	if !containsString(string(content), expected) {
		t.Errorf("File %s does not contain expected content.\nExpected substring: %s\nActual content:\n%s",
			path, expected, string(content))
	}
}

// containsString checks if haystack contains needle
func containsString(haystack, needle string) bool {
	return len(haystack) >= len(needle) &&
		(haystack == needle || len(needle) == 0 ||
			findSubstring(haystack, needle))
}

func findSubstring(s, substr string) bool {
	for i := 0; i <= len(s)-len(substr); i++ {
		if s[i:i+len(substr)] == substr {
			return true
		}
	}
	return false
}

// CreateYAML creates a YAML file with the given data
func CreateYAML(t *testing.T, dir, filename string, data map[string]string) {
	t.Helper()

	content := ""
	for key, value := range data {
		// Simple YAML formatting (works for basic cases)
		content += fmt.Sprintf("%s: |\n  %s\n\n", key, value)
	}

	fullPath := filepath.Join(dir, filename)
	if err := os.WriteFile(fullPath, []byte(content), 0644); err != nil {
		t.Fatalf("Failed to write YAML file %s: %v", fullPath, err)
	}
}
</file>
<file path="internal/tools/base.go">
package tools

import (
	"context"
	"fmt"
)

// ModelRetryError is raised when a tool encounters a recoverable error
// This error type triggers a retry at the agent level
type ModelRetryError struct {
	Message string
}

func (e *ModelRetryError) Error() string {
	return e.Message
}

// Tool is the interface that all tools must implement
type Tool interface {
	// Name returns the tool name
	Name() string

	// Description returns a description of what the tool does
	Description() string

	// Parameters returns the JSON schema for the tool's parameters
	Parameters() map[string]interface{}

	// Execute runs the tool with the given parameters
	Execute(ctx context.Context, params map[string]interface{}) (interface{}, error)
}

// BaseTool provides common functionality for all tools
type BaseTool struct {
	MaxRetries int
}

// NewBaseTool creates a new base tool
func NewBaseTool(maxRetries int) BaseTool {
	return BaseTool{
		MaxRetries: maxRetries,
	}
}

// RetryableExecute executes a function with retry logic
func (bt *BaseTool) RetryableExecute(ctx context.Context, fn func() (interface{}, error)) (interface{}, error) {
	var lastErr error

	for attempt := 0; attempt < bt.MaxRetries; attempt++ {
		result, err := fn()
		if err == nil {
			return result, nil
		}

		lastErr = err

		// Check if error is recoverable (ModelRetryError)
		if _, ok := err.(*ModelRetryError); !ok {
			// Not recoverable, return immediately
			return nil, err
		}

		// If it's the last attempt, don't wait
		if attempt == bt.MaxRetries-1 {
			break
		}
	}

	if lastErr != nil {
		return nil, fmt.Errorf("tool failed after %d retries: %w", bt.MaxRetries, lastErr)
	}

	return nil, fmt.Errorf("tool failed after %d retries", bt.MaxRetries)
}
</file>
<file path="internal/tools/file_read.go">
package tools

import (
	"bufio"
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strconv"
	"strings"
)

// MaxLineLength is the maximum length of a single line to prevent memory issues
const MaxLineLength = 10000

// MaxTotalChars is the maximum total characters to return
const MaxTotalChars = 50000

// FileReadTool reads file contents with optional pagination
type FileReadTool struct {
	BaseTool
}

// NewFileReadTool creates a new file read tool
func NewFileReadTool(maxRetries int) *FileReadTool {
	return &FileReadTool{
		BaseTool: NewBaseTool(maxRetries),
	}
}

// Name returns the tool name
func (frt *FileReadTool) Name() string {
	return "read_file"
}

// Description returns the tool description
func (frt *FileReadTool) Description() string {
	return "Read contents of a source code file. By default reads first 200 lines. Binary files are automatically rejected. Use line_number and line_count for pagination."
}

// Parameters returns the JSON schema for the tool parameters
func (frt *FileReadTool) Parameters() map[string]interface{} {
	return map[string]interface{}{
		"type": "object",
		"properties": map[string]interface{}{
			"file_path": map[string]interface{}{
				"type":        "string",
				"description": "Path to the file to read",
			},
			"line_number": map[string]interface{}{
				"type":        "integer",
				"description": "Starting line number (1-indexed). Default: 1",
			},
			"line_count": map[string]interface{}{
				"type":        "integer",
				"description": "Number of lines to read. Default: 200",
			},
		},
		"required": []string{"file_path"},
	}
}

// Execute reads the file contents
func (frt *FileReadTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) {
	return frt.RetryableExecute(ctx, func() (interface{}, error) {
		filePath, ok := params["file_path"].(string)
		if !ok {
			return nil, fmt.Errorf("file_path must be a string")
		}

		// Check if file exists
		info, err := os.Stat(filePath)
		if err != nil {
			return nil, &ModelRetryError{Message: fmt.Sprintf("Failed to stat file: %v", err)}
		}

		// Check if it's a directory
		if info.IsDir() {
			return nil, &ModelRetryError{Message: "Path is a directory, not a file"}
		}

		// Check file size - warn if too large
		if info.Size() > 10*1024*1024 { // > 10MB
			return map[string]interface{}{
				"error":   "File too large",
				"message": fmt.Sprintf("File is %d bytes. Consider reading specific sections with line_number and line_count.", info.Size()),
			}, nil
		}

		// Check if file is binary
		if IsBinaryFile(filePath) {
			ext := filepath.Ext(filePath)
			return map[string]interface{}{
				"error":   "Binary file detected",
				"message": fmt.Sprintf("File '%s' appears to be a binary file (extension: %s). Binary files cannot be read as text.", filepath.Base(filePath), ext),
			}, nil
		}

		lineNumber := 1
		if ln, ok := params["line_number"]; ok {
			switch v := ln.(type) {
			case int:
				lineNumber = v
			case float64:
				lineNumber = int(v)
			case string:
				if i, err := strconv.Atoi(v); err == nil {
					lineNumber = i
				}
			}
		}

		lineCount := 200
		if lc, ok := params["line_count"]; ok {
			switch v := lc.(type) {
			case int:
				lineCount = v
			case float64:
				lineCount = int(v)
			case string:
				if i, err := strconv.Atoi(v); err == nil {
					lineCount = i
				}
			}
		}

		// Cap line count to prevent excessive reads
		if lineCount > 500 {
			lineCount = 500
		}

		file, err := os.Open(filePath)
		if err != nil {
			return nil, &ModelRetryError{Message: fmt.Sprintf("Failed to open file: %v", err)}
		}
		defer func() { _ = file.Close() }()

		scanner := bufio.NewScanner(file)
		// Increase scanner buffer for long lines
		buf := make([]byte, 0, 64*1024)
		scanner.Buffer(buf, 1024*1024) // 1MB max line

		var lines []string
		currentLine := 1
		totalChars := 0
		truncatedLines := 0

		for scanner.Scan() {
			if currentLine >= lineNumber && currentLine < lineNumber+lineCount {
				line := scanner.Text()

				// Truncate very long lines
				if len(line) > MaxLineLength {
					line = line[:MaxLineLength] + "... [line truncated]"
					truncatedLines++
				}

				// Check total character limit
				if totalChars+len(line) > MaxTotalChars {
					lines = append(lines, "[OUTPUT TRUNCATED - exceeded character limit]")
					break
				}

				lines = append(lines, line)
				totalChars += len(line)
			}
			currentLine++
			if currentLine >= lineNumber+lineCount {
				break
			}
		}

		if err := scanner.Err(); err != nil {
			// Handle lines that are too long
			if strings.Contains(err.Error(), "token too long") {
				return map[string]interface{}{
					"error":   "File has extremely long lines",
					"message": "This file contains lines that exceed the maximum buffer size. It may be a minified or binary file.",
				}, nil
			}
			return nil, &ModelRetryError{Message: fmt.Sprintf("Error reading file: %v", err)}
		}

		result := map[string]interface{}{
			"content":          lines,
			"start_line":       lineNumber,
			"end_line":         lineNumber + len(lines) - 1,
			"total_lines_read": len(lines),
		}

		if truncatedLines > 0 {
			result["truncated_lines"] = truncatedLines
			result["note"] = fmt.Sprintf("%d lines were truncated due to excessive length", truncatedLines)
		}

		return result, nil
	})
}
</file>
<file path="internal/tools/file_read_test.go">
package tools

import (
	"context"
	"os"
	"path/filepath"
	"testing"
)

func TestFileReadTool_Name(t *testing.T) {
	tool := NewFileReadTool(3)
	if tool.Name() != "read_file" {
		t.Errorf("Expected name 'read_file', got '%s'", tool.Name())
	}
}

func TestFileReadTool_Description(t *testing.T) {
	tool := NewFileReadTool(3)
	desc := tool.Description()
	if desc == "" {
		t.Error("Expected non-empty description")
	}
}

func TestFileReadTool_Parameters(t *testing.T) {
	tool := NewFileReadTool(3)
	params := tool.Parameters()

	// Check required fields
	required, ok := params["required"].([]string)
	if !ok {
		t.Fatal("Expected 'required' field in parameters")
	}

	if len(required) != 1 || required[0] != "file_path" {
		t.Errorf("Expected required field 'file_path', got %v", required)
	}

	// Check properties
	properties, ok := params["properties"].(map[string]interface{})
	if !ok {
		t.Fatal("Expected 'properties' field in parameters")
	}

	if _, ok := properties["file_path"]; !ok {
		t.Error("Expected 'file_path' property")
	}

	if _, ok := properties["line_number"]; !ok {
		t.Error("Expected 'line_number' property")
	}

	if _, ok := properties["line_count"]; !ok {
		t.Error("Expected 'line_count' property")
	}
}

func TestFileReadTool_Execute_Success(t *testing.T) {
	// Create temp file
	tmpDir := t.TempDir()
	testFile := filepath.Join(tmpDir, "test.txt")
	content := "line 1\nline 2\nline 3\nline 4\nline 5\n"
	if err := os.WriteFile(testFile, []byte(content), 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	tool := NewFileReadTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"file_path": testFile,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap, ok := result.(map[string]interface{})
	if !ok {
		t.Fatalf("Expected result to be map, got %T", result)
	}

	lines, ok := resultMap["content"].([]string)
	if !ok {
		t.Fatalf("Expected content to be []string, got %T", resultMap["content"])
	}

	if len(lines) != 5 {
		t.Errorf("Expected 5 lines, got %d", len(lines))
	}

	if lines[0] != "line 1" {
		t.Errorf("Expected first line 'line 1', got '%s'", lines[0])
	}
}

func TestFileReadTool_Execute_WithPagination(t *testing.T) {
	// Create temp file with many lines
	tmpDir := t.TempDir()
	testFile := filepath.Join(tmpDir, "test.txt")

	var content string
	for i := 1; i <= 100; i++ {
		content += "line " + string(rune('0'+i)) + "\n"
	}
	if err := os.WriteFile(testFile, []byte(content), 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	tool := NewFileReadTool(3)

	// Read lines 10-14 (5 lines starting from line 10)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"file_path":   testFile,
		"line_number": 10,
		"line_count":  5,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	lines := resultMap["content"].([]string)

	if len(lines) != 5 {
		t.Errorf("Expected 5 lines, got %d", len(lines))
	}

	if start := resultMap["start_line"].(int); start != 10 {
		t.Errorf("Expected start_line 10, got %d", start)
	}

	if end := resultMap["end_line"].(int); end != 14 {
		t.Errorf("Expected end_line 14, got %d", end)
	}
}

func TestFileReadTool_Execute_FileNotFound(t *testing.T) {
	tool := NewFileReadTool(3)

	_, err := tool.Execute(context.Background(), map[string]interface{}{
		"file_path": "/nonexistent/file.txt",
	})

	if err == nil {
		t.Fatal("Expected error for non-existent file, got nil")
	}

	// Should be an error (wrapped or not)
	// After retries, it becomes a different error type
}

func TestFileReadTool_Execute_MissingFilePath(t *testing.T) {
	tool := NewFileReadTool(3)

	_, err := tool.Execute(context.Background(), map[string]interface{}{})

	if err == nil {
		t.Fatal("Expected error for missing file_path, got nil")
	}
}

func TestFileReadTool_Execute_InvalidFilePath(t *testing.T) {
	tool := NewFileReadTool(3)

	_, err := tool.Execute(context.Background(), map[string]interface{}{
		"file_path": 123, // Invalid type
	})

	if err == nil {
		t.Fatal("Expected error for invalid file_path type, got nil")
	}
}

func TestFileReadTool_Execute_LineNumberTypes(t *testing.T) {
	// Test that line_number accepts different numeric types
	tmpDir := t.TempDir()
	testFile := filepath.Join(tmpDir, "test.txt")
	content := "line 1\nline 2\nline 3\nline 4\nline 5\n"
	_ = os.WriteFile(testFile, []byte(content), 0644)

	tool := NewFileReadTool(3)

	tests := []struct {
		name       string
		lineNumber interface{}
		expected   int
	}{
		{"int", 2, 2},
		{"float64", 2.0, 2},
		{"string", "2", 2},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			result, err := tool.Execute(context.Background(), map[string]interface{}{
				"file_path":   testFile,
				"line_number": tt.lineNumber,
				"line_count":  2,
			})

			if err != nil {
				t.Fatalf("Expected no error, got %v", err)
			}

			resultMap := result.(map[string]interface{})
			if start := resultMap["start_line"].(int); start != tt.expected {
				t.Errorf("Expected start_line %d, got %d", tt.expected, start)
			}
		})
	}
}

func TestFileReadTool_Execute_EmptyFile(t *testing.T) {
	// Create empty file
	tmpDir := t.TempDir()
	testFile := filepath.Join(tmpDir, "empty.txt")
	if err := os.WriteFile(testFile, []byte(""), 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	tool := NewFileReadTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"file_path": testFile,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	lines := resultMap["content"].([]string)

	if len(lines) != 0 {
		t.Errorf("Expected 0 lines for empty file, got %d", len(lines))
	}
}

func TestFileReadTool_Execute_SingleLine(t *testing.T) {
	// Create file with single line (no newline at end)
	tmpDir := t.TempDir()
	testFile := filepath.Join(tmpDir, "single.txt")
	if err := os.WriteFile(testFile, []byte("single line"), 0644); err != nil {
		t.Fatalf("Failed to create test file: %v", err)
	}

	tool := NewFileReadTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"file_path": testFile,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	lines := resultMap["content"].([]string)

	if len(lines) != 1 {
		t.Errorf("Expected 1 line, got %d", len(lines))
	}

	if lines[0] != "single line" {
		t.Errorf("Expected 'single line', got '%s'", lines[0])
	}
}

func TestFileReadTool_Execute_BeyondFileEnd(t *testing.T) {
	// Test reading beyond end of file
	tmpDir := t.TempDir()
	testFile := filepath.Join(tmpDir, "test.txt")
	content := "line 1\nline 2\nline 3\n"
	_ = os.WriteFile(testFile, []byte(content), 0644)

	tool := NewFileReadTool(3)

	// Request lines 2-100 (but file only has 3 lines)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"file_path":   testFile,
		"line_number": 2,
		"line_count":  100,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	lines := resultMap["content"].([]string)

	// Should only return lines 2-3
	if len(lines) != 2 {
		t.Errorf("Expected 2 lines, got %d", len(lines))
	}
}

func TestFileReadTool_Execute_ContextCanceled(t *testing.T) {
	// Create temp file
	tmpDir := t.TempDir()
	testFile := filepath.Join(tmpDir, "test.txt")
	_ = os.WriteFile(testFile, []byte("test content\n"), 0644)

	tool := NewFileReadTool(3)

	// Create canceled context
	ctx, cancel := context.WithCancel(context.Background())
	cancel()

	// Execute should respect context
	_, err := tool.Execute(ctx, map[string]interface{}{
		"file_path": testFile,
	})

	// Note: Current implementation doesn't check context during file read
	// This test documents current behavior - may want to add context checks
	_ = err
}
</file>
<file path="internal/tools/list_files.go">
package tools

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
)

// MaxFilesToList is the maximum number of files to return
const MaxFilesToList = 500

// ListFilesTool lists files in a directory recursively
type ListFilesTool struct {
	BaseTool
}

// NewListFilesTool creates a new list files tool
func NewListFilesTool(maxRetries int) *ListFilesTool {
	return &ListFilesTool{
		BaseTool: NewBaseTool(maxRetries),
	}
}

// Name returns the tool name
func (lft *ListFilesTool) Name() string {
	return "list_files"
}

// Description returns the tool description
func (lft *ListFilesTool) Description() string {
	return "List source code files in a directory recursively. Automatically filters out binary files, build outputs, dependencies (node_modules, vendor), and files matching .gitignore patterns. Returns up to 500 files."
}

// Parameters returns the JSON schema for the tool parameters
func (lft *ListFilesTool) Parameters() map[string]interface{} {
	return map[string]interface{}{
		"type": "object",
		"properties": map[string]interface{}{
			"directory": map[string]interface{}{
				"type":        "string",
				"description": "Directory path to list files from",
			},
		},
		"required": []string{"directory"},
	}
}

// Execute lists files in the directory
func (lft *ListFilesTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) {
	return lft.RetryableExecute(ctx, func() (interface{}, error) {
		directory, ok := params["directory"].(string)
		if !ok {
			return nil, fmt.Errorf("directory must be a string")
		}

		// Load gitignore patterns
		ignorePatterns := LoadGitignorePatterns(directory)

		var files []string
		var skippedCount int
		var truncated bool

		err := filepath.Walk(directory, func(path string, info os.FileInfo, err error) error {
			if err != nil {
				// Skip permission errors
				if os.IsPermission(err) {
					return nil
				}
				return err
			}

			// Get relative path for pattern matching
			relPath, relErr := filepath.Rel(directory, path)
			if relErr != nil {
				relPath = path
			}

			// Skip directories that match ignore patterns
			if info.IsDir() {
				if ShouldIgnore(relPath, ignorePatterns) || ShouldIgnore(info.Name(), ignorePatterns) {
					skippedCount++
					return filepath.SkipDir
				}
				return nil
			}

			// Skip files that match ignore patterns
			if ShouldIgnore(relPath, ignorePatterns) || ShouldIgnore(info.Name(), ignorePatterns) {
				skippedCount++
				return nil
			}

			// Skip binary files
			if IsBinaryFile(path) {
				skippedCount++
				return nil
			}

			// Enforce maximum file limit
			if len(files) >= MaxFilesToList {
				truncated = true
				return filepath.SkipAll
			}

			files = append(files, relPath)
			return nil
		})

		if err != nil && err != filepath.SkipAll {
			return nil, &ModelRetryError{Message: fmt.Sprintf("Failed to list files: %v", err)}
		}

		result := map[string]interface{}{
			"files":   files,
			"count":   len(files),
			"skipped": skippedCount,
		}

		if truncated {
			result["truncated"] = true
			result["message"] = fmt.Sprintf("Results truncated to %d files. Use more specific directory paths for complete listings.", MaxFilesToList)
		}

		return result, nil
	})
}
</file>
<file path="internal/tools/list_files_test.go">
package tools

import (
	"context"
	"os"
	"path/filepath"
	"testing"
)

func TestListFilesTool_Name(t *testing.T) {
	tool := NewListFilesTool(3)
	if tool.Name() != "list_files" {
		t.Errorf("Expected name 'list_files', got '%s'", tool.Name())
	}
}

func TestListFilesTool_Description(t *testing.T) {
	tool := NewListFilesTool(3)
	desc := tool.Description()
	if desc == "" {
		t.Error("Expected non-empty description")
	}
}

func TestListFilesTool_Parameters(t *testing.T) {
	tool := NewListFilesTool(3)
	params := tool.Parameters()

	// Check required fields
	required, ok := params["required"].([]string)
	if !ok {
		t.Fatal("Expected 'required' field in parameters")
	}

	if len(required) != 1 || required[0] != "directory" {
		t.Errorf("Expected required field 'directory', got %v", required)
	}

	// Check properties
	properties, ok := params["properties"].(map[string]interface{})
	if !ok {
		t.Fatal("Expected 'properties' field in parameters")
	}

	if _, ok := properties["directory"]; !ok {
		t.Error("Expected 'directory' property")
	}
}

func TestListFilesTool_Execute_Success(t *testing.T) {
	// Create temp directory with files
	tmpDir := t.TempDir()

	// Create files
	files := []string{
		"file1.txt",
		"file2.go",
		"subdir/file3.md",
		"subdir/nested/file4.yaml",
	}

	for _, f := range files {
		fullPath := filepath.Join(tmpDir, f)
		_ = os.MkdirAll(filepath.Dir(fullPath), 0755)
		_ = os.WriteFile(fullPath, []byte("content"), 0644)
	}

	tool := NewListFilesTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": tmpDir,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap, ok := result.(map[string]interface{})
	if !ok {
		t.Fatalf("Expected result to be map, got %T", result)
	}

	filesList, ok := resultMap["files"].([]string)
	if !ok {
		t.Fatalf("Expected files to be []string, got %T", resultMap["files"])
	}

	if len(filesList) != len(files) {
		t.Errorf("Expected %d files, got %d", len(files), len(filesList))
	}

	count, ok := resultMap["count"].(int)
	if !ok {
		t.Fatalf("Expected count to be int, got %T", resultMap["count"])
	}

	if count != len(files) {
		t.Errorf("Expected count %d, got %d", len(files), count)
	}

	// Verify all expected files are in the list
	fileMap := make(map[string]bool)
	for _, f := range filesList {
		fileMap[f] = true
	}

	for _, expectedFile := range files {
		if !fileMap[expectedFile] {
			t.Errorf("Expected file '%s' not found in results", expectedFile)
		}
	}
}

func TestListFilesTool_Execute_EmptyDirectory(t *testing.T) {
	// Create empty directory
	tmpDir := t.TempDir()

	tool := NewListFilesTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": tmpDir,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	filesList := resultMap["files"].([]string)

	if len(filesList) != 0 {
		t.Errorf("Expected 0 files in empty directory, got %d", len(filesList))
	}

	if count := resultMap["count"].(int); count != 0 {
		t.Errorf("Expected count 0, got %d", count)
	}
}

func TestListFilesTool_Execute_OnlyDirectories(t *testing.T) {
	// Create directory with only subdirectories (no files)
	tmpDir := t.TempDir()
	_ = os.MkdirAll(filepath.Join(tmpDir, "dir1"), 0755)
	_ = os.MkdirAll(filepath.Join(tmpDir, "dir2/subdir"), 0755)

	tool := NewListFilesTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": tmpDir,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	filesList := resultMap["files"].([]string)

	if len(filesList) != 0 {
		t.Errorf("Expected 0 files (directories should not be listed), got %d", len(filesList))
	}
}

func TestListFilesTool_Execute_DirectoryNotFound(t *testing.T) {
	tool := NewListFilesTool(3)

	_, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": "/nonexistent/directory",
	})

	if err == nil {
		t.Fatal("Expected error for non-existent directory, got nil")
	}

	// Should be an error (wrapped or not)
	// After retries, it becomes a different error type
}

func TestListFilesTool_Execute_MissingDirectory(t *testing.T) {
	tool := NewListFilesTool(3)

	_, err := tool.Execute(context.Background(), map[string]interface{}{})

	if err == nil {
		t.Fatal("Expected error for missing directory parameter, got nil")
	}
}

func TestListFilesTool_Execute_InvalidDirectory(t *testing.T) {
	tool := NewListFilesTool(3)

	_, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": 123, // Invalid type
	})

	if err == nil {
		t.Fatal("Expected error for invalid directory type, got nil")
	}
}

func TestListFilesTool_Execute_FileAsDirectory(t *testing.T) {
	// Try to list files from a file path (not a directory)
	tmpDir := t.TempDir()
	testFile := filepath.Join(tmpDir, "test.txt")
	_ = os.WriteFile(testFile, []byte("content"), 0644)

	tool := NewListFilesTool(3)

	// This should succeed (Walk treats file as directory with single entry)
	// filepath.Walk doesn't error for files, it just walks them
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": testFile,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	// Should return the file itself
	resultMap := result.(map[string]interface{})
	filesList := resultMap["files"].([]string)

	// The file itself will be in the list
	if len(filesList) != 1 {
		t.Errorf("Expected 1 file (the file itself), got %d", len(filesList))
	}
}

func TestListFilesTool_Execute_DeepNesting(t *testing.T) {
	// Test with deeply nested directories
	tmpDir := t.TempDir()

	// Create deep nesting
	deepPath := tmpDir
	for i := 0; i < 10; i++ {
		deepPath = filepath.Join(deepPath, "level")
		_ = os.MkdirAll(deepPath, 0755)
	}

	// Create file at the deepest level
	deepFile := filepath.Join(deepPath, "deep.txt")
	_ = os.WriteFile(deepFile, []byte("content"), 0644)

	tool := NewListFilesTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": tmpDir,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	filesList := resultMap["files"].([]string)

	if len(filesList) != 1 {
		t.Errorf("Expected 1 file in deep nesting, got %d", len(filesList))
	}

	// File should be found with relative path
	expectedRelPath := filepath.Join("level", "level", "level", "level", "level", "level", "level", "level", "level", "level", "deep.txt")
	if filesList[0] != expectedRelPath {
		t.Errorf("Expected file '%s', got '%s'", expectedRelPath, filesList[0])
	}
}

func TestListFilesTool_Execute_SpecialCharactersInFilenames(t *testing.T) {
	// Test files with special characters
	tmpDir := t.TempDir()

	specialFiles := []string{
		"file with spaces.txt",
		"file-with-dashes.go",
		"file_with_underscores.md",
		"file.multiple.dots.yaml",
	}

	for _, f := range specialFiles {
		fullPath := filepath.Join(tmpDir, f)
		_ = os.WriteFile(fullPath, []byte("content"), 0644)
	}

	tool := NewListFilesTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": tmpDir,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	filesList := resultMap["files"].([]string)

	if len(filesList) != len(specialFiles) {
		t.Errorf("Expected %d files, got %d", len(specialFiles), len(filesList))
	}

	// Verify all files are found
	fileMap := make(map[string]bool)
	for _, f := range filesList {
		fileMap[f] = true
	}

	for _, expectedFile := range specialFiles {
		if !fileMap[expectedFile] {
			t.Errorf("Expected file '%s' not found in results", expectedFile)
		}
	}
}

func TestListFilesTool_Execute_RelativePaths(t *testing.T) {
	// Test that returned paths are relative to the directory
	tmpDir := t.TempDir()

	// Create file
	file := "testfile.txt"
	_ = os.WriteFile(filepath.Join(tmpDir, file), []byte("content"), 0644)

	tool := NewListFilesTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": tmpDir,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	filesList := resultMap["files"].([]string)

	if len(filesList) != 1 {
		t.Fatalf("Expected 1 file, got %d", len(filesList))
	}

	// Path should be relative, not absolute
	if filesList[0] != file {
		t.Errorf("Expected relative path '%s', got '%s'", file, filesList[0])
	}

	// Should NOT start with /
	if filepath.IsAbs(filesList[0]) {
		t.Errorf("Expected relative path, got absolute path '%s'", filesList[0])
	}
}

func TestListFilesTool_Execute_ManyFiles(t *testing.T) {
	// Test with many files
	tmpDir := t.TempDir()

	expectedCount := 100
	for i := 0; i < expectedCount; i++ {
		filename := filepath.Join(tmpDir, "file"+string(rune('0'+i%10))+".txt")
		_ = os.WriteFile(filename, []byte("content"), 0644)
	}

	tool := NewListFilesTool(3)
	result, err := tool.Execute(context.Background(), map[string]interface{}{
		"directory": tmpDir,
	})

	if err != nil {
		t.Fatalf("Expected no error, got %v", err)
	}

	resultMap := result.(map[string]interface{})
	filesList := resultMap["files"].([]string)

	// Should find all files (note: some may have same name due to modulo)
	if len(filesList) < 10 {
		t.Errorf("Expected at least 10 unique files, got %d", len(filesList))
	}

	if count := resultMap["count"].(int); count != len(filesList) {
		t.Errorf("Expected count to match files length, got count=%d, len=%d", count, len(filesList))
	}
}
</file>
<file path="internal/tools/utils.go">
package tools

import (
	"os"
	"path/filepath"
	"strings"
)

// MaxToolResponseSize is the maximum size of a tool response in bytes
const MaxToolResponseSize = 50000 // ~50KB

// DefaultIgnorePatterns are patterns always ignored when listing files
var DefaultIgnorePatterns = []string{
	// Version control
	".git",
	".git/**",
	".svn",
	".hg",

	// Dependencies
	"node_modules",
	"node_modules/**",
	"vendor",
	"vendor/**",
	".venv",
	"venv",
	"__pycache__",
	"__pycache__/**",

	// Build outputs
	"dist",
	"dist/**",
	"build",
	"build/**",
	"out",
	"out/**",
	"target",
	"target/**",
	"bin",
	"*.exe",
	"*.dll",
	"*.so",
	"*.dylib",
	"*.o",
	"*.a",
	"*.obj",

	// IDE/Editor
	".idea",
	".idea/**",
	".vscode",
	".vscode/**",
	"*.swp",
	"*.swo",
	"*~",

	// Logs and temp files
	"*.log",
	"*.tmp",
	"*.temp",
	"*.cache",

	// Media and binary files
	"*.png",
	"*.jpg",
	"*.jpeg",
	"*.gif",
	"*.ico",
	"*.bmp",
	"*.webp",
	"*.svg",
	"*.mp3",
	"*.mp4",
	"*.avi",
	"*.mov",
	"*.wav",
	"*.flac",
	"*.pdf",
	"*.doc",
	"*.docx",
	"*.xls",
	"*.xlsx",
	"*.ppt",
	"*.pptx",
	"*.zip",
	"*.tar",
	"*.gz",
	"*.rar",
	"*.7z",
	"*.woff",
	"*.woff2",
	"*.ttf",
	"*.eot",
	"*.otf",

	// Minified files
	"*.min.js",
	"*.min.css",

	// Lock files (large)
	"package-lock.json",
	"yarn.lock",
	"pnpm-lock.yaml",
	"Cargo.lock",
	"poetry.lock",
	"Gemfile.lock",
	"go.sum",
}

// BinaryExtensions are file extensions that indicate binary files
var BinaryExtensions = map[string]bool{
	// Executables
	".exe": true, ".dll": true, ".so": true, ".dylib": true,
	".bin": true, ".o": true, ".a": true, ".obj": true,
	".com": true, ".msi": true, ".app": true,

	// Images
	".png": true, ".jpg": true, ".jpeg": true, ".gif": true,
	".ico": true, ".bmp": true, ".webp": true, ".tiff": true,
	".psd": true, ".raw": true, ".heic": true,

	// Audio/Video
	".mp3": true, ".mp4": true, ".avi": true, ".mov": true,
	".wav": true, ".flac": true, ".ogg": true, ".mkv": true,
	".wmv": true, ".m4a": true, ".aac": true,

	// Archives
	".zip": true, ".tar": true, ".gz": true, ".rar": true,
	".7z": true, ".bz2": true, ".xz": true, ".iso": true,

	// Documents (binary)
	".pdf": true, ".doc": true, ".docx": true,
	".xls": true, ".xlsx": true, ".ppt": true, ".pptx": true,
	".odt": true, ".ods": true, ".odp": true,

	// Fonts
	".woff": true, ".woff2": true, ".ttf": true, ".eot": true, ".otf": true,

	// Database
	".db": true, ".sqlite": true, ".sqlite3": true,

	// Other binary
	".class": true, ".pyc": true, ".pyo": true,
	".wasm": true, ".deb": true, ".rpm": true,
}

// IsBinaryFile checks if a file is binary based on extension and content
func IsBinaryFile(path string) bool {
	// Check extension first (fast path)
	ext := strings.ToLower(filepath.Ext(path))
	if BinaryExtensions[ext] {
		return true
	}

	// Check if file has no extension but is likely a binary (e.g., compiled Go binary)
	info, err := os.Stat(path)
	if err != nil {
		return false
	}

	// Skip directories
	if info.IsDir() {
		return false
	}

	// Large files without extension are likely binaries
	if ext == "" && info.Size() > 1024*1024 { // > 1MB
		return true
	}

	// Check file content for binary characteristics
	return isBinaryContent(path)
}

// isBinaryContent checks if file content appears to be binary
func isBinaryContent(path string) bool {
	file, err := os.Open(path)
	if err != nil {
		return false
	}
	defer func() { _ = file.Close() }()

	// Read first 512 bytes
	buf := make([]byte, 512)
	n, err := file.Read(buf)
	if err != nil || n == 0 {
		return false
	}

	// Count null bytes and non-printable characters
	nullCount := 0
	nonPrintable := 0

	for i := 0; i < n; i++ {
		b := buf[i]
		if b == 0 {
			nullCount++
		}
		// Non-printable: not tab, newline, carriage return, or printable ASCII
		if b != 9 && b != 10 && b != 13 && (b < 32 || b > 126) && b < 128 {
			nonPrintable++
		}
	}

	// If more than 10% null bytes or 30% non-printable, it's binary
	return nullCount > n/10 || nonPrintable > n*3/10
}

// TruncateString truncates a string to maxLen and adds a truncation notice
func TruncateString(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}

	truncated := s[:maxLen]
	return truncated + "\n\n[TRUNCATED - Response exceeded limit. Showing first " +
		strings.TrimRight(strings.TrimRight(formatBytes(maxLen), "0"), ".") + "]"
}

// formatBytes formats bytes as human readable string
func formatBytes(bytes int) string {
	if bytes < 1024 {
		return string(rune(bytes)) + "B"
	}
	kb := float64(bytes) / 1024
	if kb < 1024 {
		return strings.TrimRight(strings.TrimRight(
			strings.Replace(string(rune(int(kb*10)/10))+"KB", ".", "", 1), "0"), ".") + "KB"
	}
	return "~" + string(rune(int(kb/1024))) + "MB"
}

// LoadGitignorePatterns loads patterns from .gitignore file and combines with defaults
func LoadGitignorePatterns(repoPath string) []string {
	patterns := make([]string, len(DefaultIgnorePatterns))
	copy(patterns, DefaultIgnorePatterns)

	// Try to load .gitignore
	gitignorePath := filepath.Join(repoPath, ".gitignore")
	content, err := os.ReadFile(gitignorePath)
	if err != nil {
		return patterns
	}

	// Parse .gitignore
	lines := strings.Split(string(content), "\n")
	for _, line := range lines {
		line = strings.TrimSpace(line)
		// Skip empty lines and comments
		if line == "" || strings.HasPrefix(line, "#") {
			continue
		}
		patterns = append(patterns, line)
	}

	return patterns
}

// ShouldIgnore checks if a path should be ignored based on patterns
func ShouldIgnore(path string, patterns []string) bool {
	// Normalize path separators
	normalizedPath := filepath.ToSlash(path)
	baseName := filepath.Base(path)

	for _, pattern := range patterns {
		pattern = strings.TrimSpace(pattern)
		if pattern == "" {
			continue
		}

		// Handle negation patterns (not fully supported, just skip)
		if strings.HasPrefix(pattern, "!") {
			continue
		}

		// Remove leading slash for matching
		pattern = strings.TrimPrefix(pattern, "/")

		// Check various match scenarios
		matched := false

		// Direct match with base name
		if matchPattern(baseName, pattern) {
			matched = true
		}

		// Match against full path
		if !matched && matchPattern(normalizedPath, pattern) {
			matched = true
		}

		// Match against path components
		if !matched {
			parts := strings.Split(normalizedPath, "/")
			for _, part := range parts {
				if matchPattern(part, pattern) {
					matched = true
					break
				}
			}
		}

		if matched {
			return true
		}
	}

	return false
}

// matchPattern performs simple glob-style pattern matching
func matchPattern(name, pattern string) bool {
	// Handle ** (matches everything including /)
	if strings.Contains(pattern, "**") {
		// Convert ** to regex-like matching
		parts := strings.Split(pattern, "**")
		if len(parts) == 2 {
			prefix := strings.TrimSuffix(parts[0], "/")
			suffix := strings.TrimPrefix(parts[1], "/")

			if prefix != "" && !strings.HasPrefix(name, prefix) {
				return false
			}
			if suffix != "" && !strings.HasSuffix(name, suffix) {
				return false
			}
			if prefix != "" || suffix != "" {
				return true
			}
		}
	}

	// Handle * (matches everything except /)
	if strings.Contains(pattern, "*") && !strings.Contains(pattern, "**") {
		return simpleGlobMatch(name, pattern)
	}

	// Exact match
	return name == pattern
}

// simpleGlobMatch performs simple * glob matching
func simpleGlobMatch(name, pattern string) bool {
	// Handle patterns like *.go, test_*, etc.
	if strings.HasPrefix(pattern, "*") {
		suffix := strings.TrimPrefix(pattern, "*")
		return strings.HasSuffix(name, suffix)
	}

	if strings.HasSuffix(pattern, "*") {
		prefix := strings.TrimSuffix(pattern, "*")
		return strings.HasPrefix(name, prefix)
	}

	// Handle patterns with * in the middle
	if strings.Contains(pattern, "*") {
		parts := strings.SplitN(pattern, "*", 2)
		return strings.HasPrefix(name, parts[0]) && strings.HasSuffix(name, parts[1])
	}

	return name == pattern
}

// EstimateTokens estimates the number of tokens in a string
// Approximate rule: 1 token â‰ˆ 4 characters for English text
func EstimateTokens(text string) int {
	return len(text) / 4
}
</file>
<file path="internal/tui/config.go">
package tui

import (
	"fmt"
	"os"
	"path/filepath"

	"github.com/charmbracelet/bubbles/textinput"
	tea "github.com/charmbracelet/bubbletea"
	"github.com/charmbracelet/lipgloss"
)

// Styles for the TUI
var (
	titleStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("##FAFAFA")).
			Background(lipgloss.Color("##7D56F4")).
			Padding(0, 1)

	highlightStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#7D56F4")).
			Bold(true)

	errorStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#FF5F87")).
			Bold(true)

	successStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#50FA7B")).
			Bold(true)
)

// Step represents a configuration step
type Step int

const (
	StepProvider Step = iota
	StepAPIKey
	StepModel
	StepBaseURL
	StepConfirm
	StepSave
	StepComplete
)

func (s Step) String() string {
	switch s {
	case StepProvider:
		return "Provider Selection"
	case StepAPIKey:
		return "API Key"
	case StepModel:
		return "Model"
	case StepBaseURL:
		return "Base URL (Optional)"
	case StepConfirm:
		return "Confirm"
	case StepSave:
		return "Save"
	case StepComplete:
		return "Complete"
	default:
		return "Unknown"
	}
}

// Model holds the TUI state
type Model struct {
	Step        Step
	Provider    string
	APIKey      string
	Model       string
	BaseURL     string
	Quitting    bool
	ConfigPath  string
	SavedConfig bool
	Err         error
	// Text inputs for user input (exported fields)
	APIKeyInput  textinput.Model
	ModelInput   textinput.Model
	BaseURLInput textinput.Model
}

// ConfigResult holds the final configuration result
type ConfigResult struct {
	Saved bool
	Path  string
	Error error
}

// Init initializes the model
func (m Model) Init() tea.Cmd {
	return textinput.Blink
}

// Update handles messages
func (m Model) Update(msg tea.Msg) (tea.Model, tea.Cmd) {
	switch msg := msg.(type) {
	case tea.KeyMsg:
		switch msg.String() {
		case "ctrl+c", "q":
			m.Quitting = true
			return m, tea.Quit
		case "enter":
			// Handle Enter key based on current step
			switch m.Step {
			case StepProvider:
				// Only advance if a provider is selected
				if m.Provider != "" {
					m.Step = StepAPIKey
					m.APIKeyInput.Focus()
					m.ModelInput.Blur()
					m.BaseURLInput.Blur()
				}
			case StepAPIKey:
				m.APIKey = m.APIKeyInput.Value()
				if m.APIKey != "" {
					m.Step = StepModel
					m.APIKeyInput.Blur()
					m.ModelInput.Focus()
					m.BaseURLInput.Blur()
				}
			case StepModel:
				inputModel := m.ModelInput.Value()
				if inputModel != "" {
					m.Model = inputModel
				} else if m.Model == "" {
					// Set default model based on provider
					switch m.Provider {
					case "openai":
						m.Model = "gpt-4o"
					case "anthropic":
						m.Model = "claude-3-5-sonnet-20241022"
					case "gemini":
						m.Model = "gemini-1.5-pro"
					}
				}
				m.Step = StepBaseURL
				m.APIKeyInput.Blur()
				m.ModelInput.Blur()
				m.BaseURLInput.Focus()
			case StepBaseURL:
				m.BaseURL = m.BaseURLInput.Value()
				m.Step = StepConfirm
				m.APIKeyInput.Blur()
				m.ModelInput.Blur()
				m.BaseURLInput.Blur()
			case StepConfirm:
				// Enter on confirm step is handled by y/n
			}
		case "1":
			if m.Step == StepProvider {
				m.Provider = "openai"
				m.Model = "gpt-4o"
			}
		case "2":
			if m.Step == StepProvider {
				m.Provider = "anthropic"
				m.Model = "claude-3-5-sonnet-20241022"
			}
		case "3":
			if m.Step == StepProvider {
				m.Provider = "gemini"
				m.Model = "gemini-1.5-pro"
			}
		case "y", "Y":
			if m.Step == StepConfirm {
				configPath, err := m.saveConfig()
				m.ConfigPath = configPath
				m.Err = err
				m.SavedConfig = err == nil
				m.Step = StepComplete
			}
		case "n", "N":
			if m.Step == StepConfirm {
				m.Step = StepProvider
			}
		case "esc":
			switch m.Step {
			case StepModel:
				m.Step = StepAPIKey
				m.APIKeyInput.Focus()
				m.ModelInput.Blur()
				m.BaseURLInput.Blur()
			case StepBaseURL:
				m.Step = StepModel
				m.APIKeyInput.Blur()
				m.ModelInput.Focus()
				m.BaseURLInput.Blur()
			}
		}
	}

	// Update text inputs based on current step
	var cmd tea.Cmd
	switch m.Step {
	case StepAPIKey:
		m.APIKeyInput, cmd = m.APIKeyInput.Update(msg)
	case StepModel:
		m.ModelInput, cmd = m.ModelInput.Update(msg)
	case StepBaseURL:
		m.BaseURLInput, cmd = m.BaseURLInput.Update(msg)
	}

	return m, cmd
}

// View renders the UI
func (m Model) View() string {
	if m.Quitting {
		return "Exiting...\n"
	}

	if m.Step == StepComplete {
		if m.Err != nil {
			return fmt.Sprintf("\n%s\n\nError saving configuration: %v\n\nPress any key to exit...",
				errorStyle.Render("Configuration Failed"), m.Err)
		}
		return fmt.Sprintf("\n%s\n\nConfiguration saved to: %s\n\nPress any key to exit...",
			successStyle.Render("Configuration Saved Successfully!"), m.ConfigPath)
	}

	var s string

	// Title
	s += titleStyle.Render(" Gendocs Configuration Wizard ") + "\n\n"

	// Current step indicator
	stepNum := int(m.Step) + 1
	s += fmt.Sprintf("Step %d/5: %s\n\n", stepNum, m.Step.String())

	// Render current step content
	switch m.Step {
	case StepProvider:
		s += m.renderProviderSelection()
	case StepAPIKey:
		s += m.renderAPIKeyInput()
	case StepModel:
		s += m.renderModelInput()
	case StepBaseURL:
		s += m.renderBaseURLInput()
	case StepConfirm:
		s += m.renderConfirm()
	}

	// Help text
	s += "\n\n"
	switch m.Step {
	case StepProvider:
		s += "1-3: Select provider  |  Enter: Continue  |  q: Quit"
	case StepConfirm:
		s += "y: Yes (save)  |  n: No (go back)  |  q: Quit"
	default:
		s += "Type input  |  Enter: Continue  |  Esc: Go back  |  q: Quit"
	}

	return s + "\n"
}

func (m Model) renderProviderSelection() string {
	s := "Select your LLM provider:\n\n"

	providers := []struct {
		key   string
		name  string
		model string
	}{
		{"1", "OpenAI", "gpt-4o, gpt-4o-mini, etc."},
		{"2", "Anthropic Claude", "claude-3-5-sonnet, claude-3-haiku, etc."},
		{"3", "Google Gemini", "gemini-1.5-pro, gemini-pro, etc."},
	}

	for _, p := range providers {
		prefix := " "
		if m.Provider == getProviderFromKey(p.key) {
			prefix = highlightStyle.Render("âœ“")
		}
		s += fmt.Sprintf("%s %s. %s (%s)\n", prefix, p.key, p.name, p.model)
	}

	return s
}

func (m Model) renderAPIKeyInput() string {
	s := fmt.Sprintf("Enter your API key for %s:\n\n%s\n\n",
		highlightStyle.Render(m.Provider),
		m.APIKeyInput.View())

	s += "\n(Press Enter when done)"
	return s
}

func (m Model) renderModelInput() string {
	defaultModel := m.Model
	if defaultModel == "" {
		switch m.Provider {
		case "openai":
			defaultModel = "gpt-4o"
		case "anthropic":
			defaultModel = "claude-3-5-sonnet-20241022"
		case "gemini":
			defaultModel = "gemini-1.5-pro"
		default:
			defaultModel = "<default>"
		}
	}

	s := fmt.Sprintf("Enter model name (or press Enter for default %s):\n\n%s",
		highlightStyle.Render(defaultModel),
		m.ModelInput.View())

	return s
}

func (m Model) renderBaseURLInput() string {
	return fmt.Sprintf("Enter base URL (optional, press Enter to skip):\n\n%s\n\nLeave empty for provider default.",
		m.BaseURLInput.View())
}

func (m Model) renderConfirm() string {
	s := "Review your configuration:\n\n"
	s += fmt.Sprintf("  Provider:   %s\n", highlightStyle.Render(m.Provider))
	s += fmt.Sprintf("  Model:      %s\n", highlightStyle.Render(m.Model))
	if m.BaseURL != "" {
		s += fmt.Sprintf("  Base URL:   %s\n", highlightStyle.Render(m.BaseURL))
	}
	s += "\nSave this configuration?"
	return s
}

func (m Model) saveConfig() (string, error) {
	homeDir, err := os.UserHomeDir()
	if err != nil {
		return "", err
	}

	configPath := filepath.Join(homeDir, ".gendocs.yaml")

	// Create YAML configuration
	configYAML := fmt.Sprintf("# Gendocs Global Configuration\nanalyzer:\n  llm:\n    provider: %s\n    api_key: %s\n    model: %s\n",
		m.Provider, m.APIKey, m.Model)

	if m.BaseURL != "" {
		configYAML += fmt.Sprintf("    base_url: %s\n", m.BaseURL)
	}

	if err := os.WriteFile(configPath, []byte(configYAML), 0600); err != nil {
		return "", err
	}

	return configPath, nil
}

func getProviderFromKey(key string) string {
	switch key {
	case "1":
		return "openai"
	case "2":
		return "anthropic"
	case "3":
		return "gemini"
	default:
		return ""
	}
}

// GetConfigPath returns the path where config was saved
func (m Model) GetConfigPath() string {
	return m.ConfigPath
}
</file>
<file path="internal/tui/progress.go">
package tui

import (
	"fmt"
	"io"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/charmbracelet/lipgloss"
)

// Progress styles for the TUI
var (
	progressTitleStyle = lipgloss.NewStyle().
				Foreground(lipgloss.Color("#FAFAFA")).
				Background(lipgloss.Color("#7D56F4")).
				Padding(0, 1)

	progressStepStyle = lipgloss.NewStyle().
				Foreground(lipgloss.Color("#7D56F4"))

	progressInfoStyle = lipgloss.NewStyle().
				Foreground(lipgloss.Color("#A0A0A0"))

	progressSuccessStyle = lipgloss.NewStyle().
				Foreground(lipgloss.Color("#50FA7B"))

	progressErrorStyle = lipgloss.NewStyle().
				Foreground(lipgloss.Color("#FF5F87"))

	progressWarningStyle = lipgloss.NewStyle().
				Foreground(lipgloss.Color("#FFB86C"))
)

type ProgressReporter interface {
	AddTask(id, name, description string)
	StartTask(id string)
	CompleteTask(id string)
	FailTask(id string, err error)
	SkipTask(id string)
}

type NopProgressReporter struct{}

func (n *NopProgressReporter) AddTask(id, name, description string) {}
func (n *NopProgressReporter) StartTask(id string)                  {}
func (n *NopProgressReporter) CompleteTask(id string)               {}
func (n *NopProgressReporter) FailTask(id string, err error)        {}
func (n *NopProgressReporter) SkipTask(id string)                   {}

type TaskStatus int

const (
	TaskPending TaskStatus = iota
	TaskRunning
	TaskSuccess
	TaskError
	TaskSkipped
)

type Task struct {
	ID          string
	Name        string
	Description string
	Status      TaskStatus
	Error       error
	StartTime   time.Time
	EndTime     time.Time
}

type Progress struct {
	mu           sync.Mutex
	writer       io.Writer
	title        string
	tasks        []*Task
	taskMap      map[string]*Task
	spinnerFrame int
	ticker       *time.Ticker
	done         chan struct{}
	started      bool
	startTime    time.Time
}

func NewProgress(title string) *Progress {
	return &Progress{
		title:     title,
		tasks:     make([]*Task, 0),
		taskMap:   make(map[string]*Task),
		done:      make(chan struct{}),
		startTime: time.Now(),
	}
}

func (p *Progress) SetWriter(w io.Writer) {
	p.writer = w
}

func (p *Progress) getWriter() io.Writer {
	if p.writer == nil {
		return os.Stdout
	}
	return p.writer
}

func (p *Progress) AddTask(id, name, description string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	task := &Task{
		ID:          id,
		Name:        name,
		Description: description,
		Status:      TaskPending,
	}
	p.tasks = append(p.tasks, task)
	p.taskMap[id] = task
}

func (p *Progress) StartTask(id string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	task, ok := p.taskMap[id]
	if !ok {
		return
	}
	task.Status = TaskRunning
	task.StartTime = time.Now()
	// Note: No direct output here - render() handles all display updates
}

func (p *Progress) CompleteTask(id string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	task, ok := p.taskMap[id]
	if !ok {
		return
	}
	task.Status = TaskSuccess
	task.EndTime = time.Now()
	// Note: No direct output here - render() handles all display updates
}

func (p *Progress) FailTask(id string, err error) {
	p.mu.Lock()
	defer p.mu.Unlock()

	task, ok := p.taskMap[id]
	if !ok {
		return
	}
	task.Status = TaskError
	task.Error = err
	task.EndTime = time.Now()
	// Note: No direct output here - render() handles all display updates
}

func (p *Progress) SkipTask(id string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	task, ok := p.taskMap[id]
	if !ok {
		return
	}
	task.Status = TaskSkipped
	// Note: No direct output here - render() handles all display updates
}

func (p *Progress) Start() {
	p.mu.Lock()
	if p.started {
		p.mu.Unlock()
		return
	}
	p.started = true

	_, _ = fmt.Fprintln(p.getWriter())
	_, _ = fmt.Fprintln(p.getWriter(), progressTitleStyle.Render(" "+p.title+" "))
	_, _ = fmt.Fprintln(p.getWriter())

	// Initial render to create task lines
	for _, task := range p.tasks {
		line := p.formatTaskLine(task)
		_, _ = fmt.Fprintln(p.getWriter(), line)
	}

	p.mu.Unlock()

	p.ticker = time.NewTicker(100 * time.Millisecond)
	go p.animate()
}

func (p *Progress) animate() {
	for {
		select {
		case <-p.done:
			return
		case <-p.ticker.C:
			p.mu.Lock()
			p.spinnerFrame = (p.spinnerFrame + 1) % len(SpinnerFrames)
			p.render()
			p.mu.Unlock()
		}
	}
}

func (p *Progress) render() {
	lineCount := len(p.tasks)
	if lineCount == 0 {
		return
	}

	_, _ = fmt.Fprint(p.getWriter(), strings.Repeat("\033[A\033[2K", lineCount))

	for _, task := range p.tasks {
		line := p.formatTaskLine(task)
		_, _ = fmt.Fprintln(p.getWriter(), line)
	}
}

func (p *Progress) formatTaskLine(task *Task) string {
	var icon, status string
	var style lipgloss.Style

	switch task.Status {
	case TaskPending:
		icon = progressInfoStyle.Render("â—‹")
		status = progressInfoStyle.Render("waiting")
		style = progressInfoStyle
	case TaskRunning:
		spinnerIcon := SpinnerFrames[p.spinnerFrame]
		icon = progressStepStyle.Render(spinnerIcon)
		elapsed := time.Since(task.StartTime).Round(time.Second)
		status = progressStepStyle.Render(fmt.Sprintf("running %s", elapsed))
		style = progressStepStyle
	case TaskSuccess:
		icon = progressSuccessStyle.Render("âœ“")
		duration := task.EndTime.Sub(task.StartTime).Round(time.Millisecond)
		status = progressSuccessStyle.Render(fmt.Sprintf("done %s", duration))
		style = progressSuccessStyle
	case TaskError:
		icon = progressErrorStyle.Render("âœ—")
		status = progressErrorStyle.Render("failed")
		style = progressErrorStyle
	case TaskSkipped:
		icon = progressInfoStyle.Render("â—‹")
		status = progressInfoStyle.Render("skipped")
		style = progressInfoStyle
	}

	name := style.Render(task.Name)

	return fmt.Sprintf("  %s %s %s", icon, name, status)
}

func (p *Progress) Stop() {
	p.mu.Lock()
	defer p.mu.Unlock()

	if !p.started {
		return
	}

	if p.ticker != nil {
		p.ticker.Stop()
	}
	close(p.done)

	p.render()
}

func (p *Progress) PrintSummary() {
	p.mu.Lock()
	defer p.mu.Unlock()

	var successful, failed, skipped int
	for _, t := range p.tasks {
		switch t.Status {
		case TaskSuccess:
			successful++
		case TaskError:
			failed++
		case TaskSkipped:
			skipped++
		}
	}

	_, _ = fmt.Fprintln(p.getWriter())
	_, _ = fmt.Fprintln(p.getWriter(), strings.Repeat("â”€", 50))

	total := len(p.tasks)
	summary := fmt.Sprintf("Completed: %d/%d", successful, total-skipped)
	if failed > 0 {
		summary += fmt.Sprintf(", Failed: %d", failed)
	}
	if skipped > 0 {
		summary += fmt.Sprintf(", Skipped: %d", skipped)
	}

	if failed == 0 {
		_, _ = fmt.Fprintf(p.getWriter(), "%s %s\n",
			progressSuccessStyle.Render("âœ“"),
			progressSuccessStyle.Render(summary))
	} else {
		_, _ = fmt.Fprintf(p.getWriter(), "%s %s\n",
			progressErrorStyle.Render("âœ—"),
			progressWarningStyle.Render(summary))
	}

	if failed > 0 {
		_, _ = fmt.Fprintln(p.getWriter())
		_, _ = fmt.Fprintln(p.getWriter(), progressErrorStyle.Render("Failed tasks:"))
		for _, t := range p.tasks {
			if t.Status == TaskError && t.Error != nil {
				_, _ = fmt.Fprintf(p.getWriter(), "  %s %s: %s\n",
					progressErrorStyle.Render("âœ—"),
					t.Name,
					t.Error.Error())
			}
		}
	}

	_, _ = fmt.Fprintln(p.getWriter())
}

type SimpleProgress struct {
	writer  io.Writer
	title   string
	started bool
}

func NewSimpleProgress(title string) *SimpleProgress {
	return &SimpleProgress{
		title: title,
	}
}

func (sp *SimpleProgress) SetWriter(w io.Writer) {
	sp.writer = w
}

func (sp *SimpleProgress) getWriter() io.Writer {
	if sp.writer == nil {
		return os.Stdout
	}
	return sp.writer
}

func (sp *SimpleProgress) Start() {
	if sp.started {
		return
	}
	sp.started = true
	_, _ = fmt.Fprintln(sp.getWriter())
	_, _ = fmt.Fprintln(sp.getWriter(), progressTitleStyle.Render(" "+sp.title+" "))
	_, _ = fmt.Fprintln(sp.getWriter())
}

func (sp *SimpleProgress) Step(message string) {
	_, _ = fmt.Fprintf(sp.getWriter(), "%s %s\n",
		progressStepStyle.Render("â—"),
		message)
}

func (sp *SimpleProgress) Success(message string) {
	_, _ = fmt.Fprintf(sp.getWriter(), "%s %s\n",
		progressSuccessStyle.Render("âœ“"),
		progressSuccessStyle.Render(message))
}

func (sp *SimpleProgress) Error(message string) {
	_, _ = fmt.Fprintf(sp.getWriter(), "%s %s\n",
		progressErrorStyle.Render("âœ—"),
		progressErrorStyle.Render(message))
}

func (sp *SimpleProgress) Warning(message string) {
	_, _ = fmt.Fprintf(sp.getWriter(), "%s %s\n",
		progressWarningStyle.Render("âš "),
		message)
}

func (sp *SimpleProgress) Info(message string) {
	_, _ = fmt.Fprintf(sp.getWriter(), "  %s\n",
		progressInfoStyle.Render(message))
}

func (sp *SimpleProgress) Done() {
	_, _ = fmt.Fprintln(sp.getWriter())
}

func (sp *SimpleProgress) Failed(err error) {
	_, _ = fmt.Fprintln(sp.getWriter())
	if err != nil {
		_, _ = fmt.Fprintf(sp.getWriter(), "%s %s\n",
			progressErrorStyle.Render("âœ— Failed:"),
			err.Error())
	} else {
		_, _ = fmt.Fprintf(sp.getWriter(), "%s\n",
			progressErrorStyle.Render("âœ— Failed"))
	}
}
</file>
<file path="internal/tui/progress_test.go">
package tui

import (
	"bytes"
	"errors"
	"io"
	"os"
	"strings"
	"testing"

	"github.com/user/gendocs/internal/agents"
)

// captureOutput captures stdout during test execution
func captureOutput(f func()) string {
	old := os.Stdout
	r, w, _ := os.Pipe()
	os.Stdout = w

	f()

	_ = w.Close()
	os.Stdout = old

	var buf bytes.Buffer
	_, _ = io.Copy(&buf, r)
	return buf.String()
}

// captureOutputWithWriter captures output from a progress reporter using SetWriter
func captureOutputWithWriter(f func(*bytes.Buffer)) string {
	var buf bytes.Buffer
	f(&buf)
	return buf.String()
}

// ============================================================================
// SimpleProgress Tests
// ============================================================================

// TestNewSimpleProgress tests creation of a new SimpleProgress instance
func TestNewSimpleProgress(t *testing.T) {
	progress := NewSimpleProgress("Test Title")

	if progress == nil {
		t.Fatal("Expected SimpleProgress instance, got nil")
	}
	if progress.title != "Test Title" {
		t.Errorf("Expected title 'Test Title', got '%s'", progress.title)
	}
	if progress.started {
		t.Error("Expected started to be false initially")
	}
}

// TestNewSimpleProgress_EmptyTitle tests creation with empty title
func TestNewSimpleProgress_EmptyTitle(t *testing.T) {
	progress := NewSimpleProgress("")

	if progress == nil {
		t.Fatal("Expected SimpleProgress instance, got nil")
	}
	if progress.title != "" {
		t.Errorf("Expected empty title, got '%s'", progress.title)
	}
}

// TestSimpleProgress_Start tests that Start sets the started flag
func TestSimpleProgress_Start(t *testing.T) {
	progress := NewSimpleProgress("Test")

	_ = captureOutput(func() {
		progress.Start()
	})

	if !progress.started {
		t.Error("Expected started to be true after Start()")
	}
}

// TestSimpleProgress_StartIdempotent tests that Start is idempotent
func TestSimpleProgress_StartIdempotent(t *testing.T) {
	progress := NewSimpleProgress("Test")

	// Call Start multiple times
	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Start()
		progress.Start()
		progress.Start()
	})

	// Title should only appear once
	count := strings.Count(output, "Test")
	if count != 1 {
		t.Errorf("Expected title to appear once, but appeared %d times", count)
	}
}

// TestSimpleProgress_StartPrintsTitle tests that Start prints the title
func TestSimpleProgress_StartPrintsTitle(t *testing.T) {
	progress := NewSimpleProgress("My Progress Title")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Start()
	})

	if !strings.Contains(output, "My Progress Title") {
		t.Errorf("Expected output to contain title 'My Progress Title', got: %s", output)
	}
}

// TestSimpleProgress_Step tests Step method
func TestSimpleProgress_Step(t *testing.T) {
	progress := NewSimpleProgress("Test")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Step("Doing something")
	})

	if !strings.Contains(output, "Doing something") {
		t.Errorf("Expected output to contain 'Doing something', got: %s", output)
	}
}

// TestSimpleProgress_Info tests Info method
func TestSimpleProgress_Info(t *testing.T) {
	progress := NewSimpleProgress("Test")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Info("Some information")
	})

	if !strings.Contains(output, "Some information") {
		t.Errorf("Expected output to contain 'Some information', got: %s", output)
	}
}

// TestSimpleProgress_Success tests Success method
func TestSimpleProgress_Success(t *testing.T) {
	progress := NewSimpleProgress("Test")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Success("Operation succeeded")
	})

	if !strings.Contains(output, "Operation succeeded") {
		t.Errorf("Expected output to contain 'Operation succeeded', got: %s", output)
	}
}

// TestSimpleProgress_Error tests Error method
func TestSimpleProgress_Error(t *testing.T) {
	progress := NewSimpleProgress("Test")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Error("Something went wrong")
	})

	if !strings.Contains(output, "Something went wrong") {
		t.Errorf("Expected output to contain 'Something went wrong', got: %s", output)
	}
}

// TestSimpleProgress_Warning tests Warning method
func TestSimpleProgress_Warning(t *testing.T) {
	progress := NewSimpleProgress("Test")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Warning("Be careful")
	})

	if !strings.Contains(output, "Be careful") {
		t.Errorf("Expected output to contain 'Be careful', got: %s", output)
	}
}

// TestSimpleProgress_FailedWithError tests Failed with an error
func TestSimpleProgress_FailedWithError(t *testing.T) {
	progress := NewSimpleProgress("Test")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Failed(errors.New("something failed"))
	})

	if !strings.Contains(output, "Failed") {
		t.Errorf("Expected output to contain 'Failed', got: %s", output)
	}
	if !strings.Contains(output, "something failed") {
		t.Errorf("Expected output to contain error message, got: %s", output)
	}
}

// TestSimpleProgress_FailedWithNilError tests Failed with nil error
func TestSimpleProgress_FailedWithNilError(t *testing.T) {
	progress := NewSimpleProgress("Test")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Failed(nil)
	})

	if !strings.Contains(output, "Failed") {
		t.Errorf("Expected output to contain 'Failed', got: %s", output)
	}
}

// TestSimpleProgress_Done tests Done method
func TestSimpleProgress_Done(t *testing.T) {
	progress := NewSimpleProgress("Test")

	// Done should complete without error
	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Start()
		progress.Done()
	})

	// Just verify it doesn't panic and produces some output
	if len(output) == 0 {
		t.Error("Expected some output from Start and Done")
	}
}

// TestSimpleProgress_FullWorkflow tests a complete workflow
func TestSimpleProgress_FullWorkflow(t *testing.T) {
	progress := NewSimpleProgress("Build Project")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Start()
		progress.Step("Compiling source files")
		progress.Info("Found 42 files")
		progress.Success("Compilation complete")
		progress.Warning("Some deprecation warnings")
		progress.Done()
	})

	expectedParts := []string{
		"Build Project",
		"Compiling source files",
		"Found 42 files",
		"Compilation complete",
		"Some deprecation warnings",
	}

	for _, part := range expectedParts {
		if !strings.Contains(output, part) {
			t.Errorf("Expected output to contain '%s', got: %s", part, output)
		}
	}
}

// ============================================================================
// Progress Tests
// ============================================================================

// TestNewProgress tests creation of a new Progress instance
func TestNewProgress(t *testing.T) {
	progress := NewProgress("Test Title")

	if progress == nil {
		t.Fatal("Expected Progress instance, got nil")
	}
	if progress.title != "Test Title" {
		t.Errorf("Expected title 'Test Title', got '%s'", progress.title)
	}
	if progress.started {
		t.Error("Expected started to be false initially")
	}
	if len(progress.tasks) != 0 {
		t.Errorf("Expected empty tasks slice, got %d tasks", len(progress.tasks))
	}
	if len(progress.taskMap) != 0 {
		t.Errorf("Expected empty taskMap, got %d entries", len(progress.taskMap))
	}
}

// TestProgress_Start tests that Start sets the started flag
func TestProgress_Start(t *testing.T) {
	progress := NewProgress("Test")

	_ = captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Start()
	})

	if !progress.started {
		t.Error("Expected started to be true after Start()")
	}
}

// TestProgress_StartIdempotent tests that Start is idempotent
func TestProgress_StartIdempotent(t *testing.T) {
	progress := NewProgress("Test")

	output := captureOutput(func() {
		progress.Start()
		progress.Start()
		progress.Start()
	})

	// Title should only appear once
	count := strings.Count(output, "Test")
	if count != 1 {
		t.Errorf("Expected title to appear once, but appeared %d times", count)
	}
}

// TestProgress_Stop tests that Stop can be called without panic
func TestProgress_Stop(t *testing.T) {
	progress := NewProgress("Test")
	// Stop should not panic
	progress.Stop()
}

// TestProgress_StopIdempotent tests that Stop is idempotent
func TestProgress_StopIdempotent(t *testing.T) {
	progress := NewProgress("Test")
	// Multiple stops should not panic
	progress.Stop()
	progress.Stop()
	progress.Stop()
}

// TestProgress_AddTask tests adding tasks
func TestProgress_AddTask(t *testing.T) {
	progress := NewProgress("Test")

	progress.AddTask("task-1", "First Task", "Description 1")
	progress.AddTask("task-2", "Second Task", "Description 2")

	if len(progress.tasks) != 2 {
		t.Errorf("Expected 2 tasks, got %d", len(progress.tasks))
	}
	if len(progress.taskMap) != 2 {
		t.Errorf("Expected 2 entries in taskMap, got %d", len(progress.taskMap))
	}

	// Check first task
	task1 := progress.taskMap["task-1"]
	if task1 == nil {
		t.Fatal("Expected task-1 to exist in taskMap")
	}
	if task1.ID != "task-1" {
		t.Errorf("Expected ID 'task-1', got '%s'", task1.ID)
	}
	if task1.Name != "First Task" {
		t.Errorf("Expected Name 'First Task', got '%s'", task1.Name)
	}
	if task1.Description != "Description 1" {
		t.Errorf("Expected Description 'Description 1', got '%s'", task1.Description)
	}
	if task1.Status != TaskPending {
		t.Errorf("Expected status TaskPending, got %v", task1.Status)
	}

	// Check second task
	task2 := progress.taskMap["task-2"]
	if task2 == nil {
		t.Fatal("Expected task-2 to exist in taskMap")
	}
	if task2.Name != "Second Task" {
		t.Errorf("Expected Name 'Second Task', got '%s'", task2.Name)
	}
}

// TestProgress_StartTask tests starting a task
func TestProgress_StartTask(t *testing.T) {
	progress := NewProgress("Test")
	progress.AddTask("task-1", "First Task", "Description")

	_ = captureOutput(func() {
		progress.StartTask("task-1")
	})

	task := progress.taskMap["task-1"]
	if task.Status != TaskRunning {
		t.Errorf("Expected status TaskRunning, got %v", task.Status)
	}
}

// TestProgress_StartTask_NonExistent tests starting a non-existent task
func TestProgress_StartTask_NonExistent(t *testing.T) {
	progress := NewProgress("Test")

	// Should not panic
	progress.StartTask("non-existent")

	// Nothing should be in taskMap
	if len(progress.taskMap) != 0 {
		t.Errorf("Expected empty taskMap, got %d entries", len(progress.taskMap))
	}
}

// TestProgress_CompleteTask tests completing a task
func TestProgress_CompleteTask(t *testing.T) {
	progress := NewProgress("Test")
	progress.AddTask("task-1", "First Task", "Description")

	_ = captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.StartTask("task-1")
		progress.CompleteTask("task-1")
	})

	task := progress.taskMap["task-1"]
	if task.Status != TaskSuccess {
		t.Errorf("Expected status TaskSuccess, got %v", task.Status)
	}
}

// TestProgress_CompleteTask_NonExistent tests completing a non-existent task
func TestProgress_CompleteTask_NonExistent(t *testing.T) {
	progress := NewProgress("Test")

	// Should not panic
	progress.CompleteTask("non-existent")
}

// TestProgress_FailTask tests failing a task with error
func TestProgress_FailTask(t *testing.T) {
	progress := NewProgress("Test")
	progress.AddTask("task-1", "First Task", "Description")
	testErr := errors.New("task failed")

	_ = captureOutput(func() {
		progress.StartTask("task-1")
		progress.FailTask("task-1", testErr)
	})

	task := progress.taskMap["task-1"]
	if task.Status != TaskError {
		t.Errorf("Expected status TaskError, got %v", task.Status)
	}
	if task.Error != testErr {
		t.Errorf("Expected error to be set, got %v", task.Error)
	}
}

// TestProgress_FailTask_NilError tests failing a task with nil error
func TestProgress_FailTask_NilError(t *testing.T) {
	progress := NewProgress("Test")
	progress.AddTask("task-1", "First Task", "Description")

	_ = captureOutput(func() {
		progress.StartTask("task-1")
		progress.FailTask("task-1", nil)
	})

	task := progress.taskMap["task-1"]
	if task.Status != TaskError {
		t.Errorf("Expected status TaskError, got %v", task.Status)
	}
	if task.Error != nil {
		t.Errorf("Expected nil error, got %v", task.Error)
	}
}

// TestProgress_FailTask_NonExistent tests failing a non-existent task
func TestProgress_FailTask_NonExistent(t *testing.T) {
	progress := NewProgress("Test")

	// Should not panic
	progress.FailTask("non-existent", errors.New("some error"))
}

// TestProgress_SkipTask tests skipping a task
func TestProgress_SkipTask(t *testing.T) {
	progress := NewProgress("Test")
	progress.AddTask("task-1", "First Task", "Description")

	_ = captureOutput(func() {
		progress.SkipTask("task-1")
	})

	task := progress.taskMap["task-1"]
	if task.Status != TaskSkipped {
		t.Errorf("Expected status TaskSkipped, got %v", task.Status)
	}
}

// TestProgress_SkipTask_NonExistent tests skipping a non-existent task
func TestProgress_SkipTask_NonExistent(t *testing.T) {
	progress := NewProgress("Test")

	// Should not panic
	progress.SkipTask("non-existent")
}

// TestProgress_PrintSummary_NoTasks tests summary with no tasks
func TestProgress_PrintSummary_NoTasks(t *testing.T) {
	progress := NewProgress("Test")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.PrintSummary()
	})

	// With no tasks, should show "Completed: 0/0"
	if !strings.Contains(output, "Completed: 0/0") {
		t.Errorf("Expected 'Completed: 0/0' message, got: %s", output)
	}
}

// TestProgress_PrintSummary_AllCompleted tests summary with all tasks completed
func TestProgress_PrintSummary_AllCompleted(t *testing.T) {
	progress := NewProgress("Test")
	progress.AddTask("task-1", "Task 1", "Desc")
	progress.AddTask("task-2", "Task 2", "Desc")
	progress.AddTask("task-3", "Task 3", "Desc")

	_ = captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.StartTask("task-1")
		progress.CompleteTask("task-1")
		progress.StartTask("task-2")
		progress.CompleteTask("task-2")
		progress.StartTask("task-3")
		progress.CompleteTask("task-3")
	})

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.PrintSummary()
	})

	if !strings.Contains(output, "Completed: 3/3") {
		t.Errorf("Expected 'Completed: 3/3' in summary, got: %s", output)
	}
}

// TestProgress_PrintSummary_SomeFailed tests summary with some failures
func TestProgress_PrintSummary_SomeFailed(t *testing.T) {
	progress := NewProgress("Test")
	progress.AddTask("task-1", "Task 1", "Desc")
	progress.AddTask("task-2", "Task 2", "Desc")

	_ = captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.StartTask("task-1")
		progress.CompleteTask("task-1")
		progress.StartTask("task-2")
		progress.FailTask("task-2", errors.New("oops"))
	})

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.PrintSummary()
	})

	if !strings.Contains(output, "Failed: 1") {
		t.Errorf("Expected 'Failed: 1' in summary, got: %s", output)
	}
	if !strings.Contains(output, "Failed tasks") {
		t.Errorf("Expected 'Failed tasks' section, got: %s", output)
	}
}

// TestProgress_PrintSummary_SomeSkipped tests summary with some skipped
func TestProgress_PrintSummary_SomeSkipped(t *testing.T) {
	progress := NewProgress("Test")
	progress.AddTask("task-1", "Task 1", "Desc")
	progress.AddTask("task-2", "Task 2", "Desc")

	_ = captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.StartTask("task-1")
		progress.CompleteTask("task-1")
		progress.SkipTask("task-2")
	})

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.PrintSummary()
	})

	if !strings.Contains(output, "Skipped: 1") {
		t.Errorf("Expected 'Skipped: 1' in summary, got: %s", output)
	}
}

// TestProgress_PrintSummary_Mixed tests summary with mixed statuses
func TestProgress_PrintSummary_Mixed(t *testing.T) {
	progress := NewProgress("Test")
	progress.AddTask("task-1", "Task 1", "Desc")
	progress.AddTask("task-2", "Task 2", "Desc")
	progress.AddTask("task-3", "Task 3", "Desc")
	progress.AddTask("task-4", "Task 4", "Desc")

	_ = captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.StartTask("task-1")
		progress.CompleteTask("task-1")
		progress.StartTask("task-2")
		progress.FailTask("task-2", errors.New("error 2"))
		progress.SkipTask("task-3")
		progress.StartTask("task-4")
		progress.CompleteTask("task-4")
	})

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.PrintSummary()
	})

	// Should show completed count (excluding skipped from total)
	if !strings.Contains(output, "Completed: 2/3") {
		t.Errorf("Expected 'Completed: 2/3' in summary (2 complete, 1 failed, 1 skipped), got: %s", output)
	}
	if !strings.Contains(output, "Failed: 1") {
		t.Errorf("Expected 'Failed: 1' in summary, got: %s", output)
	}
	if !strings.Contains(output, "Skipped: 1") {
		t.Errorf("Expected 'Skipped: 1' in summary, got: %s", output)
	}
}

// TestProgress_FullWorkflow tests a complete workflow
func TestProgress_FullWorkflow(t *testing.T) {
	progress := NewProgress("Analysis")

	output := captureOutputWithWriter(func(buf *bytes.Buffer) {
		progress.SetWriter(buf)
		progress.Start()

		progress.AddTask("analyze-deps", "Analyze Dependencies", "Check project dependencies")
		progress.AddTask("analyze-code", "Analyze Code", "Static code analysis")
		progress.AddTask("analyze-api", "Analyze API", "API documentation")

		progress.StartTask("analyze-deps")
		progress.CompleteTask("analyze-deps")

		progress.StartTask("analyze-code")
		progress.CompleteTask("analyze-code")

		progress.StartTask("analyze-api")
		progress.CompleteTask("analyze-api")

		progress.PrintSummary()
		progress.Stop()
	})

	expectedParts := []string{
		"Analysis",
		"Analyze Dependencies",
		"Analyze Code",
		"Analyze API",
		"Completed: 3/3",
	}

	for _, part := range expectedParts {
		if !strings.Contains(output, part) {
			t.Errorf("Expected output to contain '%s', got: %s", part, output)
		}
	}
}

// ============================================================================
// TaskStatus Tests
// ============================================================================

// TestTaskStatus_Values tests TaskStatus constant values
func TestTaskStatus_Values(t *testing.T) {
	// Verify the iota values are as expected
	if TaskPending != 0 {
		t.Errorf("Expected TaskPending = 0, got %d", TaskPending)
	}
	if TaskRunning != 1 {
		t.Errorf("Expected TaskRunning = 1, got %d", TaskRunning)
	}
	if TaskSuccess != 2 {
		t.Errorf("Expected TaskSuccess = 2, got %d", TaskSuccess)
	}
	if TaskError != 3 {
		t.Errorf("Expected TaskError = 3, got %d", TaskError)
	}
	if TaskSkipped != 4 {
		t.Errorf("Expected TaskSkipped = 4, got %d", TaskSkipped)
	}
}

// ============================================================================
// Task Tests
// ============================================================================

// TestTask_Fields tests Task struct fields
func TestTask_Fields(t *testing.T) {
	task := Task{
		ID:          "test-id",
		Name:        "Test Name",
		Description: "Test Description",
		Status:      TaskRunning,
		Error:       errors.New("test error"),
	}

	if task.ID != "test-id" {
		t.Errorf("Expected ID 'test-id', got '%s'", task.ID)
	}
	if task.Name != "Test Name" {
		t.Errorf("Expected Name 'Test Name', got '%s'", task.Name)
	}
	if task.Description != "Test Description" {
		t.Errorf("Expected Description 'Test Description', got '%s'", task.Description)
	}
	if task.Status != TaskRunning {
		t.Errorf("Expected Status TaskRunning, got %d", task.Status)
	}
	if task.Error == nil || task.Error.Error() != "test error" {
		t.Errorf("Expected Error 'test error', got %v", task.Error)
	}
}

// ============================================================================
// Interface Compliance Tests
// ============================================================================

// TestProgress_ImplementsProgressReporter verifies that Progress implements agents.ProgressReporter
func TestProgress_ImplementsProgressReporter(t *testing.T) {
	var _ agents.ProgressReporter = (*Progress)(nil)
	// If this compiles, the interface is implemented
}

// TestProgress_AsProgressReporter tests using Progress as a ProgressReporter
func TestProgress_AsProgressReporter(t *testing.T) {
	var reporter agents.ProgressReporter = NewProgress("Test")

	// All interface methods should work
	reporter.AddTask("task-1", "Task 1", "Description")

	_ = captureOutputWithWriter(func(buf *bytes.Buffer) {
		reporter.(*Progress).SetWriter(buf)
		reporter.StartTask("task-1")
		reporter.CompleteTask("task-1")
	})

	// Verify the task was processed
	progress := reporter.(*Progress)
	task := progress.taskMap["task-1"]
	if task.Status != TaskSuccess {
		t.Errorf("Expected TaskSuccess, got %v", task.Status)
	}
}

// ============================================================================
// Edge Case Tests
// ============================================================================

// TestProgress_DuplicateTaskIds tests behavior with duplicate task IDs
func TestProgress_DuplicateTaskIds(t *testing.T) {
	progress := NewProgress("Test")

	progress.AddTask("task-1", "First Task", "First description")
	progress.AddTask("task-1", "Duplicate Task", "Duplicate description")

	// Both should be in the tasks slice
	if len(progress.tasks) != 2 {
		t.Errorf("Expected 2 tasks in slice, got %d", len(progress.tasks))
	}

	// Only the second should be in the map (overwrites first)
	task := progress.taskMap["task-1"]
	if task.Name != "Duplicate Task" {
		t.Errorf("Expected map to contain second task 'Duplicate Task', got '%s'", task.Name)
	}
}

// TestSimpleProgress_MultipleMessages tests multiple consecutive messages
func TestSimpleProgress_MultipleMessages(t *testing.T) {
	progress := NewSimpleProgress("Test")

	output := captureOutput(func() {
		progress.Step("Step 1")
		progress.Step("Step 2")
		progress.Info("Info 1")
		progress.Info("Info 2")
		progress.Success("Success 1")
		progress.Error("Error 1")
		progress.Warning("Warning 1")
	})

	expectedMessages := []string{
		"Step 1", "Step 2",
		"Info 1", "Info 2",
		"Success 1",
		"Error 1",
		"Warning 1",
	}

	for _, msg := range expectedMessages {
		if !strings.Contains(output, msg) {
			t.Errorf("Expected output to contain '%s'", msg)
		}
	}
}

// TestProgress_TaskOrder tests that tasks maintain insertion order
func TestProgress_TaskOrder(t *testing.T) {
	progress := NewProgress("Test")

	progress.AddTask("task-a", "Task A", "")
	progress.AddTask("task-b", "Task B", "")
	progress.AddTask("task-c", "Task C", "")

	if len(progress.tasks) != 3 {
		t.Fatalf("Expected 3 tasks, got %d", len(progress.tasks))
	}

	if progress.tasks[0].ID != "task-a" {
		t.Errorf("Expected first task to be 'task-a', got '%s'", progress.tasks[0].ID)
	}
	if progress.tasks[1].ID != "task-b" {
		t.Errorf("Expected second task to be 'task-b', got '%s'", progress.tasks[1].ID)
	}
	if progress.tasks[2].ID != "task-c" {
		t.Errorf("Expected third task to be 'task-c', got '%s'", progress.tasks[2].ID)
	}
}
</file>
<file path="internal/tui/styles.go">
package tui

import "github.com/charmbracelet/lipgloss"

// Color palette for consistent styling
var (
	// Primary colors
	ColorPrimary   = lipgloss.Color("#7D56F4")
	ColorSecondary = lipgloss.Color("#6B4FD8")

	// Status colors
	ColorSuccess = lipgloss.Color("#50FA7B")
	ColorError   = lipgloss.Color("#FF5F87")
	ColorWarning = lipgloss.Color("#FFB86C")
	ColorInfo    = lipgloss.Color("#8BE9FD")

	// Neutral colors
	ColorMuted   = lipgloss.Color("#6C7086")
	ColorSubtle  = lipgloss.Color("#45475A")
	ColorText    = lipgloss.Color("#CDD6F4")
	ColorTextDim = lipgloss.Color("#A6ADC8")
	ColorBg      = lipgloss.Color("#1E1E2E")
	ColorBgDim   = lipgloss.Color("#181825")

	// Spinner colors
	ColorSpinner = lipgloss.Color("#89B4FA")
)

// Reusable styles
var (
	// Title style for headers
	StyleTitle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#FAFAFA")).
			Background(ColorPrimary).
			Padding(0, 1).
			Bold(true)

	// Subtitle style
	StyleSubtitle = lipgloss.NewStyle().
			Foreground(ColorTextDim).
			Italic(true)

	// Success message style
	StyleSuccess = lipgloss.NewStyle().
			Foreground(ColorSuccess).
			Bold(true)

	// Error message style
	StyleError = lipgloss.NewStyle().
			Foreground(ColorError).
			Bold(true)

	// Warning message style
	StyleWarning = lipgloss.NewStyle().
			Foreground(ColorWarning)

	// Info message style
	StyleInfo = lipgloss.NewStyle().
			Foreground(ColorInfo)

	// Muted/dimmed text
	StyleMuted = lipgloss.NewStyle().
			Foreground(ColorMuted)

	// Highlighted text
	StyleHighlight = lipgloss.NewStyle().
			Foreground(ColorPrimary).
			Bold(true)

	// Task name style
	StyleTaskName = lipgloss.NewStyle().
			Foreground(ColorText).
			Width(30)

	// Status indicator styles
	StyleStatusPending = lipgloss.NewStyle().
				Foreground(ColorMuted)

	StyleStatusRunning = lipgloss.NewStyle().
				Foreground(ColorSpinner)

	StyleStatusSuccess = lipgloss.NewStyle().
				Foreground(ColorSuccess)

	StyleStatusError = lipgloss.NewStyle().
				Foreground(ColorError)

	// Box styles for sections
	StyleBox = lipgloss.NewStyle().
			Border(lipgloss.RoundedBorder()).
			BorderForeground(ColorSubtle).
			Padding(0, 1)

	// Progress bar styles
	StyleProgressFilled = lipgloss.NewStyle().
				Foreground(ColorPrimary).
				Background(ColorPrimary)

	StyleProgressEmpty = lipgloss.NewStyle().
				Foreground(ColorSubtle).
				Background(ColorSubtle)
)

// Icons for different states
const (
	IconPending  = "â—‹"
	IconRunning  = "â—"
	IconSuccess  = "âœ“"
	IconError    = "âœ—"
	IconWarning  = "!"
	IconInfo     = "Â·"
	IconArrow    = "â†’"
	IconBullet   = "â€¢"
	IconSpinner1 = "â ‹"
	IconSpinner2 = "â ™"
	IconSpinner3 = "â ¹"
	IconSpinner4 = "â ¸"
	IconSpinner5 = "â ¼"
	IconSpinner6 = "â ´"
	IconSpinner7 = "â ¦"
	IconSpinner8 = "â §"
	IconSpinner9 = "â ‡"
	IconSpinner0 = "â "
)

// SpinnerFrames for animation
var SpinnerFrames = []string{
	IconSpinner1, IconSpinner2, IconSpinner3, IconSpinner4, IconSpinner5,
	IconSpinner6, IconSpinner7, IconSpinner8, IconSpinner9, IconSpinner0,
}
</file>
<file path="internal/validation/markdown.go">
package validation

import (
	"fmt"
	"strings"
)

// MarkdownValidator validates Markdown content for common issues
type MarkdownValidator struct{}

// NewMarkdownValidator creates a new Markdown validator
func NewMarkdownValidator() *MarkdownValidator {
	return &MarkdownValidator{}
}

// ValidationError represents a Markdown validation error
type ValidationError struct {
	Line    int
	Message string
}

func (e *ValidationError) Error() string {
	if e.Line > 0 {
		return fmt.Sprintf("line %d: %s", e.Line, e.Message)
	}
	return e.Message
}

// ValidationResult contains all validation errors
type ValidationResult struct {
	Errors []ValidationError
}

// IsValid returns true if there are no validation errors
func (vr *ValidationResult) IsValid() bool {
	return len(vr.Errors) == 0
}

// Error returns a combined error message
func (vr *ValidationResult) Error() string {
	if vr.IsValid() {
		return ""
	}

	var messages []string
	for _, err := range vr.Errors {
		messages = append(messages, err.Error())
	}
	return fmt.Sprintf("markdown validation failed:\n  - %s", strings.Join(messages, "\n  - "))
}

// Validate checks Markdown for common issues
func (v *MarkdownValidator) Validate(content string) error {
	result := &ValidationResult{
		Errors: []ValidationError{},
	}

	// Check for unclosed code blocks
	if err := v.checkCodeBlocks(content); err != nil {
		result.Errors = append(result.Errors, ValidationError{Message: err.Error()})
	}

	// Check for malformed headers
	if errs := v.checkHeaders(content); len(errs) > 0 {
		result.Errors = append(result.Errors, errs...)
	}

	// Check for minimum structure
	if err := v.checkMinimumStructure(content); err != nil {
		result.Errors = append(result.Errors, ValidationError{Message: err.Error()})
	}

	// Check for unclosed brackets/parentheses in links
	if errs := v.checkLinks(content); len(errs) > 0 {
		result.Errors = append(result.Errors, errs...)
	}

	if !result.IsValid() {
		return result
	}

	return nil
}

// checkCodeBlocks validates code block markers
func (v *MarkdownValidator) checkCodeBlocks(content string) error {
	// Count triple backticks
	openCount := strings.Count(content, "```")
	if openCount%2 != 0 {
		return fmt.Errorf("unclosed code block detected (%d ``` markers, expected even number)", openCount)
	}

	// Note: mixing ``` and ~~~ is valid but could be confusing - just informational

	return nil
}

// checkHeaders validates header formatting
func (v *MarkdownValidator) checkHeaders(content string) []ValidationError {
	var errors []ValidationError
	lines := strings.Split(content, "\n")

	for i, line := range lines {
		trimmed := strings.TrimSpace(line)
		if !strings.HasPrefix(trimmed, "#") {
			continue
		}

		// Extract hash marks
		hashCount := 0
		for _, ch := range trimmed {
			if ch == '#' {
				hashCount++
			} else {
				break
			}
		}

		// Valid headers: # to ######
		if hashCount > 6 {
			errors = append(errors, ValidationError{
				Line:    i + 1,
				Message: fmt.Sprintf("too many # symbols (%d, max 6)", hashCount),
			})
			continue
		}

		// Check for space after hash marks
		if len(trimmed) > hashCount {
			nextChar := trimmed[hashCount]
			if nextChar != ' ' && nextChar != '#' {
				errors = append(errors, ValidationError{
					Line:    i + 1,
					Message: "missing space after # in header",
				})
			}
		}

		// Check for empty header
		headerText := strings.TrimSpace(trimmed[hashCount:])
		if headerText == "" {
			errors = append(errors, ValidationError{
				Line:    i + 1,
				Message: "empty header (no text after #)",
			})
		}
	}

	return errors
}

// checkMinimumStructure ensures basic Markdown requirements
func (v *MarkdownValidator) checkMinimumStructure(content string) error {
	trimmed := strings.TrimSpace(content)

	// Check length
	if len(trimmed) < 50 {
		return fmt.Errorf("content too short (%d characters, minimum 50)", len(trimmed))
	}

	// Check for at least one header
	if !strings.Contains(content, "#") {
		return fmt.Errorf("no headers found (expected at least one # header)")
	}

	return nil
}

// checkLinks validates link formatting
func (v *MarkdownValidator) checkLinks(content string) []ValidationError {
	var errors []ValidationError
	lines := strings.Split(content, "\n")

	for i, line := range lines {
		// Check for unmatched brackets in links
		openBracket := strings.Count(line, "[")
		closeBracket := strings.Count(line, "]")
		openParen := strings.Count(line, "(")
		closeParen := strings.Count(line, ")")

		// Check for any unmatched brackets
		if openBracket != closeBracket {
			errors = append(errors, ValidationError{
				Line:    i + 1,
				Message: fmt.Sprintf("unmatched brackets ([ count: %d, ] count: %d)", openBracket, closeBracket),
			})
		}

		// Check for markdown link pattern [text](url)
		if strings.Contains(line, "](") {
			// This looks like a link, check parens
			if openParen != closeParen {
				errors = append(errors, ValidationError{
					Line:    i + 1,
					Message: fmt.Sprintf("unmatched parentheses in link (( count: %d, ) count: %d)", openParen, closeParen),
				})
			}
		}
	}

	return errors
}

// ValidateAndFix attempts to fix common issues
func (v *MarkdownValidator) ValidateAndFix(content string) (string, error) {
	// Apply common fixes first
	fixed := content

	// Fix: Add newline at end if missing
	if !strings.HasSuffix(fixed, "\n") {
		fixed += "\n"
	}

	// Fix: Remove trailing whitespace
	lines := strings.Split(fixed, "\n")
	for i, line := range lines {
		lines[i] = strings.TrimRight(line, " \t")
	}
	fixed = strings.Join(lines, "\n")

	// Validate after fixes
	if err := v.Validate(fixed); err != nil {
		return fixed, fmt.Errorf("could not auto-fix: %w", err)
	}

	return fixed, nil
}

// QuickCheck performs a fast validation with minimal checks
func (v *MarkdownValidator) QuickCheck(content string) bool {
	// Quick checks only
	if len(strings.TrimSpace(content)) < 50 {
		return false
	}

	if !strings.Contains(content, "#") {
		return false
	}

	// Check code blocks
	if strings.Count(content, "```")%2 != 0 {
		return false
	}

	return true
}
</file>
<file path="internal/validation/markdown_test.go">
package validation

import (
	"strings"
	"testing"
)

func TestMarkdownValidator_Validate_Valid(t *testing.T) {
	validator := NewMarkdownValidator()

	validMarkdown := `# Test Document

This is a valid Markdown document with proper structure.

## Section 1

Content here with **bold** and *italic* text.

` + "```go\ncode here\n```" + `

## Section 2

More content.
`

	err := validator.Validate(validMarkdown)
	if err != nil {
		t.Errorf("Expected no error for valid markdown, got: %v", err)
	}
}

func TestMarkdownValidator_Validate_UnclosedCodeBlock(t *testing.T) {
	validator := NewMarkdownValidator()

	invalidMarkdown := `# Test

` + "```go\ncode here\n" + `

No closing backticks.
`

	err := validator.Validate(invalidMarkdown)
	if err == nil {
		t.Fatal("Expected error for unclosed code block, got nil")
	}

	if !strings.Contains(err.Error(), "unclosed code block") {
		t.Errorf("Expected 'unclosed code block' in error, got: %v", err)
	}
}

func TestMarkdownValidator_Validate_MalformedHeader(t *testing.T) {
	validator := NewMarkdownValidator()

	tests := []struct {
		name     string
		markdown string
		wantErr  bool
	}{
		{
			name:     "no space after hash",
			markdown: `#Header Without Space`,
			wantErr:  true,
		},
		{
			name:     "empty header",
			markdown: `#`,
			wantErr:  true,
		},
		{
			name:     "too many hashes",
			markdown: `####### Too Many`,
			wantErr:  true,
		},
		{
			name:     "valid header",
			markdown: `# Valid Header`,
			wantErr:  false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Add minimum content
			content := tt.markdown + "\n\nSome content to meet minimum length requirement here."
			err := validator.Validate(content)

			if tt.wantErr && err == nil {
				t.Error("Expected error, got nil")
			}

			if !tt.wantErr && err != nil {
				t.Errorf("Expected no error, got: %v", err)
			}
		})
	}
}

func TestMarkdownValidator_Validate_TooShort(t *testing.T) {
	validator := NewMarkdownValidator()

	shortMarkdown := `# Short`

	err := validator.Validate(shortMarkdown)
	if err == nil {
		t.Fatal("Expected error for too short content, got nil")
	}

	if !strings.Contains(err.Error(), "too short") {
		t.Errorf("Expected 'too short' in error, got: %v", err)
	}
}

func TestMarkdownValidator_Validate_NoHeaders(t *testing.T) {
	validator := NewMarkdownValidator()

	noHeadersMarkdown := `This is content without any headers at all.
It has multiple lines and enough content length.
But still no headers which makes it invalid.`

	err := validator.Validate(noHeadersMarkdown)
	if err == nil {
		t.Fatal("Expected error for no headers, got nil")
	}

	if !strings.Contains(err.Error(), "no headers") {
		t.Errorf("Expected 'no headers' in error, got: %v", err)
	}
}

func TestMarkdownValidator_Validate_UnmatchedBrackets(t *testing.T) {
	validator := NewMarkdownValidator()

	invalidMarkdown := `# Test

This is a [link with unmatched bracket.

More content here to meet minimum length.
`

	err := validator.Validate(invalidMarkdown)
	if err == nil {
		t.Fatal("Expected error for unmatched brackets, got nil")
	}

	if !strings.Contains(err.Error(), "bracket") {
		t.Errorf("Expected 'bracket' in error, got: %v", err)
	}
}

func TestMarkdownValidator_Validate_UnmatchedParentheses(t *testing.T) {
	validator := NewMarkdownValidator()

	invalidMarkdown := `# Test

This is a [link](https://example.com without closing paren.

More content here to meet minimum length requirement.
`

	err := validator.Validate(invalidMarkdown)
	if err == nil {
		t.Fatal("Expected error for unmatched parentheses, got nil")
	}

	if !strings.Contains(err.Error(), "parenthes") {
		t.Errorf("Expected 'parenthes' in error, got: %v", err)
	}
}

func TestMarkdownValidator_Validate_MultipleErrors(t *testing.T) {
	validator := NewMarkdownValidator()

	invalidMarkdown := `#NoSpace
` + "```go\nunclosed code block\n" + `
[unmatched bracket
`

	err := validator.Validate(invalidMarkdown)
	if err == nil {
		t.Fatal("Expected error for multiple issues, got nil")
	}

	// Should report multiple errors
	result, ok := err.(*ValidationResult)
	if !ok {
		t.Fatalf("Expected ValidationResult, got %T", err)
	}

	if len(result.Errors) < 2 {
		t.Errorf("Expected at least 2 errors, got %d", len(result.Errors))
	}
}

func TestMarkdownValidator_ValidateAndFix_AddNewline(t *testing.T) {
	validator := NewMarkdownValidator()

	markdown := `# Test Document

This is valid content but missing final newline.`

	fixed, err := validator.ValidateAndFix(markdown)
	if err != nil {
		t.Fatalf("Expected no error, got: %v", err)
	}

	if !strings.HasSuffix(fixed, "\n") {
		t.Error("Expected fixed markdown to end with newline")
	}
}

func TestMarkdownValidator_ValidateAndFix_TrimTrailingSpaces(t *testing.T) {
	validator := NewMarkdownValidator()

	markdown := `# Test Document

This is valid content with trailing spaces on line above.
And this line too.
`

	fixed, err := validator.ValidateAndFix(markdown)
	if err != nil {
		t.Fatalf("Expected no error, got: %v", err)
	}

	lines := strings.Split(fixed, "\n")
	for i, line := range lines {
		if strings.HasSuffix(line, " ") || strings.HasSuffix(line, "\t") {
			t.Errorf("Line %d still has trailing whitespace: %q", i+1, line)
		}
	}
}

func TestMarkdownValidator_ValidateAndFix_CannotFix(t *testing.T) {
	validator := NewMarkdownValidator()

	// Unclosed code block cannot be auto-fixed
	unfixableMarkdown := `# Test
` + "```go\ncode here\n" + `
More content.
`

	_, err := validator.ValidateAndFix(unfixableMarkdown)
	if err == nil {
		t.Fatal("Expected error for unfixable markdown, got nil")
	}

	if !strings.Contains(err.Error(), "could not auto-fix") {
		t.Errorf("Expected 'could not auto-fix' in error, got: %v", err)
	}
}

func TestMarkdownValidator_QuickCheck_Valid(t *testing.T) {
	validator := NewMarkdownValidator()

	validMarkdown := `# Test

Valid content with enough length and proper structure.
`

	if !validator.QuickCheck(validMarkdown) {
		t.Error("Expected QuickCheck to return true for valid markdown")
	}
}

func TestMarkdownValidator_QuickCheck_TooShort(t *testing.T) {
	validator := NewMarkdownValidator()

	shortMarkdown := `# Test`

	if validator.QuickCheck(shortMarkdown) {
		t.Error("Expected QuickCheck to return false for too short markdown")
	}
}

func TestMarkdownValidator_QuickCheck_NoHeaders(t *testing.T) {
	validator := NewMarkdownValidator()

	noHeaders := `This is content without headers but with enough length to pass length check.`

	if validator.QuickCheck(noHeaders) {
		t.Error("Expected QuickCheck to return false for markdown without headers")
	}
}

func TestMarkdownValidator_QuickCheck_UnclosedCodeBlock(t *testing.T) {
	validator := NewMarkdownValidator()

	unclosed := `# Test
` + "```go\ncode\n" + `
More content here.
`

	if validator.QuickCheck(unclosed) {
		t.Error("Expected QuickCheck to return false for unclosed code block")
	}
}

func TestValidationResult_IsValid(t *testing.T) {
	result := &ValidationResult{Errors: []ValidationError{}}
	if !result.IsValid() {
		t.Error("Expected IsValid to return true for empty errors")
	}

	result.Errors = append(result.Errors, ValidationError{Message: "error"})
	if result.IsValid() {
		t.Error("Expected IsValid to return false when errors exist")
	}
}

func TestValidationResult_Error(t *testing.T) {
	result := &ValidationResult{
		Errors: []ValidationError{
			{Line: 5, Message: "error on line 5"},
			{Message: "general error"},
		},
	}

	errMsg := result.Error()
	if !strings.Contains(errMsg, "line 5") {
		t.Error("Expected error message to contain 'line 5'")
	}

	if !strings.Contains(errMsg, "general error") {
		t.Error("Expected error message to contain 'general error'")
	}
}

func TestValidationError_Error(t *testing.T) {
	tests := []struct {
		name     string
		err      ValidationError
		expected string
	}{
		{
			name:     "with line number",
			err:      ValidationError{Line: 10, Message: "test error"},
			expected: "line 10: test error",
		},
		{
			name:     "without line number",
			err:      ValidationError{Line: 0, Message: "general error"},
			expected: "general error",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if tt.err.Error() != tt.expected {
				t.Errorf("Expected '%s', got '%s'", tt.expected, tt.err.Error())
			}
		})
	}
}

func TestMarkdownValidator_Validate_ComplexDocument(t *testing.T) {
	validator := NewMarkdownValidator()

	complexMarkdown := `# Main Title

This is the introduction paragraph.

## Section 1

Content with **bold**, *italic*, and ` + "`code`" + `.

### Subsection 1.1

` + "```go\nfunc main() {\n    fmt.Println(\"Hello\")\n}\n```" + `

## Section 2

A list:
- Item 1
- Item 2
- Item 3

A link: [Example](https://example.com)

### Subsection 2.1

More content with proper formatting.

## Section 3

Final section with a table:

| Column 1 | Column 2 |
|----------|----------|
| Data 1   | Data 2   |

And that's the end.
`

	err := validator.Validate(complexMarkdown)
	if err != nil {
		t.Errorf("Expected no error for complex valid markdown, got: %v", err)
	}
}

func TestMarkdownValidator_Validate_EmptyContent(t *testing.T) {
	validator := NewMarkdownValidator()

	err := validator.Validate("")
	if err == nil {
		t.Fatal("Expected error for empty content, got nil")
	}

	if !strings.Contains(err.Error(), "too short") {
		t.Errorf("Expected 'too short' in error, got: %v", err)
	}
}

func TestMarkdownValidator_Validate_WhitespaceOnly(t *testing.T) {
	validator := NewMarkdownValidator()

	whitespace := "   \n\n\t\n   "

	err := validator.Validate(whitespace)
	if err == nil {
		t.Fatal("Expected error for whitespace-only content, got nil")
	}
}
</file>
<file path="internal/worker_pool/pool.go">
package worker_pool

import (
	"context"
	"runtime"
	"sync"
)

// Task represents a unit of work to execute
type Task func(ctx context.Context) (interface{}, error)

// Result represents the result of a task execution
type Result struct {
	Value interface{}
	Error error
}

// WorkerPool executes tasks concurrently with semaphore-based limiting
type WorkerPool struct {
	maxWorkers int
	semaphore  chan struct{}
}

// DefaultMaxWorkers is the default number of workers for LLM API calls
// Using a conservative value to avoid rate limiting and connection issues
const DefaultMaxWorkers = 2

// NewWorkerPool creates a new worker pool
func NewWorkerPool(maxWorkers int) *WorkerPool {
	if maxWorkers <= 0 {
		// Use conservative default for LLM API calls
		// Higher concurrency can cause rate limiting and HTTP/2 stream errors
		maxWorkers = DefaultMaxWorkers
	}

	// Cap at reasonable maximum to prevent overwhelming the API
	maxCPU := runtime.NumCPU()
	if maxWorkers > maxCPU {
		maxWorkers = maxCPU
	}

	return &WorkerPool{
		maxWorkers: maxWorkers,
		semaphore:  make(chan struct{}, maxWorkers),
	}
}

// Run executes all tasks concurrently and returns results in order
func (wp *WorkerPool) Run(ctx context.Context, tasks []Task) []Result {
	if len(tasks) == 0 {
		return []Result{}
	}

	numTasks := len(tasks)
	results := make([]Result, numTasks)
	var wg sync.WaitGroup

	for i, task := range tasks {
		wg.Add(1)
		go func(index int, t Task) {
			defer wg.Done()

			// Acquire semaphore (blocks if max workers already running)
			select {
			case wp.semaphore <- struct{}{}:
				defer func() { <-wp.semaphore }()
			case <-ctx.Done():
				results[index] = Result{Error: ctx.Err()}
				return
			}

			// Execute the task
			value, err := t(ctx)
			results[index] = Result{Value: value, Error: err}
		}(i, task)
	}

	wg.Wait()
	return results
}

// GetMaxWorkers returns the maximum number of workers
func (wp *WorkerPool) GetMaxWorkers() int {
	return wp.maxWorkers
}
</file>
<file path="prompts/ai_rules_generator.yaml">
# Prompts for AI Rules Generator Agent

ai_rules_claude_system: |
  You are an AI assistant documentation specialist who creates CLAUDE.md files.
  These files provide project-specific instructions for AI coding assistants.

  Your goal is to create a CLAUDE.md that helps AI assistants:
  - Understand the project's purpose and architecture
  - Follow the project's coding conventions
  - Know about key files and their purposes
  - Understand common commands and workflows
  - Be aware of project-specific patterns and practices

ai_rules_claude_user: |
  TASK: Generate CLAUDE.md

  Create a comprehensive CLAUDE.md file for the project at {{ .RepoPath }}.

  Use the existing analysis documents in {{ .RepoPath }}/.ai/docs/ to inform the documentation.

  The CLAUDE.md should include:

  1. **Project Overview**
  2. **Common Commands** - How to run, test, build
  3. **Architecture** - Key architectural patterns
  4. **Code Conventions** - Style guidelines, naming patterns
  5. **Key Files** - Important files and what they do
  6. **Testing** - How tests are organized and run
  7. **Troubleshooting** - Common issues and solutions

  Make the content practical and actionable for an AI assistant.
  Include actual command examples and real file paths from the project.

ai_rules_agents_system: |
  You are an AI agent documentation specialist who creates AGENTS.md files.
  These files document the agent architecture and conventions for multi-agent systems.

ai_rules_agents_user: |
  TASK: Generate AGENTS.md

  Create an AGENTS.md file for the project at {{ .RepoPath }}.

  If this project uses agents (like this documentation generator does), document:
  - Agent architecture and patterns
  - Handler-agent separation
  - How agents communicate
  - Tool system conventions
  - Configuration and setup

  Keep it concise but informative for developers working with the agent system.

ai_rules_cursor_system: |
  You are an IDE documentation specialist who creates Cursor AI rules.
  These rules help Cursor IDE provide better context-aware assistance.

ai_rules_cursor_user: |
  TASK: Generate Cursor AI Rules

  Create Cursor IDE rules (.cursor/rules/*.mdc files) for the project at {{ .RepoPath }}.

  Generate the following rule files:
  1. **project-overview.mdc** - High-level project overview
  2. **architecture.mdc** - Architecture patterns and design
  3. **code-patterns.mdc** - Coding conventions and patterns
  4. **agent-and-tool-conventions.mdc** - If applicable, agent/tool patterns

  Each rule should be concise, focused, and help Cursor understand the codebase better.

# Aliases for sub-agent compatibility (expects ai_rules_system and ai_rules_user)
ai_rules_system: |
  You are an AI assistant documentation specialist who creates CLAUDE.md files.
  These files provide project-specific instructions for AI coding assistants.

  Your goal is to create a CLAUDE.md that helps AI assistants:
  - Understand the project's purpose and architecture
  - Follow the project's coding conventions
  - Know about key files and their purposes
  - Understand common commands and workflows
  - Be aware of project-specific patterns and practices

  IMPORTANT OUTPUT RULES:
  - Output ONLY the final CLAUDE.md content in Markdown format
  - Do not include any preamble, explanations, or chain-of-thought
  - Do not describe your process or explain what you're doing
  - Do not wrap the output in markdown code fences
  - Your response must start directly with the heading

ai_rules_user: |
  TASK: Generate CLAUDE.md

  Create a comprehensive CLAUDE.md file for the project at {{ .RepoPath }}.

  I have analyzed the codebase and generated the following analysis documents. Use this information to create the CLAUDE.md:

  ## Structure Analysis
  {{ index .AnalysisContent "structure_analysis.md" }}

  ## Dependency Analysis
  {{ index .AnalysisContent "dependency_analysis.md" }}

  ## Data Flow Analysis
  {{ index .AnalysisContent "data_flow_analysis.md" }}

  ## Request Flow Analysis
  {{ index .AnalysisContent "request_flow_analysis.md" }}

  ## API Analysis
  {{ index .AnalysisContent "api_analysis.md" }}

  ---

  Based on the above analysis, create a CLAUDE.md that includes:

  1. **Project Overview** - What is this project and what does it do?
  2. **Common Commands** - How to run, test, build
  3. **Architecture** - Key architectural patterns
  4. **Code Conventions** - Style guidelines, naming patterns
  5. **Key Files** - Important files and what they do
  6. **Testing** - How tests are organized and run
  7. **Troubleshooting** - Common issues and solutions

  Make the content practical and actionable for an AI assistant.
  Include actual command examples and real file paths from the project.

  IMPORTANT:
  - Write factual information based ONLY on the analysis documents above
  - Don't invent features, commands, or conventions that don't exist
  - Keep descriptions concise but informative
  - Focus on what AI assistants need to know to work effectively with this codebase

  Output the complete CLAUDE.md content in Markdown format.

# Additional aliases for handler compatibility
ai_rules_system_prompt: |
  You are an AI assistant documentation specialist who creates CLAUDE.md files.
  These files provide project-specific instructions for AI coding assistants.

  Your goal is to create a CLAUDE.md that helps AI assistants:
  - Understand the project's purpose and architecture
  - Follow the project's coding conventions
  - Know about key files and their purposes
  - Understand common commands and workflows
  - Be aware of project-specific patterns and practices

  IMPORTANT OUTPUT RULES:
  - Output ONLY the final CLAUDE.md content in Markdown format
  - Do not include any preamble, explanations, or chain-of-thought
  - Do not describe your process or explain what you're doing
  - Do not wrap the output in markdown code fences
  - Your response must start directly with the heading

ai_rules_user_prompt: |
  TASK: Generate CLAUDE.md

  Create a comprehensive CLAUDE.md file for the project at {{ .RepoPath }}.

  I have analyzed the codebase and generated the following analysis documents. Use this information to create the CLAUDE.md:

  ## Structure Analysis
  {{ index .AnalysisContent "structure_analysis.md" }}

  ## Dependency Analysis
  {{ index .AnalysisContent "dependency_analysis.md" }}

  ## Data Flow Analysis
  {{ index .AnalysisContent "data_flow_analysis.md" }}

  ## Request Flow Analysis
  {{ index .AnalysisContent "request_flow_analysis.md" }}

  ## API Analysis
  {{ index .AnalysisContent "api_analysis.md" }}

  ---

  Based on the above analysis, create a CLAUDE.md that includes:

  1. **Project Overview** - What is this project and what does it do?
  2. **Common Commands** - How to run, test, build
  3. **Architecture** - Key architectural patterns
  4. **Code Conventions** - Style guidelines, naming patterns
  5. **Key Files** - Important files and what they do
  6. **Testing** - How tests are organized and run
  7. **Troubleshooting** - Common issues and solutions

  Make the content practical and actionable for an AI assistant.
  Include actual command examples and real file paths from the project.

  IMPORTANT:
  - Write factual information based ONLY on the analysis documents above
  - Don't invent features, commands, or conventions that don't exist
  - Keep descriptions concise but informative
  - Focus on what AI assistants need to know to work effectively with this codebase

  Output the complete CLAUDE.md content in Markdown format.
</file>
<file path="prompts/analyzer.yaml">
# System Prompts for Sub-Agents

structure_analyzer_system: |
  You are a code structure analyst specializing in identifying and documenting key architectural components.
  Your focus is on understanding the organization, abstraction patterns, and important services/modules in the codebase.
  You examine files, classes, interfaces, and their relationships without modifying any code.

  Your goal is to produce a comprehensive analysis of the codebase's architectural structure, key components, and design patterns.
  You will identify critical modules, interfaces, and core services that form the backbone of the application.
  You will document responsibility boundaries and how components interact at a structural level.

  IMPORTANT OUTPUT RULES:
  - Use tools to examine the codebase thoroughly
  - Output ONLY the final Markdown analysis - no preamble, no explanations, no chain-of-thought
  - Do not describe your process or explain what you're doing
  - Do not include tool outputs or intermediate results in your final response
  - Your final response must start directly with the markdown heading and contain only the analysis

structure_analyzer_user: |
  TASK: Analyze Code Structure

  Examine the project at {{ .RepoPath }} to identify and document key structural elements.

  Your analysis should clearly map the structural architecture of the codebase, highlighting key components,
  their responsibilities, and relationships.

  Start by understanding the repository's high-level organization. Then dive into identifying:
  - Core modules and their purposes
  - Key interfaces and abstractions
  - Service components and their responsibilities
  - Architectural patterns used (MVC, hexagonal, microservices, etc.)
  - Important methods and functions that define the application's capabilities
  - Code organization principles and patterns

  Focus on the "what" and "why" of components rather than implementation details.
  Be sure that you are describing existing code, not hypothetical code.

  EXPECTED OUTPUT FORMAT:

  # Code Structure Analysis

  ## Architectural Overview
  [Brief overview of the overall architecture]

  ## Core Components
  [List and describe main components]

  ## Service Definitions
  [Describe key services and their purposes]

  ## Interface Contracts
  [Document important interfaces and their contracts]

  ## Design Patterns Identified
  [List patterns found in the codebase]

  ## Component Relationships
  [Describe how components interact]

  ## Key Methods & Functions
  [Important functions that define capabilities]

dependency_analyzer_system: |
  You are a dependency analyst who traces how modules and components depend on each other.
  Your focus is on understanding import relationships, external dependencies, and coupling patterns.

  Your goal is to map the complete dependency graph of the application, identifying:
  - Internal module dependencies
  - External libraries and frameworks used
  - Circular dependencies or anti-patterns
  - Dependency injection patterns

  IMPORTANT OUTPUT RULES:
  - Use tools to examine the codebase thoroughly
  - Output ONLY the final Markdown analysis - no preamble, no explanations, no chain-of-thought
  - Do not describe your process or explain what you're doing
  - Do not include tool outputs or intermediate results in your final response
  - Your final response must start directly with the markdown heading and contain only the analysis

dependency_analyzer_user: |
  TASK: Analyze Dependencies

  Examine the project at {{ .RepoPath }} to trace and document dependencies.

  Focus on:
  - Internal module dependencies and how they relate
  - External dependencies (libraries, frameworks, services)
  - Dependency injection patterns
  - Potential issues like circular dependencies or tight coupling

  EXPECTED OUTPUT FORMAT:

  # Dependency Analysis

  ## Internal Dependencies
  [Map out how internal modules depend on each other]

  ## External Dependencies
  [List external libraries and their purposes]

  ## Dependency Graph
  [Describe the dependency structure]

  ## Dependency Injection
  [Document DI patterns if present]

  ## Potential Issues
  [Identify circular dependencies, tight coupling, etc.]

data_flow_analyzer_system: |
  You are a data flow specialist who tracks how data moves, transforms, and persists throughout an application.
  Your focus is on data structures, transformations, storage patterns, and the lifecycle of information.

  IMPORTANT OUTPUT RULES:
  - Use tools to examine the codebase thoroughly
  - Output ONLY the final Markdown analysis - no preamble, no explanations, no chain-of-thought
  - Do not describe your process or explain what you're doing
  - Do not include tool outputs or intermediate results in your final response
  - Your final response must start directly with the markdown heading and contain only the analysis

data_flow_analyzer_user: |
  TASK: Analyze Data Flow

  Examine the project at {{ .RepoPath }} to trace and document how data flows through the system.

  Focus on:
  - Data models and structures
  - How data enters the system (inputs, APIs, etc.)
  - Data transformations and validations
  - Storage mechanisms and databases
  - Data outputs and responses

  EXPECTED OUTPUT FORMAT:

  # Data Flow Analysis

  ## Data Models
  [Describe key data structures]

  ## Input Sources
  [Where data enters the system]

  ## Data Transformations
  [How data is processed and transformed]

  ## Storage Mechanisms
  [Databases, caches, files, etc.]

  ## Data Validation
  [Where and how data is validated]

  ## Output Formats
  [How data leaves the system]

request_flow_analyzer_system: |
  You are a request flow analyst who traces how requests flow through the application from entry to exit.
  Your focus is on HTTP endpoints, message handlers, and the complete lifecycle of requests.

  IMPORTANT OUTPUT RULES:
  - Use tools to examine the codebase thoroughly
  - Output ONLY the final Markdown analysis - no preamble, no explanations, no chain-of-thought
  - Do not describe your process or explain what you're doing
  - Do not include tool outputs or intermediate results in your final response
  - Your final response must start directly with the markdown heading and contain only the analysis

request_flow_analyzer_user: |
  TASK: Analyze Request Flow

  Examine the project at {{ .RepoPath }} to trace how requests are handled.

  Focus on:
  - API endpoints and their routes
  - Middleware and request processing pipeline
  - How requests are routed to handlers
  - Response generation and error handling

  EXPECTED OUTPUT FORMAT:

  # Request Flow Analysis

  ## API Endpoints
  [List all endpoints with methods and paths]

  ## Request Processing Pipeline
  [Describe middleware and processing steps]

  ## Routing Logic
  [How requests are routed to handlers]

  ## Response Generation
  [How responses are created and returned]

  ## Error Handling
  [How errors are handled in the request flow]

api_analyzer_system: |
  You are an API analyst who documents the external API surface of the application.
  Your focus is on endpoints, request/response formats, authentication, and API contracts.

  IMPORTANT OUTPUT RULES:
  - Use tools to examine the codebase thoroughly
  - Output ONLY the final Markdown analysis - no preamble, no explanations, no chain-of-thought
  - Do not describe your process or explain what you're doing
  - Do not include tool outputs or intermediate results in your final response
  - Your final response must start directly with the markdown heading and contain only the analysis

api_analyzer_user: |
  TASK: Analyze API

  Examine the project at {{ .RepoPath }} to document the API.

  IMPORTANT: First determine if this project exposes HTTP APIs. Look for:
  - HTTP server setup (e.g., net/http, gin, echo, fiber, express, fastapi, flask, etc.)
  - Route definitions or endpoint handlers
  - REST/GraphQL/gRPC endpoints

  If this is a CLI application, library, or tool WITHOUT HTTP APIs, immediately produce
  a brief report stating that and document any programmatic APIs (exported functions/interfaces)
  that consumers of the library/tool might use. Do NOT continue searching indefinitely for
  HTTP endpoints that don't exist.

  For projects WITH HTTP APIs, focus on:
  - All API endpoints (HTTP methods, paths, parameters)
  - Request formats and validation rules
  - Response formats and status codes
  - Authentication and authorization
  - Rate limiting and other API policies

  EXPECTED OUTPUT FORMAT:

  # API Analysis

  ## Project Type
  [Indicate if this is a web service, CLI, library, or other type]

  ## Endpoints Overview
  [List all endpoints grouped by resource, or state "No HTTP endpoints - this is a CLI/library"]

  ## Authentication
  [Describe auth mechanisms if applicable]

  ## Detailed Endpoints
  For each endpoint (if any):
  ### Method /path
  - Description
  - Parameters
  - Request format
  - Response format
  - Status codes

  ## Programmatic API (for libraries/CLIs)
  [Document exported functions, interfaces, or CLI commands if applicable]

  ## Common Patterns
  [Shared patterns across endpoints or APIs]
</file>
<file path="prompts/documenter.yaml">
# Prompts for Documenter Agent (README generation)

documenter_system: |
  You are a technical documentation specialist who creates comprehensive README.md files.
  You synthesize information from existing analysis documents to create user-friendly documentation.

  Your goal is to create a README that helps developers:
  - Understand what the project does
  - Get started quickly with installation and setup
  - Learn the architecture and key concepts
  - Find how to run tests and builds
  - Understand deployment processes

  IMPORTANT OUTPUT RULES:
  - Use tools to read and understand the analysis documents
  - Output ONLY the final README.md content in Markdown format
  - Do not include any preamble, explanations, or chain-of-thought
  - Do not describe your process or explain what you're doing
  - Do not wrap the output in markdown code fences
  - Your response must start directly with the README heading (e.g., "# ProjectName")

documenter_user: |
  TASK: Generate README.md

  Create a comprehensive README.md for the project at {{ .RepoPath }}.

  I have analyzed the codebase and generated the following analysis documents. Use this information to create the README:

  ## Structure Analysis
  {{ index .AnalysisContent "structure_analysis.md" }}

  ## Dependency Analysis
  {{ index .AnalysisContent "dependency_analysis.md" }}

  ## Data Flow Analysis
  {{ index .AnalysisContent "data_flow_analysis.md" }}

  ## Request Flow Analysis
  {{ index .AnalysisContent "request_flow_analysis.md" }}

  ## API Analysis
  {{ index .AnalysisContent "api_analysis.md" }}

  ---

  Based on the above analysis, create a README.md that includes:

  1. **Project Title & Brief Description** - What is this project?
  2. **Features** - Key features and capabilities
  3. **Installation** - How to install dependencies
  4. **Quick Start** - How to run the project
  5. **Architecture** - High-level architecture overview
  6. **Development** - How to run tests, lint, build
  7. **Configuration** - Environment variables and config files
  8. **Contributing** - Brief contribution guidelines
  9. **License** - Reference to LICENSE file

  Make the README professional, clear, and welcoming to new developers.
  Use code blocks for commands and examples.
  Use proper Markdown formatting throughout.

  IMPORTANT:
  - Write factual information based ONLY on the analysis documents above
  - Don't invent features that don't exist
  - Keep descriptions concise but informative
  - Focus on what developers need to know

  Output the complete README.md content in Markdown format.

# Aliases for compatibility with handler code
documenter_system_prompt: |
  You are a technical documentation specialist who creates comprehensive README.md files.
  You synthesize information from existing analysis documents to create user-friendly documentation.

  Your goal is to create a README that helps developers:
  - Understand what the project does
  - Get started quickly with installation and setup
  - Learn the architecture and key concepts
  - Find how to run tests and builds
  - Understand deployment processes

  IMPORTANT OUTPUT RULES:
  - Use tools to read and understand the analysis documents
  - Output ONLY the final README.md content in Markdown format
  - Do not include any preamble, explanations, or chain-of-thought
  - Do not describe your process or explain what you're doing
  - Do not wrap the output in markdown code fences
  - Your response must start directly with the README heading (e.g., "# ProjectName")

documenter_user_prompt: |
  TASK: Generate README.md

  Create a comprehensive README.md for the project at {{ .RepoPath }}.

  I have analyzed the codebase and generated the following analysis documents. Use this information to create the README:

  ## Structure Analysis
  {{ index .AnalysisContent "structure_analysis.md" }}

  ## Dependency Analysis
  {{ index .AnalysisContent "dependency_analysis.md" }}

  ## Data Flow Analysis
  {{ index .AnalysisContent "data_flow_analysis.md" }}

  ## Request Flow Analysis
  {{ index .AnalysisContent "request_flow_analysis.md" }}

  ## API Analysis
  {{ index .AnalysisContent "api_analysis.md" }}

  ---

  Based on the above analysis, create a README.md that includes:

  1. **Project Title & Brief Description** - What is this project?
  2. **Features** - Key features and capabilities
  3. **Installation** - How to install dependencies
  4. **Quick Start** - How to run the project
  5. **Architecture** - High-level architecture overview
  6. **Development** - How to run tests, lint, build
  7. **Configuration** - Environment variables and config files
  8. **Contributing** - Brief contribution guidelines
  9. **License** - Reference to LICENSE file

  Make the README professional, clear, and welcoming to new developers.
  Use code blocks for commands and examples.
  Use proper Markdown formatting throughout.

  IMPORTANT:
  - Write factual information based ONLY on the analysis documents above
  - Don't invent features that don't exist
  - Keep descriptions concise but informative
  - Focus on what developers need to know

  Output the complete README.md content in Markdown format.
</file>
<file path="CLAUDE.md">
# CLAUDE.md

## Project Overview

Gendocs is a CLI application written in Go that uses Large Language Models (LLMs) to analyze codebases and generate documentation. It provides commands to analyze code, generate README files, create AI assistant rules, and export documentation to HTML.

## Common Commands

*   **Run the application:** `go run cmd/root.go` (This might require specifying a subcommand, e.g., `go run cmd/root.go analyze`)
*   **Analyze the codebase:** `go run cmd/root.go analyze` (This command likely initiates the analysis process, storing results in `.ai/docs/`).
*   **Generate a README:** `go run cmd/root.go generate readme` (This command creates a `README.md` file based on the analysis).
*   **Generate AI rules:** `go run cmd/root.go generate ai_rules` (This command creates configuration files for AI assistants).
*   **Export to HTML:** `go run cmd/root.go generate export` (This command converts markdown documentation to HTML).
*   **Run a cronjob (GitLab integration):** `go run cmd/root.go cronjob`
*   **Load configuration:** `go run cmd/root.go config`

## Architecture

Gendocs follows a clean architecture with these key patterns:

*   **CLI Layer:** Uses `github.com/spf13/cobra` for command-line interface.
*   **Handlers:** Orchestrate the core logic (analysis, generation, export). Located in `internal/handlers/`.
*   **Agents:** Perform specialized tasks using LLMs. Located in `internal/agents/`.
*   **LLM Clients:** Integrate with different LLM providers (Anthropic, Gemini, OpenAI). Located in `internal/llm/`. Uses a Factory pattern (`internal/llm/factory.go`).
*   **Configuration:** Loads settings from files and environment variables using `github.com/spf13/viper` in `internal/config/`.
*   **Factory Pattern:** Used for creating LLM clients and agents, allowing for easy swapping of implementations.
*   **Strategy Pattern:**  Different LLM providers implement the same `LLMClient` interface.
*   **Worker Pool:** Employs parallel agent execution.

## Code Conventions

*   **Configuration:** Defined in `internal/config/models.go` and loaded using `internal/config/loader.go`.
*   **Logging:** Uses `go.uber.org/zap` for structured logging.
*   **Error Handling:** While not explicitly detailed, assume standard Go error handling patterns are used.

## Key Files

*   **`cmd/root.go`:** The main entry point for the CLI application. Defines the root command and global flags.
*   **`cmd/analyze.go`:** Defines the `analyze` command, triggering the codebase analysis.
*   **`cmd/generate.go`:** Defines the `generate` command with subcommands for README, AI rules, and export.
*   **`internal/agents/analyzer.go`:** The main analyzer agent that coordinates sub-agents.
*   **`internal/llm/client.go`:** Defines the `LLMClient` interface for interacting with LLMs.
*   **`internal/llm/anthropic.go`, `internal/llm/gemini.go`, `internal/llm/openai.go`:** Implementations of the `LLMClient` interface for different LLM providers.
*   **`internal/handlers/analyze.go`:** Orchestrates the codebase analysis process.
*   **`internal/handlers/readme.go`:** Handles the generation of the README file.
*   **`go.mod`:**  Lists project dependencies.

## Testing

There is no explicit information about tests provided. Look for files named `*_test.go` to locate tests. Assume standard Go testing practices are used (e.g., `go test`).

## Troubleshooting

*   **Configuration Errors:** Ensure that the configuration file is correctly formatted and that all required environment variables are set.
*   **LLM API Errors:** Check API keys and rate limits for the configured LLM provider.
*   **File Access Errors:** Verify that the application has the necessary permissions to read and write files in the specified directories.
*   **Dependency Issues:** Run `go mod tidy` and `go mod vendor` to resolve dependency conflicts.
</file>
<file path="go.mod">
module github.com/user/gendocs

go 1.25.5

require (
	github.com/joho/godotenv v1.5.1
	github.com/spf13/cobra v1.10.2
	github.com/spf13/viper v1.21.0
	go.uber.org/zap v1.27.1
	gopkg.in/yaml.v3 v3.0.1
)

require (
	github.com/alecthomas/chroma/v2 v2.21.1 // indirect
	github.com/atotto/clipboard v0.1.4 // indirect
	github.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect
	github.com/charmbracelet/bubbles v0.21.0 // indirect
	github.com/charmbracelet/bubbletea v1.3.4 // indirect
	github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc // indirect
	github.com/charmbracelet/lipgloss v1.1.0 // indirect
	github.com/charmbracelet/x/ansi v0.8.0 // indirect
	github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd // indirect
	github.com/charmbracelet/x/term v0.2.1 // indirect
	github.com/dlclark/regexp2 v1.11.5 // indirect
	github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f // indirect
	github.com/fsnotify/fsnotify v1.9.0 // indirect
	github.com/go-viper/mapstructure/v2 v2.4.0 // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/lucasb-eyer/go-colorful v1.2.0 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-localereader v0.0.1 // indirect
	github.com/mattn/go-runewidth v0.0.16 // indirect
	github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 // indirect
	github.com/muesli/cancelreader v0.2.2 // indirect
	github.com/muesli/termenv v0.16.0 // indirect
	github.com/pelletier/go-toml/v2 v2.2.4 // indirect
	github.com/rivo/uniseg v0.4.7 // indirect
	github.com/sagikazarmark/locafero v0.11.0 // indirect
	github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 // indirect
	github.com/spf13/afero v1.15.0 // indirect
	github.com/spf13/cast v1.10.0 // indirect
	github.com/spf13/pflag v1.0.10 // indirect
	github.com/subosito/gotenv v1.6.0 // indirect
	github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e // indirect
	github.com/yuin/goldmark v1.7.13 // indirect
	github.com/yuin/goldmark-highlighting/v2 v2.0.0-20230729083705-37449abec8cc // indirect
	go.uber.org/multierr v1.10.0 // indirect
	go.yaml.in/yaml/v3 v3.0.4 // indirect
	golang.org/x/sync v0.16.0 // indirect
	golang.org/x/sys v0.36.0 // indirect
	golang.org/x/text v0.28.0 // indirect
)
</file>
<file path="go.sum">
github.com/alecthomas/chroma/v2 v2.2.0/go.mod h1:vf4zrexSH54oEjJ7EdB65tGNHmH3pGZmVkgTP5RHvAs=
github.com/alecthomas/chroma/v2 v2.21.1 h1:FaSDrp6N+3pphkNKU6HPCiYLgm8dbe5UXIXcoBhZSWA=
github.com/alecthomas/chroma/v2 v2.21.1/go.mod h1:NqVhfBR0lte5Ouh3DcthuUCTUpDC9cxBOfyMbMQPs3o=
github.com/alecthomas/repr v0.0.0-20220113201626-b1b626ac65ae/go.mod h1:2kn6fqh/zIyPLmm3ugklbEi5hg5wS435eygvNfaDQL8=
github.com/atotto/clipboard v0.1.4 h1:EH0zSVneZPSuFR11BlR9YppQTVDbh5+16AmcJi4g1z4=
github.com/atotto/clipboard v0.1.4/go.mod h1:ZY9tmq7sm5xIbd9bOK4onWV4S6X0u6GY7Vn0Yu86PYI=
github.com/aymanbagabas/go-osc52/v2 v2.0.1 h1:HwpRHbFMcZLEVr42D4p7XBqjyuxQH5SMiErDT4WkJ2k=
github.com/aymanbagabas/go-osc52/v2 v2.0.1/go.mod h1:uYgXzlJ7ZpABp8OJ+exZzJJhRNQ2ASbcXHWsFqH8hp8=
github.com/charmbracelet/bubbles v0.21.0 h1:9TdC97SdRVg/1aaXNVWfFH3nnLAwOXr8Fn6u6mfQdFs=
github.com/charmbracelet/bubbles v0.21.0/go.mod h1:HF+v6QUR4HkEpz62dx7ym2xc71/KBHg+zKwJtMw+qtg=
github.com/charmbracelet/bubbletea v1.3.4 h1:kCg7B+jSCFPLYRA52SDZjr51kG/fMUEoPoZrkaDHyoI=
github.com/charmbracelet/bubbletea v1.3.4/go.mod h1:dtcUCyCGEX3g9tosuYiut3MXgY/Jsv9nKVdibKKRRXo=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc h1:4pZI35227imm7yK2bGPcfpFEmuY1gc2YSTShr4iJBfs=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc/go.mod h1:X4/0JoqgTIPSFcRA/P6INZzIuyqdFY5rm8tb41s9okk=
github.com/charmbracelet/lipgloss v1.1.0 h1:vYXsiLHVkK7fp74RkV7b2kq9+zDLoEU4MZoFqR/noCY=
github.com/charmbracelet/lipgloss v1.1.0/go.mod h1:/6Q8FR2o+kj8rz4Dq0zQc3vYf7X+B0binUUBwA0aL30=
github.com/charmbracelet/x/ansi v0.8.0 h1:9GTq3xq9caJW8ZrBTe0LIe2fvfLR/bYXKTx2llXn7xE=
github.com/charmbracelet/x/ansi v0.8.0/go.mod h1:wdYl/ONOLHLIVmQaxbIYEC/cRKOQyjTkowiI4blgS9Q=
github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd h1:vy0GVL4jeHEwG5YOXDmi86oYw2yuYUGqz6a8sLwg0X8=
github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd/go.mod h1:xe0nKWGd3eJgtqZRaN9RjMtK7xUYchjzPr7q6kcvCCs=
github.com/charmbracelet/x/term v0.2.1 h1:AQeHeLZ1OqSXhrAWpYUtZyX1T3zVxfpZuEQMIQaGIAQ=
github.com/charmbracelet/x/term v0.2.1/go.mod h1:oQ4enTYFV7QN4m0i9mzHrViD7TQKvNEEkHUMCmsxdUg=
github.com/cpuguy83/go-md2man/v2 v2.0.6/go.mod h1:oOW0eioCTA6cOiMLiUPZOpcVxMig6NIQQ7OS05n1F4g=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/dlclark/regexp2 v1.4.0/go.mod h1:2pZnwuY/m+8K6iRw6wQdMtk+rH5tNGR1i55kozfMjCc=
github.com/dlclark/regexp2 v1.7.0/go.mod h1:DHkYz0B9wPfa6wondMfaivmHpzrQ3v9q8cnmRbL6yW8=
github.com/dlclark/regexp2 v1.11.5 h1:Q/sSnsKerHeCkc/jSTNq1oCm7KiVgUMZRDUoRu0JQZQ=
github.com/dlclark/regexp2 v1.11.5/go.mod h1:DHkYz0B9wPfa6wondMfaivmHpzrQ3v9q8cnmRbL6yW8=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f h1:Y/CXytFA4m6baUTXGLOoWe4PQhGxaX0KpnayAqC48p4=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f/go.mod h1:vw97MGsxSvLiUE2X8qFplwetxpGLQrlU1Q9AUEIzCaM=
github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=
github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=
github.com/fsnotify/fsnotify v1.9.0 h1:2Ml+OJNzbYCTzsxtv8vKSFD9PbJjmhYF14k/jKC7S9k=
github.com/fsnotify/fsnotify v1.9.0/go.mod h1:8jBTzvmWwFyi3Pb8djgCCO5IBqzKJ/Jwo8TRcHyHii0=
github.com/go-viper/mapstructure/v2 v2.4.0 h1:EBsztssimR/CONLSZZ04E8qAkxNYq4Qp9LvH92wZUgs=
github.com/go-viper/mapstructure/v2 v2.4.0/go.mod h1:oJDH3BJKyqBA2TXFhDsKDGDTlndYOZ6rGS0BRZIxGhM=
github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=
github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
github.com/joho/godotenv v1.5.1 h1:7eLL/+HRGLY0ldzfGMeQkb7vMd0as4CfYvUVzLqw0N0=
github.com/joho/godotenv v1.5.1/go.mod h1:f4LDr5Voq0i2e/R5DDNOoa2zzDfwtkZa6DnEwAbqwq4=
github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/lucasb-eyer/go-colorful v1.2.0 h1:1nnpGOrhyZZuNyfu1QjKiUICQ74+3FNCN69Aj6K7nkY=
github.com/lucasb-eyer/go-colorful v1.2.0/go.mod h1:R4dSotOR9KMtayYi1e77YzuveK+i7ruzyGqttikkLy0=
github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mattn/go-localereader v0.0.1 h1:ygSAOl7ZXTx4RdPYinUpg6W99U8jWvWi9Ye2JC/oIi4=
github.com/mattn/go-localereader v0.0.1/go.mod h1:8fBrzywKY7BI3czFoHkuzRoWE9C+EiG4R1k4Cjx5p88=
github.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=
github.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 h1:ZK8zHtRHOkbHy6Mmr5D264iyp3TiX5OmNcI5cIARiQI=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6/go.mod h1:CJlz5H+gyd6CUWT45Oy4q24RdLyn7Md9Vj2/ldJBSIo=
github.com/muesli/cancelreader v0.2.2 h1:3I4Kt4BQjOR54NavqnDogx/MIoWBFa0StPA8ELUXHmA=
github.com/muesli/cancelreader v0.2.2/go.mod h1:3XuTXfFS2VjM+HTLZY9Ak0l6eUKfijIfMUZ4EgX0QYo=
github.com/muesli/termenv v0.16.0 h1:S5AlUN9dENB57rsbnkPyfdGuWIlkmzJjbFf0Tf5FWUc=
github.com/muesli/termenv v0.16.0/go.mod h1:ZRfOIKPFDYQoDFF4Olj7/QJbW60Ol/kL1pU3VfY/Cnk=
github.com/pelletier/go-toml/v2 v2.2.4 h1:mye9XuhQ6gvn5h28+VilKrrPoQVanw5PMw/TB0t5Ec4=
github.com/pelletier/go-toml/v2 v2.2.4/go.mod h1:2gIqNv+qfxSVS7cM2xJQKtLSTLUE9V8t9Stt+h56mCY=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=
github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=
github.com/rogpeppe/go-internal v1.9.0 h1:73kH8U+JUqXU8lRuOHeVHaa/SZPifC7BkcraZVejAe8=
github.com/rogpeppe/go-internal v1.9.0/go.mod h1:WtVeX8xhTBvf0smdhujwtBcq4Qrzq/fJaraNFVN+nFs=
github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/sagikazarmark/locafero v0.11.0 h1:1iurJgmM9G3PA/I+wWYIOw/5SyBtxapeHDcg+AAIFXc=
github.com/sagikazarmark/locafero v0.11.0/go.mod h1:nVIGvgyzw595SUSUE6tvCp3YYTeHs15MvlmU87WwIik=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 h1:+jumHNA0Wrelhe64i8F6HNlS8pkoyMv5sreGx2Ry5Rw=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8/go.mod h1:3n1Cwaq1E1/1lhQhtRK2ts/ZwZEhjcQeJQ1RuC6Q/8U=
github.com/spf13/afero v1.15.0 h1:b/YBCLWAJdFWJTN9cLhiXXcD7mzKn9Dm86dNnfyQw1I=
github.com/spf13/afero v1.15.0/go.mod h1:NC2ByUVxtQs4b3sIUphxK0NioZnmxgyCrfzeuq8lxMg=
github.com/spf13/cast v1.10.0 h1:h2x0u2shc1QuLHfxi+cTJvs30+ZAHOGRic8uyGTDWxY=
github.com/spf13/cast v1.10.0/go.mod h1:jNfB8QC9IA6ZuY2ZjDp0KtFO2LZZlg4S/7bzP6qqeHo=
github.com/spf13/cobra v1.10.2 h1:DMTTonx5m65Ic0GOoRY2c16WCbHxOOw6xxezuLaBpcU=
github.com/spf13/cobra v1.10.2/go.mod h1:7C1pvHqHw5A4vrJfjNwvOdzYu0Gml16OCs2GRiTUUS4=
github.com/spf13/pflag v1.0.9/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/pflag v1.0.10 h1:4EBh2KAYBwaONj6b2Ye1GiHfwjqyROoF4RwYO+vPwFk=
github.com/spf13/pflag v1.0.10/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/viper v1.21.0 h1:x5S+0EU27Lbphp4UKm1C+1oQO+rKx36vfCoaVebLFSU=
github.com/spf13/viper v1.21.0/go.mod h1:P0lhsswPGWD/1lZJ9ny3fYnVqxiegrlNrEmgLjbTCAY=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.11.1 h1:7s2iGBzp5EwR7/aIZr8ao5+dra3wiQyKjjFuvgVKu7U=
github.com/stretchr/testify v1.11.1/go.mod h1:wZwfW3scLgRK+23gO65QZefKpKQRnfz6sD981Nm4B6U=
github.com/subosito/gotenv v1.6.0 h1:9NlTDc1FTs4qu0DDq7AEtTPNw6SVm7uBMsUCUjABIf8=
github.com/subosito/gotenv v1.6.0/go.mod h1:Dk4QP5c2W3ibzajGcXpNraDfq2IrhjMIvMSWPKKo0FU=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e h1:JVG44RsyaB9T2KIHavMF/ppJZNG9ZpyihvCd0w101no=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e/go.mod h1:RbqR21r5mrJuqunuUZ/Dhy/avygyECGrLceyNeo4LiM=
github.com/yuin/goldmark v1.4.15/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=
github.com/yuin/goldmark v1.7.13 h1:GPddIs617DnBLFFVJFgpo1aBfe/4xcvMc3SB5t/D0pA=
github.com/yuin/goldmark v1.7.13/go.mod h1:ip/1k0VRfGynBgxOz0yCqHrbZXhcjxyuS66Brc7iBKg=
github.com/yuin/goldmark-highlighting/v2 v2.0.0-20230729083705-37449abec8cc h1:+IAOyRda+RLrxa1WC7umKOZRsGq4QrFFMYApOeHzQwQ=
github.com/yuin/goldmark-highlighting/v2 v2.0.0-20230729083705-37449abec8cc/go.mod h1:ovIvrum6DQJA4QsJSovrkC4saKHQVs7TvcaeO8AIl5I=
go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
go.uber.org/multierr v1.10.0 h1:S0h4aNzvfcFsC3dRF1jLoaov7oRaKqRGC/pUEJ2yvPQ=
go.uber.org/multierr v1.10.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN80Y=
go.uber.org/zap v1.27.1 h1:08RqriUEv8+ArZRYSTXy1LeBScaMpVSTBhCeaZYfMYc=
go.uber.org/zap v1.27.1/go.mod h1:GB2qFLM7cTU87MWRP2mPIjqfIDnGu+VIO4V/SdhGo2E=
go.yaml.in/yaml/v3 v3.0.4 h1:tfq32ie2Jv2UxXFdLJdh3jXuOzWiL1fo0bu/FbuKpbc=
go.yaml.in/yaml/v3 v3.0.4/go.mod h1:DhzuOOF2ATzADvBadXxruRBLzYTpT36CKvDb3+aBEFg=
golang.org/x/sync v0.16.0 h1:ycBJEhp9p4vXvUZNszeOq0kGTPghopOL8q0fq3vstxw=
golang.org/x/sync v0.16.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/sys v0.0.0-20210809222454-d867a43fc93e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.36.0 h1:KVRy2GtZBrk1cBYA7MKu5bEZFxQk4NIDV6RLVcC8o0k=
golang.org/x/sys v0.36.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
golang.org/x/text v0.28.0 h1:rhazDwis8INMIwQ4tpjLDzUhx6RlXqZNPEM0huQojng=
golang.org/x/text v0.28.0/go.mod h1:U8nCwOR8jO/marOQ0QbDiOngZVEBB7MAiitBuMjXiNU=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15 h1:YR8cESwS4TdDjEe65xsg0ogRM/Nc3DYOhEAlW+xobZo=
gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
</file>
<file path="INSTALL.md">
# Guia de InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

Este guia fornece instruÃ§Ãµes passo a passo para instalar e configurar o Gendocs Go.

## PrÃ©-requisitos

- **Go 1.22 ou posterior**
- **API key de um provedor LLM** (OpenAI, Anthropic, ou Google Gemini)
- (Opcional) **GitLab** com token OAuth para funcionalidade de cronjob

## 1. InstalaÃ§Ã£o

### OpÃ§Ã£o A: Usar Makefile (Recomendado)

```bash
# Compilar
make build

# Instalar (requer sudo)
make install

# Verificar instalaÃ§Ã£o
gendocs --version
```

### OpÃ§Ã£o B: Scripts de InstalaÃ§Ã£o

```bash
# Instalar (requer sudo)
sudo ./install.sh

# Desinstalar
sudo ./uninstall.sh
```

### OpÃ§Ã£o C: Compilar Manualmente

```bash
git clone https://github.com/divar-ir/ai-doc-gen.git
cd ai-doc-gen-feature-go-version/gendocs

# Compilar
go build -o gendocs .

# Opcional: mover para PATH global
sudo mv gendocs /usr/local/bin/
```

### OpÃ§Ã£o D: BinÃ¡rio prÃ©-compilado (quando disponÃ­vel)

```bash
# Baixar binÃ¡rio
wget https://github.com/divar-ir/ai-doc-gen/releases/latest/download/gendocs-linux-amd64
chmod +x gendocs-linux-amd64
sudo mv gendocs-linux-amd64 /usr/local/bin/gendocs
```

## 2. ConfiguraÃ§Ã£o RÃ¡pida

### MÃ©todo 1: Wizard Interativo (Recomendado)

```bash
# Inicia o wizard de configuraÃ§Ã£o
./gendocs config
```

O wizard vai te guiar atravÃ©s de:
1. SeleÃ§Ã£o do provedor (OpenAI, Anthropic, ou Gemini)
2. ConfiguraÃ§Ã£o da API key
3. SeleÃ§Ã£o do modelo
4. (Opcional) Base URL para APIs compatÃ­veis com OpenAI

A configuraÃ§Ã£o Ã© salva em `~/.gendocs.yaml`.

### MÃ©todo 2: VariÃ¡veis de Ambiente

```bash
# Configurar provedor e API key
export ANALYZER_LLM_PROVIDER="openai"
export ANALYZER_LLM_MODEL="gpt-4o"
export ANALYZER_LLM_API_KEY="sk-sua-chave-aqui"

# Para Anthropic Claude
# export ANALYZER_LLM_PROVIDER="anthropic"
# export ANALYZER_LLM_MODEL="claude-3-5-sonnet-20241022"
# export ANALYZER_LLM_API_KEY="sk-ant-sua-chave-aqui"

# Para Google Gemini
# export ANALYZER_LLM_PROVIDER="gemini"
# export ANALYZER_LLM_MODEL="gemini-1.5-pro"
# export ANALYZER_LLM_CONFIG_API_KEY="sua-chave-aqui"
```

Adicione ao seu `~/.bashrc` ou `~/.zshrc`:

```bash
echo 'export ANALYZER_LLM_PROVIDER="openai"' >> ~/.bashrc
echo 'export ANALYZER_LLM_MODEL="gpt-4o"' >> ~/.bashrc
echo 'export ANALYZER_LLM_API_KEY="sk-sua-chave"' >> ~/.bashrc
source ~/.bashrc
```

### MÃ©todo 3: Arquivo de ConfiguraÃ§Ã£o `.ai/config.yaml`

Crie um arquivo `.ai/config.yaml` no seu projeto:

```yaml
analyzer:
  llm:
    provider: openai
    model: gpt-4o
    api_key: ${ANALYZER_LLM_API_KEY}
    base_url: ""  # Opcional, para APIs compatÃ­veis
    retries: 2
    timeout: 180
    max_tokens: 8192
    temperature: 0.0
  max_workers: 0  # 0 = auto-detectar CPUs
  exclude_code_structure: false
  exclude_data_flow: false
  exclude_dependencies: false
  exclude_request_flow: false
  exclude_api_analysis: false
```

## 3. Verificar InstalaÃ§Ã£o

```bash
# Verificar versÃ£o
./gendocs --version

# Verificar ajuda
./gendocs --help

# Verificar configuraÃ§Ã£o (se usou wizard)
cat ~/.gendocs.yaml
```

## 4. Primeiro Uso

### Analisar um Projeto

```bash
# Analisar o diretÃ³rio atual
./gendocs analyze --repo-path .

# Analisar outro diretÃ³rio
./gendocs analyze --repo-path /caminho/para/projeto

# Com flags de exclusÃ£o
./gendocs analyze --repo-path . --exclude-api-analysis --exclude-dependencies

# Com depuraÃ§Ã£o
./gendocs analyze --repo-path . --debug
```

Isso vai gerar arquivos em `.ai/docs/`:
- `structure_analysis.md`
- `dependency_analysis.md`
- `data_flow_analysis.md`
- `request_flow_analysis.md`
- `api_analysis.md`

### Gerar DocumentaÃ§Ã£o

```bash
# Gerar README.md a partir das anÃ¡lises
./gendocs generate readme --repo-path .

# Gerar arquivos de configuraÃ§Ã£o para IA
./gendocs generate ai-rules --repo-path .
```

Isso vai criar:
- `README.md` no diretÃ³rio raiz
- `CLAUDE.md` (instruÃ§Ãµes para Claude)
- `AGENTS.md` (convenÃ§Ãµes de agentes)

### Processamento em Lote GitLab

```bash
# Configurar GitLab
export GITLAB_API_URL="https://gitlab.com"
export GITLAB_OAUTH_TOKEN="glpat-sua-token-aqui"
export GITLAB_USER_EMAIL="seu-email@example.com"

# Processar todos os projetos de um grupo
./gendocs cronjob analyze --group-project-id 123 --max-days-since-last-commit 14
```

Isso vai:
1. Buscar todos os projetos do grupo
2. Filtrar (pular arquivados, sem commits recentes)
3. Clonar cada projeto
4. Rodar anÃ¡lise
5. Criar branch `ai-analyzer-YYYY-MM-DD`
6. Fazer commit com resultados
7. Criar Merge Request

## 5. Exemplos de ConfiguraÃ§Ã£o

### OpenAI com Modelo Customizado

```yaml
# ~/.gendocs.yaml
analyzer:
  llm:
    provider: openai
    model: gpt-4o-mini
    max_tokens: 4096
```

### APIs CompatÃ­veis com OpenAI

```yaml
analyzer:
  llm:
    provider: openai
    model: llama-3.1-70b-instruct
    base_url: https://api.deepinfra.com/v1/openai
```

### Anthropic Claude

```yaml
analyzer:
  llm:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
```

### Google Gemini via Vertex AI

```yaml
analyzer:
  llm:
    provider: gemini
    model: gemini-1.5-pro
gemini:
  use_vertex_ai: true
  project_id: seu-project-id
  location: us-central1
```

## 6. Troubleshooting

### Erro: "Required environment variable 'ANALYZER_LLM_API_KEY' is not set"

**SoluÃ§Ã£o**: Configure a API key:
```bash
export ANALYZER_LLM_API_KEY="sk-sua-chave"
```

### Erro: "failed to load prompts"

**SoluÃ§Ã£o**: Certifique-se de estar no diretÃ³rio correto:
```bash
cd gendocs
./gendocs analyze --repo-path ../projeto-analisar
```

### Erro: "API error: status 401"

**SoluÃ§Ã£o**: Verifique sua API key. Para testar:
```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $ANALYZER_LLM_API_KEY"
```

### Ver Logs de DepuraÃ§Ã£o

```bash
# Ativar debug
./gendocs analyze --repo-path . --debug

# Logs sÃ£o salvos em
cat .ai/logs/gendocs.log
```

## 7. Estrutura de DiretÃ³rios

```
projeto-analisado/
â”œâ”€â”€ .ai/
â”‚   â”œâ”€â”€ config.yaml          # Config do projeto (opcional)
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ structure_analysis.md
â”‚   â”‚   â”œâ”€â”€ dependency_analysis.md
â”‚   â”‚   â”œâ”€â”€ data_flow_analysis.md
â”‚   â”‚   â”œâ”€â”€ request_flow_analysis.md
â”‚   â”‚   â””â”€â”€ api_analysis.md
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ gendocs.log        # Logs estruturados (JSON)
â”œâ”€â”€ README.md                 # Gerado por `gendocs generate readme`
â”œâ”€â”€ CLAUDE.md                 # Gerado por `gendocs generate ai-rules`
â””â”€â”€ AGENTS.md                 # Gerado por `gendocs generate ai-rules`
```

## 8. IntegraÃ§Ã£o CI/CD

### GitHub Actions

```yaml
- name: Run gendocs analyze
  run: |
    go install github.com/divar-ir/ai-doc-gen/gendocs@latest
    gendocs analyze --repo-path .
```

### GitLab CI

```yaml
analyze:
  script:
    - go install github.com/divar-ir/ai-doc-gen/gendocs@latest
    - gendocs analyze --repo-path .
```

## 9. AtualizaÃ§Ã£o

```bash
cd ai-doc-gen-feature-go-version/gendocs
git pull
go build -o gendocs .
```

## 10. Suporte

- **Issues**: https://github.com/divar-ai-doc-gen/issues
- **DocumentaÃ§Ã£o**: Leia PLAN.md para detalhes da arquitetura
- **Python vs Go**: A versÃ£o Go mantÃ©m paridade de recursos com a Python
</file>
<file path="install.sh">
#!/bin/bash
# Script de instalaÃ§Ã£o para Gendocs
# Uso: sudo ./install.sh

set -e

BINARY_NAME="gendocs"
BUILD_DIR="build"
BIN_DIR="/usr/local/bin"
CONFIG_DIR="$HOME/.gendocs.yaml"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

echo "=== Gendocs Installation Script ==="
echo ""

# Detect OS
OS="$(uname -s)"
case "$OS" in
    Linux*)
        BINARY="gendocs-linux-amd64"
        ;;
    Darwin*)
        BINARY="gendocs-darwin-amd64"
        ;;
    *)
        BINARY="gendocs"
        ;;
esac

echo "Detectado: $OS"
echo ""

# Check Go installation
if ! command -v go &> /dev/null; then
    echo "Erro: Go nÃ£o estÃ¡ instalado."
    echo ""
    echo "Instale Go 1.22+:"
    echo "  https://go.dev/dl/"
    exit 1
fi

GO_VERSION=$(go version | awk '{print $3}')
echo "Go encontrado: $GO_VERSION"
echo ""

# Build
echo "Compilando..."
cd "$SCRIPT_DIR"
go build -o "$BUILD_DIR/$BINARY" .

# Install binary
echo "Instalando binÃ¡rio em $BIN_DIR..."
sudo mkdir -p "$BIN_DIR"
sudo cp "$BUILD_DIR/$BINARY" "$BIN_DIR/$BINARY_NAME"
sudo chmod +x "$BIN_DIR/$BINARY_NAME"

echo ""
echo "âœ… InstalaÃ§Ã£o completa!"
echo ""
echo "BinÃ¡rio instalado em: $BIN_DIR/$BINARY_NAME"
echo ""
echo "ğŸ“– Para configuraÃ§Ã£o, execute:"
echo "  $BINARY_NAME config"
echo ""
echo "Ou configure manualmente:"
echo "  export ANALYZER_LLM_PROVIDER=\"openai\""
echo "  export ANALYZE_LLM_MODEL=\"gpt-4o\""
echo "  export ANALYZE_LLM_API_KEY=\"sk-...\""
echo ""
echo "ğŸš€ Para analisar um projeto:"
echo "  $BINARY_NAME analyze --repo-path /caminho/para/projeto"
</file>
<file path="main.go">
package main

import "github.com/user/gendocs/cmd"

func main() {
	cmd.Execute()
}
</file>
<file path="Makefile">
.PHONY: all build install uninstall clean test help

# Variables
BINARY_NAME=gendocs
BUILD_DIR=build
# InstalaÃ§Ã£o local em ~/.local/bin
BIN_DIR=$(HOME)/.local/bin
CONFIG_DIR=$(HOME)/.gendocs.yaml
PROMPTS_DIR=./prompts
GO=go
GOFLAGS=

# Detect OS
UNAME_S := $(shell uname -s)
ifeq ($(UNAME_S),Linux)
    BINARY=$(BINARY_NAME)-linux-amd64
else ifeq ($(UNAME_S),Darwin)
    BINARY=$(BINARY_NAME)-darwin-amd64
else
    BINARY=$(BINARY_NAME)
endif

all: build

help:
	@echo "Gendocs Makefile"
	@echo ""
	@echo "Available targets:"
	@echo "  make build          - Compila o binÃ¡rio"
	@echo "  make install        - Instala o binÃ¡rio em $(BIN_DIR)"
	@echo "  make uninstall      - Remove o binÃ¡rio de $(BIN_DIR)"
	@echo "  make clean          - Remove arquivos de build"
	@echo "  make test           - Executa todos os testes"
	@echo "  make test-verbose   - Executa testes com saÃ­da detalhada"
	@echo "  make test-coverage  - Executa testes com relatÃ³rio de coverage"
	@echo "  make test-short     - Executa apenas testes curtos"
	@echo "  make lint           - Executa linters"
	@echo "  make help           - Mostra esta mensagem"

build:
	@echo "Compilando $(BINARY)..."
	$(GO) $(GOFLAGS) build -o $(BUILD_DIR)/$(BINARY) .
	@echo "BinÃ¡rio criado: $(BUILD_DIR)/$(BINARY)"

install: build
	@echo "Instalando $(BINARY) em $(BIN_DIR)..."
	@mkdir -p $(BIN_DIR)
	@cp $(BUILD_DIR)/$(BINARY) $(BIN_DIR)/$(BINARY_NAME)
	@chmod +x $(BIN_DIR)/$(BINARY_NAME)
	@echo "Instalado em: $(BIN_DIR)/$(BINARY_NAME)"
	@echo ""
	@echo "Para configurar, execute:"
	@echo "  $(BINARY_NAME) config"
	@echo ""
	@echo "Ou configure manualmente:"
	@echo "  export ANALYZER_LLM_PROVIDER=\"openai\""
	@echo "  export ANALYZER_LLM_MODEL=\"gpt-4o\""
	@echo "  export ANALYZER_LLM_API_KEY=\"sk-...\""

uninstall:
	@echo "Removendo $(BINARY_NAME) de $(BIN_DIR)..."
	@rm -f $(BIN_DIR)/$(BINARY_NAME)
	@echo "Removido."
	@echo ""
	@echo "Para remover completamente (incluindo configuraÃ§Ã£o):"
	@echo "  rm -f $(CONFIG_DIR)"
	@echo "  rm -rf ~/.gendocs/prompts_backup"

clean:
	@echo "Limpando arquivos de build..."
	@rm -rf $(BUILD_DIR)
	@rm -rf coverage/
	@echo "Limpo."

test:
	@echo "Executando testes..."
	$(GO) test -race -timeout 5m ./...
	@echo "âœ“ Testes concluÃ­dos"

test-verbose:
	@echo "Executando testes (verbose)..."
	$(GO) test -v -race -timeout 5m ./...

test-coverage:
	@echo "Executando testes com coverage..."
	@mkdir -p coverage
	$(GO) test -race -timeout 5m -coverprofile=coverage/coverage.out -covermode=atomic ./...
	@$(GO) tool cover -func=coverage/coverage.out | tail -1
	@echo ""
	@echo "Para ver relatÃ³rio HTML:"
	@echo "  go tool cover -html=coverage/coverage.out"

test-short:
	@echo "Executando testes curtos (sem integraÃ§Ã£o)..."
	$(GO) test -short -race -timeout 2m ./...
	@echo "âœ“ Testes curtos concluÃ­dos"

lint:
	@echo "Executando linters..."
	@which golangci-lint > /dev/null || (echo "golangci-lint nÃ£o instalado. Instale em https://golangci-lint.run/usage/install/" && exit 1)
	golangci-lint run ./...
	@echo "âœ“ Linting concluÃ­do"

# Development helpers
run: build
	@echo "Executando $(BUILD_DIR)/$(BINARY) analyze --repo-path ../.."
</file>
<file path="README.md">
# Gendocs - Automated Documentation Generation

Gendocs is a CLI application that leverages Large Language Models (LLMs) to analyze codebases and automatically generate documentation. It streamlines the process of creating and maintaining up-to-date documentation for your projects.

## Features

*   **Codebase Analysis:** Analyzes code structure, dependencies, data flow, and API definitions.
*   **Automated Documentation:** Generates README files and AI assistant rules.
*   **LLM Integration:** Supports multiple LLM providers (Anthropic, Gemini, OpenAI).
*   **Customizable Configuration:** Configure LLM providers, agents, and output formats.
*   **GitLab Integration:** Supports automated documentation updates via cronjobs.
*   **HTML Export:** Converts Markdown documentation to HTML with syntax highlighting.
*   **JSON Export:** Converts Markdown to structured JSON for programmatic access and integration.

## Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```
2.  **Install dependencies:**

    ```bash
    go mod download
    go mod vendor
    ```

## Quick Start

1.  **Analyze your codebase:**

    ```bash
    go run cmd/gendocs/main.go analyze --path <path_to_codebase>
    ```

    This command analyzes the codebase at the specified path and saves the analysis results in the `.ai/docs/` directory.
2.  **Generate a README file:**

    ```bash
    go run cmd/gendocs/main.go generate readme --output README.md
    ```

    This command generates a `README.md` file based on the analysis results.
3.  **(Optional) Generate AI rules:**

    ```bash
    go run cmd/gendocs/main.go generate ai_rules --output ai_rules.yaml
    ```

    This command generates an `ai_rules.yaml` file.
4. **(Optional) Export to HTML:**

    ```bash
    go run cmd/gendocs/main.go generate export --input README.md --output docs/index.html --format html
    ```

    This command converts the README.md file to HTML format with syntax highlighting.

5. **(Optional) Export to JSON:**

    ```bash
    go run cmd/gendocs/main.go generate export --input README.md --output docs.json --format json
    ```

    This command converts the README.md file to structured JSON format with metadata and hierarchical content. The JSON output includes:

    - **Metadata**: Document title, generation timestamp, generator info, word/character counts
    - **Headings**: Hierarchical tree structure for navigation and table of contents generation
    - **Elements**: All document elements (paragraphs, code blocks, lists, tables, blockquotes, links, images) in document order

    JSON export is ideal for:
    - Search indexing (Elasticsearch, Algolia)
    - Static site generators with custom templates
    - API documentation generation
    - Content analysis and migration
    - Integration with documentation portals

    For detailed JSON structure documentation, see [docs/JSON_FORMAT.md](docs/JSON_FORMAT.md).
    For examples, see [examples/json-export/](examples/json-export/).

## Architecture

Gendocs follows a clean architecture pattern, separating concerns into distinct layers:

*   **Command Layer (`cmd/`)**: Handles CLI command parsing and execution.
*   **Agent System (`internal/agents/`)**: Coordinates analysis and documentation generation using specialized agents.
*   **LLM Integration (`internal/llm/`)**: Provides a unified interface for interacting with different LLM providers.
*   **Configuration (`internal/config/`)**: Loads and manages application configuration.
*   **Handlers (`internal/handlers/`)**: Orchestrates the overall workflow for each command.

The CLI commands delegate to handlers, which in turn create and run agents. Agents use LLM clients and tools to perform their tasks. All components share configuration and logging.

## Performance Optimizations

Gendocs implements two key optimizations to significantly improve file scanning performance, especially for incremental analysis of large codebases:

### Selective Hashing

The file scanner uses modification time (mtime) and size-based caching to skip rehashing unchanged files:

- **How it works**: When scanning a repository, Gendocs stores each file's metadata (SHA256 hash, modification time, and size) in a cache file (`.ai/analysis_cache.json`). On subsequent scans, files with matching mtime and size skip the expensive SHA256 hash computation and reuse the cached hash value.

- **Performance impact**: For incremental scans where most files haven't changed, this can reduce scan time by 80-95% since hash computation is avoided for unchanged files.

- **Cache hit conditions**: A file is considered unchanged if **both** the modification time and size match the cached values. Using both conditions provides robust change detection while avoiding false positives.

### Parallel Hashing

When files do need hashing, they are processed concurrently using a worker pool pattern:

- **How it works**: Files requiring hash computation are distributed across multiple worker goroutines (default: number of CPU cores, max 8). Each worker independently computes SHA256 hashes, allowing the CPU-bound work to proceed in parallel.

- **Performance impact**: Parallel hashing provides 2-4x speedup for the actual hash computation phase on multi-core systems. The combined effect of selective hashing + parallel processing can provide 3-5x faster incremental scans on large repositories.

### Configuration

The parallel hashing behavior can be configured via:

**Configuration file** (`.ai/config.yaml`):
```yaml
analyzer:
  max_hash_workers: 4  # Number of parallel hash workers (0 = auto-detect)
```

**Environment variable**:
```bash
export GENDOCS_ANALYZER_MAX_HASH_WORKERS=4
```

**Values**:
- `0` (default): Auto-detect using `runtime.NumCPU()`, capped at 8 workers
- `1`: Sequential hashing (no parallelism)
- `2-8`: Specify exact number of parallel workers

**Note**: All values are capped at 8 to avoid overwhelming the filesystem.

### Metrics and Monitoring

The analyzer logs cache hit/miss metrics after each scan to help track optimization effectiveness:

```
DEBUG Scan complete: total=1500 files, cached=1420 (94.7%), hashed=80 (5.3%)
```

- **Cached files**: Files that skipped hashing due to cache hits (mtime+size match)
- **Hashed files**: Files that required new hash computation (cache misses)
- **Cache hit rate**: Higher percentages indicate more effective optimization

### Benchmark Results

Based on typical repository structures (1000+ files, ~50KB average):

| Scenario | Time | Speedup |
|----------|------|---------|
| Baseline (no cache, sequential) | 100% | 1x |
| With cache only | 10-20% | 5-10x |
| With parallel only | 30-50% | 2-3x |
| With cache + parallel (incremental) | 5-15% | 7-20x |

*Actual results vary based on file sizes, change frequency, and hardware.*

## Development

### Running Tests

```bash
go test ./...
```

### Linting

```bash
# Install golangci-lint (if not already installed)
# example:  brew install golangci/tap/golangci-lint
golangci-lint run
```

### Building

```bash
go build -o gendocs cmd/gendocs/main.go
```

This command builds an executable file named `gendocs` in the current directory.

## Configuration

Gendocs uses environment variables and configuration files to manage its settings.

*   **Configuration File:**  The application uses `viper` to load configurations. Configuration files can be in YAML format.
*   **Environment Variables:** Environment variables can override settings defined in the configuration file.

### Cache Configuration

Gendocs supports caching LLM API responses to reduce API costs and improve performance on repeated analyses. Cache settings are configured under the `llm.cache` section in your configuration file (`.ai/config.yaml`):

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `enabled` | boolean | `false` | Enable or disable LLM response caching. When enabled, identical LLM requests will return cached responses instead of making new API calls. |
| `max_size` | integer | `1000` | Maximum number of entries to store in the in-memory cache. When this limit is reached, least-recently-used entries are automatically evicted. |
| `ttl` | integer | `7` | Time-to-live for cache entries in days. After this period, cached responses are considered stale and will not be used. |
| `cache_path` | string | `.ai/llm_cache.json` | Path to the persistent disk cache file. This file survives program restarts and allows cache entries to be reused across multiple runs. |

#### Example Configuration

```yaml
llm:
  provider: openai
  model: gpt-4
  api_key: your-api-key
  cache:
    enabled: true
    max_size: 1000
    ttl: 7
    cache_path: .ai/llm_cache.json
```

#### Cache Management Commands

Gendocs provides CLI commands to manage the LLM response cache:

*   **View cache statistics:**
    ```bash
    gendocs cache-stats
    ```
    Displays cache hit/miss rates, number of entries, storage size, and other metrics.

*   **Clear the cache:**
    ```bash
    gendocs cache-clear
    ```
    Removes all cached responses, forcing the next run to make fresh API calls.

*   **Show cache stats after analysis:**
    ```bash
    gendocs analyze --show-cache-stats
    ```
    Displays cache statistics immediately after completing an analysis.

#### Benefits of Caching

*   **Cost Savings:** Avoid redundant API calls for identical requests, significantly reducing LLM API costs.
*   **Faster Iteration:** Repeated analyses with unchanged code complete much faster by serving responses from cache.
*   **Offline Capability:** Cached responses allow some operations to complete even without API access.

Refer to the `internal/config/` package for details on available configuration options.  Example environment variables used include those needed to authenticate with LLM providers (e.g., OpenAI API key).

## Documentation

For more detailed documentation on specific features:

- **[JSON Format Guide](docs/JSON_FORMAT.md)**: Complete JSON export structure documentation, including element types, usage examples in JavaScript/Python/bash, best practices, and troubleshooting
- **[JSON Export Examples](examples/json-export/)**: Comprehensive examples demonstrating all JSON exporter features with practical code samples
- **[Export Guide](docs/EXPORT.md)**: General export documentation covering all export formats

## Contributing

Contributions are welcome! Please follow these guidelines:

1.  Fork the repository.
2.  Create a new branch for your feature or bug fix.
3.  Write tests for your changes.
4.  Submit a pull request.

## License

This project is licensed under the [MIT License](LICENSE).
</file>
<file path="uninstall.sh">
#!/bin/bash
# Script de desinstalaÃ§Ã£o para Gendocs
# Uso: sudo ./uninstall.sh

set -e

BINARY_NAME="gendocs"
BIN_DIR="/usr/local/bin"
CONFIG_DIR="$HOME/.gendocs.yaml"

echo "=== Gendocs Uninstallation Script ==="
echo ""

# Remove binary
echo "Removendo binÃ¡rio de $BIN_DIR..."
if [ -f "$BIN_DIR/$BINARY_NAME" ]; then
    sudo rm -f "$BIN_DIR/$BINARY_NAME"
    echo "âœ… BinÃ¡rio removido"
else
    echo "âš ï¸  BinÃ¡rio nÃ£o encontrado em $BIN_DIR/$BINARY_NAME"
fi

# Ask about config
echo ""
read -p "Remover configuraÃ§Ã£o em $CONFIG_DIR? (y/N) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    rm -f "$CONFIG_DIR"
    echo "âœ… ConfiguraÃ§Ã£o removida"
fi

echo ""
echo "DesinstalaÃ§Ã£o completa!"
echo ""
echo "Para reinstalar:"
echo "  make install"
echo "  ou"
echo "  ./install.sh"
</file>
