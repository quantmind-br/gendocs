# Contexto

Você é um **Arquiteto de Software Sênior e Gerente de Produto** ajudando na evolução de um código existente.

**Data Atual:** 2025-12-23 08:12:21

---

# Descrição da Tarefa

Analise a base de código fornecida e sugira novas funcionalidades (features) para a aplicação.

Suas sugestões devem ser **estritamente baseadas nos materiais do projeto fornecidos** (estrutura de arquivos e trechos de código). As propostas devem ser entregues em um formato **priorizado e acionável**.

---

# Regras e Restrições do Projeto

1. **IDIOMA OBRIGATÓRIO:** Sempre responda em **Português do Brasil (pt-BR)**.
2. Retorne apenas o que foi solicitado. A saída deve estar contida em **um único bloco de código** contendo apenas o conteúdo da resposta (sem prosa introdutória ou conclusiva fora do bloco).
3. **Não invente fatos sobre o projeto.** Se algo não estiver presente nos materiais fornecidos, rotule explicitamente como **[SUPOSIÇÃO]** e não afirme como fato.
4. **Evite o óbvio:** Não sugira melhorias genéricas de infraestrutura (ex: "Adicionar Testes Unitários", "Melhorar Logging") a menos que estejam atreladas a uma nova funcionalidade de negócio específica. Foque em valor para o usuário ou arquitetura crítica.
5. Fundamente as recomendações em evidências do repositório:
   - Para cada funcionalidade proposta, inclua **Evidência (arquivos/pastas)** referenciando caminhos de `└── gendocs/
    ├── build/
    ├── cmd/
    │   ├── analyze.go [3.1KB]
    │   ├── config.go [1.9KB]
    │   ├── cronjob.go [3.8KB]
    │   ├── generate.go [4.9KB]
    │   └── root.go [800B]
    ├── internal/
    │   ├── agents/
    │   │   ├── sub_agents/
    │   │   ├── analyzer.go [7.8KB]
    │   │   ├── base.go [4.7KB]
    │   │   ├── factory.go [3.3KB]
    │   │   └── sub_agents.go [2.9KB]
    │   ├── config/
    │   │   ├── loader.go [8.3KB]
    │   │   └── models.go [4.6KB]
    │   ├── errors/
    │   │   ├── agent.go [3.5KB]
    │   │   ├── base.go [1.5KB]
    │   │   ├── config.go [3.0KB]
    │   │   ├── context.go [1.6KB]
    │   │   ├── exit_codes.go [392B]
    │   │   ├── gitlab.go [2.7KB]
    │   │   ├── handler.go [2.4KB]
    │   │   └── validation.go [2.7KB]
    │   ├── gitlab/
    │   │   └── client.go [4.7KB]
    │   ├── handlers/
    │   │   ├── ai_rules.go [1.5KB]
    │   │   ├── analyze.go [2.1KB]
    │   │   ├── base.go [619B]
    │   │   ├── cronjob.go [6.5KB]
    │   │   └── readme.go [1.5KB]
    │   ├── llm/
    │   │   ├── anthropic.go [7.2KB]
    │   │   ├── client.go [1.6KB]
    │   │   ├── factory.go [803B]
    │   │   ├── gemini.go [8.4KB]
    │   │   ├── openai.go [6.9KB]
    │   │   └── retry_client.go [3.9KB]
    │   ├── logging/
    │   │   └── logger.go [3.7KB]
    │   ├── prompts/
    │   │   └── manager.go [4.5KB]
    │   ├── tools/
    │   │   ├── base.go [1.7KB]
    │   │   ├── file_read.go [2.9KB]
    │   │   └── list_files.go [1.8KB]
    │   ├── tui/
    │   │   └── config.go [8.0KB]
    │   └── worker_pool/
    │       └── pool.go [1.5KB]
    ├── pkg/
    ├── prompts/
    │   ├── ai_rules_generator.yaml [2.5KB]
    │   ├── analyzer.yaml [6.2KB]
    │   └── documenter.yaml [1.8KB]
    ├── INSTALL.md [7.0KB]
    ├── Makefile [2.0KB]
    ├── README.md [6.2KB]
    ├── README_OLD.md [5.4KB]
    ├── go.mod [2.0KB]
    ├── go.sum [9.3KB]
    ├── install.sh [1.5KB]
    ├── main.go [83B]
    └── uninstall.sh [795B]

<file path="cmd/analyze.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/handlers"
	"github.com/user/gendocs/internal/logging"
)

var (
	repoPath            string
	excludeStructure    bool
	excludeDataFlow     bool
	excludeDeps         bool
	excludeReqFlow      bool
	excludeAPI          bool
	maxWorkers          int
)

// analyzeCmd represents the analyze command
var analyzeCmd = &cobra.Command{
	Use:   "analyze",
	Short: "Analyze codebase structure and dependencies",
	Long: `Analyze the codebase to generate detailed documentation about:
  - Code structure and architecture
  - Dependencies and imports
  - Data flow through the system
  - Request/response flow
  - API endpoints and contracts

Results are written to .ai/docs/ directory.`,
	RunE: runAnalyze,
}

func init() {
	rootCmd.AddCommand(analyzeCmd)

	analyzeCmd.Flags().StringVar(&repoPath, "repo-path", ".", "Path to repository")
	analyzeCmd.Flags().BoolVar(&excludeStructure, "exclude-code-structure", false, "Exclude structure analysis")
	analyzeCmd.Flags().BoolVar(&excludeDataFlow, "exclude-data-flow", false, "Exclude data flow analysis")
	analyzeCmd.Flags().BoolVar(&excludeDeps, "exclude-dependencies", false, "Exclude dependency analysis")
	analyzeCmd.Flags().BoolVar(&excludeReqFlow, "exclude-request-flow", false, "Exclude request flow analysis")
	analyzeCmd.Flags().BoolVar(&excludeAPI, "exclude-api-analysis", false, "Exclude API analysis")
	analyzeCmd.Flags().IntVar(&maxWorkers, "max-workers", 0, "Maximum concurrent workers (0=auto)")
}

func runAnalyze(cmd *cobra.Command, args []string) error {
	// Build CLI overrides map
	cliOverrides := map[string]interface{}{
		"repo_path":              repoPath,
		"exclude_code_structure": excludeStructure,
		"exclude_data_flow":      excludeDataFlow,
		"exclude_dependencies":   excludeDeps,
		"exclude_request_flow":   excludeReqFlow,
		"exclude_api_analysis":   excludeAPI,
		"max_workers":            maxWorkers,
		"debug":                  debugFlag,
	}

	// Load configuration
	cfg, err := config.LoadAnalyzerConfig(repoPath, cliOverrides)
	if err != nil {
		return fmt.Errorf("failed to load configuration: %w", err)
	}

	// Initialize logger
	logDir := ".ai/logs"
	if cfg.RepoPath != "." {
		logDir = cfg.RepoPath + "/.ai/logs"
	}
	logCfg := &logging.Config{
		LogDir:       logDir,
		FileLevel:    logging.LevelFromString("info"),
		ConsoleLevel: logging.LevelFromString("debug"),
		EnableCaller: debugFlag,
	}

	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		return fmt.Errorf("failed to initialize logger: %w", err)
	}
	defer logger.Sync()

	logger.Info("Starting gendocs analyze",
		logging.String("repo_path", cfg.RepoPath),
		logging.Int("max_workers", cfg.MaxWorkers),
	)

	// Create and run AnalyzeHandler
	handler := handlers.NewAnalyzeHandler(*cfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		// Handle error with proper exit code
		if docErr, ok := err.(*errors.AIDocGenError); ok {
			fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
			return docErr
		}
		return err
	}

	logger.Info("Analysis complete")
	return nil
}
</file>
<file path="cmd/config.go">
package cmd

import (
	"fmt"
	"os"

	tea "github.com/charmbracelet/bubbletea"
	"github.com/charmbracelet/bubbles/textinput"
	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/tui"
)

// configCmd represents the config command
var configCmd = &cobra.Command{
	Use:   "config",
	Short: "Configure gendocs settings",
	Long: `Launch an interactive configuration wizard to set up your LLM provider,
API key, and model preferences. Configuration is saved to ~/.gendocs.yaml.`,
	RunE: runConfig,
}

func init() {
	rootCmd.AddCommand(configCmd)
}

func runConfig(cmd *cobra.Command, args []string) error {
	// Initialize text inputs
	apiKeyInput := textinput.New()
	apiKeyInput.Placeholder = "Enter your API key"
	apiKeyInput.EchoMode = textinput.EchoPassword
	apiKeyInput.EchoCharacter = '•'
	apiKeyInput.Focus()

	modelInput := textinput.New()
	modelInput.Placeholder = "Press Enter for default model"

	baseURLInput := textinput.New()
	baseURLInput.Placeholder = "https://api.example.com (optional)"

	// Initialize Bubble Tea model
	model := tui.Model{
		Step:         0,
		Provider:     "",
		Model:        "",
		BaseURL:      "",
		Quitting:     false,
		APIKeyInput:  apiKeyInput,
		ModelInput:   modelInput,
		BaseURLInput: baseURLInput,
	}

	// Start Bubble Tea program
	p := tea.NewProgram(
		model,
		tea.WithAltScreen(),       // Use full screen mode
		tea.WithMouseCellMotion(), // Enable mouse motion
	)

	finalModel, err := p.Run()
	if err != nil {
		return fmt.Errorf("error running config wizard: %w", err)
	}

	// Type assertion to get our model back
	m, ok := finalModel.(tui.Model)
	if !ok {
		return fmt.Errorf("unexpected model type")
	}

	// Show final status
	if m.Err != nil {
		fmt.Fprintf(os.Stderr, "\nError: %v\n", m.Err)
		return m.Err
	}

	if m.SavedConfig {
		fmt.Printf("\nConfiguration saved to: %s\n", m.GetConfigPath())
		fmt.Println("\nYou can now run:")
		fmt.Println("  gendocs analyze --repo-path .")
	}

	return nil
}
</file>
<file path="cmd/cronjob.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/handlers"
	"github.com/user/gendocs/internal/logging"
)

// cronjobCmd represents the cronjob command
var cronjobCmd = &cobra.Command{
	Use:   "cronjob",
	Short: "Automated batch processing for GitLab projects",
	Long: `Process multiple GitLab projects automatically, analyzing each
and creating merge requests with the results.`,
}

var (
	cronjobMaxDays    int
	cronjobWorkingPath string
	cronjobGroupID     int
)

// cronjobAnalyzeCmd represents the cronjob analyze command
var cronjobAnalyzeCmd = &cobra.Command{
	Use:   "analyze",
	Short: "Analyze all applicable GitLab projects in a group",
	Long: `Fetch all projects in a GitLab group, filter them based on
configuration, and run analysis on each applicable project.
Creates branches and merge requests automatically.`,
	RunE: runCronjobAnalyze,
}

func init() {
	rootCmd.AddCommand(cronjobCmd)
	cronjobCmd.AddCommand(cronjobAnalyzeCmd)

	cronjobAnalyzeCmd.Flags().IntVar(&cronjobMaxDays, "max-days-since-last-commit", 14, "Skip projects with no commits in N days")
	cronjobAnalyzeCmd.Flags().StringVar(&cronjobWorkingPath, "working-path", "./work", "Working directory for cloning repos")
	cronjobAnalyzeCmd.Flags().IntVar(&cronjobGroupID, "group-project-id", 0, "GitLab group/project ID to analyze")
	cronjobAnalyzeCmd.MarkFlagRequired("group-project-id")
}

func runCronjobAnalyze(cmd *cobra.Command, args []string) error {
	// Validate required GitLab configuration
	gitLabToken := os.Getenv("GITLAB_OAUTH_TOKEN")
	if gitLabToken == "" {
		return errors.NewMissingEnvVarError("GITLAB_OAUTH_TOKEN", "GitLab API authentication token")
	}

	gitLabURL := os.Getenv("GITLAB_API_URL")
	if gitLabURL == "" {
		gitLabURL = "https://gitlab.com"
	}

	// Build configurations
	cronjobCfg := config.CronjobConfig{
		MaxDaysSinceLastCommit: cronjobMaxDays,
		WorkingPath:            cronjobWorkingPath,
		GroupProjectID:         cronjobGroupID,
	}

	gitLabCfg := config.GitLabConfig{
		APIURL:      gitLabURL,
		OAuthToken:  gitLabToken,
		UserName:    os.Getenv("GITLAB_USER_NAME"),
		UserUsername: os.Getenv("GITLAB_USER_USERNAME"),
		UserEmail:   os.Getenv("GITLAB_USER_EMAIL"),
	}

	// Set defaults for GitLab user info
	if gitLabCfg.UserName == "" {
		gitLabCfg.UserName = "AI Analyzer"
	}
	if gitLabCfg.UserUsername == "" {
		gitLabCfg.UserUsername = "agent_doc"
	}

	// Analyzer configuration (from env vars with defaults for cronjob)
	analyzerCfg := config.AnalyzerConfig{
		BaseConfig: config.BaseConfig{
			Debug: debugFlag,
		},
		LLM: config.LLMConfig{
			Provider:    os.Getenv("ANALYZER_LLM_PROVIDER"),
			Model:       os.Getenv("ANALYZER_LLM_MODEL"),
			APIKey:      os.Getenv("ANALYZER_LLM_API_KEY"),
			BaseURL:     os.Getenv("ANALYZER_LLM_BASE_URL"),
			Retries:     2,
			Timeout:     180,
			MaxTokens:   8192,
			Temperature: 0.0,
		},
		MaxWorkers: 0, // Auto-detect
	}

	// Initialize logger
	logCfg := &logging.Config{
		LogDir:       cronjobWorkingPath + "/.ai/logs",
		FileLevel:    logging.LevelFromString("info"),
		ConsoleLevel: logging.LevelFromString("debug"),
		EnableCaller: debugFlag,
	}

	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		return fmt.Errorf("failed to initialize logger: %w", err)
	}
	defer logger.Sync()

	logger.Info("Starting cronjob analysis",
		logging.Int("group_id", cronjobGroupID),
		logging.Int("max_days", cronjobMaxDays),
	)

	// Create and run CronjobHandler
	handler := handlers.NewCronjobHandler(cronjobCfg, gitLabCfg, analyzerCfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		if docErr, ok := err.(*errors.AIDocGenError); ok {
			fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
			return docErr
		}
		return err
	}

	logger.Info("Cronjob analysis complete")
	return nil
}
</file>
<file path="cmd/generate.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/handlers"
	"github.com/user/gendocs/internal/logging"
)

// generateCmd represents the generate command
var generateCmd = &cobra.Command{
	Use:   "generate",
	Short: "Generate documentation from analysis results",
	Long:  `Generate documentation files (README.md, AI rules) from existing analysis results.`,
}

var (
	readmeRepoPath string
)

// readmeCmd represents the generate readme command
var readmeCmd = &cobra.Command{
	Use:   "readme",
	Short: "Generate README.md from analysis results",
	Long: `Generate a comprehensive README.md file based on existing analysis documents
in .ai/docs/. This synthesizes information from structure, dependency, data flow,
request flow, and API analyses into a user-friendly README.`,
	RunE: runReadme,
}

func init() {
	rootCmd.AddCommand(generateCmd)
	generateCmd.AddCommand(readmeCmd)

	readmeCmd.Flags().StringVar(&readmeRepoPath, "repo-path", ".", "Path to repository")
}

func runReadme(cmd *cobra.Command, args []string) error {
	// Build configuration
	cfg := config.DocumenterConfig{
		BaseConfig: config.BaseConfig{
			RepoPath: readmeRepoPath,
			Debug:    debugFlag,
		},
		LLM: config.LLMConfig{
			Provider:    os.Getenv("DOCUMENTER_LLM_PROVIDER"),
			Model:       os.Getenv("DOCUMENTER_LLM_MODEL"),
			APIKey:      os.Getenv("DOCUMENTER_LLM_API_KEY"),
			BaseURL:     os.Getenv("DOCUMENTER_LLM_BASE_URL"),
			Retries:     2,
			Timeout:     180,
			MaxTokens:   8192,
			Temperature: 0.0,
		},
	}

	// Set defaults from environment if not set
	if cfg.LLM.Provider == "" {
		cfg.LLM.Provider = os.Getenv("ANALYZER_LLM_PROVIDER")
	}
	if cfg.LLM.Model == "" {
		cfg.LLM.Model = os.Getenv("ANALYZER_LLM_MODEL")
	}
	if cfg.LLM.APIKey == "" {
		cfg.LLM.APIKey = os.Getenv("ANALYZER_LLM_API_KEY")
	}

	// Initialize logger
	logDir := ".ai/logs"
	if readmeRepoPath != "." {
		logDir = readmeRepoPath + "/.ai/logs"
	}
	logCfg := &logging.Config{
		LogDir:       logDir,
		FileLevel:    logging.LevelFromString("info"),
		ConsoleLevel: logging.LevelFromString("debug"),
		EnableCaller: debugFlag,
	}

	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		return fmt.Errorf("failed to initialize logger: %w", err)
	}
	defer logger.Sync()

	logger.Info("Starting README generation",
		logging.String("repo_path", readmeRepoPath),
	)

	// Create and run ReadmeHandler
	handler := handlers.NewReadmeHandler(cfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		if docErr, ok := err.(*errors.AIDocGenError); ok {
			fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
			return docErr
		}
		return err
	}

	logger.Info("README.md generation complete")
	return nil
}

// aiRulesCmd represents the generate ai-rules command
var aiRulesCmd = &cobra.Command{
	Use:   "ai-rules",
	Short: "Generate AI assistant configuration files",
	Long: `Generate AI assistant configuration files (CLAUDE.md, AGENTS.md, .cursor/rules/)
from existing analysis results. These files help AI coding assistants understand the project.`,
	RunE: runAIRules,
}

func init() {
	generateCmd.AddCommand(aiRulesCmd)
	aiRulesCmd.Flags().StringVar(&readmeRepoPath, "repo-path", ".", "Path to repository")
}

func runAIRules(cmd *cobra.Command, args []string) error {
	// Build configuration
	cfg := config.AIRulesConfig{
		BaseConfig: config.BaseConfig{
			RepoPath: readmeRepoPath,
			Debug:    debugFlag,
		},
		LLM: config.LLMConfig{
			Provider:    os.Getenv("AI_RULES_LLM_PROVIDER"),
			Model:       os.Getenv("AI_RULES_LLM_MODEL"),
			APIKey:      os.Getenv("AI_RULES_LLM_API_KEY"),
			BaseURL:     os.Getenv("AI_RULES_LLM_BASE_URL"),
			Retries:     2,
			Timeout:     240,
			MaxTokens:   8192,
			Temperature: 0.0,
		},
	}

	// Set defaults from environment if not set
	if cfg.LLM.Provider == "" {
		cfg.LLM.Provider = os.Getenv("ANALYZER_LLM_PROVIDER")
	}
	if cfg.LLM.Model == "" {
		cfg.LLM.Model = os.Getenv("ANALYZER_LLM_MODEL")
	}
	if cfg.LLM.APIKey == "" {
		cfg.LLM.APIKey = os.Getenv("ANALYZER_LLM_API_KEY")
	}

	// Initialize logger
	logDir := ".ai/logs"
	if readmeRepoPath != "." {
		logDir = readmeRepoPath + "/.ai/logs"
	}
	logCfg := &logging.Config{
		LogDir:       logDir,
		FileLevel:    logging.LevelFromString("info"),
		ConsoleLevel: logging.LevelFromString("debug"),
		EnableCaller: debugFlag,
	}

	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		return fmt.Errorf("failed to initialize logger: %w", err)
	}
	defer logger.Sync()

	logger.Info("Starting AI rules generation",
		logging.String("repo_path", readmeRepoPath),
	)

	// Create and run AIRulesHandler
	handler := handlers.NewAIRulesHandler(cfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		if docErr, ok := err.(*errors.AIDocGenError); ok {
			fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
			return docErr
		}
		return err
	}

	logger.Info("AI rules generation complete")
	return nil
}
</file>
<file path="cmd/root.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
)

var (
	debugFlag bool
)

// rootCmd represents the base command
var rootCmd = &cobra.Command{
	Use:   "gendocs",
	Short: "AI-powered code documentation generator",
	Long: `Generate comprehensive documentation for your codebase using AI.

Gendocs analyzes your codebase structure, dependencies, data flow, and APIs
to generate detailed documentation including README.md, AI assistant configs,
and more.`,
	Version: "2.0.0",
}

// Execute runs the root command
func Execute() {
	if err := rootCmd.Execute(); err != nil {
		fmt.Fprintf(os.Stderr, "Error: %v\n", err)
		os.Exit(1)
	}
}

func init() {
	// Persistent flags (available to all subcommands)
	rootCmd.PersistentFlags().BoolVar(&debugFlag, "debug", false, "Enable debug mode")
}
</file>
<file path="internal/agents/analyzer.go">
package agents

import (
	"context"
	"fmt"
	"path/filepath"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	"github.com/user/gendocs/internal/worker_pool"
)

// AnalyzerAgent orchestrates all sub-agents for code analysis
type AnalyzerAgent struct {
	config        config.AnalyzerConfig
	llmFactory    *llm.Factory
	promptManager *prompts.Manager
	logger        *logging.Logger
	workerPool    *worker_pool.WorkerPool
}

// AnalysisResult represents the result of an analysis
type AnalysisResult struct {
	Successful []string
	Failed     []FailedAnalysis
}

// FailedAnalysis represents a failed analysis
type FailedAnalysis struct {
	Name  string
	Error error
}

// NewAnalyzerAgent creates a new analyzer agent
func NewAnalyzerAgent(cfg config.AnalyzerConfig, promptManager *prompts.Manager, logger *logging.Logger) *AnalyzerAgent {
	// Create retry client
	retryClient := llm.NewRetryClient(llm.DefaultRetryConfig())

	// Create LLM factory
	factory := llm.NewFactory(retryClient)

	return &AnalyzerAgent{
		config:        cfg,
		llmFactory:    factory,
		promptManager: promptManager,
		logger:        logger,
		workerPool:    worker_pool.NewWorkerPool(cfg.MaxWorkers),
	}
}

// Run executes all sub-agents concurrently
func (aa *AnalyzerAgent) Run(ctx context.Context) (*AnalysisResult, error) {
	aa.logger.Info("Starting analysis",
		logging.String("repo_path", aa.config.RepoPath),
		logging.Int("max_workers", aa.config.MaxWorkers),
	)

	// Use the existing factory
	factory := aa.llmFactory

	// Build task list based on configuration
	var tasks []worker_pool.Task
	var outputPaths []string

	docsDir := filepath.Join(aa.config.RepoPath, ".ai", "docs")

	if !aa.config.ExcludeStructure {
		task, outputPath := aa.createTask(ctx, factory, "structure_analyzer", CreateStructureAnalyzer,
			filepath.Join(docsDir, "structure_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if !aa.config.ExcludeDeps {
		task, outputPath := aa.createTask(ctx, factory, "dependency_analyzer", CreateDependencyAnalyzer,
			filepath.Join(docsDir, "dependency_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if !aa.config.ExcludeDataFlow {
		task, outputPath := aa.createTask(ctx, factory, "data_flow_analyzer", CreateDataFlowAnalyzer,
			filepath.Join(docsDir, "data_flow_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if !aa.config.ExcludeReqFlow {
		task, outputPath := aa.createTask(ctx, factory, "request_flow_analyzer", CreateRequestFlowAnalyzer,
			filepath.Join(docsDir, "request_flow_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if !aa.config.ExcludeAPI {
		task, outputPath := aa.createTask(ctx, factory, "api_analyzer", CreateAPIAnalyzer,
			filepath.Join(docsDir, "api_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if len(tasks) == 0 {
		return nil, fmt.Errorf("no analysis tasks to run (all agents excluded)")
	}

	aa.logger.Info(fmt.Sprintf("Running %d analysis tasks concurrently", len(tasks)))

	// Execute all tasks concurrently
	results := aa.workerPool.Run(ctx, tasks)

	// Process results
	return aa.processResults(outputPaths, results), nil
}

// AgentCreator is a function that creates an agent
type AgentCreator func(llmCfg config.LLMConfig, repoPath string, factory *llm.Factory, promptMgr *prompts.Manager, logger *logging.Logger) (*SubAgent, error)

// createTask creates a task for the worker pool
func (aa *AnalyzerAgent) createTask(ctx context.Context, factory *llm.Factory, name string, creator AgentCreator, outputPath string) (worker_pool.Task, string) {
	task := func(ctx context.Context) (interface{}, error) {
		aa.logger.Info(fmt.Sprintf("Creating %s", name))

		// Create agent
		agent, err := creator(aa.config.LLM, aa.config.RepoPath, factory, aa.promptManager, aa.logger)
		if err != nil {
			return nil, fmt.Errorf("failed to create %s: %w", name, err)
		}

		// Run agent
		output, err := agent.Run(ctx)
		if err != nil {
			return nil, fmt.Errorf("%s failed: %w", name, err)
		}

		// Save output
		if err := agent.SaveOutput(output, outputPath); err != nil {
			return nil, fmt.Errorf("failed to save %s output: %w", name, err)
		}

		aa.logger.Info(fmt.Sprintf("%s completed successfully", name))
		return output, nil
	}

	return task, outputPath
}

// processResults processes worker pool results
func (aa *AnalyzerAgent) processResults(outputPaths []string, results []worker_pool.Result) *AnalysisResult {
	result := &AnalysisResult{
		Successful: []string{},
		Failed:     []FailedAnalysis{},
	}

	for i, r := range results {
		// Get agent name from output path
		name := filepath.Base(outputPaths[i])
		name = name[:len(name)-11] // Remove "_analysis.md"

		if r.Error != nil {
			result.Failed = append(result.Failed, FailedAnalysis{
				Name:  name,
				Error: r.Error,
			})
			aa.logger.Error(fmt.Sprintf("%s failed", name), logging.Error(r.Error))
		} else {
			result.Successful = append(result.Successful, name)
			aa.logger.Info(fmt.Sprintf("%s succeeded", name))
		}
	}

	aa.logger.Info(fmt.Sprintf("Analysis complete: %d/%d successful",
		len(result.Successful), len(result.Successful)+len(result.Failed)))

	return result
}

// DocumenterAgent generates README.md
type DocumenterAgent struct {
	config        config.DocumenterConfig
	promptManager *prompts.Manager
	logger        *logging.Logger
}

// NewDocumenterAgent creates a new documenter agent
func NewDocumenterAgent(cfg config.DocumenterConfig, promptManager *prompts.Manager, logger *logging.Logger) *DocumenterAgent {
	return &DocumenterAgent{
		config:        cfg,
		promptManager: promptManager,
		logger:        logger,
	}
}

// Run generates the README
func (da *DocumenterAgent) Run(ctx context.Context) error {
	retryClient := llm.NewRetryClient(llm.DefaultRetryConfig())
	factory := llm.NewFactory(retryClient)

	// Create documenter agent
	agent, err := CreateDocumenterAgent(da.config.LLM, da.config.RepoPath, factory, da.promptManager, da.logger)
	if err != nil {
		return fmt.Errorf("failed to create documenter agent: %w", err)
	}

	// Run agent
	output, err := agent.Run(ctx)
	if err != nil {
		return fmt.Errorf("documenter agent failed: %w", err)
	}

	// Save to README.md
	outputPath := filepath.Join(da.config.RepoPath, "README.md")
	if err := agent.SaveOutput(output, outputPath); err != nil {
		return err
	}

	da.logger.Info(fmt.Sprintf("README.md generated at %s", outputPath))
	return nil
}

// AIRulesGeneratorAgent generates AI assistant config files
type AIRulesGeneratorAgent struct {
	config        config.AIRulesConfig
	promptManager *prompts.Manager
	logger        *logging.Logger
}

// NewAIRulesGeneratorAgent creates a new AI rules generator agent
func NewAIRulesGeneratorAgent(cfg config.AIRulesConfig, promptManager *prompts.Manager, logger *logging.Logger) *AIRulesGeneratorAgent {
	return &AIRulesGeneratorAgent{
		config:        cfg,
		promptManager: promptManager,
		logger:        logger,
	}
}

// Run generates AI rules files
func (aa *AIRulesGeneratorAgent) Run(ctx context.Context) error {
	retryClient := llm.NewRetryClient(llm.DefaultRetryConfig())
	factory := llm.NewFactory(retryClient)

	// For now, generate CLAUDE.md
	agent, err := CreateAIRulesGeneratorAgent(aa.config.LLM, aa.config.RepoPath, factory, aa.promptManager, aa.logger)
	if err != nil {
		return fmt.Errorf("failed to create AI rules agent: %w", err)
	}

	// Run agent
	output, err := agent.Run(ctx)
	if err != nil {
		return fmt.Errorf("AI rules agent failed: %w", err)
	}

	// Save to CLAUDE.md
	outputPath := filepath.Join(aa.config.RepoPath, "CLAUDE.md")
	if err := agent.SaveOutput(output, outputPath); err != nil {
		return err
	}

	aa.logger.Info(fmt.Sprintf("CLAUDE.md generated at %s", outputPath))
	return nil
}
</file>
<file path="internal/agents/base.go">
package agents

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	"github.com/user/gendocs/internal/tools"
)

// Agent is the interface that all agents must implement
type Agent interface {
	// Run executes the agent and returns the generated output
	Run(ctx context.Context) (string, error)

	// Name returns the agent name
	Name() string
}

// BaseAgent provides common functionality for all agents
type BaseAgent struct {
	name          string
	llmClient     llm.LLMClient
	tools         []tools.Tool
	promptManager *prompts.Manager
	logger        *logging.Logger
	systemPrompt  string
	maxRetries    int
	maxTokens     int
	temperature   float64
}

// NewBaseAgent creates a new base agent
func NewBaseAgent(
	name string,
	llmClient llm.LLMClient,
	tools []tools.Tool,
	promptManager *prompts.Manager,
	logger *logging.Logger,
	systemPrompt string,
	maxRetries int,
) *BaseAgent {
	return &BaseAgent{
		name:          name,
		llmClient:     llmClient,
		tools:         tools,
		promptManager: promptManager,
		logger:        logger,
		systemPrompt:  systemPrompt,
		maxRetries:    maxRetries,
		maxTokens:     8192,
		temperature:   0.0,
	}
}

// SetMaxTokens sets the maximum tokens for LLM responses
func (ba *BaseAgent) SetMaxTokens(maxTokens int) {
	ba.maxTokens = maxTokens
}

// SetTemperature sets the temperature for LLM responses
func (ba *BaseAgent) SetTemperature(temperature float64) {
	ba.temperature = temperature
}

// RunOnce executes the agent once with the given user prompt
func (ba *BaseAgent) RunOnce(ctx context.Context, userPrompt string) (string, error) {
	// Build request
	messages := []llm.Message{
		{Role: "user", Content: userPrompt},
	}

	var conversationHistory []llm.Message

	// Tool calling loop
	for {
		req := llm.CompletionRequest{
			SystemPrompt: ba.systemPrompt,
			Messages:     append(conversationHistory, messages...),
			Tools:        ba.convertTools(),
			MaxTokens:    ba.maxTokens,
			Temperature:  ba.temperature,
		}

		ba.logger.Debug("Calling LLM",
			logging.String("agent", ba.name),
			logging.Int("tool_count", len(req.Tools)),
		)

		// Call LLM
		resp, err := ba.llmClient.GenerateCompletion(ctx, req)
		if err != nil {
			return "", fmt.Errorf("LLM call failed: %w", err)
		}

		ba.logger.Debug("LLM response received",
			logging.String("agent", ba.name),
			logging.Int("input_tokens", resp.Usage.InputTokens),
			logging.Int("output_tokens", resp.Usage.OutputTokens),
			logging.Int("tool_calls", len(resp.ToolCalls)),
		)

		// If no tool calls, return content
		if len(resp.ToolCalls) == 0 {
			return resp.Content, nil
		}

		// Add assistant response to conversation history
		conversationHistory = append(conversationHistory, llm.Message{
			Role:    "assistant",
			Content: resp.Content,
		})

		// Execute tool calls
		for _, toolCall := range resp.ToolCalls {
			tool := ba.findTool(toolCall.Name)
			if tool == nil {
				ba.logger.Warn("Tool not found", logging.String("tool", toolCall.Name))
				// Add error response
				conversationHistory = append(conversationHistory, llm.Message{
					Role:    "tool",
					Content: fmt.Sprintf("Error: Tool '%s' not found", toolCall.Name),
					ToolID:  toolCall.Name,
				})
				continue
			}

			ba.logger.Debug("Executing tool",
				logging.String("tool", tool.Name()),
				logging.String("agent", ba.name),
			)

			// Execute tool
			result, err := tool.Execute(ctx, toolCall.Arguments)
			if err != nil {
				ba.logger.Error("Tool execution failed",
					logging.String("tool", tool.Name()),
					logging.Error(err),
				)
				conversationHistory = append(conversationHistory, llm.Message{
					Role:    "tool",
					Content: fmt.Sprintf("Error: %v", err),
					ToolID:  toolCall.Name,
				})
			} else {
				conversationHistory = append(conversationHistory, llm.Message{
					Role:    "tool",
					Content: fmt.Sprintf("%v", result),
					ToolID:  toolCall.Name,
				})
			}
		}

		// Continue loop to get final response from LLM
		messages = []llm.Message{} // Clear, using conversationHistory now
	}
}

// convertTools converts agent tools to LLM tool definitions
func (ba *BaseAgent) convertTools() []llm.ToolDefinition {
	var toolDefs []llm.ToolDefinition
	for _, tool := range ba.tools {
		toolDefs = append(toolDefs, llm.ToolDefinition{
			Name:        tool.Name(),
			Description: tool.Description(),
			Parameters:  tool.Parameters(),
		})
	}
	return toolDefs
}

// findTool finds a tool by name
func (ba *BaseAgent) findTool(name string) tools.Tool {
	for _, tool := range ba.tools {
		if tool.Name() == name {
			return tool
		}
	}
	return nil
}

// Name returns the agent name
func (ba *BaseAgent) Name() string {
	return ba.name
}
</file>
<file path="internal/agents/factory.go">
package agents

import (
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// CreateStructureAnalyzer creates the structure analyzer sub-agent
func CreateStructureAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "StructureAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "structure_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateDependencyAnalyzer creates the dependency analyzer sub-agent
func CreateDependencyAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "DependencyAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "dependency_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateDataFlowAnalyzer creates the data flow analyzer sub-agent
func CreateDataFlowAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "DataFlowAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "data_flow_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateRequestFlowAnalyzer creates the request flow analyzer sub-agent
func CreateRequestFlowAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "RequestFlowAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "request_flow_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateAPIAnalyzer creates the API analyzer sub-agent
func CreateAPIAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "APIAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "api_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateDocumenterAgent creates the documenter agent (README generator)
func CreateDocumenterAgent(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "DocumenterAgent",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "documenter",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateAIRulesGeneratorAgent creates the AI rules generator agent
func CreateAIRulesGeneratorAgent(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "AIRulesGeneratorAgent",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "ai_rules",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}
</file>
<file path="internal/agents/sub_agents.go">
package agents

import (
	"context"
	"fmt"
	"os"
	"path/filepath"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	"github.com/user/gendocs/internal/tools"
)

// SubAgentConfig holds configuration for sub-agents
type SubAgentConfig struct {
	Name         string
	LLMConfig    config.LLMConfig
	RepoPath     string
	PromptSuffix string // e.g., "structure_analyzer"
}

// SubAgent is a specialized analysis agent
type SubAgent struct {
	*BaseAgent
	config SubAgentConfig
}

// NewSubAgent creates a new sub-agent
func NewSubAgent(cfg SubAgentConfig, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	// Create LLM client
	llmClient, err := llmFactory.CreateClient(cfg.LLMConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create LLM client: %w", err)
	}

	// Create tools
	toolList := []tools.Tool{
		tools.NewFileReadTool(2),
		tools.NewListFilesTool(2),
	}

	// Load system prompt
	systemPrompt, err := promptManager.Get(cfg.PromptSuffix + "_system")
	if err != nil {
		return nil, fmt.Errorf("failed to load system prompt: %w", err)
	}

	baseAgent := NewBaseAgent(
		cfg.Name,
		llmClient,
		toolList,
		promptManager,
		logger,
		systemPrompt,
		cfg.LLMConfig.GetRetries(),
	)

	return &SubAgent{
		BaseAgent: baseAgent,
		config:    cfg,
	}, nil
}

// Run executes the sub-agent
func (sa *SubAgent) Run(ctx context.Context) (string, error) {
	// Render user prompt with variables
	userPrompt, err := sa.promptManager.Render(sa.config.PromptSuffix+"_user", map[string]interface{}{
		"RepoPath": sa.config.RepoPath,
	})
	if err != nil {
		return "", fmt.Errorf("failed to render user prompt: %w", err)
	}

	// Run with retry logic
	var lastErr error
	for attempt := 0; attempt < sa.maxRetries; attempt++ {
		sa.logger.Info(fmt.Sprintf("Running sub-agent %s (attempt %d/%d)", sa.config.Name, attempt+1, sa.maxRetries))

		result, err := sa.RunOnce(ctx, userPrompt)
		if err == nil {
			sa.logger.Info(fmt.Sprintf("Sub-agent %s completed successfully", sa.config.Name))
			return result, nil
		}

		lastErr = err
		sa.logger.Warn(fmt.Sprintf("Sub-agent %s attempt %d failed: %v", sa.config.Name, attempt+1, err))
	}

	return "", fmt.Errorf("sub-agent %s failed after %d retries: %w", sa.config.Name, sa.maxRetries, lastErr)
}

// SaveOutput saves the agent output to a file
func (sa *SubAgent) SaveOutput(output, outputPath string) error {
	// Ensure directory exists
	dir := filepath.Dir(outputPath)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create directory: %w", err)
	}

	// Write output
	if err := os.WriteFile(outputPath, []byte(output), 0644); err != nil {
		return fmt.Errorf("failed to write output: %w", err)
	}

	sa.logger.Info(fmt.Sprintf("Output saved to %s", outputPath))
	return nil
}
</file>
<file path="internal/config/loader.go">
package config

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/joho/godotenv"
	"github.com/spf13/viper"
	"github.com/user/gendocs/internal/errors"
)

// Loader handles loading configuration from multiple sources
type Loader struct {
	v *viper.Viper
}

// NewLoader creates a new configuration loader
func NewLoader() *Loader {
	// Load .env file if exists
	_ = godotenv.Load()

	v := viper.New()
	v.SetConfigType("yaml")
	v.AutomaticEnv()
	v.SetEnvPrefix("GENDOCS")
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))

	return &Loader{v: v}
}

// LoadForAgent loads configuration for a specific agent section
// Precedence: CLI > .ai/config.yaml > ~/.gendocs.yaml > Environment > Defaults
func (l *Loader) LoadForAgent(repoPath, section string, cliOverrides map[string]interface{}) (*viper.Viper, error) {
	// 1. Load defaults (set via struct defaults)

	// 2. Load from ~/.gendocs.yaml (global user config)
	if err := l.loadGlobalConfig(); err != nil {
		return nil, err
	}

	// 3. Load from .ai/config.yaml (project-specific config)
	if err := l.loadProjectConfig(repoPath); err != nil {
		return nil, err
	}

	// 4. Apply CLI overrides
	if err := l.applyCLIOverrides(cliOverrides); err != nil {
		return nil, err
	}

	return l.v, nil
}

// loadGlobalConfig loads configuration from ~/.gendocs.yaml
func (l *Loader) loadGlobalConfig() error {
	homeDir, err := os.UserHomeDir()
	if err != nil {
		return nil // Not a fatal error
	}

	globalConfig := filepath.Join(homeDir, ".gendocs.yaml")
	if _, err := os.Stat(globalConfig); err != nil {
		return nil // File doesn't exist, skip
	}

	l.v.SetConfigFile(globalConfig)
	if err := l.v.ReadInConfig(); err != nil {
		return errors.NewConfigFileError(globalConfig, err)
	}

	return nil
}

// loadProjectConfig loads configuration from .ai/config.yaml
func (l *Loader) loadProjectConfig(repoPath string) error {
	if repoPath == "" {
		repoPath = "."
	}

	configPath := filepath.Join(repoPath, ".ai", "config.yaml")
	if _, err := os.Stat(configPath); err != nil {
		return nil // File doesn't exist, skip
	}

	l.v.SetConfigFile(configPath)
	if err := l.v.MergeInConfig(); err != nil {
		return errors.NewConfigFileError(configPath, err)
	}

	return nil
}

// applyCLIOverrides applies CLI flag overrides
func (l *Loader) applyCLIOverrides(overrides map[string]interface{}) error {
	for key, value := range overrides {
		// Only set if value is not nil/zero
		if value != nil {
			l.v.Set(key, value)
		}
	}
	return nil
}

// GetEnvVar gets an environment variable, returning an error if not set
func GetEnvVar(name, description string) (string, error) {
	value := os.Getenv(name)
	if value == "" {
		return "", errors.NewMissingEnvVarError(name, description)
	}
	return value, nil
}

// GetEnvVarOrDefault gets an environment variable with a default value
func GetEnvVarOrDefault(name, defaultValue string) string {
	value := os.Getenv(name)
	if value == "" {
		return defaultValue
	}
	return value
}

// MergeConfigs merges multiple configuration sources with precedence
// Precedence order (highest to lowest): cli, project, global, env, defaults
func MergeConfigs(repoPath string, section string, defaults interface{}, cliOverrides map[string]interface{}) (map[string]interface{}, error) {
	loader := NewLoader()

	// Load all config sources
	v, err := loader.LoadForAgent(repoPath, section, cliOverrides)
	if err != nil {
		return nil, err
	}

	// Get the section-specific config
	var sectionConfig map[string]interface{}
	if section != "" {
		sectionConfig = v.GetStringMap(section)
	} else {
		// Get all settings if no section specified
		sectionConfig = v.AllSettings()
	}

	// Apply CLI overrides (highest precedence)
	for key, value := range cliOverrides {
		if value != nil {
			// Convert key from snake_case to dot notation if needed
			sectionConfig[key] = value
		}
	}

	return sectionConfig, nil
}

// LoadAnalyzerConfig loads and validates analyzer configuration
func LoadAnalyzerConfig(repoPath string, cliOverrides map[string]interface{}) (*AnalyzerConfig, error) {
	configMap, err := MergeConfigs(repoPath, "analyzer", &AnalyzerConfig{}, cliOverrides)
	if err != nil {
		return nil, err
	}

	// Create config from map
	cfg := &AnalyzerConfig{
		BaseConfig: BaseConfig{
			RepoPath: getString(configMap, "repo_path", "."),
			Debug:    getBool(configMap, "debug", false),
		},
		MaxWorkers: getInt(configMap, "max_workers", 0),
	}

	// Load LLM config from environment or config
	cfg.LLM = LLMConfig{
		Provider:    getString(configMap, "llm.provider", getEnvOrDefault("ANALYZER_LLM_PROVIDER", "openai")),
		Model:       getString(configMap, "llm.model", getEnvOrDefault("ANALYZER_LLM_MODEL", "gpt-4o")),
		APIKey:      getString(configMap, "llm.api_key", getEnvOrDefault("ANALYZER_LLM_API_KEY", "")),
		BaseURL:     getString(configMap, "llm.base_url", getEnvOrDefault("ANALYZER_LLM_BASE_URL", "")),
		Retries:     getInt(configMap, "llm.retries", getEnvIntOrDefault("ANALYZER_AGENT_RETRIES", 2)),
		Timeout:     getInt(configMap, "llm.timeout", getEnvIntOrDefault("ANALYZER_LLM_TIMEOUT", 180)),
		MaxTokens:   getInt(configMap, "llm.max_tokens", getEnvIntOrDefault("ANALYZER_LLM_MAX_TOKENS", 8192)),
		Temperature: getFloat64(configMap, "llm.temperature", getEnvFloatOrDefault("ANALYZER_LLM_TEMPERATURE", 0.0)),
	}

	cfg.ExcludeStructure = getBool(configMap, "exclude_code_structure", false)
	cfg.ExcludeDataFlow = getBool(configMap, "exclude_data_flow", false)
	cfg.ExcludeDeps = getBool(configMap, "exclude_dependencies", false)
	cfg.ExcludeReqFlow = getBool(configMap, "exclude_request_flow", false)
	cfg.ExcludeAPI = getBool(configMap, "exclude_api_analysis", false)

	// Validate required fields
	if err := validateLLMConfig(&cfg.LLM, "ANALYZER"); err != nil {
		return nil, err
	}

	return cfg, nil
}

// Helper functions for type-safe config access

func getString(m map[string]interface{}, key, defaultValue string) string {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	if str, ok := val.(string); ok {
		return str
	}
	return defaultValue
}

func getInt(m map[string]interface{}, key string, defaultValue int) int {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	switch v := val.(type) {
	case int:
		return v
	case float64:
		return int(v)
	case string:
		// Try to parse string as int
		var i int
		if _, err := fmt.Sscanf(v, "%d", &i); err == nil {
			return i
		}
	}
	return defaultValue
}

func getBool(m map[string]interface{}, key string, defaultValue bool) bool {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	if b, ok := val.(bool); ok {
		return b
	}
	return defaultValue
}

func getFloat64(m map[string]interface{}, key string, defaultValue float64) float64 {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	if f, ok := val.(float64); ok {
		return f
	}
	return defaultValue
}

func getEnvOrDefault(key, defaultValue string) string {
	if val := os.Getenv(key); val != "" {
		return val
	}
	return defaultValue
}

func getEnvIntOrDefault(key string, defaultValue int) int {
	if val := os.Getenv(key); val != "" {
		var i int
		if _, err := fmt.Sscanf(val, "%d", &i); err == nil {
			return i
		}
	}
	return defaultValue
}

func getEnvFloatOrDefault(key string, defaultValue float64) float64 {
	if val := os.Getenv(key); val != "" {
		var f float64
		if _, err := fmt.Sscanf(val, "%f", &f); err == nil {
			return f
		}
	}
	return defaultValue
}

// validateLLMConfig validates LLM configuration
func validateLLMConfig(cfg *LLMConfig, prefix string) error {
	if cfg.APIKey == "" {
		return errors.NewMissingEnvVarError(prefix+"_LLM_API_KEY", "API key for LLM provider")
	}

	validProviders := map[string]bool{
		"openai":    true,
		"anthropic": true,
		"gemini":    true,
	}

	if !validProviders[cfg.Provider] {
		return errors.NewInvalidEnvVarError(prefix+"_LLM_PROVIDER", cfg.Provider, "Must be one of: openai, anthropic, gemini")
	}

	return nil
}
</file>
<file path="internal/config/models.go">
package config

import (
	"time"
)

// BaseConfig holds common configuration for all handlers
type BaseConfig struct {
	RepoPath string `mapstructure:"repo_path"`
	Debug    bool   `mapstructure:"debug"`
}

// LLMConfig holds LLM provider configuration
type LLMConfig struct {
	Provider    string  `mapstructure:"provider"`     // openai, anthropic, gemini
	Model       string  `mapstructure:"model"`
	APIKey      string  `mapstructure:"api_key"`
	BaseURL     string  `mapstructure:"base_url"`      // Optional, for OpenAI-compatible APIs
	Retries     int     `mapstructure:"retries"`
	Timeout     int     `mapstructure:"timeout"`       // Timeout in seconds
	MaxTokens   int     `mapstructure:"max_tokens"`
	Temperature float64 `mapstructure:"temperature"`
}

// GeminiConfig holds Gemini-specific configuration
type GeminiConfig struct {
	UseVertexAI bool   `mapstructure:"use_vertex_ai"`
	ProjectID   string `mapstructure:"project_id"`
	Location    string `mapstructure:"location"`
}

// RetryConfig holds HTTP retry configuration
type RetryConfig struct {
	MaxAttempts       int `mapstructure:"max_attempts"`        // Default: 5
	Multiplier        int `mapstructure:"multiplier"`          // Default: 1
	MaxWaitPerAttempt int `mapstructure:"max_wait_per_attempt"` // Default: 60 seconds
	MaxTotalWait      int `mapstructure:"max_total_wait"`      // Default: 300 seconds
}

// AnalyzerConfig holds configuration for the analyze command
type AnalyzerConfig struct {
	BaseConfig
	LLM               LLMConfig    `mapstructure:"llm"`
	ExcludeStructure  bool         `mapstructure:"exclude_code_structure"`
	ExcludeDataFlow   bool         `mapstructure:"exclude_data_flow"`
	ExcludeDeps       bool         `mapstructure:"exclude_dependencies"`
	ExcludeReqFlow    bool         `mapstructure:"exclude_request_flow"`
	ExcludeAPI        bool         `mapstructure:"exclude_api_analysis"`
	MaxWorkers        int          `mapstructure:"max_workers"`
	RetryConfig       RetryConfig  `mapstructure:"retry"`
}

// DocumenterConfig holds configuration for readme generation
type DocumenterConfig struct {
	BaseConfig
	LLM         LLMConfig   `mapstructure:"llm"`
	RetryConfig RetryConfig `mapstructure:"retry"`
}

// AIRulesConfig holds configuration for AI rules generation
type AIRulesConfig struct {
	BaseConfig
	LLM           LLMConfig   `mapstructure:"llm"`
	RetryConfig   RetryConfig `mapstructure:"retry"`
	MaxTokensMarkdown  int    `mapstructure:"max_tokens_markdown"`
	MaxTokensCursor    int    `mapstructure:"max_tokens_cursor"`
}

// CronjobConfig holds configuration for cronjob command
type CronjobConfig struct {
	MaxDaysSinceLastCommit int    `mapstructure:"max_days_since_last_commit"`
	WorkingPath            string `mapstructure:"working_path"`
	GroupProjectID         int    `mapstructure:"group_project_id"`
}

// GitLabConfig holds GitLab integration configuration
type GitLabConfig struct {
	APIURL     string `mapstructure:"api_url"`
	UserName   string `mapstructure:"user_name"`
	UserUsername string `mapstructure:"user_username"`
	UserEmail  string `mapstructure:"user_email"`
	OAuthToken string `mapstructure:"oauth_token"`
}

// LoggingConfig holds logging configuration
type LoggingConfig struct {
	LogDir       string `mapstructure:"log_dir"`
	FileLevel    string `mapstructure:"file_level"`    // debug, info, warn, error
	ConsoleLevel string `mapstructure:"console_level"` // debug, info, warn, error
}

// GlobalConfig holds top-level configuration from .ai/config.yaml
type GlobalConfig struct {
	Analyzer  AnalyzerConfig  `mapstructure:"analyzer"`
	Documenter DocumenterConfig `mapstructure:"documenter"`
	AIRules   AIRulesConfig   `mapstructure:"ai_rules"`
	Cronjob   CronjobConfig   `mapstructure:"cronjob"`
	GitLab    GitLabConfig    `mapstructure:"gitlab"`
	Gemini    GeminiConfig    `mapstructure:"gemini"`
	Logging   LoggingConfig   `mapstructure:"logging"`
}

// GetTimeout returns the timeout as a time.Duration
func (c *LLMConfig) GetTimeout() time.Duration {
	if c.Timeout == 0 {
		return 180 * time.Second // Default timeout
	}
	return time.Duration(c.Timeout) * time.Second
}

// GetMaxTokens returns the max tokens with a default
func (c *LLMConfig) GetMaxTokens() int {
	if c.MaxTokens == 0 {
		return 8192 // Default max tokens
	}
	return c.MaxTokens
}

// GetTemperature returns the temperature with a default
func (c *LLMConfig) GetTemperature() float64 {
	if c.Temperature == 0 {
		return 0.0 // Default temperature for deterministic output
	}
	return c.Temperature
}

// GetRetries returns the retry count with a default
func (c *LLMConfig) GetRetries() int {
	if c.Retries == 0 {
		return 2 // Default retries
	}
	return c.Retries
}
</file>
<file path="internal/errors/agent.go">
package errors

import (
	"fmt"
)

// AgentError is the base error for all agent-related errors
type AgentError struct {
	*AIDocGenError
}

// NewAgentError creates a new agent error
func NewAgentError(message string) *AgentError {
	return &AgentError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitAgentError,
		},
	}
}

// LLMConnectionError is raised when connection to LLM provider fails
type LLMConnectionError struct {
	*AIDocGenError
}

// NewLLMConnectionError creates a new LLM connection error
func NewLLMConnectionError(provider string, cause error) *LLMConnectionError {
	return &LLMConnectionError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to connect to LLM provider: %s", provider),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "LLM API Call",
				Component: "LLM Client",
				Details: map[string]interface{}{
					"provider": provider,
				},
				Suggestions: []string{
					"Check your internet connection",
					"Verify the API endpoint is accessible",
					"Check if the API key is valid",
					"Try again later (service may be unavailable)",
				},
				Recoverable: true,
			},
			ExitCode: ExitLLMError,
		},
	}
}

// LLMResponseError is raised when LLM response is invalid or cannot be parsed
type LLMResponseError struct {
	*AIDocGenError
}

// NewLLMResponseError creates a new LLM response error
func NewLLMResponseError(provider, reason string) *LLMResponseError {
	return &LLMResponseError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Invalid response from LLM provider: %s", provider),
			Context: &ErrorContext{
				Operation: "Parsing LLM Response",
				Component: "LLM Client",
				Details: map[string]interface{}{
					"provider": provider,
					"reason":   reason,
				},
				Suggestions: []string{
					"Check if the model name is correct",
					"Try a different model",
					"Report this issue if it persists",
				},
				Recoverable: true,
			},
			ExitCode: ExitLLMError,
		},
	}
}

// AgentTimeoutError is raised when an agent execution times out
type AgentTimeoutError struct {
	*AIDocGenError
}

// NewAgentTimeoutError creates a new agent timeout error
func NewAgentTimeoutError(agentName string, timeoutSeconds int) *AgentTimeoutError {
	return &AgentTimeoutError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Agent '%s' timed out after %d seconds", agentName, timeoutSeconds),
			Context: &ErrorContext{
				Operation: "Agent Execution",
				Component: agentName,
				Details: map[string]interface{}{
					"timeout_seconds": timeoutSeconds,
				},
				Suggestions: []string{
					"Increase the timeout via LLM_TIMEOUT environment variable",
					"Try reducing the size of the codebase",
					"Try a faster model",
				},
				Recoverable: false,
			},
			ExitCode: ExitAgentError,
		},
	}
}

// ToolExecutionError is raised when a tool execution fails
type ToolExecutionError struct {
	*AIDocGenError
}

// NewToolExecutionError creates a new tool execution error
func NewToolExecutionError(toolName string, cause error) *ToolExecutionError {
	return &ToolExecutionError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Tool '%s' execution failed", toolName),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Tool Execution",
				Component: toolName,
				Details: map[string]interface{}{
					"tool": toolName,
				},
				Suggestions: []string{
					"Check if the file/directory exists",
					"Verify file permissions",
					"Check the error details above",
				},
				Recoverable: true,
			},
			ExitCode: ExitAgentError,
		},
	}
}
</file>
<file path="internal/errors/base.go">
package errors

import (
	"fmt"
)

// AIDocGenError is the base error type for all application errors
type AIDocGenError struct {
	Message  string         // Human-readable error message
	Context  *ErrorContext  // Rich error context
	Cause    error          // Underlying error (for wrapping)
	ExitCode ExitCode       // Exit code for CLI
}

// Error returns the error message
func (e *AIDocGenError) Error() string {
	return e.Message
}

// Unwrap returns the underlying cause
func (e *AIDocGenError) Unwrap() error {
	return e.Cause
}

// GetUserMessage returns a user-friendly error message with context
func (e *AIDocGenError) GetUserMessage() string {
	msg := fmt.Sprintf("ERROR: %s", e.Message)

	if e.Context != nil {
		msg += e.Context.Format()
	}

	return msg
}

// NewError creates a new AIDocGenError with the given message and exit code
func NewError(message string, exitCode ExitCode) *AIDocGenError {
	return &AIDocGenError{
		Message:  message,
		ExitCode: exitCode,
	}
}

// WrapError wraps an existing error with additional context
func WrapError(cause error, message string, exitCode ExitCode) *AIDocGenError {
	return &AIDocGenError{
		Message:  message,
		Cause:    cause,
		ExitCode: exitCode,
	}
}

// WrapErrorWithContext wraps an error with full context
func WrapErrorWithContext(cause error, message string, exitCode ExitCode, context *ErrorContext) *AIDocGenError {
	return &AIDocGenError{
		Message:  message,
		Context:  context,
		Cause:    cause,
		ExitCode: exitCode,
	}
}
</file>
<file path="internal/errors/config.go">
package errors

import (
	"fmt"
)

// ConfigurationError is raised when configuration is invalid or missing
type ConfigurationError struct {
	*AIDocGenError
}

// NewConfigurationError creates a new configuration error
func NewConfigurationError(message string) *ConfigurationError {
	return &ConfigurationError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitConfigError,
		},
	}
}

// MissingEnvVarError is raised when a required environment variable is not set
type MissingEnvVarError struct {
	*AIDocGenError
}

// NewMissingEnvVarError creates a new missing environment variable error
func NewMissingEnvVarError(varName, description string) *MissingEnvVarError {
	return &MissingEnvVarError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Required environment variable '%s' is not set", varName),
			Context: &ErrorContext{
				Operation: "Loading configuration",
				Component: "Environment",
				Details: map[string]interface{}{
					"variable":     varName,
					"description":  description,
				},
				Suggestions: []string{
					fmt.Sprintf("Set the %s environment variable in your .env file", varName),
					fmt.Sprintf("Export the variable: export %s='your-value'", varName),
					"Check .env.example for required variables",
				},
				Recoverable: false,
			},
			ExitCode: ExitConfigError,
		},
	}
}

// InvalidEnvVarError is raised when an environment variable has an invalid value
type InvalidEnvVarError struct {
	*AIDocGenError
}

// NewInvalidEnvVarError creates a new invalid environment variable error
func NewInvalidEnvVarError(varName, value, reason string) *InvalidEnvVarError {
	return &InvalidEnvVarError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Environment variable '%s' has an invalid value", varName),
			Context: &ErrorContext{
				Operation: "Validating configuration",
				Component: "Environment",
				Details: map[string]interface{}{
					"variable": varName,
					"value":    value,
					"reason":   reason,
				},
				Suggestions: []string{
					fmt.Sprintf("Check the value of %s in your .env file", varName),
					"Refer to the documentation for valid values",
				},
				Recoverable: false,
			},
			ExitCode: ExitConfigError,
		},
	}
}

// ConfigFileError is raised when a configuration file cannot be read or parsed
type ConfigFileError struct {
	*AIDocGenError
}

// NewConfigFileError creates a new config file error
func NewConfigFileError(filePath string, cause error) *ConfigFileError {
	return &ConfigFileError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to load configuration file: %s", filePath),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Loading configuration",
				Component: "Config File",
				Details: map[string]interface{}{
					"file_path": filePath,
				},
				Suggestions: []string{
					"Check that the file exists and is readable",
					"Validate YAML syntax",
					"Check file permissions",
				},
				Recoverable: false,
			},
			ExitCode: ExitConfigError,
		},
	}
}
</file>
<file path="internal/errors/context.go">
package errors

import (
	"fmt"
	"strings"
)

// ErrorContext provides rich error information for user-friendly error messages
type ErrorContext struct {
	Operation   string                 // The operation that failed
	Component   string                 // The component that failed
	Details     map[string]interface{} // Additional details about the error
	Suggestions []string               // Actionable suggestions for the user
	Recoverable bool                   // Whether the error is recoverable
	RetryCount  int                    // Current retry count
	MaxRetries  int                    // Maximum retries allowed
}

// Format returns a formatted string representation of the error context
func (ec *ErrorContext) Format() string {
	var sb strings.Builder

	if ec.Operation != "" || ec.Component != "" {
		sb.WriteString("\nWhat happened:\n")
		if ec.Operation != "" && ec.Component != "" {
			sb.WriteString(fmt.Sprintf("  %s failed in %s.\n", ec.Operation, ec.Component))
		} else if ec.Operation != "" {
			sb.WriteString(fmt.Sprintf("  %s failed.\n", ec.Operation))
		} else if ec.Component != "" {
			sb.WriteString(fmt.Sprintf("  Failure in %s.\n", ec.Component))
		}
	}

	if len(ec.Details) > 0 {
		sb.WriteString("\nDetails:\n")
		for key, value := range ec.Details {
			sb.WriteString(fmt.Sprintf("  - %s: %v\n", key, value))
		}
	}

	if len(ec.Suggestions) > 0 {
		sb.WriteString("\nWhat you can do:\n")
		for i, suggestion := range ec.Suggestions {
			sb.WriteString(fmt.Sprintf("  %d. %s\n", i+1, suggestion))
		}
	}

	if ec.Recoverable {
		sb.WriteString(fmt.Sprintf("\nRecoverable: Yes (retry %d/%d)\n", ec.RetryCount, ec.MaxRetries))
	}

	return sb.String()
}
</file>
<file path="internal/errors/exit_codes.go">
package errors

type ExitCode int

const (
	ExitSuccess        ExitCode = 0
	ExitGeneralError   ExitCode = 1
	ExitConfigError    ExitCode = 2
	ExitValidationError ExitCode = 3
	ExitLLMError       ExitCode = 4
	ExitAgentError     ExitCode = 5
	ExitIOError        ExitCode = 6
	ExitGitLabError    ExitCode = 7
	ExitPartialSuccess ExitCode = 10
)

func (e ExitCode) Int() int {
	return int(e)
}
</file>
<file path="internal/errors/gitlab.go">
package errors

import (
	"fmt"
)

// GitLabError is the base error for all GitLab-related errors
type GitLabError struct {
	*AIDocGenError
}

// NewGitLabError creates a new GitLab error
func NewGitLabError(message string) *GitLabError {
	return &GitLabError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitGitLabError,
		},
	}
}

// GitLabAuthError is raised when GitLab authentication fails
type GitLabAuthError struct {
	*AIDocGenError
}

// NewGitLabAuthError creates a new GitLab authentication error
func NewGitLabAuthError(cause error) *GitLabAuthError {
	return &GitLabAuthError{
		AIDocGenError: &AIDocGenError{
			Message: "GitLab authentication failed",
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "GitLab Authentication",
				Component: "GitLab Client",
				Suggestions: []string{
					"Verify GITLAB_OAUTH_TOKEN is set correctly",
					"Check if the token has not expired",
					"Ensure token has required permissions (api, read_repository)",
					"Generate a new token at GitLab user settings > access tokens",
				},
				Recoverable: false,
			},
			ExitCode: ExitGitLabError,
		},
	}
}

// GitLabAPIError is raised when GitLab API call fails
type GitLabAPIError struct {
	*AIDocGenError
}

// NewGitLabAPIError creates a new GitLab API error
func NewGitLabAPIError(operation, statusCode string, cause error) *GitLabAPIError {
	return &GitLabAPIError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("GitLab API error during %s", operation),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "GitLab API Call",
				Component: "GitLab Client",
				Details: map[string]interface{}{
					"operation":   operation,
					"status_code": statusCode,
				},
				Suggestions: []string{
					"Check GitLab API URL is correct",
					"Verify the project/group exists",
					"Check API rate limits",
					"Try again later",
				},
				Recoverable: true,
			},
			ExitCode: ExitGitLabError,
		},
	}
}

// GitCloneError is raised when git clone fails
type GitCloneError struct {
	*AIDocGenError
}

// NewGitCloneError creates a new git clone error
func NewGitCloneError(repoURL, reason string, cause error) *GitCloneError {
	return &GitCloneError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to clone repository: %s", repoURL),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Git Clone",
				Component: "Git",
				Details: map[string]interface{}{
					"repo_url": repoURL,
					"reason":   reason,
				},
				Suggestions: []string{
					"Check if the repository URL is correct",
					"Verify you have access to the repository",
					"Check git is installed and accessible",
					"Ensure sufficient disk space",
				},
				Recoverable: false,
			},
			ExitCode: ExitGitLabError,
		},
	}
}
</file>
<file path="internal/errors/handler.go">
package errors

import (
	"fmt"
)

// HandlerError is the base error for all handler-related errors
type HandlerError struct {
	*AIDocGenError
}

// NewHandlerError creates a new handler error
func NewHandlerError(message string) *HandlerError {
	return &HandlerError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitGeneralError,
		},
	}
}

// AnalysisError is raised when analysis fails
type AnalysisError struct {
	*AIDocGenError
}

// NewAnalysisError creates a new analysis error
func NewAnalysisError(reason string, cause error) *AnalysisError {
	return &AnalysisError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Analysis failed: %s", reason),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Codebase Analysis",
				Component: "AnalyzerAgent",
				Suggestions: []string{
					"Check if the repository path is valid",
					"Verify LLM configuration",
					"Try with --debug flag for more information",
					"Check if any agents were excluded",
				},
				Recoverable: false,
			},
			ExitCode: ExitAgentError,
		},
	}
}

// DocumentationError is raised when documentation generation fails
type DocumentationError struct {
	*AIDocGenError
}

// NewDocumentationError creates a new documentation error
func NewDocumentationError(docType string, cause error) *DocumentationError {
	return &DocumentationError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to generate %s documentation", docType),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Documentation Generation",
				Component: docType + "Agent",
				Suggestions: []string{
					"Ensure analysis has been run first",
					"Check that analysis files exist in .ai/docs/",
					"Verify LLM configuration",
				},
				Recoverable: false,
			},
			ExitCode: ExitAgentError,
		},
	}
}

// CronjobError is raised when cronjob execution fails
type CronjobError struct {
	*AIDocGenError
}

// NewCronjobError creates a new cronjob error
func NewCronjobError(reason string, cause error) *CronjobError {
	return &CronjobError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Cronjob failed: %s", reason),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "GitLab Cronjob",
				Component: "CronjobHandler",
				Suggestions: []string{
					"Check GitLab API credentials",
					"Verify group project ID",
					"Check GitLab API URL",
					"Review cronjob logs",
				},
				Recoverable: false,
			},
			ExitCode: ExitGeneralError,
		},
	}
}
</file>
<file path="internal/errors/validation.go">
package errors

import (
	"fmt"
)

// ValidationError is the base error for all validation-related errors
type ValidationError struct {
	*AIDocGenError
}

// NewValidationError creates a new validation error
func NewValidationError(message string) *ValidationError {
	return &ValidationError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitValidationError,
		},
	}
}

// MissingFileError is raised when a required file is not found
type MissingFileError struct {
	*AIDocGenError
}

// NewMissingFileError creates a new missing file error
func NewMissingFileError(filePath string) *MissingFileError {
	return &MissingFileError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Required file not found: %s", filePath),
			Context: &ErrorContext{
				Operation: "File Validation",
				Component: "Filesystem",
				Details: map[string]interface{}{
					"file_path": filePath,
				},
				Suggestions: []string{
					"Check that the file exists",
					"Verify the file path is correct",
					"Run analysis first to generate the file",
				},
				Recoverable: false,
			},
			ExitCode: ExitValidationError,
		},
	}
}

// InvalidPathError is raised when a path is invalid
type InvalidPathError struct {
	*AIDocGenError
}

// NewInvalidPathError creates a new invalid path error
func NewInvalidPathError(path string, reason string) *InvalidPathError {
	return &InvalidPathError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Invalid path: %s", path),
			Context: &ErrorContext{
				Operation: "Path Validation",
				Component: "Filesystem",
				Details: map[string]interface{}{
					"path":   path,
					"reason": reason,
				},
				Suggestions: []string{
					"Check that the path exists",
					"Verify the path is a valid directory",
					"Use an absolute path if relative path fails",
				},
				Recoverable: false,
			},
			ExitCode: ExitValidationError,
		},
	}
}

// OutputValidationError is raised when output validation fails
type OutputValidationError struct {
	*AIDocGenError
}

// NewOutputValidationError creates a new output validation error
func NewOutputValidationError(missingFiles []string) *OutputValidationError {
	return &OutputValidationError{
		AIDocGenError: &AIDocGenError{
			Message: "Output validation failed: expected files were not generated",
			Context: &ErrorContext{
				Operation: "Output Validation",
				Component: "Validation",
				Details: map[string]interface{}{
					"missing_files": missingFiles,
				},
				Suggestions: []string{
					"Check if LLM API calls succeeded",
					"Review error logs for individual agent failures",
					"Try running with --debug flag for more details",
				},
				Recoverable: false,
			},
			ExitCode: ExitValidationError,
		},
	}
}
</file>
<file path="internal/gitlab/client.go">
package gitlab

import (
	"context"
	"fmt"
	"net/http"
	"time"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/logging"
)

// Client represents a GitLab API client
type Client struct {
	httpClient   *http.Client
	apiURL       string
	OAuthToken   string
	UserName     string
	UserUsername string
	UserEmail    string
	logger       *logging.Logger
}

// Project represents a GitLab project
type Project struct {
	ID                int       `json:"id"`
	Name              string    `json:"name"`
	PathWithNamespace string    `json:"path_with_namespace"`
	HTTPURL           string    `json:"http_url_to_repo"`
	SSHURL            string    `json:"ssh_url_to_repo"`
	DefaultBranch     string    `json:"default_branch"`
	LastActivityAt    time.Time `json:"last_activity_at"`
	CreatedAt         time.Time `json:"created_at"`
	Archived          bool      `json:"archived"`
}

// MergeRequest represents a GitLab merge request
type MergeRequest struct {
	ID          int    `json:"id"`
	IID         int    `json:"iid"`
	Title       string `json:"title"`
	Description string `json:"description"`
	SourceBranch string `json:"source_branch"`
	TargetBranch string `json:"target_branch"`
	WebURL      string `json:"web_url"`
}

// NewClient creates a new GitLab client
func NewClient(cfg config.GitLabConfig, logger *logging.Logger) *Client {
	return &Client{
		httpClient: &http.Client{
			Timeout: 30 * time.Second,
		},
		apiURL:       cfg.APIURL,
		OAuthToken:   cfg.OAuthToken,
		UserName:     cfg.UserName,
		UserUsername: cfg.UserUsername,
		UserEmail:    cfg.UserEmail,
		logger:       logger,
	}
}

// FetchProjectsInGroup fetches all projects in a group (including subgroups)
func (c *Client) FetchProjectsInGroup(ctx context.Context, groupID int) ([]Project, error) {
	c.logger.Debug(fmt.Sprintf("Fetching projects in group %d", groupID))

	// For now, return empty list - would need full GitLab API implementation
	// This would require using xanzy/go-gitlab or implementing the API calls
	return []Project{}, nil
}

// ProjectFilter determines if a project should be analyzed
type ProjectFilter struct {
	MaxDaysSinceLastCommit int
	IgnoreProjects         map[string]bool
	IgnoreSubgroups        map[string]bool
}

// ShouldAnalyze determines if a project should be analyzed
func (c *Client) ShouldAnalyze(ctx context.Context, project Project, filter ProjectFilter) (bool, error) {
	// Skip archived projects
	if project.Archived {
		return false, nil
	}

	// Skip ignored projects
	if filter.IgnoreProjects[project.PathWithNamespace] {
		return false, nil
	}

	// Check if last commit is too old
	if filter.MaxDaysSinceLastCommit > 0 {
		daysSince := time.Since(project.LastActivityAt).Hours() / 24
		if int(daysSince) > filter.MaxDaysSinceLastCommit {
			return false, nil
		}
	}

	// Check if branch already exists
	branchName := fmt.Sprintf("ai-analyzer-%s", time.Now().Format("2006-01-02"))
	if branchExists, err := c.BranchExists(ctx, project, branchName); err == nil && branchExists {
		return false, nil
	}

	// Check if open MR exists
	if hasMR, err := c.HasOpenMR(ctx, project, branchName); err == nil && hasMR {
		return false, nil
	}

	return true, nil
}

// BranchExists checks if a branch exists in a project
func (c *Client) BranchExists(ctx context.Context, project Project, branchName string) (bool, error) {
	// Placeholder - would implement GitLab API call
	return false, nil
}

// HasOpenMR checks if an open MR exists for a branch
func (c *Client) HasOpenMR(ctx context.Context, project Project, branchName string) (bool, error) {
	// Placeholder - would implement GitLab API call
	return false, nil
}

// CreateBranch creates a new branch in a project
func (c *Client) CreateBranch(ctx context.Context, project Project, branchName, fromBranch string) error {
	// Placeholder - would implement GitLab API call
	c.logger.Info(fmt.Sprintf("Would create branch '%s' in %s", branchName, project.PathWithNamespace))
	return nil
}

// CreateCommit creates a commit with the given files
func (c *Client) CreateCommit(ctx context.Context, project Project, branchName, message string, files map[string]string) error {
	// Placeholder - would implement GitLab API call
	c.logger.Info(fmt.Sprintf("Would create commit in %s on branch %s", project.PathWithNamespace, branchName))
	return nil
}

// CreateMR creates a merge request
func (c *Client) CreateMR(ctx context.Context, project Project, sourceBranch, targetBranch, title, description string) (*MergeRequest, error) {
	// Placeholder - would implement GitLab API call
	c.logger.Info(fmt.Sprintf("Would create MR in %s: %s -> %s", project.PathWithNamespace, sourceBranch, targetBranch))
	return &MergeRequest{
		Title:       title,
		Description: description,
		SourceBranch: sourceBranch,
		TargetBranch: targetBranch,
	}, nil
}
</file>
<file path="internal/handlers/ai_rules.go">
package handlers

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/agents"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// AIRulesHandler handles the generate ai-rules command
type AIRulesHandler struct {
	*BaseHandler
	config config.AIRulesConfig
}

// NewAIRulesHandler creates a new AI rules handler
func NewAIRulesHandler(cfg config.AIRulesConfig, logger *logging.Logger) *AIRulesHandler {
	return &AIRulesHandler{
		BaseHandler: &BaseHandler{
			Config: cfg.BaseConfig,
			Logger: logger,
		},
		config: cfg,
	}
}

// Handle generates AI rules files
func (h *AIRulesHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting AI rules generation",
		logging.String("repo_path", h.config.RepoPath),
	)

	// Load prompts
	promptManager, err := prompts.NewManager("./prompts")
	if err != nil {
		repoPromptsDir := fmt.Sprintf("%s/../gendocs/prompts", h.config.RepoPath)
		promptManager, err = prompts.NewManager(repoPromptsDir)
		if err != nil {
			return errors.NewConfigurationError(fmt.Sprintf("failed to load prompts: %v", err))
		}
	}

	// Create AI rules generator agent
	aiRulesAgent := agents.NewAIRulesGeneratorAgent(h.config, promptManager, h.Logger)

	// Run generation
	if err := aiRulesAgent.Run(ctx); err != nil {
		return errors.NewDocumentationError("AI rules", err)
	}

	h.Logger.Info("AI rules files generated successfully")
	return nil
}
</file>
<file path="internal/handlers/analyze.go">
package handlers

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/agents"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// AnalyzeHandler handles the analyze command
type AnalyzeHandler struct {
	*BaseHandler
	config config.AnalyzerConfig
}

// NewAnalyzeHandler creates a new analyze handler
func NewAnalyzeHandler(cfg config.AnalyzerConfig, logger *logging.Logger) *AnalyzeHandler {
	return &AnalyzeHandler{
		BaseHandler: &BaseHandler{
			Config: cfg.BaseConfig,
			Logger: logger,
		},
		config: cfg,
	}
}

// Handle executes the analysis
func (h *AnalyzeHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting analyze handler",
		logging.String("repo_path", h.config.RepoPath),
	)

	// Load prompts
	// Try to find prompts directory - check relative to binary or repo
	promptManager, err := prompts.NewManager("./prompts")
	if err != nil {
		// Try relative to repo path
		repoPromptsDir := fmt.Sprintf("%s/../gendocs/prompts", h.config.RepoPath)
		promptManager, err = prompts.NewManager(repoPromptsDir)
		if err != nil {
			return errors.NewConfigurationError(fmt.Sprintf("failed to load prompts: %v", err))
		}
	}

	// Create analyzer agent
	analyzerAgent := agents.NewAnalyzerAgent(h.config, promptManager, h.Logger)

	// Run analysis
	result, err := analyzerAgent.Run(ctx)
	if err != nil {
		return errors.NewAnalysisError("analysis execution failed", err)
	}

	// Log results
	h.Logger.Info(fmt.Sprintf("Analysis complete: %d/%d successful",
		len(result.Successful), len(result.Successful)+len(result.Failed)))

	// Determine exit code
	if len(result.Failed) > 0 && len(result.Successful) == 0 {
		return errors.NewAnalysisError("all analyses failed", fmt.Errorf("no successful analyses"))
	}

	if len(result.Failed) > 0 {
		h.Logger.Warn(fmt.Sprintf("Partial success: %d analyses failed", len(result.Failed)))
		for _, failed := range result.Failed {
			h.Logger.Error(fmt.Sprintf("  - %s: %v", failed.Name, failed.Error))
		}
	}

	return nil
}
</file>
<file path="internal/handlers/base.go">
package handlers

import (
	"context"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/logging"
)

// Handler is the interface that all handlers must implement
type Handler interface {
	// Handle executes the handler logic
	Handle(ctx context.Context) error
}

// BaseHandler provides common functionality for all handlers
type BaseHandler struct {
	Config config.BaseConfig
	Logger *logging.Logger
}

// NewBaseHandler creates a new base handler
func NewBaseHandler(cfg config.BaseConfig, logger *logging.Logger) *BaseHandler {
	return &BaseHandler{
		Config: cfg,
		Logger: logger,
	}
}
</file>
<file path="internal/handlers/cronjob.go">
package handlers

import (
	"context"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"time"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/gitlab"
	"github.com/user/gendocs/internal/logging"
)

// CronjobHandler handles the cronjob analyze command
type CronjobHandler struct {
	*BaseHandler
	config    config.CronjobConfig
	gitLabCfg config.GitLabConfig
	analyzerCfg config.AnalyzerConfig
	gitlab    *gitlab.Client
}

// NewCronjobHandler creates a new cronjob handler
func NewCronjobHandler(
	cronjobCfg config.CronjobConfig,
	gitLabCfg config.GitLabConfig,
	analyzerCfg config.AnalyzerConfig,
	logger *logging.Logger,
) *CronjobHandler {
	return &CronjobHandler{
		BaseHandler: &BaseHandler{
			Config: config.BaseConfig{
				RepoPath: cronjobCfg.WorkingPath,
				Debug:    false,
			},
			Logger: logger,
		},
		config:      cronjobCfg,
		gitLabCfg:   gitLabCfg,
		analyzerCfg: analyzerCfg,
		gitlab:      gitlab.NewClient(gitLabCfg, logger),
	}
}

// ProcessedResult holds the results of processing projects
type ProcessedResult struct {
	ProcessedCount int
	SuccessCount   int
	ErrorCount     int
	SkippedCount   int
	FailedProjects []FailedProject
}

// FailedProject represents a project that failed to process
type FailedProject struct {
	Name string
	Error error
}

// Handle executes the cronjob analysis
func (h *CronjobHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting cronjob analysis",
		logging.String("working_path", h.config.WorkingPath),
		logging.Int("group_project_id", h.config.GroupProjectID),
		logging.Int("max_days", h.config.MaxDaysSinceLastCommit),
	)

	// Fetch all projects in the group
	projects, err := h.gitlab.FetchProjectsInGroup(ctx, h.config.GroupProjectID)
	if err != nil {
		return errors.NewCronjobError("failed to fetch projects", err)
	}

	h.Logger.Info(fmt.Sprintf("Found %d projects in group", len(projects)))

	// Filter projects
	filter := gitlab.ProjectFilter{
		MaxDaysSinceLastCommit: h.config.MaxDaysSinceLastCommit,
	}

	var applicableProjects []gitlab.Project
	for _, project := range projects {
		shouldAnalyze, err := h.gitlab.ShouldAnalyze(ctx, project, filter)
		if err != nil {
			h.Logger.Warn(fmt.Sprintf("Error checking project %s: %v", project.PathWithNamespace, err))
			continue
		}
		if shouldAnalyze {
			applicableProjects = append(applicableProjects, project)
		}
	}

	h.Logger.Info(fmt.Sprintf("%d projects applicable for analysis", len(applicableProjects)))

	// Process each applicable project
	result := &ProcessedResult{
		FailedProjects: []FailedProject{},
	}

	for _, project := range applicableProjects {
		h.Logger.Info(fmt.Sprintf("Processing %s", project.PathWithNamespace))

		if err := h.processProject(ctx, project); err != nil {
			result.ErrorCount++
			result.FailedProjects = append(result.FailedProjects, FailedProject{
				Name: project.PathWithNamespace,
				Error: err,
			})
			h.Logger.Error(fmt.Sprintf("Failed to process %s: %v", project.PathWithNamespace, err))
		} else {
			result.SuccessCount++
		}
		result.ProcessedCount++
	}

	// Log summary
	h.Logger.Info(fmt.Sprintf("Cronjob complete: %d processed, %d succeeded, %d failed, %d skipped",
		result.ProcessedCount, result.SuccessCount, result.ErrorCount,
		len(projects)-len(applicableProjects)))

	if result.ErrorCount > 0 && result.SuccessCount == 0 {
		return errors.NewCronjobError("all projects failed", fmt.Errorf("%d failures", result.ErrorCount))
	}

	return nil
}

// processProject processes a single project
func (h *CronjobHandler) processProject(ctx context.Context, project gitlab.Project) error {
	// Create temp directory for cloning
	tempDir := filepath.Join(h.config.WorkingPath, "tmp", fmt.Sprintf("project_%d", project.ID))
	if err := os.MkdirAll(tempDir, 0755); err != nil {
		return fmt.Errorf("failed to create temp dir: %w", err)
	}
	defer os.RemoveAll(tempDir)

	// Clone repository
	if err := h.cloneRepository(ctx, project, tempDir); err != nil {
		return errors.NewGitCloneError(project.HTTPURL, "clone failed", err)
	}

	// Create branch
	branchName := fmt.Sprintf("ai-analyzer-%s", time.Now().Format("2006-01-02"))
	if err := h.gitlab.CreateBranch(ctx, project, branchName, project.DefaultBranch); err != nil {
		return fmt.Errorf("failed to create branch: %w", err)
	}

	// Run analysis
	analyzerCfg := h.analyzerCfg
	analyzerCfg.RepoPath = tempDir

	// Run analyze command (via subprocess for now, could be refactored to use handler directly)
	if err := h.runAnalysis(ctx, tempDir); err != nil {
		return fmt.Errorf("analysis failed: %w", err)
	}

	// Commit results
	commitMsg := fmt.Sprintf("[skip ci] AI analysis: %s", time.Now().Format("2006-01-02"))
	if err := h.gitlab.CreateCommit(ctx, project, branchName, commitMsg, nil); err != nil {
		return fmt.Errorf("failed to create commit: %w", err)
	}

	// Create merge request
	mrTitle := fmt.Sprintf("AI Analysis: %s", time.Now().Format("2006-01-02"))
	mrDescription := fmt.Sprintf("Automated AI analysis generated on %s\n\nThis MR contains:\n- Structure analysis\n- Dependency analysis\n- Data flow analysis\n- Request flow analysis\n- API analysis", time.Now().Format("2006-01-02"))
	mr, err := h.gitlab.CreateMR(ctx, project, branchName, project.DefaultBranch, mrTitle, mrDescription)
	if err != nil {
		return fmt.Errorf("failed to create MR: %w", err)
	}

	h.Logger.Info(fmt.Sprintf("Created MR %d for %s", mr.IID, project.PathWithNamespace))
	return nil
}

// cloneRepository clones a GitLab repository
func (h *CronjobHandler) cloneRepository(ctx context.Context, project gitlab.Project, destDir string) error {
	// Clone with authentication
	url := project.HTTPURL
	if h.gitlab.OAuthToken != "" {
		// Inject token into URL
		url = fmt.Sprintf("https://oauth2:%s@%s", h.gitlab.OAuthToken, project.HTTPURL[8:]) // Strip https://
	}

	cmd := exec.CommandContext(ctx, "git", "clone", "--depth", "1", url, destDir)
	output, err := cmd.CombinedOutput()
	if err != nil {
		h.Logger.Debug(fmt.Sprintf("Git clone output: %s", string(output)))
		return err
	}

	return nil
}

// runAnalysis runs the analysis on a repository
func (h *CronjobHandler) runAnalysis(ctx context.Context, repoPath string) error {
	// Run gendocs analyze command as subprocess
	cmd := exec.CommandContext(ctx, "./gendocs", "analyze", "--repo-path", repoPath)
	cmd.Dir = filepath.Dir(repoPath) // Run from parent directory to find binary

	output, err := cmd.CombinedOutput()
	if err != nil {
		h.Logger.Debug(fmt.Sprintf("Analysis output: %s", string(output)))
		return err
	}

	h.Logger.Debug(fmt.Sprintf("Analysis output: %s", string(output)))
	return nil
}
</file>
<file path="internal/handlers/readme.go">
package handlers

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/agents"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// ReadmeHandler handles the generate readme command
type ReadmeHandler struct {
	*BaseHandler
	config config.DocumenterConfig
}

// NewReadmeHandler creates a new readme handler
func NewReadmeHandler(cfg config.DocumenterConfig, logger *logging.Logger) *ReadmeHandler {
	return &ReadmeHandler{
		BaseHandler: &BaseHandler{
			Config: cfg.BaseConfig,
			Logger: logger,
		},
		config: cfg,
	}
}

// Handle generates the README
func (h *ReadmeHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting readme generation",
		logging.String("repo_path", h.config.RepoPath),
	)

	// Load prompts
	promptManager, err := prompts.NewManager("./prompts")
	if err != nil {
		repoPromptsDir := fmt.Sprintf("%s/../gendocs/prompts", h.config.RepoPath)
		promptManager, err = prompts.NewManager(repoPromptsDir)
		if err != nil {
			return errors.NewConfigurationError(fmt.Sprintf("failed to load prompts: %v", err))
		}
	}

	// Create documenter agent
	documenterAgent := agents.NewDocumenterAgent(h.config, promptManager, h.Logger)

	// Run generation
	if err := documenterAgent.Run(ctx); err != nil {
		return errors.NewDocumentationError("README", err)
	}

	h.Logger.Info("README.md generated successfully")
	return nil
}
</file>
<file path="internal/llm/anthropic.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/user/gendocs/internal/config"
)

// AnthropicClient implements LLMClient for Anthropic Claude
type AnthropicClient struct {
	*BaseLLMClient
	apiKey string
	model  string
}

// anthropicRequest represents the request body for Anthropic API
type anthropicRequest struct {
	Model         string                  `json:"model"`
	Messages      []anthropicMessage      `json:"messages"`
	System        string                  `json:"system,omitempty"`
	MaxTokens     int                     `json:"max_tokens"`
	Temperature   float64                 `json:"temperature,omitempty"`
	Tools         []anthropicTool         `json:"tools,omitempty"`
	Stream        bool                    `json:"stream,omitempty"`
}

// anthropicMessage represents a message in Anthropic format
type anthropicMessage struct {
	Role    string                 `json:"role"`
	Content []anthropicContentBlock `json:"content"`
}

// anthropicContentBlock represents a content block
type anthropicContentBlock struct {
	Type     string                 `json:"type"`
	Text     string                 `json:"text,omitempty"`
	ToolUse  *anthropicToolUseBlock `json:"tool_use,omitempty"`
	ToolResult *anthropicToolResultBlock `json:"tool_result,omitempty"`
}

// anthropicToolUseBlock represents a tool use call
type anthropicToolUseBlock struct {
	ID       string                 `json:"id"`
	Name     string                 `json:"name"`
	Input    map[string]interface{} `json:"input"`
}

// anthropicToolResultBlock represents a tool result
type anthropicToolResultBlock struct {
	ToolUseID string `json:"tool_use_id"`
	Content   string `json:"content"`
}

// anthropicTool represents a tool definition
type anthropicTool struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	InputSchema map[string]interface{} `json:"input_schema"`
}

// anthropicResponse represents the response from Anthropic API
type anthropicResponse struct {
	ID      string                `json:"id"`
	Type    string                `json:"type"`
	Role    string                `json:"role"`
	Content []anthropicContentBlock `json:"content"`
	StopReason string              `json:"stop_reason"`
	Usage   anthropicUsage        `json:"usage"`
	Error   *anthropicError       `json:"error,omitempty"`
}

// anthropicUsage represents token usage
type anthropicUsage struct {
	InputTokens  int `json:"input_tokens"`
	OutputTokens int `json:"output_tokens"`
}

// anthropicError represents an error from Anthropic
type anthropicError struct {
	Type    string `json:"type"`
	Message string `json:"message"`
}

// NewAnthropicClient creates a new Anthropic client
func NewAnthropicClient(cfg config.LLMConfig, retryClient *RetryClient) *AnthropicClient {
	return &AnthropicClient{
		BaseLLMClient: NewBaseLLMClient(retryClient),
		apiKey:        cfg.APIKey,
		model:         cfg.Model,
	}
}

// GenerateCompletion generates a completion from Anthropic
func (c *AnthropicClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	// Convert to Anthropic format
	anReq := c.convertRequest(req)

	jsonData, err := json.Marshal(anReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	url := "https://api.anthropic.com/v1/messages"
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonData))
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("x-api-key", c.apiKey)
	httpReq.Header.Set("anthropic-version", "2023-06-01")

	// Execute with retry
	resp, err := c.retryClient.Do(httpReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	// Read response
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to read response: %w", err)
	}

	// Check for error status
	if resp.StatusCode != http.StatusOK {
		return CompletionResponse{}, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))
	}

	// Parse response
	var anResp anthropicResponse
	if err := json.Unmarshal(body, &anResp); err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to parse response: %w", err)
	}

	// Check for API error
	if anResp.Error != nil {
		return CompletionResponse{}, fmt.Errorf("API error: %s", anResp.Error.Message)
	}

	return c.convertResponse(anResp), nil
}

// SupportsTools returns true
func (c *AnthropicClient) SupportsTools() bool {
	return true
}

// GetProvider returns the provider name
func (c *AnthropicClient) GetProvider() string {
	return "anthropic"
}

// convertRequest converts internal request to Anthropic format
func (c *AnthropicClient) convertRequest(req CompletionRequest) anthropicRequest {
	// Build messages from internal format
	messages := []anthropicMessage{}

	// Convert internal messages to Anthropic format
	for _, msg := range req.Messages {
		if msg.Role == "tool" {
			// Tool result message
			messages = append(messages, anthropicMessage{
				Role: "user",
				Content: []anthropicContentBlock{
					{
						Type: "tool_result",
						ToolResult: &anthropicToolResultBlock{
							Content: msg.Content,
						},
					},
				},
			})
		} else if msg.Role == "assistant" {
			// Assistant message
			contentBlock := anthropicContentBlock{
				Type: "text",
				Text: msg.Content,
			}
			messages = append(messages, anthropicMessage{
				Role:    "assistant",
				Content: []anthropicContentBlock{contentBlock},
			})
		}
	}

	// If no messages yet, add initial user message
	if len(messages) == 0 {
		messages = append(messages, anthropicMessage{
			Role: "user",
			Content: []anthropicContentBlock{
				{Type: "text", Text: "Analyze this codebase."},
			},
		})
	}

	// Build tools
	var tools []anthropicTool
	if len(req.Tools) > 0 {
		tools = make([]anthropicTool, len(req.Tools))
		for i, tool := range req.Tools {
			tools[i] = anthropicTool{
				Name:        tool.Name,
				Description: tool.Description,
				InputSchema: tool.Parameters,
			}
		}
	}

	return anthropicRequest{
		Model:       c.model,
		Messages:    messages,
		System:      req.SystemPrompt,
		MaxTokens:   req.MaxTokens,
		Temperature: req.Temperature,
		Tools:       tools,
		Stream:      false,
	}
}

// convertResponse converts Anthropic response to internal format
func (c *AnthropicClient) convertResponse(resp anthropicResponse) CompletionResponse {
	result := CompletionResponse{
		Usage: TokenUsage{
			InputTokens:  resp.Usage.InputTokens,
			OutputTokens: resp.Usage.OutputTokens,
			TotalTokens:  resp.Usage.InputTokens + resp.Usage.OutputTokens,
		},
	}

	// Extract content and tool calls
	var textContent strings.Builder
	var toolCalls []ToolCall

	for _, block := range resp.Content {
		if block.Type == "text" {
			textContent.WriteString(block.Text)
		} else if block.Type == "tool_use" && block.ToolUse != nil {
			toolCalls = append(toolCalls, ToolCall{
				Name:      block.ToolUse.Name,
				Arguments: block.ToolUse.Input,
			})
		}
	}

	result.Content = textContent.String()
	result.ToolCalls = toolCalls

	return result
}
</file>
<file path="internal/llm/client.go">
package llm

import (
	"context"
)

// Message represents a chat message
type Message struct {
	Role    string // "system", "user", "assistant", "tool"
	Content string
	ToolID  string // ID of the tool that was called (for role="tool")
}

// ToolCall represents a tool/function call from the LLM
type ToolCall struct {
	Name      string
	Arguments map[string]interface{}
}

// CompletionRequest is a request for LLM completion
type CompletionRequest struct {
	SystemPrompt string
	Messages     []Message
	Tools        []ToolDefinition
	MaxTokens    int
	Temperature  float64
}

// CompletionResponse is the response from LLM
type CompletionResponse struct {
	Content   string
	ToolCalls []ToolCall
	Usage     TokenUsage
}

// TokenUsage tracks token usage
type TokenUsage struct {
	InputTokens  int
	OutputTokens int
	TotalTokens  int
}

// ToolDefinition defines a tool for the LLM
type ToolDefinition struct {
	Name        string
	Description string
	Parameters  map[string]interface{}
}

// LLMClient is the interface for LLM providers
type LLMClient interface {
	// GenerateCompletion generates a completion from the LLM
	GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error)

	// SupportsTools returns true if the client supports tool calling
	SupportsTools() bool

	// GetProvider returns the provider name
	GetProvider() string
}

// BaseLLMClient provides common functionality for all LLM clients
type BaseLLMClient struct {
	retryClient *RetryClient
}

// NewBaseLLMClient creates a new base LLM client
func NewBaseLLMClient(retryClient *RetryClient) *BaseLLMClient {
	return &BaseLLMClient{
		retryClient: retryClient,
	}
}
</file>
<file path="internal/llm/factory.go">
package llm

import (
	"fmt"

	"github.com/user/gendocs/internal/config"
)

// Factory creates LLM clients
type Factory struct {
	retryClient *RetryClient
}

// NewFactory creates a new LLM factory
func NewFactory(retryClient *RetryClient) *Factory {
	return &Factory{
		retryClient: retryClient,
	}
}

// CreateClient creates an LLM client based on the provider configuration
func (f *Factory) CreateClient(cfg config.LLMConfig) (LLMClient, error) {
	switch cfg.Provider {
	case "openai":
		return NewOpenAIClient(cfg, f.retryClient), nil
	case "anthropic":
		return NewAnthropicClient(cfg, f.retryClient), nil
	case "gemini":
		return NewGeminiClient(cfg, f.retryClient), nil
	default:
		return nil, fmt.Errorf("unsupported LLM provider: %s (supported: openai, anthropic, gemini)", cfg.Provider)
	}
}
</file>
<file path="internal/llm/gemini.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/user/gendocs/internal/config"
)

// GeminiClient implements LLMClient for Google Gemini
type GeminiClient struct {
	*BaseLLMClient
	apiKey string
	model string
}

// geminiRequest represents the request body for Gemini API
type geminiRequest struct {
	Contents       []geminiContent    `json:"contents"`
	Tools          []geminiTool       `json:"tools,omitempty"`
	GenerationConfig geminiGenerationConfig `json:"generationConfig,omitempty"`
	SystemInstruction *geminiContent  `json:"systemInstruction,omitempty"`
}

// geminiContent represents content in Gemini format
type geminiContent struct {
	Role  string           `json:"role,omitempty"`
	Parts []geminiPart     `json:"parts"`
}

// geminiPart represents a part of content
type geminiPart struct {
	Text         string                 `json:"text,omitempty"`
	FunctionCall map[string]interface{} `json:"functionCall,omitempty"`
	FunctionResponse *geminiFunctionResponse `json:"functionResponse,omitempty"`
}

// geminiFunctionResponse represents a function response
// Gemini format: {"name": "function_name", "response": {...}}
type geminiFunctionResponse struct {
	Name     string                 `json:"name"`
	Response map[string]interface{} `json:"response,omitempty"`
}

// geminiTool represents a tool declaration
type geminiTool struct {
	FunctionDeclarations []geminiFunctionDeclaration `json:"functionDeclarations,omitempty"`
}

// geminiFunctionDeclaration represents a function declaration
type geminiFunctionDeclaration struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Parameters  map[string]interface{} `json:"parameters"`
}

// geminiGenerationConfig represents generation configuration
type geminiGenerationConfig struct {
	Temperature float64 `json:"temperature,omitempty"`
	MaxOutputTokens int  `json:"maxOutputTokens,omitempty"`
}

// geminiResponse represents the response from Gemini API
type geminiResponse struct {
	Candidates []geminiCandidate `json:"candidates"`
	UsageMetadata geminiUsageMetadata `json:"usageMetadata,omitempty"`
	Error      *geminiError      `json:"error,omitempty"`
}

// geminiCandidate represents a candidate response
type geminiCandidate struct {
	Content   geminiContent `json:"content"`
	FinishReason string     `json:"finishReason,omitempty"`
}

// geminiUsageMetadata represents token usage
type geminiUsageMetadata struct {
	PromptTokenCount     int `json:"promptTokenCount"`
	CandidatesTokenCount int `json:"candidatesTokenCount"`
	TotalTokenCount      int `json:"totalTokenCount"`
}

// geminiError represents an error
type geminiError struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Status  string `json:"status"`
}

// NewGeminiClient creates a new Gemini client
func NewGeminiClient(cfg config.LLMConfig, retryClient *RetryClient) *GeminiClient {
	return &GeminiClient{
		BaseLLMClient: NewBaseLLMClient(retryClient),
		apiKey:        cfg.APIKey,
		model:         cfg.Model,
	}
}

// GenerateCompletion generates a completion from Gemini
func (c *GeminiClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	// Convert to Gemini format
	gemReq := c.convertRequest(req)

	jsonData, err := json.Marshal(gemReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	// Model format: models/gemini-1.5-pro or models/gemini-pro
	modelName := c.model
	if !strings.HasPrefix(modelName, "models/") {
		modelName = "models/" + modelName
	}
	url := fmt.Sprintf("https://generativelanguage.googleapis.com/v1beta/%s:generateContent?key=%s", modelName, c.apiKey)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonData))
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")

	// Execute with retry
	resp, err := c.retryClient.Do(httpReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	// Read response
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to read response: %w", err)
	}

	// Check for error status
	if resp.StatusCode != http.StatusOK {
		return CompletionResponse{}, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))
	}

	// Parse response
	var gemResp geminiResponse
	if err := json.Unmarshal(body, &gemResp); err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to parse response: %w", err)
	}

	// Check for API error
	if gemResp.Error != nil {
		return CompletionResponse{}, fmt.Errorf("API error: %s", gemResp.Error.Message)
	}

	return c.convertResponse(gemResp), nil
}

// SupportsTools returns true
func (c *GeminiClient) SupportsTools() bool {
	return true
}

// GetProvider returns the provider name
func (c *GeminiClient) GetProvider() string {
	return "gemini"
}

// convertRequest converts internal request to Gemini format
func (c *GeminiClient) convertRequest(req CompletionRequest) geminiRequest {
	// Build contents
	contents := []geminiContent{}

	// Add system instruction as first content with role "user"
	// Gemini doesn't have a separate system field, it's part of content
	if req.SystemPrompt != "" {
		contents = append(contents, geminiContent{
			Role: "user",
			Parts: []geminiPart{
				{Text: req.SystemPrompt},
			},
		})
		// Add empty model response
		contents = append(contents, geminiContent{
			Role: "model",
			Parts: []geminiPart{
				{Text: "Understood. I will analyze the codebase according to your instructions."},
			},
		})
	}

	// Add messages
	for _, msg := range req.Messages {
		if msg.Role == "tool" {
			// Tool response - extract function name from tool ID or content
			// Format: {"name": "function_name", "response": {"result": "content"}}
			funcName := msg.ToolID
			if funcName == "" {
				// Try to extract from Content if it's JSON
				var toolData map[string]interface{}
				if err := json.Unmarshal([]byte(msg.Content), &toolData); err == nil {
					if name, ok := toolData["name"].(string); ok {
						funcName = name
					}
				}
			}
			// Fallback to a default name if still empty
			if funcName == "" {
				funcName = "unknown_function"
			}

			contents = append(contents, geminiContent{
				Role: "user",
				Parts: []geminiPart{
					{
						FunctionResponse: &geminiFunctionResponse{
							Name: funcName,
							Response: map[string]interface{}{
								"result": msg.Content,
							},
						},
					},
				},
			})
		} else {
			// Regular message
			role := "user"
			if msg.Role == "assistant" {
				role = "model"
			}
			// Skip empty content messages (avoid empty parts)
			if msg.Content == "" {
				continue
			}
			contents = append(contents, geminiContent{
				Role: role,
				Parts: []geminiPart{
					{Text: msg.Content},
				},
			})
		}
	}

	// Build tools
	var tools []geminiTool
	if len(req.Tools) > 0 {
		tools = make([]geminiTool, 1)
		functions := make([]geminiFunctionDeclaration, len(req.Tools))
		for i, tool := range req.Tools {
			functions[i] = geminiFunctionDeclaration{
				Name:        tool.Name,
				Description: tool.Description,
				Parameters:  tool.Parameters,
			}
		}
		tools[0] = geminiTool{
			FunctionDeclarations: functions,
		}
	}

	return geminiRequest{
		Contents: contents,
		Tools:    tools,
		GenerationConfig: geminiGenerationConfig{
			Temperature:    req.Temperature,
			MaxOutputTokens: req.MaxTokens,
		},
	}
}

// convertResponse converts Gemini response to internal format
func (c *GeminiClient) convertResponse(resp geminiResponse) CompletionResponse {
	result := CompletionResponse{
		Usage: TokenUsage{
			InputTokens:  resp.UsageMetadata.PromptTokenCount,
			OutputTokens: resp.UsageMetadata.CandidatesTokenCount,
			TotalTokens:  resp.UsageMetadata.TotalTokenCount,
		},
	}

	if len(resp.Candidates) == 0 {
		return result
	}

	candidate := resp.Candidates[0]
	var textContent string
	var toolCalls []ToolCall

	for _, part := range candidate.Content.Parts {
		if part.Text != "" {
			textContent += part.Text
		}
		if part.FunctionCall != nil {
			toolCalls = append(toolCalls, ToolCall{
				Name:      part.FunctionCall["name"].(string),
				Arguments: part.FunctionCall["args"].(map[string]interface{}),
			})
		}
	}

	result.Content = textContent
	result.ToolCalls = toolCalls

	return result
}
</file>
<file path="internal/llm/openai.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"

	"github.com/user/gendocs/internal/config"
)

// OpenAIClient implements LLMClient for OpenAI-compatible APIs
type OpenAIClient struct {
	*BaseLLMClient
	apiKey  string
	baseURL string
	model   string
}

// openaiRequest represents the request body for OpenAI API
type openaiRequest struct {
	Model       string         `json:"model"`
	Messages    []openaiMessage `json:"messages"`
	MaxTokens   int            `json:"max_tokens"`
	Temperature float64        `json:"temperature"`
	Tools       []openaiTool   `json:"tools,omitempty"`
}

// openaiMessage represents a message in OpenAI format
type openaiMessage struct {
	Role       string           `json:"role"`
	Content    string           `json:"content"`
	ToolCalls  []openaiToolCall `json:"tool_calls,omitempty"`
	ToolCallID string           `json:"tool_call_id,omitempty"`
}

// openaiTool represents a tool definition in OpenAI format
type openaiTool struct {
	Type     string              `json:"type"`
	Function openaiToolFunction  `json:"function"`
}

// openaiToolFunction represents tool function parameters
type openaiToolFunction struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Parameters  map[string]interface{} `json:"parameters"`
}

// openaiToolCall represents a tool call in OpenAI format
type openaiToolCall struct {
	ID       string                `json:"id"`
	Type     string                `json:"type"`
	Function openaiToolCallFunc    `json:"function"`
}

// openaiToolCallFunc represents function call details
type openaiToolCallFunc struct {
	Name      string `json:"name"`
	Arguments string `json:"arguments"`
}

// openaiResponse represents the response from OpenAI API
type openaiResponse struct {
	ID      string             `json:"id"`
	Object  string             `json:"object"`
	Created int64              `json:"created"`
	Model   string             `json:"model"`
	Choices []openaiChoice     `json:"choices"`
	Usage   openaiUsage        `json:"usage"`
	Error   *openaiErrorDetail `json:"error,omitempty"`
}

// openaiChoice represents a choice in the response
type openaiChoice struct {
	Index        int              `json:"index"`
	Message      openaiMessage    `json:"message"`
	FinishReason string           `json:"finish_reason"`
}

// openaiUsage represents token usage
type openaiUsage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

// openaiErrorDetail represents an error from OpenAI
type openaiErrorDetail struct {
	Message string `json:"message"`
	Type    string `json:"type"`
	Code    string `json:"code"`
}

// NewOpenAIClient creates a new OpenAI client
func NewOpenAIClient(cfg config.LLMConfig, retryClient *RetryClient) *OpenAIClient {
	baseURL := cfg.BaseURL
	if baseURL == "" {
		baseURL = "https://api.openai.com/v1"
	}

	return &OpenAIClient{
		BaseLLMClient: NewBaseLLMClient(retryClient),
		apiKey:        cfg.APIKey,
		baseURL:       baseURL,
		model:         cfg.Model,
	}
}

// GenerateCompletion generates a completion from OpenAI
func (c *OpenAIClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	// Convert to OpenAI format
	oaReq := c.convertRequest(req)

	jsonData, err := json.Marshal(oaReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	url := fmt.Sprintf("%s/chat/completions", c.baseURL)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonData))
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.apiKey))

	// Execute with retry
	resp, err := c.retryClient.Do(httpReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	// Read response
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to read response: %w", err)
	}

	// Check for error status
	if resp.StatusCode != http.StatusOK {
		return CompletionResponse{}, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))
	}

	// Parse response
	var oaResp openaiResponse
	if err := json.Unmarshal(body, &oaResp); err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to parse response: %w", err)
	}

	// Check for API error
	if oaResp.Error != nil {
		return CompletionResponse{}, fmt.Errorf("API error: %s", oaResp.Error.Message)
	}

	return c.convertResponse(oaResp), nil
}

// SupportsTools returns true
func (c *OpenAIClient) SupportsTools() bool {
	return true
}

// GetProvider returns the provider name
func (c *OpenAIClient) GetProvider() string {
	return "openai"
}

// convertRequest converts internal request to OpenAI format
func (c *OpenAIClient) convertRequest(req CompletionRequest) openaiRequest {
	messages := []openaiMessage{}

	// Add system prompt if provided
	if req.SystemPrompt != "" {
		messages = append(messages, openaiMessage{
			Role:    "system",
			Content: req.SystemPrompt,
		})
	}

	// Add messages
	for _, msg := range req.Messages {
		messages = append(messages, openaiMessage{
			Role:    msg.Role,
			Content: msg.Content,
		})
	}

	oaReq := openaiRequest{
		Model:       c.model,
		Messages:    messages,
		MaxTokens:   req.MaxTokens,
		Temperature: req.Temperature,
	}

	// Add tools if provided
	if len(req.Tools) > 0 {
		oaReq.Tools = make([]openaiTool, len(req.Tools))
		for i, tool := range req.Tools {
			oaReq.Tools[i] = openaiTool{
				Type: "function",
				Function: openaiToolFunction{
					Name:        tool.Name,
					Description: tool.Description,
					Parameters:  tool.Parameters,
				},
			}
		}
	}

	return oaReq
}

// convertResponse converts OpenAI response to internal format
func (c *OpenAIClient) convertResponse(resp openaiResponse) CompletionResponse {
	if len(resp.Choices) == 0 {
		return CompletionResponse{
			Usage: TokenUsage{
				InputTokens:  resp.Usage.PromptTokens,
				OutputTokens: resp.Usage.CompletionTokens,
				TotalTokens:  resp.Usage.TotalTokens,
			},
		}
	}

	choice := resp.Choices[0]
	result := CompletionResponse{
		Content: choice.Message.Content,
		Usage: TokenUsage{
			InputTokens:  resp.Usage.PromptTokens,
			OutputTokens: resp.Usage.CompletionTokens,
			TotalTokens:  resp.Usage.TotalTokens,
		},
	}

	// Convert tool calls
	if len(choice.Message.ToolCalls) > 0 {
		result.ToolCalls = make([]ToolCall, len(choice.Message.ToolCalls))
		for i, tc := range choice.Message.ToolCalls {
			// Parse arguments JSON string
			var args map[string]interface{}
			if tc.Function.Arguments != "" {
				json.Unmarshal([]byte(tc.Function.Arguments), &args)
			}

			result.ToolCalls[i] = ToolCall{
				Name:      tc.Function.Name,
				Arguments: args,
			}
		}
	}

	return result
}
</file>
<file path="internal/llm/retry_client.go">
package llm

import (
	"context"
	"fmt"
	"math"
	"net/http"
	"time"
)

// RetryConfig holds retry configuration
type RetryConfig struct {
	MaxAttempts       int           // Maximum number of retry attempts
	Multiplier        int           // Exponential backoff multiplier
	MaxWaitPerAttempt time.Duration // Maximum wait time per attempt
	MaxTotalWait      time.Duration // Maximum total wait time
}

// DefaultRetryConfig returns default retry configuration
func DefaultRetryConfig() *RetryConfig {
	return &RetryConfig{
		MaxAttempts:       5,
		Multiplier:        1,
		MaxWaitPerAttempt: 60 * time.Second,
		MaxTotalWait:      300 * time.Second,
	}
}

// RetryClient wraps http.Client with retry logic
type RetryClient struct {
	client *http.Client
	config *RetryConfig
}

// NewRetryClient creates a new retry client
func NewRetryClient(config *RetryConfig) *RetryClient {
	if config == nil {
		config = DefaultRetryConfig()
	}

	return &RetryClient{
		client: &http.Client{
			Timeout: 180 * time.Second, // Default timeout
		},
		config: config,
	}
}

// NewRetryClientWithTimeout creates a retry client with custom timeout
func NewRetryClientWithTimeout(timeout time.Duration, config *RetryConfig) *RetryClient {
	if config == nil {
		config = DefaultRetryConfig()
	}

	return &RetryClient{
		client: &http.Client{
			Timeout: timeout,
		},
		config: config,
	}
}

// Do executes an HTTP request with retry logic
func (rc *RetryClient) Do(req *http.Request) (*http.Response, error) {
	return rc.DoWithContext(req.Context(), req)
}

// DoWithContext executes an HTTP request with retry logic and context
func (rc *RetryClient) DoWithContext(ctx context.Context, req *http.Request) (*http.Response, error) {
	var resp *http.Response
	var err error

	totalStartTime := time.Now()

	for attempt := 0; attempt < rc.config.MaxAttempts; attempt++ {
		// Clone the request for each attempt (request body can only be read once)
		reqClone := req.Clone(ctx)

		resp, err = rc.client.Do(reqClone)

		// Check if we should NOT retry
		if err == nil && resp != nil {
			// Success on 2xx and 3xx
			// Also retry on 429 (Too Many Requests) and 5xx
			if resp.StatusCode < 500 && resp.StatusCode != 429 {
				return resp, nil
			}

			// For 4xx errors (except 429), don't retry
			if resp.StatusCode >= 400 && resp.StatusCode < 500 && resp.StatusCode != 429 {
				return resp, nil // Return the error response to caller
			}
		}

		// Calculate wait time with exponential backoff
		waitTime := rc.calculateWaitTime(attempt)

		// Check if we've exceeded max total wait time
		if time.Since(totalStartTime)+waitTime > rc.config.MaxTotalWait {
			break
		}

		// Wait before retry (but not after the last attempt)
		if attempt < rc.config.MaxAttempts-1 {
			select {
			case <-time.After(waitTime):
				// Continue to next attempt
			case <-ctx.Done():
				return nil, ctx.Err()
			}
		}
	}

	// All retries exhausted
	if err != nil {
		return nil, fmt.Errorf("request failed after %d attempts: %w", rc.config.MaxAttempts, err)
	}

	if resp != nil {
		return nil, fmt.Errorf("request failed with status %d after %d attempts", resp.StatusCode, rc.config.MaxAttempts)
	}

	return nil, fmt.Errorf("request failed after %d attempts", rc.config.MaxAttempts)
}

// calculateWaitTime calculates wait time using exponential backoff
func (rc *RetryClient) calculateWaitTime(attempt int) time.Duration {
	// Exponential backoff: 2^attempt * multiplier seconds
	baseWait := time.Duration(math.Pow(2, float64(attempt))) * time.Duration(rc.config.Multiplier) * time.Second

	// Cap at max wait per attempt
	if baseWait > rc.config.MaxWaitPerAttempt {
		baseWait = rc.config.MaxWaitPerAttempt
	}

	return baseWait
}

// SetTimeout updates the client timeout
func (rc *RetryClient) SetTimeout(timeout time.Duration) {
	rc.client.Timeout = timeout
}

// GetTimeout returns the current client timeout
func (rc *RetryClient) GetTimeout() time.Duration {
	return rc.client.Timeout
}
</file>
<file path="internal/logging/logger.go">
package logging

import (
	"os"
	"path/filepath"

	"go.uber.org/zap"
	"go.uber.org/zap/zapcore"
)

// Field is a type alias for zap.Field
type Field = zap.Field

// Common field constructors
var (
	String   = zap.String
	Int      = zap.Int
	Int64    = zap.Int64
	Float64  = zap.Float64
	Bool     = zap.Bool
	Any      = zap.Any
	Error    = zap.Error
	Err      = zap.NamedError
	Duration = zap.Duration
	Time     = zap.Time
)

// LevelFromString converts a string level to zapcore.Level
func LevelFromString(level string) zapcore.Level {
	switch level {
	case "debug":
		return zapcore.DebugLevel
	case "info":
		return zapcore.InfoLevel
	case "warn":
		return zapcore.WarnLevel
	case "error":
		return zapcore.ErrorLevel
	default:
		return zapcore.InfoLevel
	}
}

// Logger wraps zap.Logger with application-specific methods
type Logger struct {
	zap *zap.Logger
}

// Config holds logger configuration
type Config struct {
	LogDir       string
	FileLevel    zapcore.Level
	ConsoleLevel zapcore.Level
	EnableCaller bool
}

// DefaultConfig returns default logger configuration
func DefaultConfig() *Config {
	return &Config{
		LogDir:       ".ai/logs",
		FileLevel:    zapcore.InfoLevel,
		ConsoleLevel: zapcore.DebugLevel,
		EnableCaller: true,
	}
}

// NewLogger creates a new logger with file and console output
func NewLogger(cfg *Config) (*Logger, error) {
	if cfg == nil {
		cfg = DefaultConfig()
	}

	// Ensure log directory exists
	if err := os.MkdirAll(cfg.LogDir, 0755); err != nil {
		return nil, err
	}

	// File encoder (JSON)
	fileEncoderConfig := zap.NewProductionEncoderConfig()
	fileEncoderConfig.TimeKey = "timestamp"
	fileEncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
	fileEncoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder
	fileEncoder := zapcore.NewJSONEncoder(fileEncoderConfig)

	// Console encoder (human-readable with colors)
	consoleEncoderConfig := zap.NewDevelopmentEncoderConfig()
	consoleEncoderConfig.EncodeLevel = zapcore.CapitalColorLevelEncoder
	consoleEncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
	consoleEncoder := zapcore.NewConsoleEncoder(consoleEncoderConfig)

	// File writer
	logFile := filepath.Join(cfg.LogDir, "gendocs.log")
	file, err := os.OpenFile(logFile, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		return nil, err
	}
	fileWriter := zapcore.AddSync(file)

	// Console writer
	consoleWriter := zapcore.AddSync(os.Stderr)

	// Core with both outputs
	core := zapcore.NewTee(
		zapcore.NewCore(fileEncoder, fileWriter, cfg.FileLevel),
		zapcore.NewCore(consoleEncoder, consoleWriter, cfg.ConsoleLevel),
	)

	// Create logger
	zapLogger := zap.New(core, zap.AddCaller(), zap.AddStacktrace(zapcore.ErrorLevel))

	return &Logger{zap: zapLogger}, nil
}

// NewNopLogger creates a no-op logger for testing
func NewNopLogger() *Logger {
	return &Logger{zap: zap.NewNop()}
}

// Sync flushes any buffered log entries
func (l *Logger) Sync() error {
	return l.zap.Sync()
}

// Debug logs a debug message
func (l *Logger) Debug(msg string, fields ...zap.Field) {
	l.zap.Debug(msg, fields...)
}

// Info logs an info message
func (l *Logger) Info(msg string, fields ...zap.Field) {
	l.zap.Info(msg, fields...)
}

// Warn logs a warning message
func (l *Logger) Warn(msg string, fields ...zap.Field) {
	l.zap.Warn(msg, fields...)
}

// Error logs an error message
func (l *Logger) Error(msg string, fields ...zap.Field) {
	l.zap.Error(msg, fields...)
}

// Fatal logs a fatal message and exits
func (l *Logger) Fatal(msg string, fields ...zap.Field) {
	l.zap.Fatal(msg, fields...)
}

// With creates a child logger with additional fields
func (l *Logger) With(fields ...zap.Field) *Logger {
	return &Logger{zap: l.zap.With(fields...)}
}

// Named creates a named child logger
func (l *Logger) Named(name string) *Logger {
	return &Logger{zap: l.zap.Named(name)}
}
</file>
<file path="internal/prompts/manager.go">
package prompts

import (
	"bytes"
	"fmt"
	"os"
	"path/filepath"
	textTemplate "text/template"

	"gopkg.in/yaml.v3"
)

// Manager handles loading and rendering prompt templates
type Manager struct {
	prompts map[string]string
}

// PromptTemplate represents a prompt with system and user components
type PromptTemplate struct {
	SystemPrompt string `yaml:"system_prompt"`
	UserPrompt   string `yaml:"user_prompt"`
}

// NewManager creates a new prompt manager by loading prompts from a directory
func NewManager(promptsDir string) (*Manager, error) {
	pm := &Manager{
		prompts: make(map[string]string),
	}

	// Load all YAML files from the prompts directory
	entries, err := os.ReadDir(promptsDir)
	if err != nil {
		return nil, fmt.Errorf("failed to read prompts directory: %w", err)
	}

	for _, entry := range entries {
		if entry.IsDir() || filepath.Ext(entry.Name()) != ".yaml" && filepath.Ext(entry.Name()) != ".yml" {
			continue
		}

		filePath := filepath.Join(promptsDir, entry.Name())
		data, err := os.ReadFile(filePath)
		if err != nil {
			return nil, fmt.Errorf("failed to read prompt file %s: %w", filePath, err)
		}

		// Parse YAML - could be simple string -> string or nested
		var prompts map[string]string
		if err := yaml.Unmarshal(data, &prompts); err != nil {
			return nil, fmt.Errorf("failed to parse prompts from %s: %w", filePath, err)
		}

		// Merge into main prompts map
		for key, value := range prompts {
			pm.prompts[key] = value
		}
	}

	return pm, nil
}

// NewManagerFromMap creates a prompt manager from a map (useful for testing)
func NewManagerFromMap(prompts map[string]string) *Manager {
	return &Manager{
		prompts: prompts,
	}
}

// Get returns a raw prompt by name
func (pm *Manager) Get(name string) (string, error) {
	prompt, ok := pm.prompts[name]
	if !ok {
		return "", fmt.Errorf("prompt '%s' not found (available: %v)", name, pm.getAvailableNames())
	}
	return prompt, nil
}

// Render renders a prompt template with the given variables
func (pm *Manager) Render(name string, vars map[string]interface{}) (string, error) {
	promptTemplate, err := pm.Get(name)
	if err != nil {
		return "", err
	}

	// Parse and execute template
	tmpl, err := textTemplate.New(name).Option("missingkey=error").Parse(promptTemplate)
	if err != nil {
		return "", fmt.Errorf("failed to parse template '%s': %w", name, err)
	}

	var buf bytes.Buffer
	if err := tmpl.Execute(&buf, vars); err != nil {
		return "", fmt.Errorf("failed to execute template '%s': %w", name, err)
	}

	return buf.String(), nil
}

// GetPromptTemplate returns a PromptTemplate with system and user prompts
func (pm *Manager) GetPromptTemplate(name string) (*PromptTemplate, error) {
	systemPrompt, err := pm.Render(name+"_system_prompt", nil)
	if err != nil {
		// Try without suffix
		systemPrompt, err = pm.Get(name + "_system")
		if err != nil {
			return nil, fmt.Errorf("system prompt '%s' not found", name)
		}
	}

	userPrompt, err := pm.Get(name + "_user_prompt")
	if err != nil {
		return nil, fmt.Errorf("user prompt '%s' not found", name)
	}

	return &PromptTemplate{
		SystemPrompt: systemPrompt,
		UserPrompt:   userPrompt,
	}, nil
}

// RenderTemplate renders both system and user prompts with variables
func (pm *Manager) RenderTemplate(name string, vars map[string]interface{}) (*PromptTemplate, error) {
	template, err := pm.GetPromptTemplate(name)
	if err != nil {
		return nil, err
	}

	// Render system prompt with variables
	systemTmpl, err := textTemplate.New("system").Parse(template.SystemPrompt)
	if err != nil {
		return nil, fmt.Errorf("failed to parse system prompt: %w", err)
	}

	var systemBuf bytes.Buffer
	if err := systemTmpl.Execute(&systemBuf, vars); err != nil {
		return nil, fmt.Errorf("failed to render system prompt: %w", err)
	}

	// Render user prompt with variables
	userTmpl, err := textTemplate.New("user").Parse(template.UserPrompt)
	if err != nil {
		return nil, fmt.Errorf("failed to parse user prompt: %w", err)
	}

	var userBuf bytes.Buffer
	if err := userTmpl.Execute(&userBuf, vars); err != nil {
		return nil, fmt.Errorf("failed to render user prompt: %w", err)
	}

	return &PromptTemplate{
		SystemPrompt: systemBuf.String(),
		UserPrompt:   userBuf.String(),
	}, nil
}

// getAvailableNames returns a list of available prompt names
func (pm *Manager) getAvailableNames() []string {
	names := make([]string, 0, len(pm.prompts))
	for name := range pm.prompts {
		names = append(names, name)
	}
	return names
}

// HasPrompt checks if a prompt exists
func (pm *Manager) HasPrompt(name string) bool {
	_, ok := pm.prompts[name]
	return ok
}
</file>
<file path="internal/tools/base.go">
package tools

import (
	"context"
	"fmt"
)

// ModelRetryError is raised when a tool encounters a recoverable error
// This error type triggers a retry at the agent level
type ModelRetryError struct {
	Message string
}

func (e *ModelRetryError) Error() string {
	return e.Message
}

// Tool is the interface that all tools must implement
type Tool interface {
	// Name returns the tool name
	Name() string

	// Description returns a description of what the tool does
	Description() string

	// Parameters returns the JSON schema for the tool's parameters
	Parameters() map[string]interface{}

	// Execute runs the tool with the given parameters
	Execute(ctx context.Context, params map[string]interface{}) (interface{}, error)
}

// BaseTool provides common functionality for all tools
type BaseTool struct {
	MaxRetries int
}

// NewBaseTool creates a new base tool
func NewBaseTool(maxRetries int) BaseTool {
	return BaseTool{
		MaxRetries: maxRetries,
	}
}

// RetryableExecute executes a function with retry logic
func (bt *BaseTool) RetryableExecute(ctx context.Context, fn func() (interface{}, error)) (interface{}, error) {
	var lastErr error

	for attempt := 0; attempt < bt.MaxRetries; attempt++ {
		result, err := fn()
		if err == nil {
			return result, nil
		}

		lastErr = err

		// Check if error is recoverable (ModelRetryError)
		if _, ok := err.(*ModelRetryError); !ok {
			// Not recoverable, return immediately
			return nil, err
		}

		// If it's the last attempt, don't wait
		if attempt == bt.MaxRetries-1 {
			break
		}
	}

	if lastErr != nil {
		return nil, fmt.Errorf("tool failed after %d retries: %w", bt.MaxRetries, lastErr)
	}

	return nil, fmt.Errorf("tool failed after %d retries", bt.MaxRetries)
}
</file>
<file path="internal/tools/file_read.go">
package tools

import (
	"context"
	"bufio"
	"fmt"
	"os"
	"strconv"
)

// FileReadTool reads file contents with optional pagination
type FileReadTool struct {
	BaseTool
}

// NewFileReadTool creates a new file read tool
func NewFileReadTool(maxRetries int) *FileReadTool {
	return &FileReadTool{
		BaseTool: NewBaseTool(maxRetries),
	}
}

// Name returns the tool name
func (frt *FileReadTool) Name() string {
	return "read_file"
}

// Description returns the tool description
func (frt *FileReadTool) Description() string {
	return "Read contents of a file. By default reads first 200 lines. Use line_number and line_count for pagination."
}

// Parameters returns the JSON schema for the tool parameters
func (frt *FileReadTool) Parameters() map[string]interface{} {
	return map[string]interface{}{
		"type": "object",
		"properties": map[string]interface{}{
			"file_path": map[string]interface{}{
				"type":        "string",
				"description": "Path to the file to read",
			},
			"line_number": map[string]interface{}{
				"type":        "integer",
				"description": "Starting line number (1-indexed). Default: 1",
			},
			"line_count": map[string]interface{}{
				"type":        "integer",
				"description": "Number of lines to read. Default: 200",
			},
		},
		"required": []string{"file_path"},
	}
}

// Execute reads the file contents
func (frt *FileReadTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) {
	return frt.RetryableExecute(ctx, func() (interface{}, error) {
		filePath, ok := params["file_path"].(string)
		if !ok {
			return nil, fmt.Errorf("file_path must be a string")
		}

		lineNumber := 1
		if ln, ok := params["line_number"]; ok {
			switch v := ln.(type) {
			case int:
				lineNumber = v
			case float64:
				lineNumber = int(v)
			case string:
				if i, err := strconv.Atoi(v); err == nil {
					lineNumber = i
				}
			}
		}

		lineCount := 200
		if lc, ok := params["line_count"]; ok {
			switch v := lc.(type) {
			case int:
				lineCount = v
			case float64:
				lineCount = int(v)
			case string:
				if i, err := strconv.Atoi(v); err == nil {
					lineCount = i
				}
			}
		}

		file, err := os.Open(filePath)
		if err != nil {
			return nil, &ModelRetryError{Message: fmt.Sprintf("Failed to open file: %v", err)}
		}
		defer file.Close()

		scanner := bufio.NewScanner(file)
		var lines []string
		currentLine := 1

		for scanner.Scan() {
			if currentLine >= lineNumber && currentLine < lineNumber+lineCount {
				lines = append(lines, scanner.Text())
			}
			currentLine++
			if currentLine >= lineNumber+lineCount {
				break
			}
		}

		if err := scanner.Err(); err != nil {
			return nil, &ModelRetryError{Message: fmt.Sprintf("Error reading file: %v", err)}
		}

		return map[string]interface{}{
			"content":         lines,
			"start_line":      lineNumber,
			"end_line":        lineNumber + len(lines) - 1,
			"total_lines_read": len(lines),
		}, nil
	})
}
</file>
<file path="internal/tools/list_files.go">
package tools

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
)

// ListFilesTool lists files in a directory recursively
type ListFilesTool struct {
	BaseTool
}

// NewListFilesTool creates a new list files tool
func NewListFilesTool(maxRetries int) *ListFilesTool {
	return &ListFilesTool{
		BaseTool: NewBaseTool(maxRetries),
	}
}

// Name returns the tool name
func (lft *ListFilesTool) Name() string {
	return "list_files"
}

// Description returns the tool description
func (lft *ListFilesTool) Description() string {
	return "List all files in a directory recursively"
}

// Parameters returns the JSON schema for the tool parameters
func (lft *ListFilesTool) Parameters() map[string]interface{} {
	return map[string]interface{}{
		"type": "object",
		"properties": map[string]interface{}{
			"directory": map[string]interface{}{
				"type":        "string",
				"description": "Directory path to list files from",
			},
		},
		"required": []string{"directory"},
	}
}

// Execute lists files in the directory
func (lft *ListFilesTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) {
	return lft.RetryableExecute(ctx, func() (interface{}, error) {
		directory, ok := params["directory"].(string)
		if !ok {
			return nil, fmt.Errorf("directory must be a string")
		}

		var files []string
		err := filepath.Walk(directory, func(path string, info os.FileInfo, err error) error {
			if err != nil {
				return err
			}
			if !info.IsDir() {
				relPath, err := filepath.Rel(directory, path)
				if err == nil {
					files = append(files, relPath)
				}
			}
			return nil
		})

		if err != nil {
			return nil, &ModelRetryError{Message: fmt.Sprintf("Failed to list files: %v", err)}
		}

		return map[string]interface{}{
			"files": files,
			"count": len(files),
		}, nil
	})
}
</file>
<file path="internal/tui/config.go">
package tui

import (
	"fmt"
	"os"
	"path/filepath"

	tea "github.com/charmbracelet/bubbletea"
	"github.com/charmbracelet/bubbles/textinput"
	"github.com/charmbracelet/lipgloss"
)

// Styles for the TUI
var (
	titleStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("##FAFAFA")).
			Background(lipgloss.Color("##7D56F4")).
			Padding(0, 1)

	highlightStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#7D56F4")).
			Bold(true)

	errorStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#FF5F87")).
			Bold(true)

	successStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#50FA7B")).
			Bold(true)
)

// Step represents a configuration step
type Step int

const (
	StepProvider Step = iota
	StepAPIKey
	StepModel
	StepBaseURL
	StepConfirm
	StepSave
	StepComplete
)

func (s Step) String() string {
	switch s {
	case StepProvider:
		return "Provider Selection"
	case StepAPIKey:
		return "API Key"
	case StepModel:
		return "Model"
	case StepBaseURL:
		return "Base URL (Optional)"
	case StepConfirm:
		return "Confirm"
	case StepSave:
		return "Save"
	case StepComplete:
		return "Complete"
	default:
		return "Unknown"
	}
}

// Model holds the TUI state
type Model struct {
	Step         Step
	Provider     string
	APIKey       string
	Model        string
	BaseURL      string
	Quitting     bool
	ConfigPath   string
	SavedConfig  bool
	Err          error
	// Text inputs for user input (exported fields)
	APIKeyInput  textinput.Model
	ModelInput   textinput.Model
	BaseURLInput textinput.Model
}

// ConfigResult holds the final configuration result
type ConfigResult struct {
	Saved bool
	Path  string
	Error error
}

// Init initializes the model
func (m Model) Init() tea.Cmd {
	return textinput.Blink
}

// Update handles messages
func (m Model) Update(msg tea.Msg) (tea.Model, tea.Cmd) {
	switch msg := msg.(type) {
	case tea.KeyMsg:
		switch msg.String() {
		case "ctrl+c", "q":
			m.Quitting = true
			return m, tea.Quit
		case "enter":
			// Handle Enter key based on current step
			switch m.Step {
			case StepProvider:
				// Only advance if a provider is selected
				if m.Provider != "" {
					m.Step = StepAPIKey
					m.APIKeyInput.Focus()
					m.ModelInput.Blur()
					m.BaseURLInput.Blur()
				}
			case StepAPIKey:
				m.APIKey = m.APIKeyInput.Value()
				if m.APIKey != "" {
					m.Step = StepModel
					m.APIKeyInput.Blur()
					m.ModelInput.Focus()
					m.BaseURLInput.Blur()
				}
			case StepModel:
				inputModel := m.ModelInput.Value()
				if inputModel != "" {
					m.Model = inputModel
				} else if m.Model == "" {
					// Set default model based on provider
					switch m.Provider {
					case "openai":
						m.Model = "gpt-4o"
					case "anthropic":
						m.Model = "claude-3-5-sonnet-20241022"
					case "gemini":
						m.Model = "gemini-1.5-pro"
					}
				}
				m.Step = StepBaseURL
				m.APIKeyInput.Blur()
				m.ModelInput.Blur()
				m.BaseURLInput.Focus()
			case StepBaseURL:
				m.BaseURL = m.BaseURLInput.Value()
				m.Step = StepConfirm
				m.APIKeyInput.Blur()
				m.ModelInput.Blur()
				m.BaseURLInput.Blur()
			case StepConfirm:
				// Enter on confirm step is handled by y/n
			}
		case "1":
			if m.Step == StepProvider {
				m.Provider = "openai"
				m.Model = "gpt-4o"
			}
		case "2":
			if m.Step == StepProvider {
				m.Provider = "anthropic"
				m.Model = "claude-3-5-sonnet-20241022"
			}
		case "3":
			if m.Step == StepProvider {
				m.Provider = "gemini"
				m.Model = "gemini-1.5-pro"
			}
		case "y", "Y":
			if m.Step == StepConfirm {
				m.saveConfig()
				m.Step = StepComplete
			}
		case "n", "N":
			if m.Step == StepConfirm {
				m.Step = StepProvider
			}
		case "esc":
			if m.Step == StepModel {
				m.Step = StepAPIKey
				m.APIKeyInput.Focus()
				m.ModelInput.Blur()
				m.BaseURLInput.Blur()
			} else if m.Step == StepBaseURL {
				m.Step = StepModel
				m.APIKeyInput.Blur()
				m.ModelInput.Focus()
				m.BaseURLInput.Blur()
			}
		}
	}

	// Update text inputs based on current step
	var cmd tea.Cmd
	switch m.Step {
	case StepAPIKey:
		m.APIKeyInput, cmd = m.APIKeyInput.Update(msg)
	case StepModel:
		m.ModelInput, cmd = m.ModelInput.Update(msg)
	case StepBaseURL:
		m.BaseURLInput, cmd = m.BaseURLInput.Update(msg)
	}

	return m, cmd
}

// View renders the UI
func (m Model) View() string {
	if m.Quitting {
		return "Exiting...\n"
	}

	if m.Step == StepComplete {
		if m.Err != nil {
			return fmt.Sprintf("\n%s\n\nError saving configuration: %v\n\nPress any key to exit...",
				errorStyle.Render("Configuration Failed"), m.Err)
		}
		return fmt.Sprintf("\n%s\n\nConfiguration saved to: %s\n\nPress any key to exit...",
			successStyle.Render("Configuration Saved Successfully!"), m.ConfigPath)
	}

	var s string

	// Title
	s += titleStyle.Render(" Gendocs Configuration Wizard ") + "\n\n"

	// Current step indicator
	stepNum := int(m.Step) + 1
	s += fmt.Sprintf("Step %d/5: %s\n\n", stepNum, m.Step.String())

	// Render current step content
	switch m.Step {
	case StepProvider:
		s += m.renderProviderSelection()
	case StepAPIKey:
		s += m.renderAPIKeyInput()
	case StepModel:
		s += m.renderModelInput()
	case StepBaseURL:
		s += m.renderBaseURLInput()
	case StepConfirm:
		s += m.renderConfirm()
	}

	// Help text
	s += "\n\n"
	if m.Step == StepProvider {
		s += "1-3: Select provider  |  Enter: Continue  |  q: Quit"
	} else if m.Step == StepConfirm {
		s += "y: Yes (save)  |  n: No (go back)  |  q: Quit"
	} else {
		s += "Type input  |  Enter: Continue  |  Esc: Go back  |  q: Quit"
	}

	return s + "\n"
}

func (m Model) renderProviderSelection() string {
	s := "Select your LLM provider:\n\n"
	
	providers := []struct {
		key   string
		name  string
		model string
	}{
		{"1", "OpenAI", "gpt-4o, gpt-4o-mini, etc."},
		{"2", "Anthropic Claude", "claude-3-5-sonnet, claude-3-haiku, etc."},
		{"3", "Google Gemini", "gemini-1.5-pro, gemini-pro, etc."},
	}

	for _, p := range providers {
		prefix := " "
		if m.Provider == getProviderFromKey(p.key) {
			prefix = highlightStyle.Render("✓")
		}
		s += fmt.Sprintf("%s %s. %s (%s)\n", prefix, p.key, p.name, p.model)
	}

	return s
}

func (m Model) renderAPIKeyInput() string {
	return fmt.Sprintf("Enter your API key for %s:\n\n%s\n\n(Press Enter when done)",
		highlightStyle.Render(m.Provider),
		m.APIKeyInput.View())
}

func (m Model) renderModelInput() string {
	defaultModel := m.Model
	if defaultModel == "" {
		switch m.Provider {
		case "openai":
			defaultModel = "gpt-4o"
		case "anthropic":
			defaultModel = "claude-3-5-sonnet-20241022"
		case "gemini":
			defaultModel = "gemini-1.5-pro"
		default:
			defaultModel = "<default>"
		}
	}
	return fmt.Sprintf("Enter model name (or press Enter for default %s):\n\n%s",
		highlightStyle.Render(defaultModel),
		m.ModelInput.View())
}

func (m Model) renderBaseURLInput() string {
	return fmt.Sprintf("Enter base URL (optional, press Enter to skip):\n\n%s\n\nLeave empty for provider default.",
		m.BaseURLInput.View())
}

func (m Model) renderConfirm() string {
	s := "Review your configuration:\n\n"
	s += fmt.Sprintf("  Provider:   %s\n", highlightStyle.Render(m.Provider))
	s += fmt.Sprintf("  Model:      %s\n", highlightStyle.Render(m.Model))
	if m.BaseURL != "" {
		s += fmt.Sprintf("  Base URL:   %s\n", highlightStyle.Render(m.BaseURL))
	}
	s += "\nSave this configuration?"
	return s
}

func (m Model) saveConfig() {
	homeDir, err := os.UserHomeDir()
	if err != nil {
		m.Err = err
		return
	}

	m.ConfigPath = filepath.Join(homeDir, ".gendocs.yaml")

	// Create YAML configuration
	configYAML := fmt.Sprintf("# Gendocs Global Configuration\nanalyzer:\n  llm:\n    provider: %s\n    api_key: %s\n    model: %s\n",
		m.Provider, m.APIKey, m.Model)

	if m.BaseURL != "" {
		configYAML += fmt.Sprintf("    base_url: %s\n", m.BaseURL)
	}

	if err := os.WriteFile(m.ConfigPath, []byte(configYAML), 0600); err != nil {
		m.Err = err
		return
	}

	m.SavedConfig = true
}

func getProviderFromKey(key string) string {
	switch key {
	case "1":
		return "openai"
	case "2":
		return "anthropic"
	case "3":
		return "gemini"
	default:
		return ""
	}
}

// GetConfigPath returns the path where config was saved
func (m Model) GetConfigPath() string {
	return m.ConfigPath
}
</file>
<file path="internal/worker_pool/pool.go">
package worker_pool

import (
	"context"
	"runtime"
	"sync"
)

// Task represents a unit of work to execute
type Task func(ctx context.Context) (interface{}, error)

// Result represents the result of a task execution
type Result struct {
	Value interface{}
	Error error
}

// WorkerPool executes tasks concurrently with semaphore-based limiting
type WorkerPool struct {
	maxWorkers int
	semaphore  chan struct{}
}

// NewWorkerPool creates a new worker pool
func NewWorkerPool(maxWorkers int) *WorkerPool {
	if maxWorkers <= 0 {
		maxWorkers = runtime.NumCPU()
	}

	return &WorkerPool{
		maxWorkers: maxWorkers,
		semaphore:  make(chan struct{}, maxWorkers),
	}
}

// Run executes all tasks concurrently and returns results in order
func (wp *WorkerPool) Run(ctx context.Context, tasks []Task) []Result {
	if len(tasks) == 0 {
		return []Result{}
	}

	numTasks := len(tasks)
	results := make([]Result, numTasks)
	var wg sync.WaitGroup

	for i, task := range tasks {
		wg.Add(1)
		go func(index int, t Task) {
			defer wg.Done()

			// Acquire semaphore (blocks if max workers already running)
			select {
			case wp.semaphore <- struct{}{}:
				defer func() { <-wp.semaphore }()
			case <-ctx.Done():
				results[index] = Result{Error: ctx.Err()}
				return
			}

			// Execute the task
			value, err := t(ctx)
			results[index] = Result{Value: value, Error: err}
		}(i, task)
	}

	wg.Wait()
	return results
}

// GetMaxWorkers returns the maximum number of workers
func (wp *WorkerPool) GetMaxWorkers() int {
	return wp.maxWorkers
}
</file>
<file path="prompts/ai_rules_generator.yaml">
# Prompts for AI Rules Generator Agent

ai_rules_claude_system: |
  You are an AI assistant documentation specialist who creates CLAUDE.md files.
  These files provide project-specific instructions for AI coding assistants.

  Your goal is to create a CLAUDE.md that helps AI assistants:
  - Understand the project's purpose and architecture
  - Follow the project's coding conventions
  - Know about key files and their purposes
  - Understand common commands and workflows
  - Be aware of project-specific patterns and practices

ai_rules_claude_user: |
  TASK: Generate CLAUDE.md

  Create a comprehensive CLAUDE.md file for the project at {{ .RepoPath }}.

  Use the existing analysis documents in {{ .RepoPath }}/.ai/docs/ to inform the documentation.

  The CLAUDE.md should include:

  1. **Project Overview**
  2. **Common Commands** - How to run, test, build
  3. **Architecture** - Key architectural patterns
  4. **Code Conventions** - Style guidelines, naming patterns
  5. **Key Files** - Important files and what they do
  6. **Testing** - How tests are organized and run
  7. **Troubleshooting** - Common issues and solutions

  Make the content practical and actionable for an AI assistant.
  Include actual command examples and real file paths from the project.

ai_rules_agents_system: |
  You are an AI agent documentation specialist who creates AGENTS.md files.
  These files document the agent architecture and conventions for multi-agent systems.

ai_rules_agents_user: |
  TASK: Generate AGENTS.md

  Create an AGENTS.md file for the project at {{ .RepoPath }}.

  If this project uses agents (like this documentation generator does), document:
  - Agent architecture and patterns
  - Handler-agent separation
  - How agents communicate
  - Tool system conventions
  - Configuration and setup

  Keep it concise but informative for developers working with the agent system.

ai_rules_cursor_system: |
  You are an IDE documentation specialist who creates Cursor AI rules.
  These rules help Cursor IDE provide better context-aware assistance.

ai_rules_cursor_user: |
  TASK: Generate Cursor AI Rules

  Create Cursor IDE rules (.cursor/rules/*.mdc files) for the project at {{ .RepoPath }}.

  Generate the following rule files:
  1. **project-overview.mdc** - High-level project overview
  2. **architecture.mdc** - Architecture patterns and design
  3. **code-patterns.mdc** - Coding conventions and patterns
  4. **agent-and-tool-conventions.mdc** - If applicable, agent/tool patterns

  Each rule should be concise, focused, and help Cursor understand the codebase better.
</file>
<file path="prompts/analyzer.yaml">
# System Prompts for Sub-Agents

structure_analyzer_system: |
  You are a code structure analyst specializing in identifying and documenting key architectural components.
  Your focus is on understanding the organization, abstraction patterns, and important services/modules in the codebase.
  You examine files, classes, interfaces, and their relationships without modifying any code.

  Your goal is to produce a comprehensive analysis of the codebase's architectural structure, key components, and design patterns.
  You will identify critical modules, interfaces, and core services that form the backbone of the application.
  You will document responsibility boundaries and how components interact at a structural level.

  Always return your response in Markdown format using the structure specified.

structure_analyzer_user: |
  TASK: Analyze Code Structure

  Examine the project at {{ .RepoPath }} to identify and document key structural elements.

  Your analysis should clearly map the structural architecture of the codebase, highlighting key components,
  their responsibilities, and relationships.

  Start by understanding the repository's high-level organization. Then dive into identifying:
  - Core modules and their purposes
  - Key interfaces and abstractions
  - Service components and their responsibilities
  - Architectural patterns used (MVC, hexagonal, microservices, etc.)
  - Important methods and functions that define the application's capabilities
  - Code organization principles and patterns

  Focus on the "what" and "why" of components rather than implementation details.
  Be sure that you are describing existing code, not hypothetical code.

  EXPECTED OUTPUT FORMAT:

  # Code Structure Analysis

  ## Architectural Overview
  [Brief overview of the overall architecture]

  ## Core Components
  [List and describe main components]

  ## Service Definitions
  [Describe key services and their purposes]

  ## Interface Contracts
  [Document important interfaces and their contracts]

  ## Design Patterns Identified
  [List patterns found in the codebase]

  ## Component Relationships
  [Describe how components interact]

  ## Key Methods & Functions
  [Important functions that define capabilities]

dependency_analyzer_system: |
  You are a dependency analyst who traces how modules and components depend on each other.
  Your focus is on understanding import relationships, external dependencies, and coupling patterns.

  Your goal is to map the complete dependency graph of the application, identifying:
  - Internal module dependencies
  - External libraries and frameworks used
  - Circular dependencies or anti-patterns
  - Dependency injection patterns

dependency_analyzer_user: |
  TASK: Analyze Dependencies

  Examine the project at {{ .RepoPath }} to trace and document dependencies.

  Focus on:
  - Internal module dependencies and how they relate
  - External dependencies (libraries, frameworks, services)
  - Dependency injection patterns
  - Potential issues like circular dependencies or tight coupling

  EXPECTED OUTPUT FORMAT:

  # Dependency Analysis

  ## Internal Dependencies
  [Map out how internal modules depend on each other]

  ## External Dependencies
  [List external libraries and their purposes]

  ## Dependency Graph
  [Describe the dependency structure]

  ## Dependency Injection
  [Document DI patterns if present]

  ## Potential Issues
  [Identify circular dependencies, tight coupling, etc.]

data_flow_analyzer_system: |
  You are a data flow specialist who tracks how data moves, transforms, and persists throughout an application.
  Your focus is on data structures, transformations, storage patterns, and the lifecycle of information.

data_flow_analyzer_user: |
  TASK: Analyze Data Flow

  Examine the project at {{ .RepoPath }} to trace and document how data flows through the system.

  Focus on:
  - Data models and structures
  - How data enters the system (inputs, APIs, etc.)
  - Data transformations and validations
  - Storage mechanisms and databases
  - Data outputs and responses

  EXPECTED OUTPUT FORMAT:

  # Data Flow Analysis

  ## Data Models
  [Describe key data structures]

  ## Input Sources
  [Where data enters the system]

  ## Data Transformations
  [How data is processed and transformed]

  ## Storage Mechanisms
  [Databases, caches, files, etc.]

  ## Data Validation
  [Where and how data is validated]

  ## Output Formats
  [How data leaves the system]

request_flow_analyzer_system: |
  You are a request flow analyst who traces how requests flow through the application from entry to exit.
  Your focus is on HTTP endpoints, message handlers, and the complete lifecycle of requests.

request_flow_analyzer_user: |
  TASK: Analyze Request Flow

  Examine the project at {{ .RepoPath }} to trace how requests are handled.

  Focus on:
  - API endpoints and their routes
  - Middleware and request processing pipeline
  - How requests are routed to handlers
  - Response generation and error handling

  EXPECTED OUTPUT FORMAT:

  # Request Flow Analysis

  ## API Endpoints
  [List all endpoints with methods and paths]

  ## Request Processing Pipeline
  [Describe middleware and processing steps]

  ## Routing Logic
  [How requests are routed to handlers]

  ## Response Generation
  [How responses are created and returned]

  ## Error Handling
  [How errors are handled in the request flow]

api_analyzer_system: |
  You are an API analyst who documents the external API surface of the application.
  Your focus is on endpoints, request/response formats, authentication, and API contracts.

api_analyzer_user: |
  TASK: Analyze API

  Examine the project at {{ .RepoPath }} to document the API.

  Focus on:
  - All API endpoints (HTTP methods, paths, parameters)
  - Request formats and validation rules
  - Response formats and status codes
  - Authentication and authorization
  - Rate limiting and other API policies

  EXPECTED OUTPUT FORMAT:

  # API Analysis

  ## Endpoints Overview
  [List all endpoints grouped by resource]

  ## Authentication
  [Describe auth mechanisms]

  ## Detailed Endpoints
  For each endpoint:
  ### Method /path
  - Description
  - Parameters
  - Request format
  - Response format
  - Status codes

  ## Common Patterns
  [Shared patterns across endpoints]
</file>
<file path="prompts/documenter.yaml">
# Prompts for Documenter Agent (README generation)

documenter_system: |
  You are a technical documentation specialist who creates comprehensive README.md files.
  You synthesize information from existing analysis documents to create user-friendly documentation.

  Your goal is to create a README that helps developers:
  - Understand what the project does
  - Get started quickly with installation and setup
  - Learn the architecture and key concepts
  - Find how to run tests and builds
  - Understand deployment processes

  Always write clear, concise, and well-organized Markdown.

documenter_user: |
  TASK: Generate README.md

  Create a comprehensive README.md for the project at {{ .RepoPath }}.

  Use the existing analysis documents in {{ .RepoPath }}/.ai/docs/ to inform your README.
  Combine information from structure, dependency, data flow, request flow, and API analyses.

  The README should include:

  1. **Project Title & Brief Description**
  2. **Features** - Key features and capabilities
  3. **Installation** - How to install dependencies
  4. **Quick Start** - How to run the project
  5. **Architecture** - High-level architecture overview
  6. **Development** - How to run tests, lint, build
  7. **Configuration** - Environment variables and config files
  8. **Contributing** - Brief contribution guidelines
  9. **License** - Reference to LICENSE file

  Make the README professional, clear, and welcoming to new developers.
  Use code blocks for commands and examples.
  Use proper Markdown formatting throughout.

  IMPORTANT:
  - Write factual information based on the actual codebase
  - Don't invent features that don't exist
  - Keep descriptions concise but informative
  - Focus on what developers need to know

  Output the complete README.md content in Markdown format.
</file>
<file path="go.mod">
module github.com/user/gendocs

go 1.25.5

require (
	github.com/joho/godotenv v1.5.1
	github.com/spf13/cobra v1.10.2
	github.com/spf13/viper v1.21.0
	go.uber.org/zap v1.27.1
	gopkg.in/yaml.v3 v3.0.1
)

require (
	github.com/atotto/clipboard v0.1.4 // indirect
	github.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect
	github.com/charmbracelet/bubbles v0.21.0 // indirect
	github.com/charmbracelet/bubbletea v1.3.4 // indirect
	github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc // indirect
	github.com/charmbracelet/lipgloss v1.1.0 // indirect
	github.com/charmbracelet/x/ansi v0.8.0 // indirect
	github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd // indirect
	github.com/charmbracelet/x/term v0.2.1 // indirect
	github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f // indirect
	github.com/fsnotify/fsnotify v1.9.0 // indirect
	github.com/go-viper/mapstructure/v2 v2.4.0 // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/lucasb-eyer/go-colorful v1.2.0 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-localereader v0.0.1 // indirect
	github.com/mattn/go-runewidth v0.0.16 // indirect
	github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 // indirect
	github.com/muesli/cancelreader v0.2.2 // indirect
	github.com/muesli/termenv v0.16.0 // indirect
	github.com/pelletier/go-toml/v2 v2.2.4 // indirect
	github.com/rivo/uniseg v0.4.7 // indirect
	github.com/sagikazarmark/locafero v0.11.0 // indirect
	github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 // indirect
	github.com/spf13/afero v1.15.0 // indirect
	github.com/spf13/cast v1.10.0 // indirect
	github.com/spf13/pflag v1.0.10 // indirect
	github.com/subosito/gotenv v1.6.0 // indirect
	github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e // indirect
	go.uber.org/multierr v1.10.0 // indirect
	go.yaml.in/yaml/v3 v3.0.4 // indirect
	golang.org/x/sync v0.16.0 // indirect
	golang.org/x/sys v0.36.0 // indirect
	golang.org/x/text v0.28.0 // indirect
)
</file>
<file path="go.sum">
github.com/atotto/clipboard v0.1.4 h1:EH0zSVneZPSuFR11BlR9YppQTVDbh5+16AmcJi4g1z4=
github.com/atotto/clipboard v0.1.4/go.mod h1:ZY9tmq7sm5xIbd9bOK4onWV4S6X0u6GY7Vn0Yu86PYI=
github.com/aymanbagabas/go-osc52/v2 v2.0.1 h1:HwpRHbFMcZLEVr42D4p7XBqjyuxQH5SMiErDT4WkJ2k=
github.com/aymanbagabas/go-osc52/v2 v2.0.1/go.mod h1:uYgXzlJ7ZpABp8OJ+exZzJJhRNQ2ASbcXHWsFqH8hp8=
github.com/charmbracelet/bubbles v0.21.0 h1:9TdC97SdRVg/1aaXNVWfFH3nnLAwOXr8Fn6u6mfQdFs=
github.com/charmbracelet/bubbles v0.21.0/go.mod h1:HF+v6QUR4HkEpz62dx7ym2xc71/KBHg+zKwJtMw+qtg=
github.com/charmbracelet/bubbletea v1.3.4 h1:kCg7B+jSCFPLYRA52SDZjr51kG/fMUEoPoZrkaDHyoI=
github.com/charmbracelet/bubbletea v1.3.4/go.mod h1:dtcUCyCGEX3g9tosuYiut3MXgY/Jsv9nKVdibKKRRXo=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc h1:4pZI35227imm7yK2bGPcfpFEmuY1gc2YSTShr4iJBfs=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc/go.mod h1:X4/0JoqgTIPSFcRA/P6INZzIuyqdFY5rm8tb41s9okk=
github.com/charmbracelet/lipgloss v1.1.0 h1:vYXsiLHVkK7fp74RkV7b2kq9+zDLoEU4MZoFqR/noCY=
github.com/charmbracelet/lipgloss v1.1.0/go.mod h1:/6Q8FR2o+kj8rz4Dq0zQc3vYf7X+B0binUUBwA0aL30=
github.com/charmbracelet/x/ansi v0.8.0 h1:9GTq3xq9caJW8ZrBTe0LIe2fvfLR/bYXKTx2llXn7xE=
github.com/charmbracelet/x/ansi v0.8.0/go.mod h1:wdYl/ONOLHLIVmQaxbIYEC/cRKOQyjTkowiI4blgS9Q=
github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd h1:vy0GVL4jeHEwG5YOXDmi86oYw2yuYUGqz6a8sLwg0X8=
github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd/go.mod h1:xe0nKWGd3eJgtqZRaN9RjMtK7xUYchjzPr7q6kcvCCs=
github.com/charmbracelet/x/term v0.2.1 h1:AQeHeLZ1OqSXhrAWpYUtZyX1T3zVxfpZuEQMIQaGIAQ=
github.com/charmbracelet/x/term v0.2.1/go.mod h1:oQ4enTYFV7QN4m0i9mzHrViD7TQKvNEEkHUMCmsxdUg=
github.com/cpuguy83/go-md2man/v2 v2.0.6/go.mod h1:oOW0eioCTA6cOiMLiUPZOpcVxMig6NIQQ7OS05n1F4g=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f h1:Y/CXytFA4m6baUTXGLOoWe4PQhGxaX0KpnayAqC48p4=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f/go.mod h1:vw97MGsxSvLiUE2X8qFplwetxpGLQrlU1Q9AUEIzCaM=
github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=
github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=
github.com/fsnotify/fsnotify v1.9.0 h1:2Ml+OJNzbYCTzsxtv8vKSFD9PbJjmhYF14k/jKC7S9k=
github.com/fsnotify/fsnotify v1.9.0/go.mod h1:8jBTzvmWwFyi3Pb8djgCCO5IBqzKJ/Jwo8TRcHyHii0=
github.com/go-viper/mapstructure/v2 v2.4.0 h1:EBsztssimR/CONLSZZ04E8qAkxNYq4Qp9LvH92wZUgs=
github.com/go-viper/mapstructure/v2 v2.4.0/go.mod h1:oJDH3BJKyqBA2TXFhDsKDGDTlndYOZ6rGS0BRZIxGhM=
github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=
github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
github.com/joho/godotenv v1.5.1 h1:7eLL/+HRGLY0ldzfGMeQkb7vMd0as4CfYvUVzLqw0N0=
github.com/joho/godotenv v1.5.1/go.mod h1:f4LDr5Voq0i2e/R5DDNOoa2zzDfwtkZa6DnEwAbqwq4=
github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/lucasb-eyer/go-colorful v1.2.0 h1:1nnpGOrhyZZuNyfu1QjKiUICQ74+3FNCN69Aj6K7nkY=
github.com/lucasb-eyer/go-colorful v1.2.0/go.mod h1:R4dSotOR9KMtayYi1e77YzuveK+i7ruzyGqttikkLy0=
github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mattn/go-localereader v0.0.1 h1:ygSAOl7ZXTx4RdPYinUpg6W99U8jWvWi9Ye2JC/oIi4=
github.com/mattn/go-localereader v0.0.1/go.mod h1:8fBrzywKY7BI3czFoHkuzRoWE9C+EiG4R1k4Cjx5p88=
github.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=
github.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 h1:ZK8zHtRHOkbHy6Mmr5D264iyp3TiX5OmNcI5cIARiQI=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6/go.mod h1:CJlz5H+gyd6CUWT45Oy4q24RdLyn7Md9Vj2/ldJBSIo=
github.com/muesli/cancelreader v0.2.2 h1:3I4Kt4BQjOR54NavqnDogx/MIoWBFa0StPA8ELUXHmA=
github.com/muesli/cancelreader v0.2.2/go.mod h1:3XuTXfFS2VjM+HTLZY9Ak0l6eUKfijIfMUZ4EgX0QYo=
github.com/muesli/termenv v0.16.0 h1:S5AlUN9dENB57rsbnkPyfdGuWIlkmzJjbFf0Tf5FWUc=
github.com/muesli/termenv v0.16.0/go.mod h1:ZRfOIKPFDYQoDFF4Olj7/QJbW60Ol/kL1pU3VfY/Cnk=
github.com/pelletier/go-toml/v2 v2.2.4 h1:mye9XuhQ6gvn5h28+VilKrrPoQVanw5PMw/TB0t5Ec4=
github.com/pelletier/go-toml/v2 v2.2.4/go.mod h1:2gIqNv+qfxSVS7cM2xJQKtLSTLUE9V8t9Stt+h56mCY=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=
github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=
github.com/rogpeppe/go-internal v1.9.0 h1:73kH8U+JUqXU8lRuOHeVHaa/SZPifC7BkcraZVejAe8=
github.com/rogpeppe/go-internal v1.9.0/go.mod h1:WtVeX8xhTBvf0smdhujwtBcq4Qrzq/fJaraNFVN+nFs=
github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/sagikazarmark/locafero v0.11.0 h1:1iurJgmM9G3PA/I+wWYIOw/5SyBtxapeHDcg+AAIFXc=
github.com/sagikazarmark/locafero v0.11.0/go.mod h1:nVIGvgyzw595SUSUE6tvCp3YYTeHs15MvlmU87WwIik=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 h1:+jumHNA0Wrelhe64i8F6HNlS8pkoyMv5sreGx2Ry5Rw=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8/go.mod h1:3n1Cwaq1E1/1lhQhtRK2ts/ZwZEhjcQeJQ1RuC6Q/8U=
github.com/spf13/afero v1.15.0 h1:b/YBCLWAJdFWJTN9cLhiXXcD7mzKn9Dm86dNnfyQw1I=
github.com/spf13/afero v1.15.0/go.mod h1:NC2ByUVxtQs4b3sIUphxK0NioZnmxgyCrfzeuq8lxMg=
github.com/spf13/cast v1.10.0 h1:h2x0u2shc1QuLHfxi+cTJvs30+ZAHOGRic8uyGTDWxY=
github.com/spf13/cast v1.10.0/go.mod h1:jNfB8QC9IA6ZuY2ZjDp0KtFO2LZZlg4S/7bzP6qqeHo=
github.com/spf13/cobra v1.10.2 h1:DMTTonx5m65Ic0GOoRY2c16WCbHxOOw6xxezuLaBpcU=
github.com/spf13/cobra v1.10.2/go.mod h1:7C1pvHqHw5A4vrJfjNwvOdzYu0Gml16OCs2GRiTUUS4=
github.com/spf13/pflag v1.0.9/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/pflag v1.0.10 h1:4EBh2KAYBwaONj6b2Ye1GiHfwjqyROoF4RwYO+vPwFk=
github.com/spf13/pflag v1.0.10/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/viper v1.21.0 h1:x5S+0EU27Lbphp4UKm1C+1oQO+rKx36vfCoaVebLFSU=
github.com/spf13/viper v1.21.0/go.mod h1:P0lhsswPGWD/1lZJ9ny3fYnVqxiegrlNrEmgLjbTCAY=
github.com/stretchr/testify v1.11.1 h1:7s2iGBzp5EwR7/aIZr8ao5+dra3wiQyKjjFuvgVKu7U=
github.com/stretchr/testify v1.11.1/go.mod h1:wZwfW3scLgRK+23gO65QZefKpKQRnfz6sD981Nm4B6U=
github.com/subosito/gotenv v1.6.0 h1:9NlTDc1FTs4qu0DDq7AEtTPNw6SVm7uBMsUCUjABIf8=
github.com/subosito/gotenv v1.6.0/go.mod h1:Dk4QP5c2W3ibzajGcXpNraDfq2IrhjMIvMSWPKKo0FU=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e h1:JVG44RsyaB9T2KIHavMF/ppJZNG9ZpyihvCd0w101no=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e/go.mod h1:RbqR21r5mrJuqunuUZ/Dhy/avygyECGrLceyNeo4LiM=
go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
go.uber.org/multierr v1.10.0 h1:S0h4aNzvfcFsC3dRF1jLoaov7oRaKqRGC/pUEJ2yvPQ=
go.uber.org/multierr v1.10.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN80Y=
go.uber.org/zap v1.27.1 h1:08RqriUEv8+ArZRYSTXy1LeBScaMpVSTBhCeaZYfMYc=
go.uber.org/zap v1.27.1/go.mod h1:GB2qFLM7cTU87MWRP2mPIjqfIDnGu+VIO4V/SdhGo2E=
go.yaml.in/yaml/v3 v3.0.4 h1:tfq32ie2Jv2UxXFdLJdh3jXuOzWiL1fo0bu/FbuKpbc=
go.yaml.in/yaml/v3 v3.0.4/go.mod h1:DhzuOOF2ATzADvBadXxruRBLzYTpT36CKvDb3+aBEFg=
golang.org/x/sync v0.16.0 h1:ycBJEhp9p4vXvUZNszeOq0kGTPghopOL8q0fq3vstxw=
golang.org/x/sync v0.16.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/sys v0.0.0-20210809222454-d867a43fc93e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.36.0 h1:KVRy2GtZBrk1cBYA7MKu5bEZFxQk4NIDV6RLVcC8o0k=
golang.org/x/sys v0.36.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
golang.org/x/text v0.28.0 h1:rhazDwis8INMIwQ4tpjLDzUhx6RlXqZNPEM0huQojng=
golang.org/x/text v0.28.0/go.mod h1:U8nCwOR8jO/marOQ0QbDiOngZVEBB7MAiitBuMjXiNU=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15 h1:YR8cESwS4TdDjEe65xsg0ogRM/Nc3DYOhEAlW+xobZo=
gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
</file>
<file path="INSTALL.md">
# Guia de Instalação e Configuração

Este guia fornece instruções passo a passo para instalar e configurar o Gendocs Go.

## Pré-requisitos

- **Go 1.22 ou posterior**
- **API key de um provedor LLM** (OpenAI, Anthropic, ou Google Gemini)
- (Opcional) **GitLab** com token OAuth para funcionalidade de cronjob

## 1. Instalação

### Opção A: Usar Makefile (Recomendado)

```bash
# Compilar
make build

# Instalar (requer sudo)
make install

# Verificar instalação
gendocs --version
```

### Opção B: Scripts de Instalação

```bash
# Instalar (requer sudo)
sudo ./install.sh

# Desinstalar
sudo ./uninstall.sh
```

### Opção C: Compilar Manualmente

```bash
git clone https://github.com/divar-ir/ai-doc-gen.git
cd ai-doc-gen-feature-go-version/gendocs

# Compilar
go build -o gendocs .

# Opcional: mover para PATH global
sudo mv gendocs /usr/local/bin/
```

### Opção D: Binário pré-compilado (quando disponível)

```bash
# Baixar binário
wget https://github.com/divar-ir/ai-doc-gen/releases/latest/download/gendocs-linux-amd64
chmod +x gendocs-linux-amd64
sudo mv gendocs-linux-amd64 /usr/local/bin/gendocs
```

## 2. Configuração Rápida

### Método 1: Wizard Interativo (Recomendado)

```bash
# Inicia o wizard de configuração
./gendocs config
```

O wizard vai te guiar através de:
1. Seleção do provedor (OpenAI, Anthropic, ou Gemini)
2. Configuração da API key
3. Seleção do modelo
4. (Opcional) Base URL para APIs compatíveis com OpenAI

A configuração é salva em `~/.gendocs.yaml`.

### Método 2: Variáveis de Ambiente

```bash
# Configurar provedor e API key
export ANALYZER_LLM_PROVIDER="openai"
export ANALYZER_LLM_MODEL="gpt-4o"
export ANALYZER_LLM_API_KEY="sk-sua-chave-aqui"

# Para Anthropic Claude
# export ANALYZER_LLM_PROVIDER="anthropic"
# export ANALYZER_LLM_MODEL="claude-3-5-sonnet-20241022"
# export ANALYZER_LLM_API_KEY="sk-ant-sua-chave-aqui"

# Para Google Gemini
# export ANALYZER_LLM_PROVIDER="gemini"
# export ANALYZER_LLM_MODEL="gemini-1.5-pro"
# export ANALYZER_LLM_CONFIG_API_KEY="sua-chave-aqui"
```

Adicione ao seu `~/.bashrc` ou `~/.zshrc`:

```bash
echo 'export ANALYZER_LLM_PROVIDER="openai"' >> ~/.bashrc
echo 'export ANALYZER_LLM_MODEL="gpt-4o"' >> ~/.bashrc
echo 'export ANALYZER_LLM_API_KEY="sk-sua-chave"' >> ~/.bashrc
source ~/.bashrc
```

### Método 3: Arquivo de Configuração `.ai/config.yaml`

Crie um arquivo `.ai/config.yaml` no seu projeto:

```yaml
analyzer:
  llm:
    provider: openai
    model: gpt-4o
    api_key: ${ANALYZER_LLM_API_KEY}
    base_url: ""  # Opcional, para APIs compatíveis
    retries: 2
    timeout: 180
    max_tokens: 8192
    temperature: 0.0
  max_workers: 0  # 0 = auto-detectar CPUs
  exclude_code_structure: false
  exclude_data_flow: false
  exclude_dependencies: false
  exclude_request_flow: false
  exclude_api_analysis: false
```

## 3. Verificar Instalação

```bash
# Verificar versão
./gendocs --version

# Verificar ajuda
./gendocs --help

# Verificar configuração (se usou wizard)
cat ~/.gendocs.yaml
```

## 4. Primeiro Uso

### Analisar um Projeto

```bash
# Analisar o diretório atual
./gendocs analyze --repo-path .

# Analisar outro diretório
./gendocs analyze --repo-path /caminho/para/projeto

# Com flags de exclusão
./gendocs analyze --repo-path . --exclude-api-analysis --exclude-dependencies

# Com depuração
./gendocs analyze --repo-path . --debug
```

Isso vai gerar arquivos em `.ai/docs/`:
- `structure_analysis.md`
- `dependency_analysis.md`
- `data_flow_analysis.md`
- `request_flow_analysis.md`
- `api_analysis.md`

### Gerar Documentação

```bash
# Gerar README.md a partir das análises
./gendocs generate readme --repo-path .

# Gerar arquivos de configuração para IA
./gendocs generate ai-rules --repo-path .
```

Isso vai criar:
- `README.md` no diretório raiz
- `CLAUDE.md` (instruções para Claude)
- `AGENTS.md` (convenções de agentes)

### Processamento em Lote GitLab

```bash
# Configurar GitLab
export GITLAB_API_URL="https://gitlab.com"
export GITLAB_OAUTH_TOKEN="glpat-sua-token-aqui"
export GITLAB_USER_EMAIL="seu-email@example.com"

# Processar todos os projetos de um grupo
./gendocs cronjob analyze --group-project-id 123 --max-days-since-last-commit 14
```

Isso vai:
1. Buscar todos os projetos do grupo
2. Filtrar (pular arquivados, sem commits recentes)
3. Clonar cada projeto
4. Rodar análise
5. Criar branch `ai-analyzer-YYYY-MM-DD`
6. Fazer commit com resultados
7. Criar Merge Request

## 5. Exemplos de Configuração

### OpenAI com Modelo Customizado

```yaml
# ~/.gendocs.yaml
analyzer:
  llm:
    provider: openai
    model: gpt-4o-mini
    max_tokens: 4096
```

### APIs Compatíveis com OpenAI

```yaml
analyzer:
  llm:
    provider: openai
    model: llama-3.1-70b-instruct
    base_url: https://api.deepinfra.com/v1/openai
```

### Anthropic Claude

```yaml
analyzer:
  llm:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
```

### Google Gemini via Vertex AI

```yaml
analyzer:
  llm:
    provider: gemini
    model: gemini-1.5-pro
gemini:
  use_vertex_ai: true
  project_id: seu-project-id
  location: us-central1
```

## 6. Troubleshooting

### Erro: "Required environment variable 'ANALYZER_LLM_API_KEY' is not set"

**Solução**: Configure a API key:
```bash
export ANALYZER_LLM_API_KEY="sk-sua-chave"
```

### Erro: "failed to load prompts"

**Solução**: Certifique-se de estar no diretório correto:
```bash
cd gendocs
./gendocs analyze --repo-path ../projeto-analisar
```

### Erro: "API error: status 401"

**Solução**: Verifique sua API key. Para testar:
```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $ANALYZER_LLM_API_KEY"
```

### Ver Logs de Depuração

```bash
# Ativar debug
./gendocs analyze --repo-path . --debug

# Logs são salvos em
cat .ai/logs/gendocs.log
```

## 7. Estrutura de Diretórios

```
projeto-analisado/
├── .ai/
│   ├── config.yaml          # Config do projeto (opcional)
│   ├── docs/
│   │   ├── structure_analysis.md
│   │   ├── dependency_analysis.md
│   │   ├── data_flow_analysis.md
│   │   ├── request_flow_analysis.md
│   │   └── api_analysis.md
│   └── logs/
│       └── gendocs.log        # Logs estruturados (JSON)
├── README.md                 # Gerado por `gendocs generate readme`
├── CLAUDE.md                 # Gerado por `gendocs generate ai-rules`
└── AGENTS.md                 # Gerado por `gendocs generate ai-rules`
```

## 8. Integração CI/CD

### GitHub Actions

```yaml
- name: Run gendocs analyze
  run: |
    go install github.com/divar-ir/ai-doc-gen/gendocs@latest
    gendocs analyze --repo-path .
```

### GitLab CI

```yaml
analyze:
  script:
    - go install github.com/divar-ir/ai-doc-gen/gendocs@latest
    - gendocs analyze --repo-path .
```

## 9. Atualização

```bash
cd ai-doc-gen-feature-go-version/gendocs
git pull
go build -o gendocs .
```

## 10. Suporte

- **Issues**: https://github.com/divar-ai-doc-gen/issues
- **Documentação**: Leia PLAN.md para detalhes da arquitetura
- **Python vs Go**: A versão Go mantém paridade de recursos com a Python
</file>
<file path="install.sh">
#!/bin/bash
# Script de instalação para Gendocs
# Uso: sudo ./install.sh

set -e

BINARY_NAME="gendocs"
BUILD_DIR="build"
BIN_DIR="/usr/local/bin"
CONFIG_DIR="$HOME/.gendocs.yaml"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

echo "=== Gendocs Installation Script ==="
echo ""

# Detect OS
OS="$(uname -s)"
case "$OS" in
    Linux*)
        BINARY="gendocs-linux-amd64"
        ;;
    Darwin*)
        BINARY="gendocs-darwin-amd64"
        ;;
    *)
        BINARY="gendocs"
        ;;
esac

echo "Detectado: $OS"
echo ""

# Check Go installation
if ! command -v go &> /dev/null; then
    echo "Erro: Go não está instalado."
    echo ""
    echo "Instale Go 1.22+:"
    echo "  https://go.dev/dl/"
    exit 1
fi

GO_VERSION=$(go version | awk '{print $3}')
echo "Go encontrado: $GO_VERSION"
echo ""

# Build
echo "Compilando..."
cd "$SCRIPT_DIR"
go build -o "$BUILD_DIR/$BINARY" .

# Install binary
echo "Instalando binário em $BIN_DIR..."
sudo mkdir -p "$BIN_DIR"
sudo cp "$BUILD_DIR/$BINARY" "$BIN_DIR/$BINARY_NAME"
sudo chmod +x "$BIN_DIR/$BINARY_NAME"

echo ""
echo "✅ Instalação completa!"
echo ""
echo "Binário instalado em: $BIN_DIR/$BINARY_NAME"
echo ""
echo "📖 Para configuração, execute:"
echo "  $BINARY_NAME config"
echo ""
echo "Ou configure manualmente:"
echo "  export ANALYZER_LLM_PROVIDER=\"openai\""
echo "  export ANALYZE_LLM_MODEL=\"gpt-4o\""
echo "  export ANALYZE_LLM_API_KEY=\"sk-...\""
echo ""
echo "🚀 Para analisar um projeto:"
echo "  $BINARY_NAME analyze --repo-path /caminho/para/projeto"
</file>
<file path="main.go">
package main

import "github.com/user/gendocs/cmd"

func main() {
	cmd.Execute()
}
</file>
<file path="Makefile">
.PHONY: all build install uninstall clean test help

# Variables
BINARY_NAME=gendocs
BUILD_DIR=build
# Instalação local em ~/.local/bin
BIN_DIR=$(HOME)/.local/bin
CONFIG_DIR=$(HOME)/.gendocs.yaml
PROMPTS_DIR=./prompts
GO=go
GOFLAGS=

# Detect OS
UNAME_S := $(shell uname -s)
ifeq ($(UNAME_S),Linux)
    BINARY=$(BINARY_NAME)-linux-amd64
else ifeq ($(UNAME_S),Darwin)
    BINARY=$(BINARY_NAME)-darwin-amd64
else
    BINARY=$(BINARY_NAME)
endif

all: build

help:
	@echo "Gendocs Makefile"
	@echo ""
	@echo "Available targets:"
	@echo "  make build        - Compila o binário"
	@echo "  make install      - Instala o binário em $(BIN_DIR)"
	@echo "  make uninstall    - Remove o binário de $(BIN_DIR)"
	@echo "  make clean        - Remove arquivos de build"
	@echo "  make test         - Executa testes"
	@echo "  make help         - Mostra esta mensagem"

build:
	@echo "Compilando $(BINARY)..."
	$(GO) $(GOFLAGS) build -o $(BUILD_DIR)/$(BINARY) .
	@echo "Binário criado: $(BUILD_DIR)/$(BINARY)"

install: build
	@echo "Instalando $(BINARY) em $(BIN_DIR)..."
	@mkdir -p $(BIN_DIR)
	@cp $(BUILD_DIR)/$(BINARY) $(BIN_DIR)/$(BINARY_NAME)
	@chmod +x $(BIN_DIR)/$(BINARY_NAME)
	@echo "Instalado em: $(BIN_DIR)/$(BINARY_NAME)"
	@echo ""
	@echo "Para configurar, execute:"
	@echo "  $(BINARY_NAME) config"
	@echo ""
	@echo "Ou configure manualmente:"
	@echo "  export ANALYZER_LLM_PROVIDER=\"openai\""
	@echo "  export ANALYZER_LLM_MODEL=\"gpt-4o\""
	@echo "  export ANALYZER_LLM_API_KEY=\"sk-...\""

uninstall:
	@echo "Removendo $(BINARY_NAME) de $(BIN_DIR)..."
	@rm -f $(BIN_DIR)/$(BINARY_NAME)
	@echo "Removido."
	@echo ""
	@echo "Para remover completamente (incluindo configuração):"
	@echo "  rm -f $(CONFIG_DIR)"
	@echo "  rm -rf ~/.gendocs/prompts_backup"

clean:
	@echo "Limpando arquivos de build..."
	@rm -rf $(BUILD_DIR)
	@echo "Limpo."

test:
	@echo "Executando testes..."
	$(GO) test -v ./...

# Development helpers
run: build
	@echo "Executando $(BUILD_DIR)/$(BINARY) analyze --repo-path ../.."
</file>
<file path="README.md">
# Gendocs Go Implementation

This is the Go port of the AI Documentation Generator. The Go version provides better performance, easier distribution, and simplified deployment while maintaining complete feature parity with the Python version.

## Status: ✅ IMPLEMENTATION COMPLETE

**Progress: ~95% of PLAN.md**

All major features have been implemented and the binary compiles successfully.

## Quick Start

```bash
# 1. Install Go 1.22+
# 2. Build
cd gendocs && go build -o gendocs .

# 3. Configure (option A: wizard, option B: env vars)
./gendocs config
# OR
export ANALYZER_LLM_PROVIDER="openai"
export ANALYZER_LLM_MODEL="gendocs analyze --repo-path ."
```

See [INSTALL.md](INSTALL.md) for detailed installation and configuration instructions.

## Features

### Commands

- ✅ `gendocs analyze` - Analyze codebase structure and dependencies
- ✅ `gendocs generate readme` - Generate README.md from analysis
- ✅ `gendocs generate ai-rules` - Generate AI assistant configs (CLAUDE.md, AGENTS.md)
- ✅ `gendocs cronjob analyze` - GitLab automated batch processing
- ✅ `gendocs config` - Interactive TUI configuration wizard

### LLM Providers

- ✅ OpenAI (including OpenAI-compatible APIs)
- ✅ Anthropic Claude
- ✅ Google Gemini

### Architecture

- ✅ **7 Agents**: AnalyzerAgent (orchestrator) + 5 sub-agents + DocumenterAgent + AIRulesAgent
- ✅ **Handler-Agent Pattern**: Clean separation between CLI, handlers, and agents
- ✅ **Tool System**: FileReadTool, ListFilesTool with retry logic
- ✅ **Worker Pool**: Semaphore-based concurrent execution
- ✅ **Configuration**: Multi-source (CLI > YAML > env > defaults)
- ✅ **Error Handling**: 14 exception types with rich context
- ✅ **Logging**: Structured JSON + colored console output

## Building

```bash
cd gendocs
go build -o gendocs .
```

## Usage

### Basic Usage

```bash
# Analyze a codebase
./gendocs analyze --repo-path ../my-project

# Generate README from analysis
./gendocs generate readme --repo-path ../my-project

# Generate AI assistant configs
./gendocs generate ai-rules --repo-path ../my-project

# Configure with interactive wizard
./gendocs config

# GitLab batch processing
./gendocs cronjob analyze --group-project-id 123 --max-days-since-last-commit 14
```

### Configuration

The Go version supports the same configuration sources as the Python version:

1. **CLI arguments** (highest priority)
2. **`.ai/config.yaml`** (project-specific)
3. **`~/.gendocs.yaml`** (global user config, from TUI)
4. **Environment variables**
5. **Defaults** (lowest priority)

### Environment Variables

```bash
# Analyzer configuration
export ANALYZER_LLM_PROVIDER="openai"  # openai, anthropic, gemini
export ANALYZER_LLM_MODEL="gpt-4o"
export ANALYZER_LLM_API_KEY="sk-..."
export ANALYZER_MAX_WORKERS=0  # 0 = auto-detect CPU count

# Documenter configuration
export DOCUMENTER_LLM_PROVIDER="openai"
export DOCUMENTER_LLM_MODEL="gpt-4o"
export DOCUMENTER_LLM_API_KEY="sk-..."

# GitLab configuration (for cronjob)
export GITLAB_API_URL="https://gitlab.example.com"
export GITLAB_OAUTH_TOKEN="glpat-..."
```

## Project Structure

```
gendocs/
├── cmd/                      # CLI commands (Cobra)
│   ├── root.go
│   ├── analyze.go
│   ├── generate.go
│   ├── cronjob.go
│   └── config.go
├── internal/
│   ├── agents/              # AI agents
│   │   ├── base.go
│   │   ├── analyzer.go      # AnalyzerAgent orchestrator
│   │   ├── sub_agents.go    # Sub-agent implementations
│   │   └── factory.go
│   ├── config/              # Configuration loading
│   ├── errors/              # 14 exception types
│   ├── gitlab/              # GitLab client
│   ├── handlers/            # Command handlers
│   │   ├── base.go
│   │   ├── analyze.go
│   │   ├── readme.go
│   │   ├── ai_rules.go
│   │   └── cronjob.go
│   ├── llm/                 # LLM providers
│   │   ├── client.go        # LLMClient interface
│   │   ├── openai.go
│   │   ├── anthropic.go
│   │   ├── gemini.go
│   │   ├── retry_client.go  # HTTP with retry
│   │   └── factory.go
│   ├── logging/             # Structured logging (zap)
│   ├── prompts/             # Prompt template manager
│   ├── tools/               # Agent tools
│   │   ├── base.go
│   │   ├── file_read.go
│   │   └── list_files.go
│   ├── tui/                 # TUI config wizard
│   │   └── config.go        # Bubble Tea UI
│   └── worker_pool/         # Concurrent execution
├── prompts/                 # YAML prompt templates
│   ├── analyzer.yaml
│   ├── documenter.yaml
│   └── ai_rules_generator.yaml
├── main.go
├── go.mod
├── go.sum
└── README.md
```

## Implementation Status

| Phase | Component | Status |
|-------|-----------|--------|
| 1 | Foundation (project, errors, logging, config) | ✅ 100% |
| 2 | LLM Integration (OpenAI, Anthropic, Gemini) | ✅ 100% |
| 3 | Tools & Worker Pool | ✅ 100% |
| 4 | Agents (7 agents with tool calling) | ✅ 100% |
| 5 | CLI & Handlers (5 commands) | ✅ 100% |
| 6 | GitLab Integration (cronjob) | ✅ 100% |
| 7 | TUI Config Wizard (Bubble Tea) | ✅ 100% |
| 8 | Testing | ⚠️ 0% |

## What Works

All CLI commands are implemented and functional:
- `gendocs analyze` with all exclusion flags
- `gendocs generate readme`
- `gendocs generate ai-rules`
- `gendocs cronjob analyze` (GitLab integration)
- `gendocs config` (interactive TUI wizard)

## What's Next

The implementation is functionally complete. Remaining work:
1. End-to-end testing with real LLM APIs
2. Unit tests for better coverage
3. Integration tests

## Migration from Python

The Go version maintains feature parity with the Python version:
- Same `.ai/config.yaml` format
- Same environment variable names
- Same CLI command structure
- Same output file formats

## Development

### Prerequisites

- Go 1.22 or later
- Access to LLM provider API

### Running

```bash
# Build
go build -o gendocs .

# Run
./gendocs --help
./gendocs analyze --repo-path ../my-project
```

## License

Same as the parent project.
</file>
<file path="uninstall.sh">
#!/bin/bash
# Script de desinstalação para Gendocs
# Uso: sudo ./uninstall.sh

set -e

BINARY_NAME="gendocs"
BIN_DIR="/usr/local/bin"
CONFIG_DIR="$HOME/.gendocs.yaml"

echo "=== Gendocs Uninstallation Script ==="
echo ""

# Remove binary
echo "Removendo binário de $BIN_DIR..."
if [ -f "$BIN_DIR/$BINARY_NAME" ]; then
    sudo rm -f "$BIN_DIR/$BINARY_NAME"
    echo "✅ Binário removido"
else
    echo "⚠️  Binário não encontrado em $BIN_DIR/$BINARY_NAME"
fi

# Ask about config
echo ""
read -p "Remover configuração em $CONFIG_DIR? (y/N) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    rm -f "$CONFIG_DIR"
    echo "✅ Configuração removida"
fi

echo ""
echo "Desinstalação completa!"
echo ""
echo "Para reinstalar:"
echo "  make install"
echo "  ou"
echo "  ./install.sh"
</file>
` ou trechos fornecidos.
   - Se não conseguir identificar a evidência, escreva "a confirmar".
6. Siga exatamente o **Formato de Saída Obrigatório**. Não adicione, remova ou renomeie seções ou campos.
7. Considere requisitos não funcionais onde relevante: segurança, privacidade, performance, confiabilidade, acessibilidade e manutenibilidade.
8. Mantenha as propostas realistas e implementáveis em passos incrementais.
9. Prefira **5 a 12 propostas de funcionalidades**, mais até **3 vitórias rápidas (quick wins)**.

---

# Estrutura do Projeto

└── gendocs/
    ├── build/
    ├── cmd/
    │   ├── analyze.go [3.1KB]
    │   ├── config.go [1.9KB]
    │   ├── cronjob.go [3.8KB]
    │   ├── generate.go [4.9KB]
    │   └── root.go [800B]
    ├── internal/
    │   ├── agents/
    │   │   ├── sub_agents/
    │   │   ├── analyzer.go [7.8KB]
    │   │   ├── base.go [4.7KB]
    │   │   ├── factory.go [3.3KB]
    │   │   └── sub_agents.go [2.9KB]
    │   ├── config/
    │   │   ├── loader.go [8.3KB]
    │   │   └── models.go [4.6KB]
    │   ├── errors/
    │   │   ├── agent.go [3.5KB]
    │   │   ├── base.go [1.5KB]
    │   │   ├── config.go [3.0KB]
    │   │   ├── context.go [1.6KB]
    │   │   ├── exit_codes.go [392B]
    │   │   ├── gitlab.go [2.7KB]
    │   │   ├── handler.go [2.4KB]
    │   │   └── validation.go [2.7KB]
    │   ├── gitlab/
    │   │   └── client.go [4.7KB]
    │   ├── handlers/
    │   │   ├── ai_rules.go [1.5KB]
    │   │   ├── analyze.go [2.1KB]
    │   │   ├── base.go [619B]
    │   │   ├── cronjob.go [6.5KB]
    │   │   └── readme.go [1.5KB]
    │   ├── llm/
    │   │   ├── anthropic.go [7.2KB]
    │   │   ├── client.go [1.6KB]
    │   │   ├── factory.go [803B]
    │   │   ├── gemini.go [8.4KB]
    │   │   ├── openai.go [6.9KB]
    │   │   └── retry_client.go [3.9KB]
    │   ├── logging/
    │   │   └── logger.go [3.7KB]
    │   ├── prompts/
    │   │   └── manager.go [4.5KB]
    │   ├── tools/
    │   │   ├── base.go [1.7KB]
    │   │   ├── file_read.go [2.9KB]
    │   │   └── list_files.go [1.8KB]
    │   ├── tui/
    │   │   └── config.go [8.0KB]
    │   └── worker_pool/
    │       └── pool.go [1.5KB]
    ├── pkg/
    ├── prompts/
    │   ├── ai_rules_generator.yaml [2.5KB]
    │   ├── analyzer.yaml [6.2KB]
    │   └── documenter.yaml [1.8KB]
    ├── INSTALL.md [7.0KB]
    ├── Makefile [2.0KB]
    ├── README.md [6.2KB]
    ├── README_OLD.md [5.4KB]
    ├── go.mod [2.0KB]
    ├── go.sum [9.3KB]
    ├── install.sh [1.5KB]
    ├── main.go [83B]
    └── uninstall.sh [795B]

<file path="cmd/analyze.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/handlers"
	"github.com/user/gendocs/internal/logging"
)

var (
	repoPath            string
	excludeStructure    bool
	excludeDataFlow     bool
	excludeDeps         bool
	excludeReqFlow      bool
	excludeAPI          bool
	maxWorkers          int
)

// analyzeCmd represents the analyze command
var analyzeCmd = &cobra.Command{
	Use:   "analyze",
	Short: "Analyze codebase structure and dependencies",
	Long: `Analyze the codebase to generate detailed documentation about:
  - Code structure and architecture
  - Dependencies and imports
  - Data flow through the system
  - Request/response flow
  - API endpoints and contracts

Results are written to .ai/docs/ directory.`,
	RunE: runAnalyze,
}

func init() {
	rootCmd.AddCommand(analyzeCmd)

	analyzeCmd.Flags().StringVar(&repoPath, "repo-path", ".", "Path to repository")
	analyzeCmd.Flags().BoolVar(&excludeStructure, "exclude-code-structure", false, "Exclude structure analysis")
	analyzeCmd.Flags().BoolVar(&excludeDataFlow, "exclude-data-flow", false, "Exclude data flow analysis")
	analyzeCmd.Flags().BoolVar(&excludeDeps, "exclude-dependencies", false, "Exclude dependency analysis")
	analyzeCmd.Flags().BoolVar(&excludeReqFlow, "exclude-request-flow", false, "Exclude request flow analysis")
	analyzeCmd.Flags().BoolVar(&excludeAPI, "exclude-api-analysis", false, "Exclude API analysis")
	analyzeCmd.Flags().IntVar(&maxWorkers, "max-workers", 0, "Maximum concurrent workers (0=auto)")
}

func runAnalyze(cmd *cobra.Command, args []string) error {
	// Build CLI overrides map
	cliOverrides := map[string]interface{}{
		"repo_path":              repoPath,
		"exclude_code_structure": excludeStructure,
		"exclude_data_flow":      excludeDataFlow,
		"exclude_dependencies":   excludeDeps,
		"exclude_request_flow":   excludeReqFlow,
		"exclude_api_analysis":   excludeAPI,
		"max_workers":            maxWorkers,
		"debug":                  debugFlag,
	}

	// Load configuration
	cfg, err := config.LoadAnalyzerConfig(repoPath, cliOverrides)
	if err != nil {
		return fmt.Errorf("failed to load configuration: %w", err)
	}

	// Initialize logger
	logDir := ".ai/logs"
	if cfg.RepoPath != "." {
		logDir = cfg.RepoPath + "/.ai/logs"
	}
	logCfg := &logging.Config{
		LogDir:       logDir,
		FileLevel:    logging.LevelFromString("info"),
		ConsoleLevel: logging.LevelFromString("debug"),
		EnableCaller: debugFlag,
	}

	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		return fmt.Errorf("failed to initialize logger: %w", err)
	}
	defer logger.Sync()

	logger.Info("Starting gendocs analyze",
		logging.String("repo_path", cfg.RepoPath),
		logging.Int("max_workers", cfg.MaxWorkers),
	)

	// Create and run AnalyzeHandler
	handler := handlers.NewAnalyzeHandler(*cfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		// Handle error with proper exit code
		if docErr, ok := err.(*errors.AIDocGenError); ok {
			fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
			return docErr
		}
		return err
	}

	logger.Info("Analysis complete")
	return nil
}
</file>
<file path="cmd/config.go">
package cmd

import (
	"fmt"
	"os"

	tea "github.com/charmbracelet/bubbletea"
	"github.com/charmbracelet/bubbles/textinput"
	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/tui"
)

// configCmd represents the config command
var configCmd = &cobra.Command{
	Use:   "config",
	Short: "Configure gendocs settings",
	Long: `Launch an interactive configuration wizard to set up your LLM provider,
API key, and model preferences. Configuration is saved to ~/.gendocs.yaml.`,
	RunE: runConfig,
}

func init() {
	rootCmd.AddCommand(configCmd)
}

func runConfig(cmd *cobra.Command, args []string) error {
	// Initialize text inputs
	apiKeyInput := textinput.New()
	apiKeyInput.Placeholder = "Enter your API key"
	apiKeyInput.EchoMode = textinput.EchoPassword
	apiKeyInput.EchoCharacter = '•'
	apiKeyInput.Focus()

	modelInput := textinput.New()
	modelInput.Placeholder = "Press Enter for default model"

	baseURLInput := textinput.New()
	baseURLInput.Placeholder = "https://api.example.com (optional)"

	// Initialize Bubble Tea model
	model := tui.Model{
		Step:         0,
		Provider:     "",
		Model:        "",
		BaseURL:      "",
		Quitting:     false,
		APIKeyInput:  apiKeyInput,
		ModelInput:   modelInput,
		BaseURLInput: baseURLInput,
	}

	// Start Bubble Tea program
	p := tea.NewProgram(
		model,
		tea.WithAltScreen(),       // Use full screen mode
		tea.WithMouseCellMotion(), // Enable mouse motion
	)

	finalModel, err := p.Run()
	if err != nil {
		return fmt.Errorf("error running config wizard: %w", err)
	}

	// Type assertion to get our model back
	m, ok := finalModel.(tui.Model)
	if !ok {
		return fmt.Errorf("unexpected model type")
	}

	// Show final status
	if m.Err != nil {
		fmt.Fprintf(os.Stderr, "\nError: %v\n", m.Err)
		return m.Err
	}

	if m.SavedConfig {
		fmt.Printf("\nConfiguration saved to: %s\n", m.GetConfigPath())
		fmt.Println("\nYou can now run:")
		fmt.Println("  gendocs analyze --repo-path .")
	}

	return nil
}
</file>
<file path="cmd/cronjob.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/handlers"
	"github.com/user/gendocs/internal/logging"
)

// cronjobCmd represents the cronjob command
var cronjobCmd = &cobra.Command{
	Use:   "cronjob",
	Short: "Automated batch processing for GitLab projects",
	Long: `Process multiple GitLab projects automatically, analyzing each
and creating merge requests with the results.`,
}

var (
	cronjobMaxDays    int
	cronjobWorkingPath string
	cronjobGroupID     int
)

// cronjobAnalyzeCmd represents the cronjob analyze command
var cronjobAnalyzeCmd = &cobra.Command{
	Use:   "analyze",
	Short: "Analyze all applicable GitLab projects in a group",
	Long: `Fetch all projects in a GitLab group, filter them based on
configuration, and run analysis on each applicable project.
Creates branches and merge requests automatically.`,
	RunE: runCronjobAnalyze,
}

func init() {
	rootCmd.AddCommand(cronjobCmd)
	cronjobCmd.AddCommand(cronjobAnalyzeCmd)

	cronjobAnalyzeCmd.Flags().IntVar(&cronjobMaxDays, "max-days-since-last-commit", 14, "Skip projects with no commits in N days")
	cronjobAnalyzeCmd.Flags().StringVar(&cronjobWorkingPath, "working-path", "./work", "Working directory for cloning repos")
	cronjobAnalyzeCmd.Flags().IntVar(&cronjobGroupID, "group-project-id", 0, "GitLab group/project ID to analyze")
	cronjobAnalyzeCmd.MarkFlagRequired("group-project-id")
}

func runCronjobAnalyze(cmd *cobra.Command, args []string) error {
	// Validate required GitLab configuration
	gitLabToken := os.Getenv("GITLAB_OAUTH_TOKEN")
	if gitLabToken == "" {
		return errors.NewMissingEnvVarError("GITLAB_OAUTH_TOKEN", "GitLab API authentication token")
	}

	gitLabURL := os.Getenv("GITLAB_API_URL")
	if gitLabURL == "" {
		gitLabURL = "https://gitlab.com"
	}

	// Build configurations
	cronjobCfg := config.CronjobConfig{
		MaxDaysSinceLastCommit: cronjobMaxDays,
		WorkingPath:            cronjobWorkingPath,
		GroupProjectID:         cronjobGroupID,
	}

	gitLabCfg := config.GitLabConfig{
		APIURL:      gitLabURL,
		OAuthToken:  gitLabToken,
		UserName:    os.Getenv("GITLAB_USER_NAME"),
		UserUsername: os.Getenv("GITLAB_USER_USERNAME"),
		UserEmail:   os.Getenv("GITLAB_USER_EMAIL"),
	}

	// Set defaults for GitLab user info
	if gitLabCfg.UserName == "" {
		gitLabCfg.UserName = "AI Analyzer"
	}
	if gitLabCfg.UserUsername == "" {
		gitLabCfg.UserUsername = "agent_doc"
	}

	// Analyzer configuration (from env vars with defaults for cronjob)
	analyzerCfg := config.AnalyzerConfig{
		BaseConfig: config.BaseConfig{
			Debug: debugFlag,
		},
		LLM: config.LLMConfig{
			Provider:    os.Getenv("ANALYZER_LLM_PROVIDER"),
			Model:       os.Getenv("ANALYZER_LLM_MODEL"),
			APIKey:      os.Getenv("ANALYZER_LLM_API_KEY"),
			BaseURL:     os.Getenv("ANALYZER_LLM_BASE_URL"),
			Retries:     2,
			Timeout:     180,
			MaxTokens:   8192,
			Temperature: 0.0,
		},
		MaxWorkers: 0, // Auto-detect
	}

	// Initialize logger
	logCfg := &logging.Config{
		LogDir:       cronjobWorkingPath + "/.ai/logs",
		FileLevel:    logging.LevelFromString("info"),
		ConsoleLevel: logging.LevelFromString("debug"),
		EnableCaller: debugFlag,
	}

	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		return fmt.Errorf("failed to initialize logger: %w", err)
	}
	defer logger.Sync()

	logger.Info("Starting cronjob analysis",
		logging.Int("group_id", cronjobGroupID),
		logging.Int("max_days", cronjobMaxDays),
	)

	// Create and run CronjobHandler
	handler := handlers.NewCronjobHandler(cronjobCfg, gitLabCfg, analyzerCfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		if docErr, ok := err.(*errors.AIDocGenError); ok {
			fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
			return docErr
		}
		return err
	}

	logger.Info("Cronjob analysis complete")
	return nil
}
</file>
<file path="cmd/generate.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/handlers"
	"github.com/user/gendocs/internal/logging"
)

// generateCmd represents the generate command
var generateCmd = &cobra.Command{
	Use:   "generate",
	Short: "Generate documentation from analysis results",
	Long:  `Generate documentation files (README.md, AI rules) from existing analysis results.`,
}

var (
	readmeRepoPath string
)

// readmeCmd represents the generate readme command
var readmeCmd = &cobra.Command{
	Use:   "readme",
	Short: "Generate README.md from analysis results",
	Long: `Generate a comprehensive README.md file based on existing analysis documents
in .ai/docs/. This synthesizes information from structure, dependency, data flow,
request flow, and API analyses into a user-friendly README.`,
	RunE: runReadme,
}

func init() {
	rootCmd.AddCommand(generateCmd)
	generateCmd.AddCommand(readmeCmd)

	readmeCmd.Flags().StringVar(&readmeRepoPath, "repo-path", ".", "Path to repository")
}

func runReadme(cmd *cobra.Command, args []string) error {
	// Build configuration
	cfg := config.DocumenterConfig{
		BaseConfig: config.BaseConfig{
			RepoPath: readmeRepoPath,
			Debug:    debugFlag,
		},
		LLM: config.LLMConfig{
			Provider:    os.Getenv("DOCUMENTER_LLM_PROVIDER"),
			Model:       os.Getenv("DOCUMENTER_LLM_MODEL"),
			APIKey:      os.Getenv("DOCUMENTER_LLM_API_KEY"),
			BaseURL:     os.Getenv("DOCUMENTER_LLM_BASE_URL"),
			Retries:     2,
			Timeout:     180,
			MaxTokens:   8192,
			Temperature: 0.0,
		},
	}

	// Set defaults from environment if not set
	if cfg.LLM.Provider == "" {
		cfg.LLM.Provider = os.Getenv("ANALYZER_LLM_PROVIDER")
	}
	if cfg.LLM.Model == "" {
		cfg.LLM.Model = os.Getenv("ANALYZER_LLM_MODEL")
	}
	if cfg.LLM.APIKey == "" {
		cfg.LLM.APIKey = os.Getenv("ANALYZER_LLM_API_KEY")
	}

	// Initialize logger
	logDir := ".ai/logs"
	if readmeRepoPath != "." {
		logDir = readmeRepoPath + "/.ai/logs"
	}
	logCfg := &logging.Config{
		LogDir:       logDir,
		FileLevel:    logging.LevelFromString("info"),
		ConsoleLevel: logging.LevelFromString("debug"),
		EnableCaller: debugFlag,
	}

	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		return fmt.Errorf("failed to initialize logger: %w", err)
	}
	defer logger.Sync()

	logger.Info("Starting README generation",
		logging.String("repo_path", readmeRepoPath),
	)

	// Create and run ReadmeHandler
	handler := handlers.NewReadmeHandler(cfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		if docErr, ok := err.(*errors.AIDocGenError); ok {
			fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
			return docErr
		}
		return err
	}

	logger.Info("README.md generation complete")
	return nil
}

// aiRulesCmd represents the generate ai-rules command
var aiRulesCmd = &cobra.Command{
	Use:   "ai-rules",
	Short: "Generate AI assistant configuration files",
	Long: `Generate AI assistant configuration files (CLAUDE.md, AGENTS.md, .cursor/rules/)
from existing analysis results. These files help AI coding assistants understand the project.`,
	RunE: runAIRules,
}

func init() {
	generateCmd.AddCommand(aiRulesCmd)
	aiRulesCmd.Flags().StringVar(&readmeRepoPath, "repo-path", ".", "Path to repository")
}

func runAIRules(cmd *cobra.Command, args []string) error {
	// Build configuration
	cfg := config.AIRulesConfig{
		BaseConfig: config.BaseConfig{
			RepoPath: readmeRepoPath,
			Debug:    debugFlag,
		},
		LLM: config.LLMConfig{
			Provider:    os.Getenv("AI_RULES_LLM_PROVIDER"),
			Model:       os.Getenv("AI_RULES_LLM_MODEL"),
			APIKey:      os.Getenv("AI_RULES_LLM_API_KEY"),
			BaseURL:     os.Getenv("AI_RULES_LLM_BASE_URL"),
			Retries:     2,
			Timeout:     240,
			MaxTokens:   8192,
			Temperature: 0.0,
		},
	}

	// Set defaults from environment if not set
	if cfg.LLM.Provider == "" {
		cfg.LLM.Provider = os.Getenv("ANALYZER_LLM_PROVIDER")
	}
	if cfg.LLM.Model == "" {
		cfg.LLM.Model = os.Getenv("ANALYZER_LLM_MODEL")
	}
	if cfg.LLM.APIKey == "" {
		cfg.LLM.APIKey = os.Getenv("ANALYZER_LLM_API_KEY")
	}

	// Initialize logger
	logDir := ".ai/logs"
	if readmeRepoPath != "." {
		logDir = readmeRepoPath + "/.ai/logs"
	}
	logCfg := &logging.Config{
		LogDir:       logDir,
		FileLevel:    logging.LevelFromString("info"),
		ConsoleLevel: logging.LevelFromString("debug"),
		EnableCaller: debugFlag,
	}

	logger, err := logging.NewLogger(logCfg)
	if err != nil {
		return fmt.Errorf("failed to initialize logger: %w", err)
	}
	defer logger.Sync()

	logger.Info("Starting AI rules generation",
		logging.String("repo_path", readmeRepoPath),
	)

	// Create and run AIRulesHandler
	handler := handlers.NewAIRulesHandler(cfg, logger)

	if err := handler.Handle(cmd.Context()); err != nil {
		if docErr, ok := err.(*errors.AIDocGenError); ok {
			fmt.Fprintf(os.Stderr, "%s\n", docErr.GetUserMessage())
			return docErr
		}
		return err
	}

	logger.Info("AI rules generation complete")
	return nil
}
</file>
<file path="cmd/root.go">
package cmd

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
)

var (
	debugFlag bool
)

// rootCmd represents the base command
var rootCmd = &cobra.Command{
	Use:   "gendocs",
	Short: "AI-powered code documentation generator",
	Long: `Generate comprehensive documentation for your codebase using AI.

Gendocs analyzes your codebase structure, dependencies, data flow, and APIs
to generate detailed documentation including README.md, AI assistant configs,
and more.`,
	Version: "2.0.0",
}

// Execute runs the root command
func Execute() {
	if err := rootCmd.Execute(); err != nil {
		fmt.Fprintf(os.Stderr, "Error: %v\n", err)
		os.Exit(1)
	}
}

func init() {
	// Persistent flags (available to all subcommands)
	rootCmd.PersistentFlags().BoolVar(&debugFlag, "debug", false, "Enable debug mode")
}
</file>
<file path="internal/agents/analyzer.go">
package agents

import (
	"context"
	"fmt"
	"path/filepath"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	"github.com/user/gendocs/internal/worker_pool"
)

// AnalyzerAgent orchestrates all sub-agents for code analysis
type AnalyzerAgent struct {
	config        config.AnalyzerConfig
	llmFactory    *llm.Factory
	promptManager *prompts.Manager
	logger        *logging.Logger
	workerPool    *worker_pool.WorkerPool
}

// AnalysisResult represents the result of an analysis
type AnalysisResult struct {
	Successful []string
	Failed     []FailedAnalysis
}

// FailedAnalysis represents a failed analysis
type FailedAnalysis struct {
	Name  string
	Error error
}

// NewAnalyzerAgent creates a new analyzer agent
func NewAnalyzerAgent(cfg config.AnalyzerConfig, promptManager *prompts.Manager, logger *logging.Logger) *AnalyzerAgent {
	// Create retry client
	retryClient := llm.NewRetryClient(llm.DefaultRetryConfig())

	// Create LLM factory
	factory := llm.NewFactory(retryClient)

	return &AnalyzerAgent{
		config:        cfg,
		llmFactory:    factory,
		promptManager: promptManager,
		logger:        logger,
		workerPool:    worker_pool.NewWorkerPool(cfg.MaxWorkers),
	}
}

// Run executes all sub-agents concurrently
func (aa *AnalyzerAgent) Run(ctx context.Context) (*AnalysisResult, error) {
	aa.logger.Info("Starting analysis",
		logging.String("repo_path", aa.config.RepoPath),
		logging.Int("max_workers", aa.config.MaxWorkers),
	)

	// Use the existing factory
	factory := aa.llmFactory

	// Build task list based on configuration
	var tasks []worker_pool.Task
	var outputPaths []string

	docsDir := filepath.Join(aa.config.RepoPath, ".ai", "docs")

	if !aa.config.ExcludeStructure {
		task, outputPath := aa.createTask(ctx, factory, "structure_analyzer", CreateStructureAnalyzer,
			filepath.Join(docsDir, "structure_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if !aa.config.ExcludeDeps {
		task, outputPath := aa.createTask(ctx, factory, "dependency_analyzer", CreateDependencyAnalyzer,
			filepath.Join(docsDir, "dependency_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if !aa.config.ExcludeDataFlow {
		task, outputPath := aa.createTask(ctx, factory, "data_flow_analyzer", CreateDataFlowAnalyzer,
			filepath.Join(docsDir, "data_flow_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if !aa.config.ExcludeReqFlow {
		task, outputPath := aa.createTask(ctx, factory, "request_flow_analyzer", CreateRequestFlowAnalyzer,
			filepath.Join(docsDir, "request_flow_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if !aa.config.ExcludeAPI {
		task, outputPath := aa.createTask(ctx, factory, "api_analyzer", CreateAPIAnalyzer,
			filepath.Join(docsDir, "api_analysis.md"))
		tasks = append(tasks, task)
		outputPaths = append(outputPaths, outputPath)
	}

	if len(tasks) == 0 {
		return nil, fmt.Errorf("no analysis tasks to run (all agents excluded)")
	}

	aa.logger.Info(fmt.Sprintf("Running %d analysis tasks concurrently", len(tasks)))

	// Execute all tasks concurrently
	results := aa.workerPool.Run(ctx, tasks)

	// Process results
	return aa.processResults(outputPaths, results), nil
}

// AgentCreator is a function that creates an agent
type AgentCreator func(llmCfg config.LLMConfig, repoPath string, factory *llm.Factory, promptMgr *prompts.Manager, logger *logging.Logger) (*SubAgent, error)

// createTask creates a task for the worker pool
func (aa *AnalyzerAgent) createTask(ctx context.Context, factory *llm.Factory, name string, creator AgentCreator, outputPath string) (worker_pool.Task, string) {
	task := func(ctx context.Context) (interface{}, error) {
		aa.logger.Info(fmt.Sprintf("Creating %s", name))

		// Create agent
		agent, err := creator(aa.config.LLM, aa.config.RepoPath, factory, aa.promptManager, aa.logger)
		if err != nil {
			return nil, fmt.Errorf("failed to create %s: %w", name, err)
		}

		// Run agent
		output, err := agent.Run(ctx)
		if err != nil {
			return nil, fmt.Errorf("%s failed: %w", name, err)
		}

		// Save output
		if err := agent.SaveOutput(output, outputPath); err != nil {
			return nil, fmt.Errorf("failed to save %s output: %w", name, err)
		}

		aa.logger.Info(fmt.Sprintf("%s completed successfully", name))
		return output, nil
	}

	return task, outputPath
}

// processResults processes worker pool results
func (aa *AnalyzerAgent) processResults(outputPaths []string, results []worker_pool.Result) *AnalysisResult {
	result := &AnalysisResult{
		Successful: []string{},
		Failed:     []FailedAnalysis{},
	}

	for i, r := range results {
		// Get agent name from output path
		name := filepath.Base(outputPaths[i])
		name = name[:len(name)-11] // Remove "_analysis.md"

		if r.Error != nil {
			result.Failed = append(result.Failed, FailedAnalysis{
				Name:  name,
				Error: r.Error,
			})
			aa.logger.Error(fmt.Sprintf("%s failed", name), logging.Error(r.Error))
		} else {
			result.Successful = append(result.Successful, name)
			aa.logger.Info(fmt.Sprintf("%s succeeded", name))
		}
	}

	aa.logger.Info(fmt.Sprintf("Analysis complete: %d/%d successful",
		len(result.Successful), len(result.Successful)+len(result.Failed)))

	return result
}

// DocumenterAgent generates README.md
type DocumenterAgent struct {
	config        config.DocumenterConfig
	promptManager *prompts.Manager
	logger        *logging.Logger
}

// NewDocumenterAgent creates a new documenter agent
func NewDocumenterAgent(cfg config.DocumenterConfig, promptManager *prompts.Manager, logger *logging.Logger) *DocumenterAgent {
	return &DocumenterAgent{
		config:        cfg,
		promptManager: promptManager,
		logger:        logger,
	}
}

// Run generates the README
func (da *DocumenterAgent) Run(ctx context.Context) error {
	retryClient := llm.NewRetryClient(llm.DefaultRetryConfig())
	factory := llm.NewFactory(retryClient)

	// Create documenter agent
	agent, err := CreateDocumenterAgent(da.config.LLM, da.config.RepoPath, factory, da.promptManager, da.logger)
	if err != nil {
		return fmt.Errorf("failed to create documenter agent: %w", err)
	}

	// Run agent
	output, err := agent.Run(ctx)
	if err != nil {
		return fmt.Errorf("documenter agent failed: %w", err)
	}

	// Save to README.md
	outputPath := filepath.Join(da.config.RepoPath, "README.md")
	if err := agent.SaveOutput(output, outputPath); err != nil {
		return err
	}

	da.logger.Info(fmt.Sprintf("README.md generated at %s", outputPath))
	return nil
}

// AIRulesGeneratorAgent generates AI assistant config files
type AIRulesGeneratorAgent struct {
	config        config.AIRulesConfig
	promptManager *prompts.Manager
	logger        *logging.Logger
}

// NewAIRulesGeneratorAgent creates a new AI rules generator agent
func NewAIRulesGeneratorAgent(cfg config.AIRulesConfig, promptManager *prompts.Manager, logger *logging.Logger) *AIRulesGeneratorAgent {
	return &AIRulesGeneratorAgent{
		config:        cfg,
		promptManager: promptManager,
		logger:        logger,
	}
}

// Run generates AI rules files
func (aa *AIRulesGeneratorAgent) Run(ctx context.Context) error {
	retryClient := llm.NewRetryClient(llm.DefaultRetryConfig())
	factory := llm.NewFactory(retryClient)

	// For now, generate CLAUDE.md
	agent, err := CreateAIRulesGeneratorAgent(aa.config.LLM, aa.config.RepoPath, factory, aa.promptManager, aa.logger)
	if err != nil {
		return fmt.Errorf("failed to create AI rules agent: %w", err)
	}

	// Run agent
	output, err := agent.Run(ctx)
	if err != nil {
		return fmt.Errorf("AI rules agent failed: %w", err)
	}

	// Save to CLAUDE.md
	outputPath := filepath.Join(aa.config.RepoPath, "CLAUDE.md")
	if err := agent.SaveOutput(output, outputPath); err != nil {
		return err
	}

	aa.logger.Info(fmt.Sprintf("CLAUDE.md generated at %s", outputPath))
	return nil
}
</file>
<file path="internal/agents/base.go">
package agents

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	"github.com/user/gendocs/internal/tools"
)

// Agent is the interface that all agents must implement
type Agent interface {
	// Run executes the agent and returns the generated output
	Run(ctx context.Context) (string, error)

	// Name returns the agent name
	Name() string
}

// BaseAgent provides common functionality for all agents
type BaseAgent struct {
	name          string
	llmClient     llm.LLMClient
	tools         []tools.Tool
	promptManager *prompts.Manager
	logger        *logging.Logger
	systemPrompt  string
	maxRetries    int
	maxTokens     int
	temperature   float64
}

// NewBaseAgent creates a new base agent
func NewBaseAgent(
	name string,
	llmClient llm.LLMClient,
	tools []tools.Tool,
	promptManager *prompts.Manager,
	logger *logging.Logger,
	systemPrompt string,
	maxRetries int,
) *BaseAgent {
	return &BaseAgent{
		name:          name,
		llmClient:     llmClient,
		tools:         tools,
		promptManager: promptManager,
		logger:        logger,
		systemPrompt:  systemPrompt,
		maxRetries:    maxRetries,
		maxTokens:     8192,
		temperature:   0.0,
	}
}

// SetMaxTokens sets the maximum tokens for LLM responses
func (ba *BaseAgent) SetMaxTokens(maxTokens int) {
	ba.maxTokens = maxTokens
}

// SetTemperature sets the temperature for LLM responses
func (ba *BaseAgent) SetTemperature(temperature float64) {
	ba.temperature = temperature
}

// RunOnce executes the agent once with the given user prompt
func (ba *BaseAgent) RunOnce(ctx context.Context, userPrompt string) (string, error) {
	// Build request
	messages := []llm.Message{
		{Role: "user", Content: userPrompt},
	}

	var conversationHistory []llm.Message

	// Tool calling loop
	for {
		req := llm.CompletionRequest{
			SystemPrompt: ba.systemPrompt,
			Messages:     append(conversationHistory, messages...),
			Tools:        ba.convertTools(),
			MaxTokens:    ba.maxTokens,
			Temperature:  ba.temperature,
		}

		ba.logger.Debug("Calling LLM",
			logging.String("agent", ba.name),
			logging.Int("tool_count", len(req.Tools)),
		)

		// Call LLM
		resp, err := ba.llmClient.GenerateCompletion(ctx, req)
		if err != nil {
			return "", fmt.Errorf("LLM call failed: %w", err)
		}

		ba.logger.Debug("LLM response received",
			logging.String("agent", ba.name),
			logging.Int("input_tokens", resp.Usage.InputTokens),
			logging.Int("output_tokens", resp.Usage.OutputTokens),
			logging.Int("tool_calls", len(resp.ToolCalls)),
		)

		// If no tool calls, return content
		if len(resp.ToolCalls) == 0 {
			return resp.Content, nil
		}

		// Add assistant response to conversation history
		conversationHistory = append(conversationHistory, llm.Message{
			Role:    "assistant",
			Content: resp.Content,
		})

		// Execute tool calls
		for _, toolCall := range resp.ToolCalls {
			tool := ba.findTool(toolCall.Name)
			if tool == nil {
				ba.logger.Warn("Tool not found", logging.String("tool", toolCall.Name))
				// Add error response
				conversationHistory = append(conversationHistory, llm.Message{
					Role:    "tool",
					Content: fmt.Sprintf("Error: Tool '%s' not found", toolCall.Name),
					ToolID:  toolCall.Name,
				})
				continue
			}

			ba.logger.Debug("Executing tool",
				logging.String("tool", tool.Name()),
				logging.String("agent", ba.name),
			)

			// Execute tool
			result, err := tool.Execute(ctx, toolCall.Arguments)
			if err != nil {
				ba.logger.Error("Tool execution failed",
					logging.String("tool", tool.Name()),
					logging.Error(err),
				)
				conversationHistory = append(conversationHistory, llm.Message{
					Role:    "tool",
					Content: fmt.Sprintf("Error: %v", err),
					ToolID:  toolCall.Name,
				})
			} else {
				conversationHistory = append(conversationHistory, llm.Message{
					Role:    "tool",
					Content: fmt.Sprintf("%v", result),
					ToolID:  toolCall.Name,
				})
			}
		}

		// Continue loop to get final response from LLM
		messages = []llm.Message{} // Clear, using conversationHistory now
	}
}

// convertTools converts agent tools to LLM tool definitions
func (ba *BaseAgent) convertTools() []llm.ToolDefinition {
	var toolDefs []llm.ToolDefinition
	for _, tool := range ba.tools {
		toolDefs = append(toolDefs, llm.ToolDefinition{
			Name:        tool.Name(),
			Description: tool.Description(),
			Parameters:  tool.Parameters(),
		})
	}
	return toolDefs
}

// findTool finds a tool by name
func (ba *BaseAgent) findTool(name string) tools.Tool {
	for _, tool := range ba.tools {
		if tool.Name() == name {
			return tool
		}
	}
	return nil
}

// Name returns the agent name
func (ba *BaseAgent) Name() string {
	return ba.name
}
</file>
<file path="internal/agents/factory.go">
package agents

import (
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// CreateStructureAnalyzer creates the structure analyzer sub-agent
func CreateStructureAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "StructureAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "structure_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateDependencyAnalyzer creates the dependency analyzer sub-agent
func CreateDependencyAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "DependencyAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "dependency_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateDataFlowAnalyzer creates the data flow analyzer sub-agent
func CreateDataFlowAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "DataFlowAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "data_flow_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateRequestFlowAnalyzer creates the request flow analyzer sub-agent
func CreateRequestFlowAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "RequestFlowAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "request_flow_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateAPIAnalyzer creates the API analyzer sub-agent
func CreateAPIAnalyzer(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "APIAnalyzer",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "api_analyzer",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateDocumenterAgent creates the documenter agent (README generator)
func CreateDocumenterAgent(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "DocumenterAgent",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "documenter",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}

// CreateAIRulesGeneratorAgent creates the AI rules generator agent
func CreateAIRulesGeneratorAgent(llmCfg config.LLMConfig, repoPath string, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	cfg := SubAgentConfig{
		Name:         "AIRulesGeneratorAgent",
		LLMConfig:    llmCfg,
		RepoPath:     repoPath,
		PromptSuffix: "ai_rules",
	}
	return NewSubAgent(cfg, llmFactory, promptManager, logger)
}
</file>
<file path="internal/agents/sub_agents.go">
package agents

import (
	"context"
	"fmt"
	"os"
	"path/filepath"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/llm"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
	"github.com/user/gendocs/internal/tools"
)

// SubAgentConfig holds configuration for sub-agents
type SubAgentConfig struct {
	Name         string
	LLMConfig    config.LLMConfig
	RepoPath     string
	PromptSuffix string // e.g., "structure_analyzer"
}

// SubAgent is a specialized analysis agent
type SubAgent struct {
	*BaseAgent
	config SubAgentConfig
}

// NewSubAgent creates a new sub-agent
func NewSubAgent(cfg SubAgentConfig, llmFactory *llm.Factory, promptManager *prompts.Manager, logger *logging.Logger) (*SubAgent, error) {
	// Create LLM client
	llmClient, err := llmFactory.CreateClient(cfg.LLMConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create LLM client: %w", err)
	}

	// Create tools
	toolList := []tools.Tool{
		tools.NewFileReadTool(2),
		tools.NewListFilesTool(2),
	}

	// Load system prompt
	systemPrompt, err := promptManager.Get(cfg.PromptSuffix + "_system")
	if err != nil {
		return nil, fmt.Errorf("failed to load system prompt: %w", err)
	}

	baseAgent := NewBaseAgent(
		cfg.Name,
		llmClient,
		toolList,
		promptManager,
		logger,
		systemPrompt,
		cfg.LLMConfig.GetRetries(),
	)

	return &SubAgent{
		BaseAgent: baseAgent,
		config:    cfg,
	}, nil
}

// Run executes the sub-agent
func (sa *SubAgent) Run(ctx context.Context) (string, error) {
	// Render user prompt with variables
	userPrompt, err := sa.promptManager.Render(sa.config.PromptSuffix+"_user", map[string]interface{}{
		"RepoPath": sa.config.RepoPath,
	})
	if err != nil {
		return "", fmt.Errorf("failed to render user prompt: %w", err)
	}

	// Run with retry logic
	var lastErr error
	for attempt := 0; attempt < sa.maxRetries; attempt++ {
		sa.logger.Info(fmt.Sprintf("Running sub-agent %s (attempt %d/%d)", sa.config.Name, attempt+1, sa.maxRetries))

		result, err := sa.RunOnce(ctx, userPrompt)
		if err == nil {
			sa.logger.Info(fmt.Sprintf("Sub-agent %s completed successfully", sa.config.Name))
			return result, nil
		}

		lastErr = err
		sa.logger.Warn(fmt.Sprintf("Sub-agent %s attempt %d failed: %v", sa.config.Name, attempt+1, err))
	}

	return "", fmt.Errorf("sub-agent %s failed after %d retries: %w", sa.config.Name, sa.maxRetries, lastErr)
}

// SaveOutput saves the agent output to a file
func (sa *SubAgent) SaveOutput(output, outputPath string) error {
	// Ensure directory exists
	dir := filepath.Dir(outputPath)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create directory: %w", err)
	}

	// Write output
	if err := os.WriteFile(outputPath, []byte(output), 0644); err != nil {
		return fmt.Errorf("failed to write output: %w", err)
	}

	sa.logger.Info(fmt.Sprintf("Output saved to %s", outputPath))
	return nil
}
</file>
<file path="internal/config/loader.go">
package config

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/joho/godotenv"
	"github.com/spf13/viper"
	"github.com/user/gendocs/internal/errors"
)

// Loader handles loading configuration from multiple sources
type Loader struct {
	v *viper.Viper
}

// NewLoader creates a new configuration loader
func NewLoader() *Loader {
	// Load .env file if exists
	_ = godotenv.Load()

	v := viper.New()
	v.SetConfigType("yaml")
	v.AutomaticEnv()
	v.SetEnvPrefix("GENDOCS")
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))

	return &Loader{v: v}
}

// LoadForAgent loads configuration for a specific agent section
// Precedence: CLI > .ai/config.yaml > ~/.gendocs.yaml > Environment > Defaults
func (l *Loader) LoadForAgent(repoPath, section string, cliOverrides map[string]interface{}) (*viper.Viper, error) {
	// 1. Load defaults (set via struct defaults)

	// 2. Load from ~/.gendocs.yaml (global user config)
	if err := l.loadGlobalConfig(); err != nil {
		return nil, err
	}

	// 3. Load from .ai/config.yaml (project-specific config)
	if err := l.loadProjectConfig(repoPath); err != nil {
		return nil, err
	}

	// 4. Apply CLI overrides
	if err := l.applyCLIOverrides(cliOverrides); err != nil {
		return nil, err
	}

	return l.v, nil
}

// loadGlobalConfig loads configuration from ~/.gendocs.yaml
func (l *Loader) loadGlobalConfig() error {
	homeDir, err := os.UserHomeDir()
	if err != nil {
		return nil // Not a fatal error
	}

	globalConfig := filepath.Join(homeDir, ".gendocs.yaml")
	if _, err := os.Stat(globalConfig); err != nil {
		return nil // File doesn't exist, skip
	}

	l.v.SetConfigFile(globalConfig)
	if err := l.v.ReadInConfig(); err != nil {
		return errors.NewConfigFileError(globalConfig, err)
	}

	return nil
}

// loadProjectConfig loads configuration from .ai/config.yaml
func (l *Loader) loadProjectConfig(repoPath string) error {
	if repoPath == "" {
		repoPath = "."
	}

	configPath := filepath.Join(repoPath, ".ai", "config.yaml")
	if _, err := os.Stat(configPath); err != nil {
		return nil // File doesn't exist, skip
	}

	l.v.SetConfigFile(configPath)
	if err := l.v.MergeInConfig(); err != nil {
		return errors.NewConfigFileError(configPath, err)
	}

	return nil
}

// applyCLIOverrides applies CLI flag overrides
func (l *Loader) applyCLIOverrides(overrides map[string]interface{}) error {
	for key, value := range overrides {
		// Only set if value is not nil/zero
		if value != nil {
			l.v.Set(key, value)
		}
	}
	return nil
}

// GetEnvVar gets an environment variable, returning an error if not set
func GetEnvVar(name, description string) (string, error) {
	value := os.Getenv(name)
	if value == "" {
		return "", errors.NewMissingEnvVarError(name, description)
	}
	return value, nil
}

// GetEnvVarOrDefault gets an environment variable with a default value
func GetEnvVarOrDefault(name, defaultValue string) string {
	value := os.Getenv(name)
	if value == "" {
		return defaultValue
	}
	return value
}

// MergeConfigs merges multiple configuration sources with precedence
// Precedence order (highest to lowest): cli, project, global, env, defaults
func MergeConfigs(repoPath string, section string, defaults interface{}, cliOverrides map[string]interface{}) (map[string]interface{}, error) {
	loader := NewLoader()

	// Load all config sources
	v, err := loader.LoadForAgent(repoPath, section, cliOverrides)
	if err != nil {
		return nil, err
	}

	// Get the section-specific config
	var sectionConfig map[string]interface{}
	if section != "" {
		sectionConfig = v.GetStringMap(section)
	} else {
		// Get all settings if no section specified
		sectionConfig = v.AllSettings()
	}

	// Apply CLI overrides (highest precedence)
	for key, value := range cliOverrides {
		if value != nil {
			// Convert key from snake_case to dot notation if needed
			sectionConfig[key] = value
		}
	}

	return sectionConfig, nil
}

// LoadAnalyzerConfig loads and validates analyzer configuration
func LoadAnalyzerConfig(repoPath string, cliOverrides map[string]interface{}) (*AnalyzerConfig, error) {
	configMap, err := MergeConfigs(repoPath, "analyzer", &AnalyzerConfig{}, cliOverrides)
	if err != nil {
		return nil, err
	}

	// Create config from map
	cfg := &AnalyzerConfig{
		BaseConfig: BaseConfig{
			RepoPath: getString(configMap, "repo_path", "."),
			Debug:    getBool(configMap, "debug", false),
		},
		MaxWorkers: getInt(configMap, "max_workers", 0),
	}

	// Load LLM config from environment or config
	cfg.LLM = LLMConfig{
		Provider:    getString(configMap, "llm.provider", getEnvOrDefault("ANALYZER_LLM_PROVIDER", "openai")),
		Model:       getString(configMap, "llm.model", getEnvOrDefault("ANALYZER_LLM_MODEL", "gpt-4o")),
		APIKey:      getString(configMap, "llm.api_key", getEnvOrDefault("ANALYZER_LLM_API_KEY", "")),
		BaseURL:     getString(configMap, "llm.base_url", getEnvOrDefault("ANALYZER_LLM_BASE_URL", "")),
		Retries:     getInt(configMap, "llm.retries", getEnvIntOrDefault("ANALYZER_AGENT_RETRIES", 2)),
		Timeout:     getInt(configMap, "llm.timeout", getEnvIntOrDefault("ANALYZER_LLM_TIMEOUT", 180)),
		MaxTokens:   getInt(configMap, "llm.max_tokens", getEnvIntOrDefault("ANALYZER_LLM_MAX_TOKENS", 8192)),
		Temperature: getFloat64(configMap, "llm.temperature", getEnvFloatOrDefault("ANALYZER_LLM_TEMPERATURE", 0.0)),
	}

	cfg.ExcludeStructure = getBool(configMap, "exclude_code_structure", false)
	cfg.ExcludeDataFlow = getBool(configMap, "exclude_data_flow", false)
	cfg.ExcludeDeps = getBool(configMap, "exclude_dependencies", false)
	cfg.ExcludeReqFlow = getBool(configMap, "exclude_request_flow", false)
	cfg.ExcludeAPI = getBool(configMap, "exclude_api_analysis", false)

	// Validate required fields
	if err := validateLLMConfig(&cfg.LLM, "ANALYZER"); err != nil {
		return nil, err
	}

	return cfg, nil
}

// Helper functions for type-safe config access

func getString(m map[string]interface{}, key, defaultValue string) string {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	if str, ok := val.(string); ok {
		return str
	}
	return defaultValue
}

func getInt(m map[string]interface{}, key string, defaultValue int) int {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	switch v := val.(type) {
	case int:
		return v
	case float64:
		return int(v)
	case string:
		// Try to parse string as int
		var i int
		if _, err := fmt.Sscanf(v, "%d", &i); err == nil {
			return i
		}
	}
	return defaultValue
}

func getBool(m map[string]interface{}, key string, defaultValue bool) bool {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	if b, ok := val.(bool); ok {
		return b
	}
	return defaultValue
}

func getFloat64(m map[string]interface{}, key string, defaultValue float64) float64 {
	parts := strings.Split(key, ".")
	var val interface{} = m

	for _, part := range parts {
		if subMap, ok := val.(map[string]interface{}); ok {
			val = subMap[part]
		} else {
			return defaultValue
		}
	}

	if f, ok := val.(float64); ok {
		return f
	}
	return defaultValue
}

func getEnvOrDefault(key, defaultValue string) string {
	if val := os.Getenv(key); val != "" {
		return val
	}
	return defaultValue
}

func getEnvIntOrDefault(key string, defaultValue int) int {
	if val := os.Getenv(key); val != "" {
		var i int
		if _, err := fmt.Sscanf(val, "%d", &i); err == nil {
			return i
		}
	}
	return defaultValue
}

func getEnvFloatOrDefault(key string, defaultValue float64) float64 {
	if val := os.Getenv(key); val != "" {
		var f float64
		if _, err := fmt.Sscanf(val, "%f", &f); err == nil {
			return f
		}
	}
	return defaultValue
}

// validateLLMConfig validates LLM configuration
func validateLLMConfig(cfg *LLMConfig, prefix string) error {
	if cfg.APIKey == "" {
		return errors.NewMissingEnvVarError(prefix+"_LLM_API_KEY", "API key for LLM provider")
	}

	validProviders := map[string]bool{
		"openai":    true,
		"anthropic": true,
		"gemini":    true,
	}

	if !validProviders[cfg.Provider] {
		return errors.NewInvalidEnvVarError(prefix+"_LLM_PROVIDER", cfg.Provider, "Must be one of: openai, anthropic, gemini")
	}

	return nil
}
</file>
<file path="internal/config/models.go">
package config

import (
	"time"
)

// BaseConfig holds common configuration for all handlers
type BaseConfig struct {
	RepoPath string `mapstructure:"repo_path"`
	Debug    bool   `mapstructure:"debug"`
}

// LLMConfig holds LLM provider configuration
type LLMConfig struct {
	Provider    string  `mapstructure:"provider"`     // openai, anthropic, gemini
	Model       string  `mapstructure:"model"`
	APIKey      string  `mapstructure:"api_key"`
	BaseURL     string  `mapstructure:"base_url"`      // Optional, for OpenAI-compatible APIs
	Retries     int     `mapstructure:"retries"`
	Timeout     int     `mapstructure:"timeout"`       // Timeout in seconds
	MaxTokens   int     `mapstructure:"max_tokens"`
	Temperature float64 `mapstructure:"temperature"`
}

// GeminiConfig holds Gemini-specific configuration
type GeminiConfig struct {
	UseVertexAI bool   `mapstructure:"use_vertex_ai"`
	ProjectID   string `mapstructure:"project_id"`
	Location    string `mapstructure:"location"`
}

// RetryConfig holds HTTP retry configuration
type RetryConfig struct {
	MaxAttempts       int `mapstructure:"max_attempts"`        // Default: 5
	Multiplier        int `mapstructure:"multiplier"`          // Default: 1
	MaxWaitPerAttempt int `mapstructure:"max_wait_per_attempt"` // Default: 60 seconds
	MaxTotalWait      int `mapstructure:"max_total_wait"`      // Default: 300 seconds
}

// AnalyzerConfig holds configuration for the analyze command
type AnalyzerConfig struct {
	BaseConfig
	LLM               LLMConfig    `mapstructure:"llm"`
	ExcludeStructure  bool         `mapstructure:"exclude_code_structure"`
	ExcludeDataFlow   bool         `mapstructure:"exclude_data_flow"`
	ExcludeDeps       bool         `mapstructure:"exclude_dependencies"`
	ExcludeReqFlow    bool         `mapstructure:"exclude_request_flow"`
	ExcludeAPI        bool         `mapstructure:"exclude_api_analysis"`
	MaxWorkers        int          `mapstructure:"max_workers"`
	RetryConfig       RetryConfig  `mapstructure:"retry"`
}

// DocumenterConfig holds configuration for readme generation
type DocumenterConfig struct {
	BaseConfig
	LLM         LLMConfig   `mapstructure:"llm"`
	RetryConfig RetryConfig `mapstructure:"retry"`
}

// AIRulesConfig holds configuration for AI rules generation
type AIRulesConfig struct {
	BaseConfig
	LLM           LLMConfig   `mapstructure:"llm"`
	RetryConfig   RetryConfig `mapstructure:"retry"`
	MaxTokensMarkdown  int    `mapstructure:"max_tokens_markdown"`
	MaxTokensCursor    int    `mapstructure:"max_tokens_cursor"`
}

// CronjobConfig holds configuration for cronjob command
type CronjobConfig struct {
	MaxDaysSinceLastCommit int    `mapstructure:"max_days_since_last_commit"`
	WorkingPath            string `mapstructure:"working_path"`
	GroupProjectID         int    `mapstructure:"group_project_id"`
}

// GitLabConfig holds GitLab integration configuration
type GitLabConfig struct {
	APIURL     string `mapstructure:"api_url"`
	UserName   string `mapstructure:"user_name"`
	UserUsername string `mapstructure:"user_username"`
	UserEmail  string `mapstructure:"user_email"`
	OAuthToken string `mapstructure:"oauth_token"`
}

// LoggingConfig holds logging configuration
type LoggingConfig struct {
	LogDir       string `mapstructure:"log_dir"`
	FileLevel    string `mapstructure:"file_level"`    // debug, info, warn, error
	ConsoleLevel string `mapstructure:"console_level"` // debug, info, warn, error
}

// GlobalConfig holds top-level configuration from .ai/config.yaml
type GlobalConfig struct {
	Analyzer  AnalyzerConfig  `mapstructure:"analyzer"`
	Documenter DocumenterConfig `mapstructure:"documenter"`
	AIRules   AIRulesConfig   `mapstructure:"ai_rules"`
	Cronjob   CronjobConfig   `mapstructure:"cronjob"`
	GitLab    GitLabConfig    `mapstructure:"gitlab"`
	Gemini    GeminiConfig    `mapstructure:"gemini"`
	Logging   LoggingConfig   `mapstructure:"logging"`
}

// GetTimeout returns the timeout as a time.Duration
func (c *LLMConfig) GetTimeout() time.Duration {
	if c.Timeout == 0 {
		return 180 * time.Second // Default timeout
	}
	return time.Duration(c.Timeout) * time.Second
}

// GetMaxTokens returns the max tokens with a default
func (c *LLMConfig) GetMaxTokens() int {
	if c.MaxTokens == 0 {
		return 8192 // Default max tokens
	}
	return c.MaxTokens
}

// GetTemperature returns the temperature with a default
func (c *LLMConfig) GetTemperature() float64 {
	if c.Temperature == 0 {
		return 0.0 // Default temperature for deterministic output
	}
	return c.Temperature
}

// GetRetries returns the retry count with a default
func (c *LLMConfig) GetRetries() int {
	if c.Retries == 0 {
		return 2 // Default retries
	}
	return c.Retries
}
</file>
<file path="internal/errors/agent.go">
package errors

import (
	"fmt"
)

// AgentError is the base error for all agent-related errors
type AgentError struct {
	*AIDocGenError
}

// NewAgentError creates a new agent error
func NewAgentError(message string) *AgentError {
	return &AgentError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitAgentError,
		},
	}
}

// LLMConnectionError is raised when connection to LLM provider fails
type LLMConnectionError struct {
	*AIDocGenError
}

// NewLLMConnectionError creates a new LLM connection error
func NewLLMConnectionError(provider string, cause error) *LLMConnectionError {
	return &LLMConnectionError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to connect to LLM provider: %s", provider),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "LLM API Call",
				Component: "LLM Client",
				Details: map[string]interface{}{
					"provider": provider,
				},
				Suggestions: []string{
					"Check your internet connection",
					"Verify the API endpoint is accessible",
					"Check if the API key is valid",
					"Try again later (service may be unavailable)",
				},
				Recoverable: true,
			},
			ExitCode: ExitLLMError,
		},
	}
}

// LLMResponseError is raised when LLM response is invalid or cannot be parsed
type LLMResponseError struct {
	*AIDocGenError
}

// NewLLMResponseError creates a new LLM response error
func NewLLMResponseError(provider, reason string) *LLMResponseError {
	return &LLMResponseError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Invalid response from LLM provider: %s", provider),
			Context: &ErrorContext{
				Operation: "Parsing LLM Response",
				Component: "LLM Client",
				Details: map[string]interface{}{
					"provider": provider,
					"reason":   reason,
				},
				Suggestions: []string{
					"Check if the model name is correct",
					"Try a different model",
					"Report this issue if it persists",
				},
				Recoverable: true,
			},
			ExitCode: ExitLLMError,
		},
	}
}

// AgentTimeoutError is raised when an agent execution times out
type AgentTimeoutError struct {
	*AIDocGenError
}

// NewAgentTimeoutError creates a new agent timeout error
func NewAgentTimeoutError(agentName string, timeoutSeconds int) *AgentTimeoutError {
	return &AgentTimeoutError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Agent '%s' timed out after %d seconds", agentName, timeoutSeconds),
			Context: &ErrorContext{
				Operation: "Agent Execution",
				Component: agentName,
				Details: map[string]interface{}{
					"timeout_seconds": timeoutSeconds,
				},
				Suggestions: []string{
					"Increase the timeout via LLM_TIMEOUT environment variable",
					"Try reducing the size of the codebase",
					"Try a faster model",
				},
				Recoverable: false,
			},
			ExitCode: ExitAgentError,
		},
	}
}

// ToolExecutionError is raised when a tool execution fails
type ToolExecutionError struct {
	*AIDocGenError
}

// NewToolExecutionError creates a new tool execution error
func NewToolExecutionError(toolName string, cause error) *ToolExecutionError {
	return &ToolExecutionError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Tool '%s' execution failed", toolName),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Tool Execution",
				Component: toolName,
				Details: map[string]interface{}{
					"tool": toolName,
				},
				Suggestions: []string{
					"Check if the file/directory exists",
					"Verify file permissions",
					"Check the error details above",
				},
				Recoverable: true,
			},
			ExitCode: ExitAgentError,
		},
	}
}
</file>
<file path="internal/errors/base.go">
package errors

import (
	"fmt"
)

// AIDocGenError is the base error type for all application errors
type AIDocGenError struct {
	Message  string         // Human-readable error message
	Context  *ErrorContext  // Rich error context
	Cause    error          // Underlying error (for wrapping)
	ExitCode ExitCode       // Exit code for CLI
}

// Error returns the error message
func (e *AIDocGenError) Error() string {
	return e.Message
}

// Unwrap returns the underlying cause
func (e *AIDocGenError) Unwrap() error {
	return e.Cause
}

// GetUserMessage returns a user-friendly error message with context
func (e *AIDocGenError) GetUserMessage() string {
	msg := fmt.Sprintf("ERROR: %s", e.Message)

	if e.Context != nil {
		msg += e.Context.Format()
	}

	return msg
}

// NewError creates a new AIDocGenError with the given message and exit code
func NewError(message string, exitCode ExitCode) *AIDocGenError {
	return &AIDocGenError{
		Message:  message,
		ExitCode: exitCode,
	}
}

// WrapError wraps an existing error with additional context
func WrapError(cause error, message string, exitCode ExitCode) *AIDocGenError {
	return &AIDocGenError{
		Message:  message,
		Cause:    cause,
		ExitCode: exitCode,
	}
}

// WrapErrorWithContext wraps an error with full context
func WrapErrorWithContext(cause error, message string, exitCode ExitCode, context *ErrorContext) *AIDocGenError {
	return &AIDocGenError{
		Message:  message,
		Context:  context,
		Cause:    cause,
		ExitCode: exitCode,
	}
}
</file>
<file path="internal/errors/config.go">
package errors

import (
	"fmt"
)

// ConfigurationError is raised when configuration is invalid or missing
type ConfigurationError struct {
	*AIDocGenError
}

// NewConfigurationError creates a new configuration error
func NewConfigurationError(message string) *ConfigurationError {
	return &ConfigurationError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitConfigError,
		},
	}
}

// MissingEnvVarError is raised when a required environment variable is not set
type MissingEnvVarError struct {
	*AIDocGenError
}

// NewMissingEnvVarError creates a new missing environment variable error
func NewMissingEnvVarError(varName, description string) *MissingEnvVarError {
	return &MissingEnvVarError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Required environment variable '%s' is not set", varName),
			Context: &ErrorContext{
				Operation: "Loading configuration",
				Component: "Environment",
				Details: map[string]interface{}{
					"variable":     varName,
					"description":  description,
				},
				Suggestions: []string{
					fmt.Sprintf("Set the %s environment variable in your .env file", varName),
					fmt.Sprintf("Export the variable: export %s='your-value'", varName),
					"Check .env.example for required variables",
				},
				Recoverable: false,
			},
			ExitCode: ExitConfigError,
		},
	}
}

// InvalidEnvVarError is raised when an environment variable has an invalid value
type InvalidEnvVarError struct {
	*AIDocGenError
}

// NewInvalidEnvVarError creates a new invalid environment variable error
func NewInvalidEnvVarError(varName, value, reason string) *InvalidEnvVarError {
	return &InvalidEnvVarError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Environment variable '%s' has an invalid value", varName),
			Context: &ErrorContext{
				Operation: "Validating configuration",
				Component: "Environment",
				Details: map[string]interface{}{
					"variable": varName,
					"value":    value,
					"reason":   reason,
				},
				Suggestions: []string{
					fmt.Sprintf("Check the value of %s in your .env file", varName),
					"Refer to the documentation for valid values",
				},
				Recoverable: false,
			},
			ExitCode: ExitConfigError,
		},
	}
}

// ConfigFileError is raised when a configuration file cannot be read or parsed
type ConfigFileError struct {
	*AIDocGenError
}

// NewConfigFileError creates a new config file error
func NewConfigFileError(filePath string, cause error) *ConfigFileError {
	return &ConfigFileError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to load configuration file: %s", filePath),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Loading configuration",
				Component: "Config File",
				Details: map[string]interface{}{
					"file_path": filePath,
				},
				Suggestions: []string{
					"Check that the file exists and is readable",
					"Validate YAML syntax",
					"Check file permissions",
				},
				Recoverable: false,
			},
			ExitCode: ExitConfigError,
		},
	}
}
</file>
<file path="internal/errors/context.go">
package errors

import (
	"fmt"
	"strings"
)

// ErrorContext provides rich error information for user-friendly error messages
type ErrorContext struct {
	Operation   string                 // The operation that failed
	Component   string                 // The component that failed
	Details     map[string]interface{} // Additional details about the error
	Suggestions []string               // Actionable suggestions for the user
	Recoverable bool                   // Whether the error is recoverable
	RetryCount  int                    // Current retry count
	MaxRetries  int                    // Maximum retries allowed
}

// Format returns a formatted string representation of the error context
func (ec *ErrorContext) Format() string {
	var sb strings.Builder

	if ec.Operation != "" || ec.Component != "" {
		sb.WriteString("\nWhat happened:\n")
		if ec.Operation != "" && ec.Component != "" {
			sb.WriteString(fmt.Sprintf("  %s failed in %s.\n", ec.Operation, ec.Component))
		} else if ec.Operation != "" {
			sb.WriteString(fmt.Sprintf("  %s failed.\n", ec.Operation))
		} else if ec.Component != "" {
			sb.WriteString(fmt.Sprintf("  Failure in %s.\n", ec.Component))
		}
	}

	if len(ec.Details) > 0 {
		sb.WriteString("\nDetails:\n")
		for key, value := range ec.Details {
			sb.WriteString(fmt.Sprintf("  - %s: %v\n", key, value))
		}
	}

	if len(ec.Suggestions) > 0 {
		sb.WriteString("\nWhat you can do:\n")
		for i, suggestion := range ec.Suggestions {
			sb.WriteString(fmt.Sprintf("  %d. %s\n", i+1, suggestion))
		}
	}

	if ec.Recoverable {
		sb.WriteString(fmt.Sprintf("\nRecoverable: Yes (retry %d/%d)\n", ec.RetryCount, ec.MaxRetries))
	}

	return sb.String()
}
</file>
<file path="internal/errors/exit_codes.go">
package errors

type ExitCode int

const (
	ExitSuccess        ExitCode = 0
	ExitGeneralError   ExitCode = 1
	ExitConfigError    ExitCode = 2
	ExitValidationError ExitCode = 3
	ExitLLMError       ExitCode = 4
	ExitAgentError     ExitCode = 5
	ExitIOError        ExitCode = 6
	ExitGitLabError    ExitCode = 7
	ExitPartialSuccess ExitCode = 10
)

func (e ExitCode) Int() int {
	return int(e)
}
</file>
<file path="internal/errors/gitlab.go">
package errors

import (
	"fmt"
)

// GitLabError is the base error for all GitLab-related errors
type GitLabError struct {
	*AIDocGenError
}

// NewGitLabError creates a new GitLab error
func NewGitLabError(message string) *GitLabError {
	return &GitLabError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitGitLabError,
		},
	}
}

// GitLabAuthError is raised when GitLab authentication fails
type GitLabAuthError struct {
	*AIDocGenError
}

// NewGitLabAuthError creates a new GitLab authentication error
func NewGitLabAuthError(cause error) *GitLabAuthError {
	return &GitLabAuthError{
		AIDocGenError: &AIDocGenError{
			Message: "GitLab authentication failed",
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "GitLab Authentication",
				Component: "GitLab Client",
				Suggestions: []string{
					"Verify GITLAB_OAUTH_TOKEN is set correctly",
					"Check if the token has not expired",
					"Ensure token has required permissions (api, read_repository)",
					"Generate a new token at GitLab user settings > access tokens",
				},
				Recoverable: false,
			},
			ExitCode: ExitGitLabError,
		},
	}
}

// GitLabAPIError is raised when GitLab API call fails
type GitLabAPIError struct {
	*AIDocGenError
}

// NewGitLabAPIError creates a new GitLab API error
func NewGitLabAPIError(operation, statusCode string, cause error) *GitLabAPIError {
	return &GitLabAPIError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("GitLab API error during %s", operation),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "GitLab API Call",
				Component: "GitLab Client",
				Details: map[string]interface{}{
					"operation":   operation,
					"status_code": statusCode,
				},
				Suggestions: []string{
					"Check GitLab API URL is correct",
					"Verify the project/group exists",
					"Check API rate limits",
					"Try again later",
				},
				Recoverable: true,
			},
			ExitCode: ExitGitLabError,
		},
	}
}

// GitCloneError is raised when git clone fails
type GitCloneError struct {
	*AIDocGenError
}

// NewGitCloneError creates a new git clone error
func NewGitCloneError(repoURL, reason string, cause error) *GitCloneError {
	return &GitCloneError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to clone repository: %s", repoURL),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Git Clone",
				Component: "Git",
				Details: map[string]interface{}{
					"repo_url": repoURL,
					"reason":   reason,
				},
				Suggestions: []string{
					"Check if the repository URL is correct",
					"Verify you have access to the repository",
					"Check git is installed and accessible",
					"Ensure sufficient disk space",
				},
				Recoverable: false,
			},
			ExitCode: ExitGitLabError,
		},
	}
}
</file>
<file path="internal/errors/handler.go">
package errors

import (
	"fmt"
)

// HandlerError is the base error for all handler-related errors
type HandlerError struct {
	*AIDocGenError
}

// NewHandlerError creates a new handler error
func NewHandlerError(message string) *HandlerError {
	return &HandlerError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitGeneralError,
		},
	}
}

// AnalysisError is raised when analysis fails
type AnalysisError struct {
	*AIDocGenError
}

// NewAnalysisError creates a new analysis error
func NewAnalysisError(reason string, cause error) *AnalysisError {
	return &AnalysisError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Analysis failed: %s", reason),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Codebase Analysis",
				Component: "AnalyzerAgent",
				Suggestions: []string{
					"Check if the repository path is valid",
					"Verify LLM configuration",
					"Try with --debug flag for more information",
					"Check if any agents were excluded",
				},
				Recoverable: false,
			},
			ExitCode: ExitAgentError,
		},
	}
}

// DocumentationError is raised when documentation generation fails
type DocumentationError struct {
	*AIDocGenError
}

// NewDocumentationError creates a new documentation error
func NewDocumentationError(docType string, cause error) *DocumentationError {
	return &DocumentationError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Failed to generate %s documentation", docType),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "Documentation Generation",
				Component: docType + "Agent",
				Suggestions: []string{
					"Ensure analysis has been run first",
					"Check that analysis files exist in .ai/docs/",
					"Verify LLM configuration",
				},
				Recoverable: false,
			},
			ExitCode: ExitAgentError,
		},
	}
}

// CronjobError is raised when cronjob execution fails
type CronjobError struct {
	*AIDocGenError
}

// NewCronjobError creates a new cronjob error
func NewCronjobError(reason string, cause error) *CronjobError {
	return &CronjobError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Cronjob failed: %s", reason),
			Cause:   cause,
			Context: &ErrorContext{
				Operation: "GitLab Cronjob",
				Component: "CronjobHandler",
				Suggestions: []string{
					"Check GitLab API credentials",
					"Verify group project ID",
					"Check GitLab API URL",
					"Review cronjob logs",
				},
				Recoverable: false,
			},
			ExitCode: ExitGeneralError,
		},
	}
}
</file>
<file path="internal/errors/validation.go">
package errors

import (
	"fmt"
)

// ValidationError is the base error for all validation-related errors
type ValidationError struct {
	*AIDocGenError
}

// NewValidationError creates a new validation error
func NewValidationError(message string) *ValidationError {
	return &ValidationError{
		AIDocGenError: &AIDocGenError{
			Message:  message,
			ExitCode: ExitValidationError,
		},
	}
}

// MissingFileError is raised when a required file is not found
type MissingFileError struct {
	*AIDocGenError
}

// NewMissingFileError creates a new missing file error
func NewMissingFileError(filePath string) *MissingFileError {
	return &MissingFileError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Required file not found: %s", filePath),
			Context: &ErrorContext{
				Operation: "File Validation",
				Component: "Filesystem",
				Details: map[string]interface{}{
					"file_path": filePath,
				},
				Suggestions: []string{
					"Check that the file exists",
					"Verify the file path is correct",
					"Run analysis first to generate the file",
				},
				Recoverable: false,
			},
			ExitCode: ExitValidationError,
		},
	}
}

// InvalidPathError is raised when a path is invalid
type InvalidPathError struct {
	*AIDocGenError
}

// NewInvalidPathError creates a new invalid path error
func NewInvalidPathError(path string, reason string) *InvalidPathError {
	return &InvalidPathError{
		AIDocGenError: &AIDocGenError{
			Message: fmt.Sprintf("Invalid path: %s", path),
			Context: &ErrorContext{
				Operation: "Path Validation",
				Component: "Filesystem",
				Details: map[string]interface{}{
					"path":   path,
					"reason": reason,
				},
				Suggestions: []string{
					"Check that the path exists",
					"Verify the path is a valid directory",
					"Use an absolute path if relative path fails",
				},
				Recoverable: false,
			},
			ExitCode: ExitValidationError,
		},
	}
}

// OutputValidationError is raised when output validation fails
type OutputValidationError struct {
	*AIDocGenError
}

// NewOutputValidationError creates a new output validation error
func NewOutputValidationError(missingFiles []string) *OutputValidationError {
	return &OutputValidationError{
		AIDocGenError: &AIDocGenError{
			Message: "Output validation failed: expected files were not generated",
			Context: &ErrorContext{
				Operation: "Output Validation",
				Component: "Validation",
				Details: map[string]interface{}{
					"missing_files": missingFiles,
				},
				Suggestions: []string{
					"Check if LLM API calls succeeded",
					"Review error logs for individual agent failures",
					"Try running with --debug flag for more details",
				},
				Recoverable: false,
			},
			ExitCode: ExitValidationError,
		},
	}
}
</file>
<file path="internal/gitlab/client.go">
package gitlab

import (
	"context"
	"fmt"
	"net/http"
	"time"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/logging"
)

// Client represents a GitLab API client
type Client struct {
	httpClient   *http.Client
	apiURL       string
	OAuthToken   string
	UserName     string
	UserUsername string
	UserEmail    string
	logger       *logging.Logger
}

// Project represents a GitLab project
type Project struct {
	ID                int       `json:"id"`
	Name              string    `json:"name"`
	PathWithNamespace string    `json:"path_with_namespace"`
	HTTPURL           string    `json:"http_url_to_repo"`
	SSHURL            string    `json:"ssh_url_to_repo"`
	DefaultBranch     string    `json:"default_branch"`
	LastActivityAt    time.Time `json:"last_activity_at"`
	CreatedAt         time.Time `json:"created_at"`
	Archived          bool      `json:"archived"`
}

// MergeRequest represents a GitLab merge request
type MergeRequest struct {
	ID          int    `json:"id"`
	IID         int    `json:"iid"`
	Title       string `json:"title"`
	Description string `json:"description"`
	SourceBranch string `json:"source_branch"`
	TargetBranch string `json:"target_branch"`
	WebURL      string `json:"web_url"`
}

// NewClient creates a new GitLab client
func NewClient(cfg config.GitLabConfig, logger *logging.Logger) *Client {
	return &Client{
		httpClient: &http.Client{
			Timeout: 30 * time.Second,
		},
		apiURL:       cfg.APIURL,
		OAuthToken:   cfg.OAuthToken,
		UserName:     cfg.UserName,
		UserUsername: cfg.UserUsername,
		UserEmail:    cfg.UserEmail,
		logger:       logger,
	}
}

// FetchProjectsInGroup fetches all projects in a group (including subgroups)
func (c *Client) FetchProjectsInGroup(ctx context.Context, groupID int) ([]Project, error) {
	c.logger.Debug(fmt.Sprintf("Fetching projects in group %d", groupID))

	// For now, return empty list - would need full GitLab API implementation
	// This would require using xanzy/go-gitlab or implementing the API calls
	return []Project{}, nil
}

// ProjectFilter determines if a project should be analyzed
type ProjectFilter struct {
	MaxDaysSinceLastCommit int
	IgnoreProjects         map[string]bool
	IgnoreSubgroups        map[string]bool
}

// ShouldAnalyze determines if a project should be analyzed
func (c *Client) ShouldAnalyze(ctx context.Context, project Project, filter ProjectFilter) (bool, error) {
	// Skip archived projects
	if project.Archived {
		return false, nil
	}

	// Skip ignored projects
	if filter.IgnoreProjects[project.PathWithNamespace] {
		return false, nil
	}

	// Check if last commit is too old
	if filter.MaxDaysSinceLastCommit > 0 {
		daysSince := time.Since(project.LastActivityAt).Hours() / 24
		if int(daysSince) > filter.MaxDaysSinceLastCommit {
			return false, nil
		}
	}

	// Check if branch already exists
	branchName := fmt.Sprintf("ai-analyzer-%s", time.Now().Format("2006-01-02"))
	if branchExists, err := c.BranchExists(ctx, project, branchName); err == nil && branchExists {
		return false, nil
	}

	// Check if open MR exists
	if hasMR, err := c.HasOpenMR(ctx, project, branchName); err == nil && hasMR {
		return false, nil
	}

	return true, nil
}

// BranchExists checks if a branch exists in a project
func (c *Client) BranchExists(ctx context.Context, project Project, branchName string) (bool, error) {
	// Placeholder - would implement GitLab API call
	return false, nil
}

// HasOpenMR checks if an open MR exists for a branch
func (c *Client) HasOpenMR(ctx context.Context, project Project, branchName string) (bool, error) {
	// Placeholder - would implement GitLab API call
	return false, nil
}

// CreateBranch creates a new branch in a project
func (c *Client) CreateBranch(ctx context.Context, project Project, branchName, fromBranch string) error {
	// Placeholder - would implement GitLab API call
	c.logger.Info(fmt.Sprintf("Would create branch '%s' in %s", branchName, project.PathWithNamespace))
	return nil
}

// CreateCommit creates a commit with the given files
func (c *Client) CreateCommit(ctx context.Context, project Project, branchName, message string, files map[string]string) error {
	// Placeholder - would implement GitLab API call
	c.logger.Info(fmt.Sprintf("Would create commit in %s on branch %s", project.PathWithNamespace, branchName))
	return nil
}

// CreateMR creates a merge request
func (c *Client) CreateMR(ctx context.Context, project Project, sourceBranch, targetBranch, title, description string) (*MergeRequest, error) {
	// Placeholder - would implement GitLab API call
	c.logger.Info(fmt.Sprintf("Would create MR in %s: %s -> %s", project.PathWithNamespace, sourceBranch, targetBranch))
	return &MergeRequest{
		Title:       title,
		Description: description,
		SourceBranch: sourceBranch,
		TargetBranch: targetBranch,
	}, nil
}
</file>
<file path="internal/handlers/ai_rules.go">
package handlers

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/agents"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// AIRulesHandler handles the generate ai-rules command
type AIRulesHandler struct {
	*BaseHandler
	config config.AIRulesConfig
}

// NewAIRulesHandler creates a new AI rules handler
func NewAIRulesHandler(cfg config.AIRulesConfig, logger *logging.Logger) *AIRulesHandler {
	return &AIRulesHandler{
		BaseHandler: &BaseHandler{
			Config: cfg.BaseConfig,
			Logger: logger,
		},
		config: cfg,
	}
}

// Handle generates AI rules files
func (h *AIRulesHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting AI rules generation",
		logging.String("repo_path", h.config.RepoPath),
	)

	// Load prompts
	promptManager, err := prompts.NewManager("./prompts")
	if err != nil {
		repoPromptsDir := fmt.Sprintf("%s/../gendocs/prompts", h.config.RepoPath)
		promptManager, err = prompts.NewManager(repoPromptsDir)
		if err != nil {
			return errors.NewConfigurationError(fmt.Sprintf("failed to load prompts: %v", err))
		}
	}

	// Create AI rules generator agent
	aiRulesAgent := agents.NewAIRulesGeneratorAgent(h.config, promptManager, h.Logger)

	// Run generation
	if err := aiRulesAgent.Run(ctx); err != nil {
		return errors.NewDocumentationError("AI rules", err)
	}

	h.Logger.Info("AI rules files generated successfully")
	return nil
}
</file>
<file path="internal/handlers/analyze.go">
package handlers

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/agents"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// AnalyzeHandler handles the analyze command
type AnalyzeHandler struct {
	*BaseHandler
	config config.AnalyzerConfig
}

// NewAnalyzeHandler creates a new analyze handler
func NewAnalyzeHandler(cfg config.AnalyzerConfig, logger *logging.Logger) *AnalyzeHandler {
	return &AnalyzeHandler{
		BaseHandler: &BaseHandler{
			Config: cfg.BaseConfig,
			Logger: logger,
		},
		config: cfg,
	}
}

// Handle executes the analysis
func (h *AnalyzeHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting analyze handler",
		logging.String("repo_path", h.config.RepoPath),
	)

	// Load prompts
	// Try to find prompts directory - check relative to binary or repo
	promptManager, err := prompts.NewManager("./prompts")
	if err != nil {
		// Try relative to repo path
		repoPromptsDir := fmt.Sprintf("%s/../gendocs/prompts", h.config.RepoPath)
		promptManager, err = prompts.NewManager(repoPromptsDir)
		if err != nil {
			return errors.NewConfigurationError(fmt.Sprintf("failed to load prompts: %v", err))
		}
	}

	// Create analyzer agent
	analyzerAgent := agents.NewAnalyzerAgent(h.config, promptManager, h.Logger)

	// Run analysis
	result, err := analyzerAgent.Run(ctx)
	if err != nil {
		return errors.NewAnalysisError("analysis execution failed", err)
	}

	// Log results
	h.Logger.Info(fmt.Sprintf("Analysis complete: %d/%d successful",
		len(result.Successful), len(result.Successful)+len(result.Failed)))

	// Determine exit code
	if len(result.Failed) > 0 && len(result.Successful) == 0 {
		return errors.NewAnalysisError("all analyses failed", fmt.Errorf("no successful analyses"))
	}

	if len(result.Failed) > 0 {
		h.Logger.Warn(fmt.Sprintf("Partial success: %d analyses failed", len(result.Failed)))
		for _, failed := range result.Failed {
			h.Logger.Error(fmt.Sprintf("  - %s: %v", failed.Name, failed.Error))
		}
	}

	return nil
}
</file>
<file path="internal/handlers/base.go">
package handlers

import (
	"context"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/logging"
)

// Handler is the interface that all handlers must implement
type Handler interface {
	// Handle executes the handler logic
	Handle(ctx context.Context) error
}

// BaseHandler provides common functionality for all handlers
type BaseHandler struct {
	Config config.BaseConfig
	Logger *logging.Logger
}

// NewBaseHandler creates a new base handler
func NewBaseHandler(cfg config.BaseConfig, logger *logging.Logger) *BaseHandler {
	return &BaseHandler{
		Config: cfg,
		Logger: logger,
	}
}
</file>
<file path="internal/handlers/cronjob.go">
package handlers

import (
	"context"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"time"

	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/gitlab"
	"github.com/user/gendocs/internal/logging"
)

// CronjobHandler handles the cronjob analyze command
type CronjobHandler struct {
	*BaseHandler
	config    config.CronjobConfig
	gitLabCfg config.GitLabConfig
	analyzerCfg config.AnalyzerConfig
	gitlab    *gitlab.Client
}

// NewCronjobHandler creates a new cronjob handler
func NewCronjobHandler(
	cronjobCfg config.CronjobConfig,
	gitLabCfg config.GitLabConfig,
	analyzerCfg config.AnalyzerConfig,
	logger *logging.Logger,
) *CronjobHandler {
	return &CronjobHandler{
		BaseHandler: &BaseHandler{
			Config: config.BaseConfig{
				RepoPath: cronjobCfg.WorkingPath,
				Debug:    false,
			},
			Logger: logger,
		},
		config:      cronjobCfg,
		gitLabCfg:   gitLabCfg,
		analyzerCfg: analyzerCfg,
		gitlab:      gitlab.NewClient(gitLabCfg, logger),
	}
}

// ProcessedResult holds the results of processing projects
type ProcessedResult struct {
	ProcessedCount int
	SuccessCount   int
	ErrorCount     int
	SkippedCount   int
	FailedProjects []FailedProject
}

// FailedProject represents a project that failed to process
type FailedProject struct {
	Name string
	Error error
}

// Handle executes the cronjob analysis
func (h *CronjobHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting cronjob analysis",
		logging.String("working_path", h.config.WorkingPath),
		logging.Int("group_project_id", h.config.GroupProjectID),
		logging.Int("max_days", h.config.MaxDaysSinceLastCommit),
	)

	// Fetch all projects in the group
	projects, err := h.gitlab.FetchProjectsInGroup(ctx, h.config.GroupProjectID)
	if err != nil {
		return errors.NewCronjobError("failed to fetch projects", err)
	}

	h.Logger.Info(fmt.Sprintf("Found %d projects in group", len(projects)))

	// Filter projects
	filter := gitlab.ProjectFilter{
		MaxDaysSinceLastCommit: h.config.MaxDaysSinceLastCommit,
	}

	var applicableProjects []gitlab.Project
	for _, project := range projects {
		shouldAnalyze, err := h.gitlab.ShouldAnalyze(ctx, project, filter)
		if err != nil {
			h.Logger.Warn(fmt.Sprintf("Error checking project %s: %v", project.PathWithNamespace, err))
			continue
		}
		if shouldAnalyze {
			applicableProjects = append(applicableProjects, project)
		}
	}

	h.Logger.Info(fmt.Sprintf("%d projects applicable for analysis", len(applicableProjects)))

	// Process each applicable project
	result := &ProcessedResult{
		FailedProjects: []FailedProject{},
	}

	for _, project := range applicableProjects {
		h.Logger.Info(fmt.Sprintf("Processing %s", project.PathWithNamespace))

		if err := h.processProject(ctx, project); err != nil {
			result.ErrorCount++
			result.FailedProjects = append(result.FailedProjects, FailedProject{
				Name: project.PathWithNamespace,
				Error: err,
			})
			h.Logger.Error(fmt.Sprintf("Failed to process %s: %v", project.PathWithNamespace, err))
		} else {
			result.SuccessCount++
		}
		result.ProcessedCount++
	}

	// Log summary
	h.Logger.Info(fmt.Sprintf("Cronjob complete: %d processed, %d succeeded, %d failed, %d skipped",
		result.ProcessedCount, result.SuccessCount, result.ErrorCount,
		len(projects)-len(applicableProjects)))

	if result.ErrorCount > 0 && result.SuccessCount == 0 {
		return errors.NewCronjobError("all projects failed", fmt.Errorf("%d failures", result.ErrorCount))
	}

	return nil
}

// processProject processes a single project
func (h *CronjobHandler) processProject(ctx context.Context, project gitlab.Project) error {
	// Create temp directory for cloning
	tempDir := filepath.Join(h.config.WorkingPath, "tmp", fmt.Sprintf("project_%d", project.ID))
	if err := os.MkdirAll(tempDir, 0755); err != nil {
		return fmt.Errorf("failed to create temp dir: %w", err)
	}
	defer os.RemoveAll(tempDir)

	// Clone repository
	if err := h.cloneRepository(ctx, project, tempDir); err != nil {
		return errors.NewGitCloneError(project.HTTPURL, "clone failed", err)
	}

	// Create branch
	branchName := fmt.Sprintf("ai-analyzer-%s", time.Now().Format("2006-01-02"))
	if err := h.gitlab.CreateBranch(ctx, project, branchName, project.DefaultBranch); err != nil {
		return fmt.Errorf("failed to create branch: %w", err)
	}

	// Run analysis
	analyzerCfg := h.analyzerCfg
	analyzerCfg.RepoPath = tempDir

	// Run analyze command (via subprocess for now, could be refactored to use handler directly)
	if err := h.runAnalysis(ctx, tempDir); err != nil {
		return fmt.Errorf("analysis failed: %w", err)
	}

	// Commit results
	commitMsg := fmt.Sprintf("[skip ci] AI analysis: %s", time.Now().Format("2006-01-02"))
	if err := h.gitlab.CreateCommit(ctx, project, branchName, commitMsg, nil); err != nil {
		return fmt.Errorf("failed to create commit: %w", err)
	}

	// Create merge request
	mrTitle := fmt.Sprintf("AI Analysis: %s", time.Now().Format("2006-01-02"))
	mrDescription := fmt.Sprintf("Automated AI analysis generated on %s\n\nThis MR contains:\n- Structure analysis\n- Dependency analysis\n- Data flow analysis\n- Request flow analysis\n- API analysis", time.Now().Format("2006-01-02"))
	mr, err := h.gitlab.CreateMR(ctx, project, branchName, project.DefaultBranch, mrTitle, mrDescription)
	if err != nil {
		return fmt.Errorf("failed to create MR: %w", err)
	}

	h.Logger.Info(fmt.Sprintf("Created MR %d for %s", mr.IID, project.PathWithNamespace))
	return nil
}

// cloneRepository clones a GitLab repository
func (h *CronjobHandler) cloneRepository(ctx context.Context, project gitlab.Project, destDir string) error {
	// Clone with authentication
	url := project.HTTPURL
	if h.gitlab.OAuthToken != "" {
		// Inject token into URL
		url = fmt.Sprintf("https://oauth2:%s@%s", h.gitlab.OAuthToken, project.HTTPURL[8:]) // Strip https://
	}

	cmd := exec.CommandContext(ctx, "git", "clone", "--depth", "1", url, destDir)
	output, err := cmd.CombinedOutput()
	if err != nil {
		h.Logger.Debug(fmt.Sprintf("Git clone output: %s", string(output)))
		return err
	}

	return nil
}

// runAnalysis runs the analysis on a repository
func (h *CronjobHandler) runAnalysis(ctx context.Context, repoPath string) error {
	// Run gendocs analyze command as subprocess
	cmd := exec.CommandContext(ctx, "./gendocs", "analyze", "--repo-path", repoPath)
	cmd.Dir = filepath.Dir(repoPath) // Run from parent directory to find binary

	output, err := cmd.CombinedOutput()
	if err != nil {
		h.Logger.Debug(fmt.Sprintf("Analysis output: %s", string(output)))
		return err
	}

	h.Logger.Debug(fmt.Sprintf("Analysis output: %s", string(output)))
	return nil
}
</file>
<file path="internal/handlers/readme.go">
package handlers

import (
	"context"
	"fmt"

	"github.com/user/gendocs/internal/agents"
	"github.com/user/gendocs/internal/config"
	"github.com/user/gendocs/internal/errors"
	"github.com/user/gendocs/internal/logging"
	"github.com/user/gendocs/internal/prompts"
)

// ReadmeHandler handles the generate readme command
type ReadmeHandler struct {
	*BaseHandler
	config config.DocumenterConfig
}

// NewReadmeHandler creates a new readme handler
func NewReadmeHandler(cfg config.DocumenterConfig, logger *logging.Logger) *ReadmeHandler {
	return &ReadmeHandler{
		BaseHandler: &BaseHandler{
			Config: cfg.BaseConfig,
			Logger: logger,
		},
		config: cfg,
	}
}

// Handle generates the README
func (h *ReadmeHandler) Handle(ctx context.Context) error {
	h.Logger.Info("Starting readme generation",
		logging.String("repo_path", h.config.RepoPath),
	)

	// Load prompts
	promptManager, err := prompts.NewManager("./prompts")
	if err != nil {
		repoPromptsDir := fmt.Sprintf("%s/../gendocs/prompts", h.config.RepoPath)
		promptManager, err = prompts.NewManager(repoPromptsDir)
		if err != nil {
			return errors.NewConfigurationError(fmt.Sprintf("failed to load prompts: %v", err))
		}
	}

	// Create documenter agent
	documenterAgent := agents.NewDocumenterAgent(h.config, promptManager, h.Logger)

	// Run generation
	if err := documenterAgent.Run(ctx); err != nil {
		return errors.NewDocumentationError("README", err)
	}

	h.Logger.Info("README.md generated successfully")
	return nil
}
</file>
<file path="internal/llm/anthropic.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/user/gendocs/internal/config"
)

// AnthropicClient implements LLMClient for Anthropic Claude
type AnthropicClient struct {
	*BaseLLMClient
	apiKey string
	model  string
}

// anthropicRequest represents the request body for Anthropic API
type anthropicRequest struct {
	Model         string                  `json:"model"`
	Messages      []anthropicMessage      `json:"messages"`
	System        string                  `json:"system,omitempty"`
	MaxTokens     int                     `json:"max_tokens"`
	Temperature   float64                 `json:"temperature,omitempty"`
	Tools         []anthropicTool         `json:"tools,omitempty"`
	Stream        bool                    `json:"stream,omitempty"`
}

// anthropicMessage represents a message in Anthropic format
type anthropicMessage struct {
	Role    string                 `json:"role"`
	Content []anthropicContentBlock `json:"content"`
}

// anthropicContentBlock represents a content block
type anthropicContentBlock struct {
	Type     string                 `json:"type"`
	Text     string                 `json:"text,omitempty"`
	ToolUse  *anthropicToolUseBlock `json:"tool_use,omitempty"`
	ToolResult *anthropicToolResultBlock `json:"tool_result,omitempty"`
}

// anthropicToolUseBlock represents a tool use call
type anthropicToolUseBlock struct {
	ID       string                 `json:"id"`
	Name     string                 `json:"name"`
	Input    map[string]interface{} `json:"input"`
}

// anthropicToolResultBlock represents a tool result
type anthropicToolResultBlock struct {
	ToolUseID string `json:"tool_use_id"`
	Content   string `json:"content"`
}

// anthropicTool represents a tool definition
type anthropicTool struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	InputSchema map[string]interface{} `json:"input_schema"`
}

// anthropicResponse represents the response from Anthropic API
type anthropicResponse struct {
	ID      string                `json:"id"`
	Type    string                `json:"type"`
	Role    string                `json:"role"`
	Content []anthropicContentBlock `json:"content"`
	StopReason string              `json:"stop_reason"`
	Usage   anthropicUsage        `json:"usage"`
	Error   *anthropicError       `json:"error,omitempty"`
}

// anthropicUsage represents token usage
type anthropicUsage struct {
	InputTokens  int `json:"input_tokens"`
	OutputTokens int `json:"output_tokens"`
}

// anthropicError represents an error from Anthropic
type anthropicError struct {
	Type    string `json:"type"`
	Message string `json:"message"`
}

// NewAnthropicClient creates a new Anthropic client
func NewAnthropicClient(cfg config.LLMConfig, retryClient *RetryClient) *AnthropicClient {
	return &AnthropicClient{
		BaseLLMClient: NewBaseLLMClient(retryClient),
		apiKey:        cfg.APIKey,
		model:         cfg.Model,
	}
}

// GenerateCompletion generates a completion from Anthropic
func (c *AnthropicClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	// Convert to Anthropic format
	anReq := c.convertRequest(req)

	jsonData, err := json.Marshal(anReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	url := "https://api.anthropic.com/v1/messages"
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonData))
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("x-api-key", c.apiKey)
	httpReq.Header.Set("anthropic-version", "2023-06-01")

	// Execute with retry
	resp, err := c.retryClient.Do(httpReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	// Read response
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to read response: %w", err)
	}

	// Check for error status
	if resp.StatusCode != http.StatusOK {
		return CompletionResponse{}, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))
	}

	// Parse response
	var anResp anthropicResponse
	if err := json.Unmarshal(body, &anResp); err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to parse response: %w", err)
	}

	// Check for API error
	if anResp.Error != nil {
		return CompletionResponse{}, fmt.Errorf("API error: %s", anResp.Error.Message)
	}

	return c.convertResponse(anResp), nil
}

// SupportsTools returns true
func (c *AnthropicClient) SupportsTools() bool {
	return true
}

// GetProvider returns the provider name
func (c *AnthropicClient) GetProvider() string {
	return "anthropic"
}

// convertRequest converts internal request to Anthropic format
func (c *AnthropicClient) convertRequest(req CompletionRequest) anthropicRequest {
	// Build messages from internal format
	messages := []anthropicMessage{}

	// Convert internal messages to Anthropic format
	for _, msg := range req.Messages {
		if msg.Role == "tool" {
			// Tool result message
			messages = append(messages, anthropicMessage{
				Role: "user",
				Content: []anthropicContentBlock{
					{
						Type: "tool_result",
						ToolResult: &anthropicToolResultBlock{
							Content: msg.Content,
						},
					},
				},
			})
		} else if msg.Role == "assistant" {
			// Assistant message
			contentBlock := anthropicContentBlock{
				Type: "text",
				Text: msg.Content,
			}
			messages = append(messages, anthropicMessage{
				Role:    "assistant",
				Content: []anthropicContentBlock{contentBlock},
			})
		}
	}

	// If no messages yet, add initial user message
	if len(messages) == 0 {
		messages = append(messages, anthropicMessage{
			Role: "user",
			Content: []anthropicContentBlock{
				{Type: "text", Text: "Analyze this codebase."},
			},
		})
	}

	// Build tools
	var tools []anthropicTool
	if len(req.Tools) > 0 {
		tools = make([]anthropicTool, len(req.Tools))
		for i, tool := range req.Tools {
			tools[i] = anthropicTool{
				Name:        tool.Name,
				Description: tool.Description,
				InputSchema: tool.Parameters,
			}
		}
	}

	return anthropicRequest{
		Model:       c.model,
		Messages:    messages,
		System:      req.SystemPrompt,
		MaxTokens:   req.MaxTokens,
		Temperature: req.Temperature,
		Tools:       tools,
		Stream:      false,
	}
}

// convertResponse converts Anthropic response to internal format
func (c *AnthropicClient) convertResponse(resp anthropicResponse) CompletionResponse {
	result := CompletionResponse{
		Usage: TokenUsage{
			InputTokens:  resp.Usage.InputTokens,
			OutputTokens: resp.Usage.OutputTokens,
			TotalTokens:  resp.Usage.InputTokens + resp.Usage.OutputTokens,
		},
	}

	// Extract content and tool calls
	var textContent strings.Builder
	var toolCalls []ToolCall

	for _, block := range resp.Content {
		if block.Type == "text" {
			textContent.WriteString(block.Text)
		} else if block.Type == "tool_use" && block.ToolUse != nil {
			toolCalls = append(toolCalls, ToolCall{
				Name:      block.ToolUse.Name,
				Arguments: block.ToolUse.Input,
			})
		}
	}

	result.Content = textContent.String()
	result.ToolCalls = toolCalls

	return result
}
</file>
<file path="internal/llm/client.go">
package llm

import (
	"context"
)

// Message represents a chat message
type Message struct {
	Role    string // "system", "user", "assistant", "tool"
	Content string
	ToolID  string // ID of the tool that was called (for role="tool")
}

// ToolCall represents a tool/function call from the LLM
type ToolCall struct {
	Name      string
	Arguments map[string]interface{}
}

// CompletionRequest is a request for LLM completion
type CompletionRequest struct {
	SystemPrompt string
	Messages     []Message
	Tools        []ToolDefinition
	MaxTokens    int
	Temperature  float64
}

// CompletionResponse is the response from LLM
type CompletionResponse struct {
	Content   string
	ToolCalls []ToolCall
	Usage     TokenUsage
}

// TokenUsage tracks token usage
type TokenUsage struct {
	InputTokens  int
	OutputTokens int
	TotalTokens  int
}

// ToolDefinition defines a tool for the LLM
type ToolDefinition struct {
	Name        string
	Description string
	Parameters  map[string]interface{}
}

// LLMClient is the interface for LLM providers
type LLMClient interface {
	// GenerateCompletion generates a completion from the LLM
	GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error)

	// SupportsTools returns true if the client supports tool calling
	SupportsTools() bool

	// GetProvider returns the provider name
	GetProvider() string
}

// BaseLLMClient provides common functionality for all LLM clients
type BaseLLMClient struct {
	retryClient *RetryClient
}

// NewBaseLLMClient creates a new base LLM client
func NewBaseLLMClient(retryClient *RetryClient) *BaseLLMClient {
	return &BaseLLMClient{
		retryClient: retryClient,
	}
}
</file>
<file path="internal/llm/factory.go">
package llm

import (
	"fmt"

	"github.com/user/gendocs/internal/config"
)

// Factory creates LLM clients
type Factory struct {
	retryClient *RetryClient
}

// NewFactory creates a new LLM factory
func NewFactory(retryClient *RetryClient) *Factory {
	return &Factory{
		retryClient: retryClient,
	}
}

// CreateClient creates an LLM client based on the provider configuration
func (f *Factory) CreateClient(cfg config.LLMConfig) (LLMClient, error) {
	switch cfg.Provider {
	case "openai":
		return NewOpenAIClient(cfg, f.retryClient), nil
	case "anthropic":
		return NewAnthropicClient(cfg, f.retryClient), nil
	case "gemini":
		return NewGeminiClient(cfg, f.retryClient), nil
	default:
		return nil, fmt.Errorf("unsupported LLM provider: %s (supported: openai, anthropic, gemini)", cfg.Provider)
	}
}
</file>
<file path="internal/llm/gemini.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/user/gendocs/internal/config"
)

// GeminiClient implements LLMClient for Google Gemini
type GeminiClient struct {
	*BaseLLMClient
	apiKey string
	model string
}

// geminiRequest represents the request body for Gemini API
type geminiRequest struct {
	Contents       []geminiContent    `json:"contents"`
	Tools          []geminiTool       `json:"tools,omitempty"`
	GenerationConfig geminiGenerationConfig `json:"generationConfig,omitempty"`
	SystemInstruction *geminiContent  `json:"systemInstruction,omitempty"`
}

// geminiContent represents content in Gemini format
type geminiContent struct {
	Role  string           `json:"role,omitempty"`
	Parts []geminiPart     `json:"parts"`
}

// geminiPart represents a part of content
type geminiPart struct {
	Text         string                 `json:"text,omitempty"`
	FunctionCall map[string]interface{} `json:"functionCall,omitempty"`
	FunctionResponse *geminiFunctionResponse `json:"functionResponse,omitempty"`
}

// geminiFunctionResponse represents a function response
// Gemini format: {"name": "function_name", "response": {...}}
type geminiFunctionResponse struct {
	Name     string                 `json:"name"`
	Response map[string]interface{} `json:"response,omitempty"`
}

// geminiTool represents a tool declaration
type geminiTool struct {
	FunctionDeclarations []geminiFunctionDeclaration `json:"functionDeclarations,omitempty"`
}

// geminiFunctionDeclaration represents a function declaration
type geminiFunctionDeclaration struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Parameters  map[string]interface{} `json:"parameters"`
}

// geminiGenerationConfig represents generation configuration
type geminiGenerationConfig struct {
	Temperature float64 `json:"temperature,omitempty"`
	MaxOutputTokens int  `json:"maxOutputTokens,omitempty"`
}

// geminiResponse represents the response from Gemini API
type geminiResponse struct {
	Candidates []geminiCandidate `json:"candidates"`
	UsageMetadata geminiUsageMetadata `json:"usageMetadata,omitempty"`
	Error      *geminiError      `json:"error,omitempty"`
}

// geminiCandidate represents a candidate response
type geminiCandidate struct {
	Content   geminiContent `json:"content"`
	FinishReason string     `json:"finishReason,omitempty"`
}

// geminiUsageMetadata represents token usage
type geminiUsageMetadata struct {
	PromptTokenCount     int `json:"promptTokenCount"`
	CandidatesTokenCount int `json:"candidatesTokenCount"`
	TotalTokenCount      int `json:"totalTokenCount"`
}

// geminiError represents an error
type geminiError struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Status  string `json:"status"`
}

// NewGeminiClient creates a new Gemini client
func NewGeminiClient(cfg config.LLMConfig, retryClient *RetryClient) *GeminiClient {
	return &GeminiClient{
		BaseLLMClient: NewBaseLLMClient(retryClient),
		apiKey:        cfg.APIKey,
		model:         cfg.Model,
	}
}

// GenerateCompletion generates a completion from Gemini
func (c *GeminiClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	// Convert to Gemini format
	gemReq := c.convertRequest(req)

	jsonData, err := json.Marshal(gemReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	// Model format: models/gemini-1.5-pro or models/gemini-pro
	modelName := c.model
	if !strings.HasPrefix(modelName, "models/") {
		modelName = "models/" + modelName
	}
	url := fmt.Sprintf("https://generativelanguage.googleapis.com/v1beta/%s:generateContent?key=%s", modelName, c.apiKey)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonData))
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")

	// Execute with retry
	resp, err := c.retryClient.Do(httpReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	// Read response
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to read response: %w", err)
	}

	// Check for error status
	if resp.StatusCode != http.StatusOK {
		return CompletionResponse{}, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))
	}

	// Parse response
	var gemResp geminiResponse
	if err := json.Unmarshal(body, &gemResp); err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to parse response: %w", err)
	}

	// Check for API error
	if gemResp.Error != nil {
		return CompletionResponse{}, fmt.Errorf("API error: %s", gemResp.Error.Message)
	}

	return c.convertResponse(gemResp), nil
}

// SupportsTools returns true
func (c *GeminiClient) SupportsTools() bool {
	return true
}

// GetProvider returns the provider name
func (c *GeminiClient) GetProvider() string {
	return "gemini"
}

// convertRequest converts internal request to Gemini format
func (c *GeminiClient) convertRequest(req CompletionRequest) geminiRequest {
	// Build contents
	contents := []geminiContent{}

	// Add system instruction as first content with role "user"
	// Gemini doesn't have a separate system field, it's part of content
	if req.SystemPrompt != "" {
		contents = append(contents, geminiContent{
			Role: "user",
			Parts: []geminiPart{
				{Text: req.SystemPrompt},
			},
		})
		// Add empty model response
		contents = append(contents, geminiContent{
			Role: "model",
			Parts: []geminiPart{
				{Text: "Understood. I will analyze the codebase according to your instructions."},
			},
		})
	}

	// Add messages
	for _, msg := range req.Messages {
		if msg.Role == "tool" {
			// Tool response - extract function name from tool ID or content
			// Format: {"name": "function_name", "response": {"result": "content"}}
			funcName := msg.ToolID
			if funcName == "" {
				// Try to extract from Content if it's JSON
				var toolData map[string]interface{}
				if err := json.Unmarshal([]byte(msg.Content), &toolData); err == nil {
					if name, ok := toolData["name"].(string); ok {
						funcName = name
					}
				}
			}
			// Fallback to a default name if still empty
			if funcName == "" {
				funcName = "unknown_function"
			}

			contents = append(contents, geminiContent{
				Role: "user",
				Parts: []geminiPart{
					{
						FunctionResponse: &geminiFunctionResponse{
							Name: funcName,
							Response: map[string]interface{}{
								"result": msg.Content,
							},
						},
					},
				},
			})
		} else {
			// Regular message
			role := "user"
			if msg.Role == "assistant" {
				role = "model"
			}
			// Skip empty content messages (avoid empty parts)
			if msg.Content == "" {
				continue
			}
			contents = append(contents, geminiContent{
				Role: role,
				Parts: []geminiPart{
					{Text: msg.Content},
				},
			})
		}
	}

	// Build tools
	var tools []geminiTool
	if len(req.Tools) > 0 {
		tools = make([]geminiTool, 1)
		functions := make([]geminiFunctionDeclaration, len(req.Tools))
		for i, tool := range req.Tools {
			functions[i] = geminiFunctionDeclaration{
				Name:        tool.Name,
				Description: tool.Description,
				Parameters:  tool.Parameters,
			}
		}
		tools[0] = geminiTool{
			FunctionDeclarations: functions,
		}
	}

	return geminiRequest{
		Contents: contents,
		Tools:    tools,
		GenerationConfig: geminiGenerationConfig{
			Temperature:    req.Temperature,
			MaxOutputTokens: req.MaxTokens,
		},
	}
}

// convertResponse converts Gemini response to internal format
func (c *GeminiClient) convertResponse(resp geminiResponse) CompletionResponse {
	result := CompletionResponse{
		Usage: TokenUsage{
			InputTokens:  resp.UsageMetadata.PromptTokenCount,
			OutputTokens: resp.UsageMetadata.CandidatesTokenCount,
			TotalTokens:  resp.UsageMetadata.TotalTokenCount,
		},
	}

	if len(resp.Candidates) == 0 {
		return result
	}

	candidate := resp.Candidates[0]
	var textContent string
	var toolCalls []ToolCall

	for _, part := range candidate.Content.Parts {
		if part.Text != "" {
			textContent += part.Text
		}
		if part.FunctionCall != nil {
			toolCalls = append(toolCalls, ToolCall{
				Name:      part.FunctionCall["name"].(string),
				Arguments: part.FunctionCall["args"].(map[string]interface{}),
			})
		}
	}

	result.Content = textContent
	result.ToolCalls = toolCalls

	return result
}
</file>
<file path="internal/llm/openai.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"

	"github.com/user/gendocs/internal/config"
)

// OpenAIClient implements LLMClient for OpenAI-compatible APIs
type OpenAIClient struct {
	*BaseLLMClient
	apiKey  string
	baseURL string
	model   string
}

// openaiRequest represents the request body for OpenAI API
type openaiRequest struct {
	Model       string         `json:"model"`
	Messages    []openaiMessage `json:"messages"`
	MaxTokens   int            `json:"max_tokens"`
	Temperature float64        `json:"temperature"`
	Tools       []openaiTool   `json:"tools,omitempty"`
}

// openaiMessage represents a message in OpenAI format
type openaiMessage struct {
	Role       string           `json:"role"`
	Content    string           `json:"content"`
	ToolCalls  []openaiToolCall `json:"tool_calls,omitempty"`
	ToolCallID string           `json:"tool_call_id,omitempty"`
}

// openaiTool represents a tool definition in OpenAI format
type openaiTool struct {
	Type     string              `json:"type"`
	Function openaiToolFunction  `json:"function"`
}

// openaiToolFunction represents tool function parameters
type openaiToolFunction struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Parameters  map[string]interface{} `json:"parameters"`
}

// openaiToolCall represents a tool call in OpenAI format
type openaiToolCall struct {
	ID       string                `json:"id"`
	Type     string                `json:"type"`
	Function openaiToolCallFunc    `json:"function"`
}

// openaiToolCallFunc represents function call details
type openaiToolCallFunc struct {
	Name      string `json:"name"`
	Arguments string `json:"arguments"`
}

// openaiResponse represents the response from OpenAI API
type openaiResponse struct {
	ID      string             `json:"id"`
	Object  string             `json:"object"`
	Created int64              `json:"created"`
	Model   string             `json:"model"`
	Choices []openaiChoice     `json:"choices"`
	Usage   openaiUsage        `json:"usage"`
	Error   *openaiErrorDetail `json:"error,omitempty"`
}

// openaiChoice represents a choice in the response
type openaiChoice struct {
	Index        int              `json:"index"`
	Message      openaiMessage    `json:"message"`
	FinishReason string           `json:"finish_reason"`
}

// openaiUsage represents token usage
type openaiUsage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

// openaiErrorDetail represents an error from OpenAI
type openaiErrorDetail struct {
	Message string `json:"message"`
	Type    string `json:"type"`
	Code    string `json:"code"`
}

// NewOpenAIClient creates a new OpenAI client
func NewOpenAIClient(cfg config.LLMConfig, retryClient *RetryClient) *OpenAIClient {
	baseURL := cfg.BaseURL
	if baseURL == "" {
		baseURL = "https://api.openai.com/v1"
	}

	return &OpenAIClient{
		BaseLLMClient: NewBaseLLMClient(retryClient),
		apiKey:        cfg.APIKey,
		baseURL:       baseURL,
		model:         cfg.Model,
	}
}

// GenerateCompletion generates a completion from OpenAI
func (c *OpenAIClient) GenerateCompletion(ctx context.Context, req CompletionRequest) (CompletionResponse, error) {
	// Convert to OpenAI format
	oaReq := c.convertRequest(req)

	jsonData, err := json.Marshal(oaReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Create HTTP request
	url := fmt.Sprintf("%s/chat/completions", c.baseURL)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(jsonData))
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.apiKey))

	// Execute with retry
	resp, err := c.retryClient.Do(httpReq)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	// Read response
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to read response: %w", err)
	}

	// Check for error status
	if resp.StatusCode != http.StatusOK {
		return CompletionResponse{}, fmt.Errorf("API error: status %d, body: %s", resp.StatusCode, string(body))
	}

	// Parse response
	var oaResp openaiResponse
	if err := json.Unmarshal(body, &oaResp); err != nil {
		return CompletionResponse{}, fmt.Errorf("failed to parse response: %w", err)
	}

	// Check for API error
	if oaResp.Error != nil {
		return CompletionResponse{}, fmt.Errorf("API error: %s", oaResp.Error.Message)
	}

	return c.convertResponse(oaResp), nil
}

// SupportsTools returns true
func (c *OpenAIClient) SupportsTools() bool {
	return true
}

// GetProvider returns the provider name
func (c *OpenAIClient) GetProvider() string {
	return "openai"
}

// convertRequest converts internal request to OpenAI format
func (c *OpenAIClient) convertRequest(req CompletionRequest) openaiRequest {
	messages := []openaiMessage{}

	// Add system prompt if provided
	if req.SystemPrompt != "" {
		messages = append(messages, openaiMessage{
			Role:    "system",
			Content: req.SystemPrompt,
		})
	}

	// Add messages
	for _, msg := range req.Messages {
		messages = append(messages, openaiMessage{
			Role:    msg.Role,
			Content: msg.Content,
		})
	}

	oaReq := openaiRequest{
		Model:       c.model,
		Messages:    messages,
		MaxTokens:   req.MaxTokens,
		Temperature: req.Temperature,
	}

	// Add tools if provided
	if len(req.Tools) > 0 {
		oaReq.Tools = make([]openaiTool, len(req.Tools))
		for i, tool := range req.Tools {
			oaReq.Tools[i] = openaiTool{
				Type: "function",
				Function: openaiToolFunction{
					Name:        tool.Name,
					Description: tool.Description,
					Parameters:  tool.Parameters,
				},
			}
		}
	}

	return oaReq
}

// convertResponse converts OpenAI response to internal format
func (c *OpenAIClient) convertResponse(resp openaiResponse) CompletionResponse {
	if len(resp.Choices) == 0 {
		return CompletionResponse{
			Usage: TokenUsage{
				InputTokens:  resp.Usage.PromptTokens,
				OutputTokens: resp.Usage.CompletionTokens,
				TotalTokens:  resp.Usage.TotalTokens,
			},
		}
	}

	choice := resp.Choices[0]
	result := CompletionResponse{
		Content: choice.Message.Content,
		Usage: TokenUsage{
			InputTokens:  resp.Usage.PromptTokens,
			OutputTokens: resp.Usage.CompletionTokens,
			TotalTokens:  resp.Usage.TotalTokens,
		},
	}

	// Convert tool calls
	if len(choice.Message.ToolCalls) > 0 {
		result.ToolCalls = make([]ToolCall, len(choice.Message.ToolCalls))
		for i, tc := range choice.Message.ToolCalls {
			// Parse arguments JSON string
			var args map[string]interface{}
			if tc.Function.Arguments != "" {
				json.Unmarshal([]byte(tc.Function.Arguments), &args)
			}

			result.ToolCalls[i] = ToolCall{
				Name:      tc.Function.Name,
				Arguments: args,
			}
		}
	}

	return result
}
</file>
<file path="internal/llm/retry_client.go">
package llm

import (
	"context"
	"fmt"
	"math"
	"net/http"
	"time"
)

// RetryConfig holds retry configuration
type RetryConfig struct {
	MaxAttempts       int           // Maximum number of retry attempts
	Multiplier        int           // Exponential backoff multiplier
	MaxWaitPerAttempt time.Duration // Maximum wait time per attempt
	MaxTotalWait      time.Duration // Maximum total wait time
}

// DefaultRetryConfig returns default retry configuration
func DefaultRetryConfig() *RetryConfig {
	return &RetryConfig{
		MaxAttempts:       5,
		Multiplier:        1,
		MaxWaitPerAttempt: 60 * time.Second,
		MaxTotalWait:      300 * time.Second,
	}
}

// RetryClient wraps http.Client with retry logic
type RetryClient struct {
	client *http.Client
	config *RetryConfig
}

// NewRetryClient creates a new retry client
func NewRetryClient(config *RetryConfig) *RetryClient {
	if config == nil {
		config = DefaultRetryConfig()
	}

	return &RetryClient{
		client: &http.Client{
			Timeout: 180 * time.Second, // Default timeout
		},
		config: config,
	}
}

// NewRetryClientWithTimeout creates a retry client with custom timeout
func NewRetryClientWithTimeout(timeout time.Duration, config *RetryConfig) *RetryClient {
	if config == nil {
		config = DefaultRetryConfig()
	}

	return &RetryClient{
		client: &http.Client{
			Timeout: timeout,
		},
		config: config,
	}
}

// Do executes an HTTP request with retry logic
func (rc *RetryClient) Do(req *http.Request) (*http.Response, error) {
	return rc.DoWithContext(req.Context(), req)
}

// DoWithContext executes an HTTP request with retry logic and context
func (rc *RetryClient) DoWithContext(ctx context.Context, req *http.Request) (*http.Response, error) {
	var resp *http.Response
	var err error

	totalStartTime := time.Now()

	for attempt := 0; attempt < rc.config.MaxAttempts; attempt++ {
		// Clone the request for each attempt (request body can only be read once)
		reqClone := req.Clone(ctx)

		resp, err = rc.client.Do(reqClone)

		// Check if we should NOT retry
		if err == nil && resp != nil {
			// Success on 2xx and 3xx
			// Also retry on 429 (Too Many Requests) and 5xx
			if resp.StatusCode < 500 && resp.StatusCode != 429 {
				return resp, nil
			}

			// For 4xx errors (except 429), don't retry
			if resp.StatusCode >= 400 && resp.StatusCode < 500 && resp.StatusCode != 429 {
				return resp, nil // Return the error response to caller
			}
		}

		// Calculate wait time with exponential backoff
		waitTime := rc.calculateWaitTime(attempt)

		// Check if we've exceeded max total wait time
		if time.Since(totalStartTime)+waitTime > rc.config.MaxTotalWait {
			break
		}

		// Wait before retry (but not after the last attempt)
		if attempt < rc.config.MaxAttempts-1 {
			select {
			case <-time.After(waitTime):
				// Continue to next attempt
			case <-ctx.Done():
				return nil, ctx.Err()
			}
		}
	}

	// All retries exhausted
	if err != nil {
		return nil, fmt.Errorf("request failed after %d attempts: %w", rc.config.MaxAttempts, err)
	}

	if resp != nil {
		return nil, fmt.Errorf("request failed with status %d after %d attempts", resp.StatusCode, rc.config.MaxAttempts)
	}

	return nil, fmt.Errorf("request failed after %d attempts", rc.config.MaxAttempts)
}

// calculateWaitTime calculates wait time using exponential backoff
func (rc *RetryClient) calculateWaitTime(attempt int) time.Duration {
	// Exponential backoff: 2^attempt * multiplier seconds
	baseWait := time.Duration(math.Pow(2, float64(attempt))) * time.Duration(rc.config.Multiplier) * time.Second

	// Cap at max wait per attempt
	if baseWait > rc.config.MaxWaitPerAttempt {
		baseWait = rc.config.MaxWaitPerAttempt
	}

	return baseWait
}

// SetTimeout updates the client timeout
func (rc *RetryClient) SetTimeout(timeout time.Duration) {
	rc.client.Timeout = timeout
}

// GetTimeout returns the current client timeout
func (rc *RetryClient) GetTimeout() time.Duration {
	return rc.client.Timeout
}
</file>
<file path="internal/logging/logger.go">
package logging

import (
	"os"
	"path/filepath"

	"go.uber.org/zap"
	"go.uber.org/zap/zapcore"
)

// Field is a type alias for zap.Field
type Field = zap.Field

// Common field constructors
var (
	String   = zap.String
	Int      = zap.Int
	Int64    = zap.Int64
	Float64  = zap.Float64
	Bool     = zap.Bool
	Any      = zap.Any
	Error    = zap.Error
	Err      = zap.NamedError
	Duration = zap.Duration
	Time     = zap.Time
)

// LevelFromString converts a string level to zapcore.Level
func LevelFromString(level string) zapcore.Level {
	switch level {
	case "debug":
		return zapcore.DebugLevel
	case "info":
		return zapcore.InfoLevel
	case "warn":
		return zapcore.WarnLevel
	case "error":
		return zapcore.ErrorLevel
	default:
		return zapcore.InfoLevel
	}
}

// Logger wraps zap.Logger with application-specific methods
type Logger struct {
	zap *zap.Logger
}

// Config holds logger configuration
type Config struct {
	LogDir       string
	FileLevel    zapcore.Level
	ConsoleLevel zapcore.Level
	EnableCaller bool
}

// DefaultConfig returns default logger configuration
func DefaultConfig() *Config {
	return &Config{
		LogDir:       ".ai/logs",
		FileLevel:    zapcore.InfoLevel,
		ConsoleLevel: zapcore.DebugLevel,
		EnableCaller: true,
	}
}

// NewLogger creates a new logger with file and console output
func NewLogger(cfg *Config) (*Logger, error) {
	if cfg == nil {
		cfg = DefaultConfig()
	}

	// Ensure log directory exists
	if err := os.MkdirAll(cfg.LogDir, 0755); err != nil {
		return nil, err
	}

	// File encoder (JSON)
	fileEncoderConfig := zap.NewProductionEncoderConfig()
	fileEncoderConfig.TimeKey = "timestamp"
	fileEncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
	fileEncoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder
	fileEncoder := zapcore.NewJSONEncoder(fileEncoderConfig)

	// Console encoder (human-readable with colors)
	consoleEncoderConfig := zap.NewDevelopmentEncoderConfig()
	consoleEncoderConfig.EncodeLevel = zapcore.CapitalColorLevelEncoder
	consoleEncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
	consoleEncoder := zapcore.NewConsoleEncoder(consoleEncoderConfig)

	// File writer
	logFile := filepath.Join(cfg.LogDir, "gendocs.log")
	file, err := os.OpenFile(logFile, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		return nil, err
	}
	fileWriter := zapcore.AddSync(file)

	// Console writer
	consoleWriter := zapcore.AddSync(os.Stderr)

	// Core with both outputs
	core := zapcore.NewTee(
		zapcore.NewCore(fileEncoder, fileWriter, cfg.FileLevel),
		zapcore.NewCore(consoleEncoder, consoleWriter, cfg.ConsoleLevel),
	)

	// Create logger
	zapLogger := zap.New(core, zap.AddCaller(), zap.AddStacktrace(zapcore.ErrorLevel))

	return &Logger{zap: zapLogger}, nil
}

// NewNopLogger creates a no-op logger for testing
func NewNopLogger() *Logger {
	return &Logger{zap: zap.NewNop()}
}

// Sync flushes any buffered log entries
func (l *Logger) Sync() error {
	return l.zap.Sync()
}

// Debug logs a debug message
func (l *Logger) Debug(msg string, fields ...zap.Field) {
	l.zap.Debug(msg, fields...)
}

// Info logs an info message
func (l *Logger) Info(msg string, fields ...zap.Field) {
	l.zap.Info(msg, fields...)
}

// Warn logs a warning message
func (l *Logger) Warn(msg string, fields ...zap.Field) {
	l.zap.Warn(msg, fields...)
}

// Error logs an error message
func (l *Logger) Error(msg string, fields ...zap.Field) {
	l.zap.Error(msg, fields...)
}

// Fatal logs a fatal message and exits
func (l *Logger) Fatal(msg string, fields ...zap.Field) {
	l.zap.Fatal(msg, fields...)
}

// With creates a child logger with additional fields
func (l *Logger) With(fields ...zap.Field) *Logger {
	return &Logger{zap: l.zap.With(fields...)}
}

// Named creates a named child logger
func (l *Logger) Named(name string) *Logger {
	return &Logger{zap: l.zap.Named(name)}
}
</file>
<file path="internal/prompts/manager.go">
package prompts

import (
	"bytes"
	"fmt"
	"os"
	"path/filepath"
	textTemplate "text/template"

	"gopkg.in/yaml.v3"
)

// Manager handles loading and rendering prompt templates
type Manager struct {
	prompts map[string]string
}

// PromptTemplate represents a prompt with system and user components
type PromptTemplate struct {
	SystemPrompt string `yaml:"system_prompt"`
	UserPrompt   string `yaml:"user_prompt"`
}

// NewManager creates a new prompt manager by loading prompts from a directory
func NewManager(promptsDir string) (*Manager, error) {
	pm := &Manager{
		prompts: make(map[string]string),
	}

	// Load all YAML files from the prompts directory
	entries, err := os.ReadDir(promptsDir)
	if err != nil {
		return nil, fmt.Errorf("failed to read prompts directory: %w", err)
	}

	for _, entry := range entries {
		if entry.IsDir() || filepath.Ext(entry.Name()) != ".yaml" && filepath.Ext(entry.Name()) != ".yml" {
			continue
		}

		filePath := filepath.Join(promptsDir, entry.Name())
		data, err := os.ReadFile(filePath)
		if err != nil {
			return nil, fmt.Errorf("failed to read prompt file %s: %w", filePath, err)
		}

		// Parse YAML - could be simple string -> string or nested
		var prompts map[string]string
		if err := yaml.Unmarshal(data, &prompts); err != nil {
			return nil, fmt.Errorf("failed to parse prompts from %s: %w", filePath, err)
		}

		// Merge into main prompts map
		for key, value := range prompts {
			pm.prompts[key] = value
		}
	}

	return pm, nil
}

// NewManagerFromMap creates a prompt manager from a map (useful for testing)
func NewManagerFromMap(prompts map[string]string) *Manager {
	return &Manager{
		prompts: prompts,
	}
}

// Get returns a raw prompt by name
func (pm *Manager) Get(name string) (string, error) {
	prompt, ok := pm.prompts[name]
	if !ok {
		return "", fmt.Errorf("prompt '%s' not found (available: %v)", name, pm.getAvailableNames())
	}
	return prompt, nil
}

// Render renders a prompt template with the given variables
func (pm *Manager) Render(name string, vars map[string]interface{}) (string, error) {
	promptTemplate, err := pm.Get(name)
	if err != nil {
		return "", err
	}

	// Parse and execute template
	tmpl, err := textTemplate.New(name).Option("missingkey=error").Parse(promptTemplate)
	if err != nil {
		return "", fmt.Errorf("failed to parse template '%s': %w", name, err)
	}

	var buf bytes.Buffer
	if err := tmpl.Execute(&buf, vars); err != nil {
		return "", fmt.Errorf("failed to execute template '%s': %w", name, err)
	}

	return buf.String(), nil
}

// GetPromptTemplate returns a PromptTemplate with system and user prompts
func (pm *Manager) GetPromptTemplate(name string) (*PromptTemplate, error) {
	systemPrompt, err := pm.Render(name+"_system_prompt", nil)
	if err != nil {
		// Try without suffix
		systemPrompt, err = pm.Get(name + "_system")
		if err != nil {
			return nil, fmt.Errorf("system prompt '%s' not found", name)
		}
	}

	userPrompt, err := pm.Get(name + "_user_prompt")
	if err != nil {
		return nil, fmt.Errorf("user prompt '%s' not found", name)
	}

	return &PromptTemplate{
		SystemPrompt: systemPrompt,
		UserPrompt:   userPrompt,
	}, nil
}

// RenderTemplate renders both system and user prompts with variables
func (pm *Manager) RenderTemplate(name string, vars map[string]interface{}) (*PromptTemplate, error) {
	template, err := pm.GetPromptTemplate(name)
	if err != nil {
		return nil, err
	}

	// Render system prompt with variables
	systemTmpl, err := textTemplate.New("system").Parse(template.SystemPrompt)
	if err != nil {
		return nil, fmt.Errorf("failed to parse system prompt: %w", err)
	}

	var systemBuf bytes.Buffer
	if err := systemTmpl.Execute(&systemBuf, vars); err != nil {
		return nil, fmt.Errorf("failed to render system prompt: %w", err)
	}

	// Render user prompt with variables
	userTmpl, err := textTemplate.New("user").Parse(template.UserPrompt)
	if err != nil {
		return nil, fmt.Errorf("failed to parse user prompt: %w", err)
	}

	var userBuf bytes.Buffer
	if err := userTmpl.Execute(&userBuf, vars); err != nil {
		return nil, fmt.Errorf("failed to render user prompt: %w", err)
	}

	return &PromptTemplate{
		SystemPrompt: systemBuf.String(),
		UserPrompt:   userBuf.String(),
	}, nil
}

// getAvailableNames returns a list of available prompt names
func (pm *Manager) getAvailableNames() []string {
	names := make([]string, 0, len(pm.prompts))
	for name := range pm.prompts {
		names = append(names, name)
	}
	return names
}

// HasPrompt checks if a prompt exists
func (pm *Manager) HasPrompt(name string) bool {
	_, ok := pm.prompts[name]
	return ok
}
</file>
<file path="internal/tools/base.go">
package tools

import (
	"context"
	"fmt"
)

// ModelRetryError is raised when a tool encounters a recoverable error
// This error type triggers a retry at the agent level
type ModelRetryError struct {
	Message string
}

func (e *ModelRetryError) Error() string {
	return e.Message
}

// Tool is the interface that all tools must implement
type Tool interface {
	// Name returns the tool name
	Name() string

	// Description returns a description of what the tool does
	Description() string

	// Parameters returns the JSON schema for the tool's parameters
	Parameters() map[string]interface{}

	// Execute runs the tool with the given parameters
	Execute(ctx context.Context, params map[string]interface{}) (interface{}, error)
}

// BaseTool provides common functionality for all tools
type BaseTool struct {
	MaxRetries int
}

// NewBaseTool creates a new base tool
func NewBaseTool(maxRetries int) BaseTool {
	return BaseTool{
		MaxRetries: maxRetries,
	}
}

// RetryableExecute executes a function with retry logic
func (bt *BaseTool) RetryableExecute(ctx context.Context, fn func() (interface{}, error)) (interface{}, error) {
	var lastErr error

	for attempt := 0; attempt < bt.MaxRetries; attempt++ {
		result, err := fn()
		if err == nil {
			return result, nil
		}

		lastErr = err

		// Check if error is recoverable (ModelRetryError)
		if _, ok := err.(*ModelRetryError); !ok {
			// Not recoverable, return immediately
			return nil, err
		}

		// If it's the last attempt, don't wait
		if attempt == bt.MaxRetries-1 {
			break
		}
	}

	if lastErr != nil {
		return nil, fmt.Errorf("tool failed after %d retries: %w", bt.MaxRetries, lastErr)
	}

	return nil, fmt.Errorf("tool failed after %d retries", bt.MaxRetries)
}
</file>
<file path="internal/tools/file_read.go">
package tools

import (
	"context"
	"bufio"
	"fmt"
	"os"
	"strconv"
)

// FileReadTool reads file contents with optional pagination
type FileReadTool struct {
	BaseTool
}

// NewFileReadTool creates a new file read tool
func NewFileReadTool(maxRetries int) *FileReadTool {
	return &FileReadTool{
		BaseTool: NewBaseTool(maxRetries),
	}
}

// Name returns the tool name
func (frt *FileReadTool) Name() string {
	return "read_file"
}

// Description returns the tool description
func (frt *FileReadTool) Description() string {
	return "Read contents of a file. By default reads first 200 lines. Use line_number and line_count for pagination."
}

// Parameters returns the JSON schema for the tool parameters
func (frt *FileReadTool) Parameters() map[string]interface{} {
	return map[string]interface{}{
		"type": "object",
		"properties": map[string]interface{}{
			"file_path": map[string]interface{}{
				"type":        "string",
				"description": "Path to the file to read",
			},
			"line_number": map[string]interface{}{
				"type":        "integer",
				"description": "Starting line number (1-indexed). Default: 1",
			},
			"line_count": map[string]interface{}{
				"type":        "integer",
				"description": "Number of lines to read. Default: 200",
			},
		},
		"required": []string{"file_path"},
	}
}

// Execute reads the file contents
func (frt *FileReadTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) {
	return frt.RetryableExecute(ctx, func() (interface{}, error) {
		filePath, ok := params["file_path"].(string)
		if !ok {
			return nil, fmt.Errorf("file_path must be a string")
		}

		lineNumber := 1
		if ln, ok := params["line_number"]; ok {
			switch v := ln.(type) {
			case int:
				lineNumber = v
			case float64:
				lineNumber = int(v)
			case string:
				if i, err := strconv.Atoi(v); err == nil {
					lineNumber = i
				}
			}
		}

		lineCount := 200
		if lc, ok := params["line_count"]; ok {
			switch v := lc.(type) {
			case int:
				lineCount = v
			case float64:
				lineCount = int(v)
			case string:
				if i, err := strconv.Atoi(v); err == nil {
					lineCount = i
				}
			}
		}

		file, err := os.Open(filePath)
		if err != nil {
			return nil, &ModelRetryError{Message: fmt.Sprintf("Failed to open file: %v", err)}
		}
		defer file.Close()

		scanner := bufio.NewScanner(file)
		var lines []string
		currentLine := 1

		for scanner.Scan() {
			if currentLine >= lineNumber && currentLine < lineNumber+lineCount {
				lines = append(lines, scanner.Text())
			}
			currentLine++
			if currentLine >= lineNumber+lineCount {
				break
			}
		}

		if err := scanner.Err(); err != nil {
			return nil, &ModelRetryError{Message: fmt.Sprintf("Error reading file: %v", err)}
		}

		return map[string]interface{}{
			"content":         lines,
			"start_line":      lineNumber,
			"end_line":        lineNumber + len(lines) - 1,
			"total_lines_read": len(lines),
		}, nil
	})
}
</file>
<file path="internal/tools/list_files.go">
package tools

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
)

// ListFilesTool lists files in a directory recursively
type ListFilesTool struct {
	BaseTool
}

// NewListFilesTool creates a new list files tool
func NewListFilesTool(maxRetries int) *ListFilesTool {
	return &ListFilesTool{
		BaseTool: NewBaseTool(maxRetries),
	}
}

// Name returns the tool name
func (lft *ListFilesTool) Name() string {
	return "list_files"
}

// Description returns the tool description
func (lft *ListFilesTool) Description() string {
	return "List all files in a directory recursively"
}

// Parameters returns the JSON schema for the tool parameters
func (lft *ListFilesTool) Parameters() map[string]interface{} {
	return map[string]interface{}{
		"type": "object",
		"properties": map[string]interface{}{
			"directory": map[string]interface{}{
				"type":        "string",
				"description": "Directory path to list files from",
			},
		},
		"required": []string{"directory"},
	}
}

// Execute lists files in the directory
func (lft *ListFilesTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) {
	return lft.RetryableExecute(ctx, func() (interface{}, error) {
		directory, ok := params["directory"].(string)
		if !ok {
			return nil, fmt.Errorf("directory must be a string")
		}

		var files []string
		err := filepath.Walk(directory, func(path string, info os.FileInfo, err error) error {
			if err != nil {
				return err
			}
			if !info.IsDir() {
				relPath, err := filepath.Rel(directory, path)
				if err == nil {
					files = append(files, relPath)
				}
			}
			return nil
		})

		if err != nil {
			return nil, &ModelRetryError{Message: fmt.Sprintf("Failed to list files: %v", err)}
		}

		return map[string]interface{}{
			"files": files,
			"count": len(files),
		}, nil
	})
}
</file>
<file path="internal/tui/config.go">
package tui

import (
	"fmt"
	"os"
	"path/filepath"

	tea "github.com/charmbracelet/bubbletea"
	"github.com/charmbracelet/bubbles/textinput"
	"github.com/charmbracelet/lipgloss"
)

// Styles for the TUI
var (
	titleStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("##FAFAFA")).
			Background(lipgloss.Color("##7D56F4")).
			Padding(0, 1)

	highlightStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#7D56F4")).
			Bold(true)

	errorStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#FF5F87")).
			Bold(true)

	successStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("#50FA7B")).
			Bold(true)
)

// Step represents a configuration step
type Step int

const (
	StepProvider Step = iota
	StepAPIKey
	StepModel
	StepBaseURL
	StepConfirm
	StepSave
	StepComplete
)

func (s Step) String() string {
	switch s {
	case StepProvider:
		return "Provider Selection"
	case StepAPIKey:
		return "API Key"
	case StepModel:
		return "Model"
	case StepBaseURL:
		return "Base URL (Optional)"
	case StepConfirm:
		return "Confirm"
	case StepSave:
		return "Save"
	case StepComplete:
		return "Complete"
	default:
		return "Unknown"
	}
}

// Model holds the TUI state
type Model struct {
	Step         Step
	Provider     string
	APIKey       string
	Model        string
	BaseURL      string
	Quitting     bool
	ConfigPath   string
	SavedConfig  bool
	Err          error
	// Text inputs for user input (exported fields)
	APIKeyInput  textinput.Model
	ModelInput   textinput.Model
	BaseURLInput textinput.Model
}

// ConfigResult holds the final configuration result
type ConfigResult struct {
	Saved bool
	Path  string
	Error error
}

// Init initializes the model
func (m Model) Init() tea.Cmd {
	return textinput.Blink
}

// Update handles messages
func (m Model) Update(msg tea.Msg) (tea.Model, tea.Cmd) {
	switch msg := msg.(type) {
	case tea.KeyMsg:
		switch msg.String() {
		case "ctrl+c", "q":
			m.Quitting = true
			return m, tea.Quit
		case "enter":
			// Handle Enter key based on current step
			switch m.Step {
			case StepProvider:
				// Only advance if a provider is selected
				if m.Provider != "" {
					m.Step = StepAPIKey
					m.APIKeyInput.Focus()
					m.ModelInput.Blur()
					m.BaseURLInput.Blur()
				}
			case StepAPIKey:
				m.APIKey = m.APIKeyInput.Value()
				if m.APIKey != "" {
					m.Step = StepModel
					m.APIKeyInput.Blur()
					m.ModelInput.Focus()
					m.BaseURLInput.Blur()
				}
			case StepModel:
				inputModel := m.ModelInput.Value()
				if inputModel != "" {
					m.Model = inputModel
				} else if m.Model == "" {
					// Set default model based on provider
					switch m.Provider {
					case "openai":
						m.Model = "gpt-4o"
					case "anthropic":
						m.Model = "claude-3-5-sonnet-20241022"
					case "gemini":
						m.Model = "gemini-1.5-pro"
					}
				}
				m.Step = StepBaseURL
				m.APIKeyInput.Blur()
				m.ModelInput.Blur()
				m.BaseURLInput.Focus()
			case StepBaseURL:
				m.BaseURL = m.BaseURLInput.Value()
				m.Step = StepConfirm
				m.APIKeyInput.Blur()
				m.ModelInput.Blur()
				m.BaseURLInput.Blur()
			case StepConfirm:
				// Enter on confirm step is handled by y/n
			}
		case "1":
			if m.Step == StepProvider {
				m.Provider = "openai"
				m.Model = "gpt-4o"
			}
		case "2":
			if m.Step == StepProvider {
				m.Provider = "anthropic"
				m.Model = "claude-3-5-sonnet-20241022"
			}
		case "3":
			if m.Step == StepProvider {
				m.Provider = "gemini"
				m.Model = "gemini-1.5-pro"
			}
		case "y", "Y":
			if m.Step == StepConfirm {
				m.saveConfig()
				m.Step = StepComplete
			}
		case "n", "N":
			if m.Step == StepConfirm {
				m.Step = StepProvider
			}
		case "esc":
			if m.Step == StepModel {
				m.Step = StepAPIKey
				m.APIKeyInput.Focus()
				m.ModelInput.Blur()
				m.BaseURLInput.Blur()
			} else if m.Step == StepBaseURL {
				m.Step = StepModel
				m.APIKeyInput.Blur()
				m.ModelInput.Focus()
				m.BaseURLInput.Blur()
			}
		}
	}

	// Update text inputs based on current step
	var cmd tea.Cmd
	switch m.Step {
	case StepAPIKey:
		m.APIKeyInput, cmd = m.APIKeyInput.Update(msg)
	case StepModel:
		m.ModelInput, cmd = m.ModelInput.Update(msg)
	case StepBaseURL:
		m.BaseURLInput, cmd = m.BaseURLInput.Update(msg)
	}

	return m, cmd
}

// View renders the UI
func (m Model) View() string {
	if m.Quitting {
		return "Exiting...\n"
	}

	if m.Step == StepComplete {
		if m.Err != nil {
			return fmt.Sprintf("\n%s\n\nError saving configuration: %v\n\nPress any key to exit...",
				errorStyle.Render("Configuration Failed"), m.Err)
		}
		return fmt.Sprintf("\n%s\n\nConfiguration saved to: %s\n\nPress any key to exit...",
			successStyle.Render("Configuration Saved Successfully!"), m.ConfigPath)
	}

	var s string

	// Title
	s += titleStyle.Render(" Gendocs Configuration Wizard ") + "\n\n"

	// Current step indicator
	stepNum := int(m.Step) + 1
	s += fmt.Sprintf("Step %d/5: %s\n\n", stepNum, m.Step.String())

	// Render current step content
	switch m.Step {
	case StepProvider:
		s += m.renderProviderSelection()
	case StepAPIKey:
		s += m.renderAPIKeyInput()
	case StepModel:
		s += m.renderModelInput()
	case StepBaseURL:
		s += m.renderBaseURLInput()
	case StepConfirm:
		s += m.renderConfirm()
	}

	// Help text
	s += "\n\n"
	if m.Step == StepProvider {
		s += "1-3: Select provider  |  Enter: Continue  |  q: Quit"
	} else if m.Step == StepConfirm {
		s += "y: Yes (save)  |  n: No (go back)  |  q: Quit"
	} else {
		s += "Type input  |  Enter: Continue  |  Esc: Go back  |  q: Quit"
	}

	return s + "\n"
}

func (m Model) renderProviderSelection() string {
	s := "Select your LLM provider:\n\n"
	
	providers := []struct {
		key   string
		name  string
		model string
	}{
		{"1", "OpenAI", "gpt-4o, gpt-4o-mini, etc."},
		{"2", "Anthropic Claude", "claude-3-5-sonnet, claude-3-haiku, etc."},
		{"3", "Google Gemini", "gemini-1.5-pro, gemini-pro, etc."},
	}

	for _, p := range providers {
		prefix := " "
		if m.Provider == getProviderFromKey(p.key) {
			prefix = highlightStyle.Render("✓")
		}
		s += fmt.Sprintf("%s %s. %s (%s)\n", prefix, p.key, p.name, p.model)
	}

	return s
}

func (m Model) renderAPIKeyInput() string {
	return fmt.Sprintf("Enter your API key for %s:\n\n%s\n\n(Press Enter when done)",
		highlightStyle.Render(m.Provider),
		m.APIKeyInput.View())
}

func (m Model) renderModelInput() string {
	defaultModel := m.Model
	if defaultModel == "" {
		switch m.Provider {
		case "openai":
			defaultModel = "gpt-4o"
		case "anthropic":
			defaultModel = "claude-3-5-sonnet-20241022"
		case "gemini":
			defaultModel = "gemini-1.5-pro"
		default:
			defaultModel = "<default>"
		}
	}
	return fmt.Sprintf("Enter model name (or press Enter for default %s):\n\n%s",
		highlightStyle.Render(defaultModel),
		m.ModelInput.View())
}

func (m Model) renderBaseURLInput() string {
	return fmt.Sprintf("Enter base URL (optional, press Enter to skip):\n\n%s\n\nLeave empty for provider default.",
		m.BaseURLInput.View())
}

func (m Model) renderConfirm() string {
	s := "Review your configuration:\n\n"
	s += fmt.Sprintf("  Provider:   %s\n", highlightStyle.Render(m.Provider))
	s += fmt.Sprintf("  Model:      %s\n", highlightStyle.Render(m.Model))
	if m.BaseURL != "" {
		s += fmt.Sprintf("  Base URL:   %s\n", highlightStyle.Render(m.BaseURL))
	}
	s += "\nSave this configuration?"
	return s
}

func (m Model) saveConfig() {
	homeDir, err := os.UserHomeDir()
	if err != nil {
		m.Err = err
		return
	}

	m.ConfigPath = filepath.Join(homeDir, ".gendocs.yaml")

	// Create YAML configuration
	configYAML := fmt.Sprintf("# Gendocs Global Configuration\nanalyzer:\n  llm:\n    provider: %s\n    api_key: %s\n    model: %s\n",
		m.Provider, m.APIKey, m.Model)

	if m.BaseURL != "" {
		configYAML += fmt.Sprintf("    base_url: %s\n", m.BaseURL)
	}

	if err := os.WriteFile(m.ConfigPath, []byte(configYAML), 0600); err != nil {
		m.Err = err
		return
	}

	m.SavedConfig = true
}

func getProviderFromKey(key string) string {
	switch key {
	case "1":
		return "openai"
	case "2":
		return "anthropic"
	case "3":
		return "gemini"
	default:
		return ""
	}
}

// GetConfigPath returns the path where config was saved
func (m Model) GetConfigPath() string {
	return m.ConfigPath
}
</file>
<file path="internal/worker_pool/pool.go">
package worker_pool

import (
	"context"
	"runtime"
	"sync"
)

// Task represents a unit of work to execute
type Task func(ctx context.Context) (interface{}, error)

// Result represents the result of a task execution
type Result struct {
	Value interface{}
	Error error
}

// WorkerPool executes tasks concurrently with semaphore-based limiting
type WorkerPool struct {
	maxWorkers int
	semaphore  chan struct{}
}

// NewWorkerPool creates a new worker pool
func NewWorkerPool(maxWorkers int) *WorkerPool {
	if maxWorkers <= 0 {
		maxWorkers = runtime.NumCPU()
	}

	return &WorkerPool{
		maxWorkers: maxWorkers,
		semaphore:  make(chan struct{}, maxWorkers),
	}
}

// Run executes all tasks concurrently and returns results in order
func (wp *WorkerPool) Run(ctx context.Context, tasks []Task) []Result {
	if len(tasks) == 0 {
		return []Result{}
	}

	numTasks := len(tasks)
	results := make([]Result, numTasks)
	var wg sync.WaitGroup

	for i, task := range tasks {
		wg.Add(1)
		go func(index int, t Task) {
			defer wg.Done()

			// Acquire semaphore (blocks if max workers already running)
			select {
			case wp.semaphore <- struct{}{}:
				defer func() { <-wp.semaphore }()
			case <-ctx.Done():
				results[index] = Result{Error: ctx.Err()}
				return
			}

			// Execute the task
			value, err := t(ctx)
			results[index] = Result{Value: value, Error: err}
		}(i, task)
	}

	wg.Wait()
	return results
}

// GetMaxWorkers returns the maximum number of workers
func (wp *WorkerPool) GetMaxWorkers() int {
	return wp.maxWorkers
}
</file>
<file path="prompts/ai_rules_generator.yaml">
# Prompts for AI Rules Generator Agent

ai_rules_claude_system: |
  You are an AI assistant documentation specialist who creates CLAUDE.md files.
  These files provide project-specific instructions for AI coding assistants.

  Your goal is to create a CLAUDE.md that helps AI assistants:
  - Understand the project's purpose and architecture
  - Follow the project's coding conventions
  - Know about key files and their purposes
  - Understand common commands and workflows
  - Be aware of project-specific patterns and practices

ai_rules_claude_user: |
  TASK: Generate CLAUDE.md

  Create a comprehensive CLAUDE.md file for the project at {{ .RepoPath }}.

  Use the existing analysis documents in {{ .RepoPath }}/.ai/docs/ to inform the documentation.

  The CLAUDE.md should include:

  1. **Project Overview**
  2. **Common Commands** - How to run, test, build
  3. **Architecture** - Key architectural patterns
  4. **Code Conventions** - Style guidelines, naming patterns
  5. **Key Files** - Important files and what they do
  6. **Testing** - How tests are organized and run
  7. **Troubleshooting** - Common issues and solutions

  Make the content practical and actionable for an AI assistant.
  Include actual command examples and real file paths from the project.

ai_rules_agents_system: |
  You are an AI agent documentation specialist who creates AGENTS.md files.
  These files document the agent architecture and conventions for multi-agent systems.

ai_rules_agents_user: |
  TASK: Generate AGENTS.md

  Create an AGENTS.md file for the project at {{ .RepoPath }}.

  If this project uses agents (like this documentation generator does), document:
  - Agent architecture and patterns
  - Handler-agent separation
  - How agents communicate
  - Tool system conventions
  - Configuration and setup

  Keep it concise but informative for developers working with the agent system.

ai_rules_cursor_system: |
  You are an IDE documentation specialist who creates Cursor AI rules.
  These rules help Cursor IDE provide better context-aware assistance.

ai_rules_cursor_user: |
  TASK: Generate Cursor AI Rules

  Create Cursor IDE rules (.cursor/rules/*.mdc files) for the project at {{ .RepoPath }}.

  Generate the following rule files:
  1. **project-overview.mdc** - High-level project overview
  2. **architecture.mdc** - Architecture patterns and design
  3. **code-patterns.mdc** - Coding conventions and patterns
  4. **agent-and-tool-conventions.mdc** - If applicable, agent/tool patterns

  Each rule should be concise, focused, and help Cursor understand the codebase better.
</file>
<file path="prompts/analyzer.yaml">
# System Prompts for Sub-Agents

structure_analyzer_system: |
  You are a code structure analyst specializing in identifying and documenting key architectural components.
  Your focus is on understanding the organization, abstraction patterns, and important services/modules in the codebase.
  You examine files, classes, interfaces, and their relationships without modifying any code.

  Your goal is to produce a comprehensive analysis of the codebase's architectural structure, key components, and design patterns.
  You will identify critical modules, interfaces, and core services that form the backbone of the application.
  You will document responsibility boundaries and how components interact at a structural level.

  Always return your response in Markdown format using the structure specified.

structure_analyzer_user: |
  TASK: Analyze Code Structure

  Examine the project at {{ .RepoPath }} to identify and document key structural elements.

  Your analysis should clearly map the structural architecture of the codebase, highlighting key components,
  their responsibilities, and relationships.

  Start by understanding the repository's high-level organization. Then dive into identifying:
  - Core modules and their purposes
  - Key interfaces and abstractions
  - Service components and their responsibilities
  - Architectural patterns used (MVC, hexagonal, microservices, etc.)
  - Important methods and functions that define the application's capabilities
  - Code organization principles and patterns

  Focus on the "what" and "why" of components rather than implementation details.
  Be sure that you are describing existing code, not hypothetical code.

  EXPECTED OUTPUT FORMAT:

  # Code Structure Analysis

  ## Architectural Overview
  [Brief overview of the overall architecture]

  ## Core Components
  [List and describe main components]

  ## Service Definitions
  [Describe key services and their purposes]

  ## Interface Contracts
  [Document important interfaces and their contracts]

  ## Design Patterns Identified
  [List patterns found in the codebase]

  ## Component Relationships
  [Describe how components interact]

  ## Key Methods & Functions
  [Important functions that define capabilities]

dependency_analyzer_system: |
  You are a dependency analyst who traces how modules and components depend on each other.
  Your focus is on understanding import relationships, external dependencies, and coupling patterns.

  Your goal is to map the complete dependency graph of the application, identifying:
  - Internal module dependencies
  - External libraries and frameworks used
  - Circular dependencies or anti-patterns
  - Dependency injection patterns

dependency_analyzer_user: |
  TASK: Analyze Dependencies

  Examine the project at {{ .RepoPath }} to trace and document dependencies.

  Focus on:
  - Internal module dependencies and how they relate
  - External dependencies (libraries, frameworks, services)
  - Dependency injection patterns
  - Potential issues like circular dependencies or tight coupling

  EXPECTED OUTPUT FORMAT:

  # Dependency Analysis

  ## Internal Dependencies
  [Map out how internal modules depend on each other]

  ## External Dependencies
  [List external libraries and their purposes]

  ## Dependency Graph
  [Describe the dependency structure]

  ## Dependency Injection
  [Document DI patterns if present]

  ## Potential Issues
  [Identify circular dependencies, tight coupling, etc.]

data_flow_analyzer_system: |
  You are a data flow specialist who tracks how data moves, transforms, and persists throughout an application.
  Your focus is on data structures, transformations, storage patterns, and the lifecycle of information.

data_flow_analyzer_user: |
  TASK: Analyze Data Flow

  Examine the project at {{ .RepoPath }} to trace and document how data flows through the system.

  Focus on:
  - Data models and structures
  - How data enters the system (inputs, APIs, etc.)
  - Data transformations and validations
  - Storage mechanisms and databases
  - Data outputs and responses

  EXPECTED OUTPUT FORMAT:

  # Data Flow Analysis

  ## Data Models
  [Describe key data structures]

  ## Input Sources
  [Where data enters the system]

  ## Data Transformations
  [How data is processed and transformed]

  ## Storage Mechanisms
  [Databases, caches, files, etc.]

  ## Data Validation
  [Where and how data is validated]

  ## Output Formats
  [How data leaves the system]

request_flow_analyzer_system: |
  You are a request flow analyst who traces how requests flow through the application from entry to exit.
  Your focus is on HTTP endpoints, message handlers, and the complete lifecycle of requests.

request_flow_analyzer_user: |
  TASK: Analyze Request Flow

  Examine the project at {{ .RepoPath }} to trace how requests are handled.

  Focus on:
  - API endpoints and their routes
  - Middleware and request processing pipeline
  - How requests are routed to handlers
  - Response generation and error handling

  EXPECTED OUTPUT FORMAT:

  # Request Flow Analysis

  ## API Endpoints
  [List all endpoints with methods and paths]

  ## Request Processing Pipeline
  [Describe middleware and processing steps]

  ## Routing Logic
  [How requests are routed to handlers]

  ## Response Generation
  [How responses are created and returned]

  ## Error Handling
  [How errors are handled in the request flow]

api_analyzer_system: |
  You are an API analyst who documents the external API surface of the application.
  Your focus is on endpoints, request/response formats, authentication, and API contracts.

api_analyzer_user: |
  TASK: Analyze API

  Examine the project at {{ .RepoPath }} to document the API.

  Focus on:
  - All API endpoints (HTTP methods, paths, parameters)
  - Request formats and validation rules
  - Response formats and status codes
  - Authentication and authorization
  - Rate limiting and other API policies

  EXPECTED OUTPUT FORMAT:

  # API Analysis

  ## Endpoints Overview
  [List all endpoints grouped by resource]

  ## Authentication
  [Describe auth mechanisms]

  ## Detailed Endpoints
  For each endpoint:
  ### Method /path
  - Description
  - Parameters
  - Request format
  - Response format
  - Status codes

  ## Common Patterns
  [Shared patterns across endpoints]
</file>
<file path="prompts/documenter.yaml">
# Prompts for Documenter Agent (README generation)

documenter_system: |
  You are a technical documentation specialist who creates comprehensive README.md files.
  You synthesize information from existing analysis documents to create user-friendly documentation.

  Your goal is to create a README that helps developers:
  - Understand what the project does
  - Get started quickly with installation and setup
  - Learn the architecture and key concepts
  - Find how to run tests and builds
  - Understand deployment processes

  Always write clear, concise, and well-organized Markdown.

documenter_user: |
  TASK: Generate README.md

  Create a comprehensive README.md for the project at {{ .RepoPath }}.

  Use the existing analysis documents in {{ .RepoPath }}/.ai/docs/ to inform your README.
  Combine information from structure, dependency, data flow, request flow, and API analyses.

  The README should include:

  1. **Project Title & Brief Description**
  2. **Features** - Key features and capabilities
  3. **Installation** - How to install dependencies
  4. **Quick Start** - How to run the project
  5. **Architecture** - High-level architecture overview
  6. **Development** - How to run tests, lint, build
  7. **Configuration** - Environment variables and config files
  8. **Contributing** - Brief contribution guidelines
  9. **License** - Reference to LICENSE file

  Make the README professional, clear, and welcoming to new developers.
  Use code blocks for commands and examples.
  Use proper Markdown formatting throughout.

  IMPORTANT:
  - Write factual information based on the actual codebase
  - Don't invent features that don't exist
  - Keep descriptions concise but informative
  - Focus on what developers need to know

  Output the complete README.md content in Markdown format.
</file>
<file path="go.mod">
module github.com/user/gendocs

go 1.25.5

require (
	github.com/joho/godotenv v1.5.1
	github.com/spf13/cobra v1.10.2
	github.com/spf13/viper v1.21.0
	go.uber.org/zap v1.27.1
	gopkg.in/yaml.v3 v3.0.1
)

require (
	github.com/atotto/clipboard v0.1.4 // indirect
	github.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect
	github.com/charmbracelet/bubbles v0.21.0 // indirect
	github.com/charmbracelet/bubbletea v1.3.4 // indirect
	github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc // indirect
	github.com/charmbracelet/lipgloss v1.1.0 // indirect
	github.com/charmbracelet/x/ansi v0.8.0 // indirect
	github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd // indirect
	github.com/charmbracelet/x/term v0.2.1 // indirect
	github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f // indirect
	github.com/fsnotify/fsnotify v1.9.0 // indirect
	github.com/go-viper/mapstructure/v2 v2.4.0 // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/lucasb-eyer/go-colorful v1.2.0 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-localereader v0.0.1 // indirect
	github.com/mattn/go-runewidth v0.0.16 // indirect
	github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 // indirect
	github.com/muesli/cancelreader v0.2.2 // indirect
	github.com/muesli/termenv v0.16.0 // indirect
	github.com/pelletier/go-toml/v2 v2.2.4 // indirect
	github.com/rivo/uniseg v0.4.7 // indirect
	github.com/sagikazarmark/locafero v0.11.0 // indirect
	github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 // indirect
	github.com/spf13/afero v1.15.0 // indirect
	github.com/spf13/cast v1.10.0 // indirect
	github.com/spf13/pflag v1.0.10 // indirect
	github.com/subosito/gotenv v1.6.0 // indirect
	github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e // indirect
	go.uber.org/multierr v1.10.0 // indirect
	go.yaml.in/yaml/v3 v3.0.4 // indirect
	golang.org/x/sync v0.16.0 // indirect
	golang.org/x/sys v0.36.0 // indirect
	golang.org/x/text v0.28.0 // indirect
)
</file>
<file path="go.sum">
github.com/atotto/clipboard v0.1.4 h1:EH0zSVneZPSuFR11BlR9YppQTVDbh5+16AmcJi4g1z4=
github.com/atotto/clipboard v0.1.4/go.mod h1:ZY9tmq7sm5xIbd9bOK4onWV4S6X0u6GY7Vn0Yu86PYI=
github.com/aymanbagabas/go-osc52/v2 v2.0.1 h1:HwpRHbFMcZLEVr42D4p7XBqjyuxQH5SMiErDT4WkJ2k=
github.com/aymanbagabas/go-osc52/v2 v2.0.1/go.mod h1:uYgXzlJ7ZpABp8OJ+exZzJJhRNQ2ASbcXHWsFqH8hp8=
github.com/charmbracelet/bubbles v0.21.0 h1:9TdC97SdRVg/1aaXNVWfFH3nnLAwOXr8Fn6u6mfQdFs=
github.com/charmbracelet/bubbles v0.21.0/go.mod h1:HF+v6QUR4HkEpz62dx7ym2xc71/KBHg+zKwJtMw+qtg=
github.com/charmbracelet/bubbletea v1.3.4 h1:kCg7B+jSCFPLYRA52SDZjr51kG/fMUEoPoZrkaDHyoI=
github.com/charmbracelet/bubbletea v1.3.4/go.mod h1:dtcUCyCGEX3g9tosuYiut3MXgY/Jsv9nKVdibKKRRXo=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc h1:4pZI35227imm7yK2bGPcfpFEmuY1gc2YSTShr4iJBfs=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc/go.mod h1:X4/0JoqgTIPSFcRA/P6INZzIuyqdFY5rm8tb41s9okk=
github.com/charmbracelet/lipgloss v1.1.0 h1:vYXsiLHVkK7fp74RkV7b2kq9+zDLoEU4MZoFqR/noCY=
github.com/charmbracelet/lipgloss v1.1.0/go.mod h1:/6Q8FR2o+kj8rz4Dq0zQc3vYf7X+B0binUUBwA0aL30=
github.com/charmbracelet/x/ansi v0.8.0 h1:9GTq3xq9caJW8ZrBTe0LIe2fvfLR/bYXKTx2llXn7xE=
github.com/charmbracelet/x/ansi v0.8.0/go.mod h1:wdYl/ONOLHLIVmQaxbIYEC/cRKOQyjTkowiI4blgS9Q=
github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd h1:vy0GVL4jeHEwG5YOXDmi86oYw2yuYUGqz6a8sLwg0X8=
github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd/go.mod h1:xe0nKWGd3eJgtqZRaN9RjMtK7xUYchjzPr7q6kcvCCs=
github.com/charmbracelet/x/term v0.2.1 h1:AQeHeLZ1OqSXhrAWpYUtZyX1T3zVxfpZuEQMIQaGIAQ=
github.com/charmbracelet/x/term v0.2.1/go.mod h1:oQ4enTYFV7QN4m0i9mzHrViD7TQKvNEEkHUMCmsxdUg=
github.com/cpuguy83/go-md2man/v2 v2.0.6/go.mod h1:oOW0eioCTA6cOiMLiUPZOpcVxMig6NIQQ7OS05n1F4g=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f h1:Y/CXytFA4m6baUTXGLOoWe4PQhGxaX0KpnayAqC48p4=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f/go.mod h1:vw97MGsxSvLiUE2X8qFplwetxpGLQrlU1Q9AUEIzCaM=
github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=
github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=
github.com/fsnotify/fsnotify v1.9.0 h1:2Ml+OJNzbYCTzsxtv8vKSFD9PbJjmhYF14k/jKC7S9k=
github.com/fsnotify/fsnotify v1.9.0/go.mod h1:8jBTzvmWwFyi3Pb8djgCCO5IBqzKJ/Jwo8TRcHyHii0=
github.com/go-viper/mapstructure/v2 v2.4.0 h1:EBsztssimR/CONLSZZ04E8qAkxNYq4Qp9LvH92wZUgs=
github.com/go-viper/mapstructure/v2 v2.4.0/go.mod h1:oJDH3BJKyqBA2TXFhDsKDGDTlndYOZ6rGS0BRZIxGhM=
github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=
github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
github.com/joho/godotenv v1.5.1 h1:7eLL/+HRGLY0ldzfGMeQkb7vMd0as4CfYvUVzLqw0N0=
github.com/joho/godotenv v1.5.1/go.mod h1:f4LDr5Voq0i2e/R5DDNOoa2zzDfwtkZa6DnEwAbqwq4=
github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/lucasb-eyer/go-colorful v1.2.0 h1:1nnpGOrhyZZuNyfu1QjKiUICQ74+3FNCN69Aj6K7nkY=
github.com/lucasb-eyer/go-colorful v1.2.0/go.mod h1:R4dSotOR9KMtayYi1e77YzuveK+i7ruzyGqttikkLy0=
github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mattn/go-localereader v0.0.1 h1:ygSAOl7ZXTx4RdPYinUpg6W99U8jWvWi9Ye2JC/oIi4=
github.com/mattn/go-localereader v0.0.1/go.mod h1:8fBrzywKY7BI3czFoHkuzRoWE9C+EiG4R1k4Cjx5p88=
github.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=
github.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 h1:ZK8zHtRHOkbHy6Mmr5D264iyp3TiX5OmNcI5cIARiQI=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6/go.mod h1:CJlz5H+gyd6CUWT45Oy4q24RdLyn7Md9Vj2/ldJBSIo=
github.com/muesli/cancelreader v0.2.2 h1:3I4Kt4BQjOR54NavqnDogx/MIoWBFa0StPA8ELUXHmA=
github.com/muesli/cancelreader v0.2.2/go.mod h1:3XuTXfFS2VjM+HTLZY9Ak0l6eUKfijIfMUZ4EgX0QYo=
github.com/muesli/termenv v0.16.0 h1:S5AlUN9dENB57rsbnkPyfdGuWIlkmzJjbFf0Tf5FWUc=
github.com/muesli/termenv v0.16.0/go.mod h1:ZRfOIKPFDYQoDFF4Olj7/QJbW60Ol/kL1pU3VfY/Cnk=
github.com/pelletier/go-toml/v2 v2.2.4 h1:mye9XuhQ6gvn5h28+VilKrrPoQVanw5PMw/TB0t5Ec4=
github.com/pelletier/go-toml/v2 v2.2.4/go.mod h1:2gIqNv+qfxSVS7cM2xJQKtLSTLUE9V8t9Stt+h56mCY=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=
github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=
github.com/rogpeppe/go-internal v1.9.0 h1:73kH8U+JUqXU8lRuOHeVHaa/SZPifC7BkcraZVejAe8=
github.com/rogpeppe/go-internal v1.9.0/go.mod h1:WtVeX8xhTBvf0smdhujwtBcq4Qrzq/fJaraNFVN+nFs=
github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/sagikazarmark/locafero v0.11.0 h1:1iurJgmM9G3PA/I+wWYIOw/5SyBtxapeHDcg+AAIFXc=
github.com/sagikazarmark/locafero v0.11.0/go.mod h1:nVIGvgyzw595SUSUE6tvCp3YYTeHs15MvlmU87WwIik=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 h1:+jumHNA0Wrelhe64i8F6HNlS8pkoyMv5sreGx2Ry5Rw=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8/go.mod h1:3n1Cwaq1E1/1lhQhtRK2ts/ZwZEhjcQeJQ1RuC6Q/8U=
github.com/spf13/afero v1.15.0 h1:b/YBCLWAJdFWJTN9cLhiXXcD7mzKn9Dm86dNnfyQw1I=
github.com/spf13/afero v1.15.0/go.mod h1:NC2ByUVxtQs4b3sIUphxK0NioZnmxgyCrfzeuq8lxMg=
github.com/spf13/cast v1.10.0 h1:h2x0u2shc1QuLHfxi+cTJvs30+ZAHOGRic8uyGTDWxY=
github.com/spf13/cast v1.10.0/go.mod h1:jNfB8QC9IA6ZuY2ZjDp0KtFO2LZZlg4S/7bzP6qqeHo=
github.com/spf13/cobra v1.10.2 h1:DMTTonx5m65Ic0GOoRY2c16WCbHxOOw6xxezuLaBpcU=
github.com/spf13/cobra v1.10.2/go.mod h1:7C1pvHqHw5A4vrJfjNwvOdzYu0Gml16OCs2GRiTUUS4=
github.com/spf13/pflag v1.0.9/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/pflag v1.0.10 h1:4EBh2KAYBwaONj6b2Ye1GiHfwjqyROoF4RwYO+vPwFk=
github.com/spf13/pflag v1.0.10/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/viper v1.21.0 h1:x5S+0EU27Lbphp4UKm1C+1oQO+rKx36vfCoaVebLFSU=
github.com/spf13/viper v1.21.0/go.mod h1:P0lhsswPGWD/1lZJ9ny3fYnVqxiegrlNrEmgLjbTCAY=
github.com/stretchr/testify v1.11.1 h1:7s2iGBzp5EwR7/aIZr8ao5+dra3wiQyKjjFuvgVKu7U=
github.com/stretchr/testify v1.11.1/go.mod h1:wZwfW3scLgRK+23gO65QZefKpKQRnfz6sD981Nm4B6U=
github.com/subosito/gotenv v1.6.0 h1:9NlTDc1FTs4qu0DDq7AEtTPNw6SVm7uBMsUCUjABIf8=
github.com/subosito/gotenv v1.6.0/go.mod h1:Dk4QP5c2W3ibzajGcXpNraDfq2IrhjMIvMSWPKKo0FU=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e h1:JVG44RsyaB9T2KIHavMF/ppJZNG9ZpyihvCd0w101no=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e/go.mod h1:RbqR21r5mrJuqunuUZ/Dhy/avygyECGrLceyNeo4LiM=
go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
go.uber.org/multierr v1.10.0 h1:S0h4aNzvfcFsC3dRF1jLoaov7oRaKqRGC/pUEJ2yvPQ=
go.uber.org/multierr v1.10.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN80Y=
go.uber.org/zap v1.27.1 h1:08RqriUEv8+ArZRYSTXy1LeBScaMpVSTBhCeaZYfMYc=
go.uber.org/zap v1.27.1/go.mod h1:GB2qFLM7cTU87MWRP2mPIjqfIDnGu+VIO4V/SdhGo2E=
go.yaml.in/yaml/v3 v3.0.4 h1:tfq32ie2Jv2UxXFdLJdh3jXuOzWiL1fo0bu/FbuKpbc=
go.yaml.in/yaml/v3 v3.0.4/go.mod h1:DhzuOOF2ATzADvBadXxruRBLzYTpT36CKvDb3+aBEFg=
golang.org/x/sync v0.16.0 h1:ycBJEhp9p4vXvUZNszeOq0kGTPghopOL8q0fq3vstxw=
golang.org/x/sync v0.16.0/go.mod h1:1dzgHSNfp02xaA81J2MS99Qcpr2w7fw1gpm99rleRqA=
golang.org/x/sys v0.0.0-20210809222454-d867a43fc93e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.36.0 h1:KVRy2GtZBrk1cBYA7MKu5bEZFxQk4NIDV6RLVcC8o0k=
golang.org/x/sys v0.36.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
golang.org/x/text v0.28.0 h1:rhazDwis8INMIwQ4tpjLDzUhx6RlXqZNPEM0huQojng=
golang.org/x/text v0.28.0/go.mod h1:U8nCwOR8jO/marOQ0QbDiOngZVEBB7MAiitBuMjXiNU=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15 h1:YR8cESwS4TdDjEe65xsg0ogRM/Nc3DYOhEAlW+xobZo=
gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
</file>
<file path="INSTALL.md">
# Guia de Instalação e Configuração

Este guia fornece instruções passo a passo para instalar e configurar o Gendocs Go.

## Pré-requisitos

- **Go 1.22 ou posterior**
- **API key de um provedor LLM** (OpenAI, Anthropic, ou Google Gemini)
- (Opcional) **GitLab** com token OAuth para funcionalidade de cronjob

## 1. Instalação

### Opção A: Usar Makefile (Recomendado)

```bash
# Compilar
make build

# Instalar (requer sudo)
make install

# Verificar instalação
gendocs --version
```

### Opção B: Scripts de Instalação

```bash
# Instalar (requer sudo)
sudo ./install.sh

# Desinstalar
sudo ./uninstall.sh
```

### Opção C: Compilar Manualmente

```bash
git clone https://github.com/divar-ir/ai-doc-gen.git
cd ai-doc-gen-feature-go-version/gendocs

# Compilar
go build -o gendocs .

# Opcional: mover para PATH global
sudo mv gendocs /usr/local/bin/
```

### Opção D: Binário pré-compilado (quando disponível)

```bash
# Baixar binário
wget https://github.com/divar-ir/ai-doc-gen/releases/latest/download/gendocs-linux-amd64
chmod +x gendocs-linux-amd64
sudo mv gendocs-linux-amd64 /usr/local/bin/gendocs
```

## 2. Configuração Rápida

### Método 1: Wizard Interativo (Recomendado)

```bash
# Inicia o wizard de configuração
./gendocs config
```

O wizard vai te guiar através de:
1. Seleção do provedor (OpenAI, Anthropic, ou Gemini)
2. Configuração da API key
3. Seleção do modelo
4. (Opcional) Base URL para APIs compatíveis com OpenAI

A configuração é salva em `~/.gendocs.yaml`.

### Método 2: Variáveis de Ambiente

```bash
# Configurar provedor e API key
export ANALYZER_LLM_PROVIDER="openai"
export ANALYZER_LLM_MODEL="gpt-4o"
export ANALYZER_LLM_API_KEY="sk-sua-chave-aqui"

# Para Anthropic Claude
# export ANALYZER_LLM_PROVIDER="anthropic"
# export ANALYZER_LLM_MODEL="claude-3-5-sonnet-20241022"
# export ANALYZER_LLM_API_KEY="sk-ant-sua-chave-aqui"

# Para Google Gemini
# export ANALYZER_LLM_PROVIDER="gemini"
# export ANALYZER_LLM_MODEL="gemini-1.5-pro"
# export ANALYZER_LLM_CONFIG_API_KEY="sua-chave-aqui"
```

Adicione ao seu `~/.bashrc` ou `~/.zshrc`:

```bash
echo 'export ANALYZER_LLM_PROVIDER="openai"' >> ~/.bashrc
echo 'export ANALYZER_LLM_MODEL="gpt-4o"' >> ~/.bashrc
echo 'export ANALYZER_LLM_API_KEY="sk-sua-chave"' >> ~/.bashrc
source ~/.bashrc
```

### Método 3: Arquivo de Configuração `.ai/config.yaml`

Crie um arquivo `.ai/config.yaml` no seu projeto:

```yaml
analyzer:
  llm:
    provider: openai
    model: gpt-4o
    api_key: ${ANALYZER_LLM_API_KEY}
    base_url: ""  # Opcional, para APIs compatíveis
    retries: 2
    timeout: 180
    max_tokens: 8192
    temperature: 0.0
  max_workers: 0  # 0 = auto-detectar CPUs
  exclude_code_structure: false
  exclude_data_flow: false
  exclude_dependencies: false
  exclude_request_flow: false
  exclude_api_analysis: false
```

## 3. Verificar Instalação

```bash
# Verificar versão
./gendocs --version

# Verificar ajuda
./gendocs --help

# Verificar configuração (se usou wizard)
cat ~/.gendocs.yaml
```

## 4. Primeiro Uso

### Analisar um Projeto

```bash
# Analisar o diretório atual
./gendocs analyze --repo-path .

# Analisar outro diretório
./gendocs analyze --repo-path /caminho/para/projeto

# Com flags de exclusão
./gendocs analyze --repo-path . --exclude-api-analysis --exclude-dependencies

# Com depuração
./gendocs analyze --repo-path . --debug
```

Isso vai gerar arquivos em `.ai/docs/`:
- `structure_analysis.md`
- `dependency_analysis.md`
- `data_flow_analysis.md`
- `request_flow_analysis.md`
- `api_analysis.md`

### Gerar Documentação

```bash
# Gerar README.md a partir das análises
./gendocs generate readme --repo-path .

# Gerar arquivos de configuração para IA
./gendocs generate ai-rules --repo-path .
```

Isso vai criar:
- `README.md` no diretório raiz
- `CLAUDE.md` (instruções para Claude)
- `AGENTS.md` (convenções de agentes)

### Processamento em Lote GitLab

```bash
# Configurar GitLab
export GITLAB_API_URL="https://gitlab.com"
export GITLAB_OAUTH_TOKEN="glpat-sua-token-aqui"
export GITLAB_USER_EMAIL="seu-email@example.com"

# Processar todos os projetos de um grupo
./gendocs cronjob analyze --group-project-id 123 --max-days-since-last-commit 14
```

Isso vai:
1. Buscar todos os projetos do grupo
2. Filtrar (pular arquivados, sem commits recentes)
3. Clonar cada projeto
4. Rodar análise
5. Criar branch `ai-analyzer-YYYY-MM-DD`
6. Fazer commit com resultados
7. Criar Merge Request

## 5. Exemplos de Configuração

### OpenAI com Modelo Customizado

```yaml
# ~/.gendocs.yaml
analyzer:
  llm:
    provider: openai
    model: gpt-4o-mini
    max_tokens: 4096
```

### APIs Compatíveis com OpenAI

```yaml
analyzer:
  llm:
    provider: openai
    model: llama-3.1-70b-instruct
    base_url: https://api.deepinfra.com/v1/openai
```

### Anthropic Claude

```yaml
analyzer:
  llm:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
```

### Google Gemini via Vertex AI

```yaml
analyzer:
  llm:
    provider: gemini
    model: gemini-1.5-pro
gemini:
  use_vertex_ai: true
  project_id: seu-project-id
  location: us-central1
```

## 6. Troubleshooting

### Erro: "Required environment variable 'ANALYZER_LLM_API_KEY' is not set"

**Solução**: Configure a API key:
```bash
export ANALYZER_LLM_API_KEY="sk-sua-chave"
```

### Erro: "failed to load prompts"

**Solução**: Certifique-se de estar no diretório correto:
```bash
cd gendocs
./gendocs analyze --repo-path ../projeto-analisar
```

### Erro: "API error: status 401"

**Solução**: Verifique sua API key. Para testar:
```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $ANALYZER_LLM_API_KEY"
```

### Ver Logs de Depuração

```bash
# Ativar debug
./gendocs analyze --repo-path . --debug

# Logs são salvos em
cat .ai/logs/gendocs.log
```

## 7. Estrutura de Diretórios

```
projeto-analisado/
├── .ai/
│   ├── config.yaml          # Config do projeto (opcional)
│   ├── docs/
│   │   ├── structure_analysis.md
│   │   ├── dependency_analysis.md
│   │   ├── data_flow_analysis.md
│   │   ├── request_flow_analysis.md
│   │   └── api_analysis.md
│   └── logs/
│       └── gendocs.log        # Logs estruturados (JSON)
├── README.md                 # Gerado por `gendocs generate readme`
├── CLAUDE.md                 # Gerado por `gendocs generate ai-rules`
└── AGENTS.md                 # Gerado por `gendocs generate ai-rules`
```

## 8. Integração CI/CD

### GitHub Actions

```yaml
- name: Run gendocs analyze
  run: |
    go install github.com/divar-ir/ai-doc-gen/gendocs@latest
    gendocs analyze --repo-path .
```

### GitLab CI

```yaml
analyze:
  script:
    - go install github.com/divar-ir/ai-doc-gen/gendocs@latest
    - gendocs analyze --repo-path .
```

## 9. Atualização

```bash
cd ai-doc-gen-feature-go-version/gendocs
git pull
go build -o gendocs .
```

## 10. Suporte

- **Issues**: https://github.com/divar-ai-doc-gen/issues
- **Documentação**: Leia PLAN.md para detalhes da arquitetura
- **Python vs Go**: A versão Go mantém paridade de recursos com a Python
</file>
<file path="install.sh">
#!/bin/bash
# Script de instalação para Gendocs
# Uso: sudo ./install.sh

set -e

BINARY_NAME="gendocs"
BUILD_DIR="build"
BIN_DIR="/usr/local/bin"
CONFIG_DIR="$HOME/.gendocs.yaml"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

echo "=== Gendocs Installation Script ==="
echo ""

# Detect OS
OS="$(uname -s)"
case "$OS" in
    Linux*)
        BINARY="gendocs-linux-amd64"
        ;;
    Darwin*)
        BINARY="gendocs-darwin-amd64"
        ;;
    *)
        BINARY="gendocs"
        ;;
esac

echo "Detectado: $OS"
echo ""

# Check Go installation
if ! command -v go &> /dev/null; then
    echo "Erro: Go não está instalado."
    echo ""
    echo "Instale Go 1.22+:"
    echo "  https://go.dev/dl/"
    exit 1
fi

GO_VERSION=$(go version | awk '{print $3}')
echo "Go encontrado: $GO_VERSION"
echo ""

# Build
echo "Compilando..."
cd "$SCRIPT_DIR"
go build -o "$BUILD_DIR/$BINARY" .

# Install binary
echo "Instalando binário em $BIN_DIR..."
sudo mkdir -p "$BIN_DIR"
sudo cp "$BUILD_DIR/$BINARY" "$BIN_DIR/$BINARY_NAME"
sudo chmod +x "$BIN_DIR/$BINARY_NAME"

echo ""
echo "✅ Instalação completa!"
echo ""
echo "Binário instalado em: $BIN_DIR/$BINARY_NAME"
echo ""
echo "📖 Para configuração, execute:"
echo "  $BINARY_NAME config"
echo ""
echo "Ou configure manualmente:"
echo "  export ANALYZER_LLM_PROVIDER=\"openai\""
echo "  export ANALYZE_LLM_MODEL=\"gpt-4o\""
echo "  export ANALYZE_LLM_API_KEY=\"sk-...\""
echo ""
echo "🚀 Para analisar um projeto:"
echo "  $BINARY_NAME analyze --repo-path /caminho/para/projeto"
</file>
<file path="main.go">
package main

import "github.com/user/gendocs/cmd"

func main() {
	cmd.Execute()
}
</file>
<file path="Makefile">
.PHONY: all build install uninstall clean test help

# Variables
BINARY_NAME=gendocs
BUILD_DIR=build
# Instalação local em ~/.local/bin
BIN_DIR=$(HOME)/.local/bin
CONFIG_DIR=$(HOME)/.gendocs.yaml
PROMPTS_DIR=./prompts
GO=go
GOFLAGS=

# Detect OS
UNAME_S := $(shell uname -s)
ifeq ($(UNAME_S),Linux)
    BINARY=$(BINARY_NAME)-linux-amd64
else ifeq ($(UNAME_S),Darwin)
    BINARY=$(BINARY_NAME)-darwin-amd64
else
    BINARY=$(BINARY_NAME)
endif

all: build

help:
	@echo "Gendocs Makefile"
	@echo ""
	@echo "Available targets:"
	@echo "  make build        - Compila o binário"
	@echo "  make install      - Instala o binário em $(BIN_DIR)"
	@echo "  make uninstall    - Remove o binário de $(BIN_DIR)"
	@echo "  make clean        - Remove arquivos de build"
	@echo "  make test         - Executa testes"
	@echo "  make help         - Mostra esta mensagem"

build:
	@echo "Compilando $(BINARY)..."
	$(GO) $(GOFLAGS) build -o $(BUILD_DIR)/$(BINARY) .
	@echo "Binário criado: $(BUILD_DIR)/$(BINARY)"

install: build
	@echo "Instalando $(BINARY) em $(BIN_DIR)..."
	@mkdir -p $(BIN_DIR)
	@cp $(BUILD_DIR)/$(BINARY) $(BIN_DIR)/$(BINARY_NAME)
	@chmod +x $(BIN_DIR)/$(BINARY_NAME)
	@echo "Instalado em: $(BIN_DIR)/$(BINARY_NAME)"
	@echo ""
	@echo "Para configurar, execute:"
	@echo "  $(BINARY_NAME) config"
	@echo ""
	@echo "Ou configure manualmente:"
	@echo "  export ANALYZER_LLM_PROVIDER=\"openai\""
	@echo "  export ANALYZER_LLM_MODEL=\"gpt-4o\""
	@echo "  export ANALYZER_LLM_API_KEY=\"sk-...\""

uninstall:
	@echo "Removendo $(BINARY_NAME) de $(BIN_DIR)..."
	@rm -f $(BIN_DIR)/$(BINARY_NAME)
	@echo "Removido."
	@echo ""
	@echo "Para remover completamente (incluindo configuração):"
	@echo "  rm -f $(CONFIG_DIR)"
	@echo "  rm -rf ~/.gendocs/prompts_backup"

clean:
	@echo "Limpando arquivos de build..."
	@rm -rf $(BUILD_DIR)
	@echo "Limpo."

test:
	@echo "Executando testes..."
	$(GO) test -v ./...

# Development helpers
run: build
	@echo "Executando $(BUILD_DIR)/$(BINARY) analyze --repo-path ../.."
</file>
<file path="README.md">
# Gendocs Go Implementation

This is the Go port of the AI Documentation Generator. The Go version provides better performance, easier distribution, and simplified deployment while maintaining complete feature parity with the Python version.

## Status: ✅ IMPLEMENTATION COMPLETE

**Progress: ~95% of PLAN.md**

All major features have been implemented and the binary compiles successfully.

## Quick Start

```bash
# 1. Install Go 1.22+
# 2. Build
cd gendocs && go build -o gendocs .

# 3. Configure (option A: wizard, option B: env vars)
./gendocs config
# OR
export ANALYZER_LLM_PROVIDER="openai"
export ANALYZER_LLM_MODEL="gendocs analyze --repo-path ."
```

See [INSTALL.md](INSTALL.md) for detailed installation and configuration instructions.

## Features

### Commands

- ✅ `gendocs analyze` - Analyze codebase structure and dependencies
- ✅ `gendocs generate readme` - Generate README.md from analysis
- ✅ `gendocs generate ai-rules` - Generate AI assistant configs (CLAUDE.md, AGENTS.md)
- ✅ `gendocs cronjob analyze` - GitLab automated batch processing
- ✅ `gendocs config` - Interactive TUI configuration wizard

### LLM Providers

- ✅ OpenAI (including OpenAI-compatible APIs)
- ✅ Anthropic Claude
- ✅ Google Gemini

### Architecture

- ✅ **7 Agents**: AnalyzerAgent (orchestrator) + 5 sub-agents + DocumenterAgent + AIRulesAgent
- ✅ **Handler-Agent Pattern**: Clean separation between CLI, handlers, and agents
- ✅ **Tool System**: FileReadTool, ListFilesTool with retry logic
- ✅ **Worker Pool**: Semaphore-based concurrent execution
- ✅ **Configuration**: Multi-source (CLI > YAML > env > defaults)
- ✅ **Error Handling**: 14 exception types with rich context
- ✅ **Logging**: Structured JSON + colored console output

## Building

```bash
cd gendocs
go build -o gendocs .
```

## Usage

### Basic Usage

```bash
# Analyze a codebase
./gendocs analyze --repo-path ../my-project

# Generate README from analysis
./gendocs generate readme --repo-path ../my-project

# Generate AI assistant configs
./gendocs generate ai-rules --repo-path ../my-project

# Configure with interactive wizard
./gendocs config

# GitLab batch processing
./gendocs cronjob analyze --group-project-id 123 --max-days-since-last-commit 14
```

### Configuration

The Go version supports the same configuration sources as the Python version:

1. **CLI arguments** (highest priority)
2. **`.ai/config.yaml`** (project-specific)
3. **`~/.gendocs.yaml`** (global user config, from TUI)
4. **Environment variables**
5. **Defaults** (lowest priority)

### Environment Variables

```bash
# Analyzer configuration
export ANALYZER_LLM_PROVIDER="openai"  # openai, anthropic, gemini
export ANALYZER_LLM_MODEL="gpt-4o"
export ANALYZER_LLM_API_KEY="sk-..."
export ANALYZER_MAX_WORKERS=0  # 0 = auto-detect CPU count

# Documenter configuration
export DOCUMENTER_LLM_PROVIDER="openai"
export DOCUMENTER_LLM_MODEL="gpt-4o"
export DOCUMENTER_LLM_API_KEY="sk-..."

# GitLab configuration (for cronjob)
export GITLAB_API_URL="https://gitlab.example.com"
export GITLAB_OAUTH_TOKEN="glpat-..."
```

## Project Structure

```
gendocs/
├── cmd/                      # CLI commands (Cobra)
│   ├── root.go
│   ├── analyze.go
│   ├── generate.go
│   ├── cronjob.go
│   └── config.go
├── internal/
│   ├── agents/              # AI agents
│   │   ├── base.go
│   │   ├── analyzer.go      # AnalyzerAgent orchestrator
│   │   ├── sub_agents.go    # Sub-agent implementations
│   │   └── factory.go
│   ├── config/              # Configuration loading
│   ├── errors/              # 14 exception types
│   ├── gitlab/              # GitLab client
│   ├── handlers/            # Command handlers
│   │   ├── base.go
│   │   ├── analyze.go
│   │   ├── readme.go
│   │   ├── ai_rules.go
│   │   └── cronjob.go
│   ├── llm/                 # LLM providers
│   │   ├── client.go        # LLMClient interface
│   │   ├── openai.go
│   │   ├── anthropic.go
│   │   ├── gemini.go
│   │   ├── retry_client.go  # HTTP with retry
│   │   └── factory.go
│   ├── logging/             # Structured logging (zap)
│   ├── prompts/             # Prompt template manager
│   ├── tools/               # Agent tools
│   │   ├── base.go
│   │   ├── file_read.go
│   │   └── list_files.go
│   ├── tui/                 # TUI config wizard
│   │   └── config.go        # Bubble Tea UI
│   └── worker_pool/         # Concurrent execution
├── prompts/                 # YAML prompt templates
│   ├── analyzer.yaml
│   ├── documenter.yaml
│   └── ai_rules_generator.yaml
├── main.go
├── go.mod
├── go.sum
└── README.md
```

## Implementation Status

| Phase | Component | Status |
|-------|-----------|--------|
| 1 | Foundation (project, errors, logging, config) | ✅ 100% |
| 2 | LLM Integration (OpenAI, Anthropic, Gemini) | ✅ 100% |
| 3 | Tools & Worker Pool | ✅ 100% |
| 4 | Agents (7 agents with tool calling) | ✅ 100% |
| 5 | CLI & Handlers (5 commands) | ✅ 100% |
| 6 | GitLab Integration (cronjob) | ✅ 100% |
| 7 | TUI Config Wizard (Bubble Tea) | ✅ 100% |
| 8 | Testing | ⚠️ 0% |

## What Works

All CLI commands are implemented and functional:
- `gendocs analyze` with all exclusion flags
- `gendocs generate readme`
- `gendocs generate ai-rules`
- `gendocs cronjob analyze` (GitLab integration)
- `gendocs config` (interactive TUI wizard)

## What's Next

The implementation is functionally complete. Remaining work:
1. End-to-end testing with real LLM APIs
2. Unit tests for better coverage
3. Integration tests

## Migration from Python

The Go version maintains feature parity with the Python version:
- Same `.ai/config.yaml` format
- Same environment variable names
- Same CLI command structure
- Same output file formats

## Development

### Prerequisites

- Go 1.22 or later
- Access to LLM provider API

### Running

```bash
# Build
go build -o gendocs .

# Run
./gendocs --help
./gendocs analyze --repo-path ../my-project
```

## License

Same as the parent project.
</file>
<file path="uninstall.sh">
#!/bin/bash
# Script de desinstalação para Gendocs
# Uso: sudo ./uninstall.sh

set -e

BINARY_NAME="gendocs"
BIN_DIR="/usr/local/bin"
CONFIG_DIR="$HOME/.gendocs.yaml"

echo "=== Gendocs Uninstallation Script ==="
echo ""

# Remove binary
echo "Removendo binário de $BIN_DIR..."
if [ -f "$BIN_DIR/$BINARY_NAME" ]; then
    sudo rm -f "$BIN_DIR/$BINARY_NAME"
    echo "✅ Binário removido"
else
    echo "⚠️  Binário não encontrado em $BIN_DIR/$BINARY_NAME"
fi

# Ask about config
echo ""
read -p "Remover configuração em $CONFIG_DIR? (y/N) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    rm -f "$CONFIG_DIR"
    echo "✅ Configuração removida"
fi

echo ""
echo "Desinstalação completa!"
echo ""
echo "Para reinstalar:"
echo "  make install"
echo "  ou"
echo "  ./install.sh"
</file>


---

# Instruções

Por favor, analise as informações fornecidas e:

1. Entenda os requisitos da tarefa.
2. Revise a estrutura do projeto e quaisquer trechos/snippets.
3. Identifique oportunidades técnicas e de produto baseadas nas pastas/módulos/configurações existentes.
4. Proponha funcionalidades que se alinhem com a arquitetura e restrições atuais.
5. Forneça uma solução detalhada e priorizada usando o formato de saída obrigatório abaixo.

---

# Formato de Saída Obrigatório (Mandatório)

Retorne sua resposta em **Português do Brasil** e siga estritamente esta estrutura:

1) Contexto Inferido do Projeto (3–6 linhas)
- Use [SUPOSIÇÃO] onde necessário.

2) Funcionalidades Propostas (Priorizadas)
- 5–12 itens. Para cada item, use exatamente os campos abaixo:

- **Nome:**
- **Problema/Oportunidade:**
- **Descrição da Funcionalidade:**
- **Valor Esperado (usuário/negócio):**
- **Complexidade (P/M/G):**
- **Justificativa da Complexidade (breve):**
- **Riscos e Dependências:**
- **Evidência (arquivos/pastas):**
- **Impacto Técnico (prováveis módulos/arquivos):**
- **Plano de Implementação (passos):**
- **Critérios de Aceite:**
- **Testes Recomendados:**

3) Vitórias Rápidas (Quick Wins) (até 3)
- Itens pequenos, de alto impacto e baixa complexidade.

